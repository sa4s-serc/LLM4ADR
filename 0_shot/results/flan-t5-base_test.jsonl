{"File Name":"cdh-adrs\/0010-etl_tenancy_relationship.md","Context":"## Context\\nThe PriveXMl results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read\/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.\\n","Decision":"We increase complexity during insertion by including all necessary tenant information as part of a PriveXML event and also provide event objects which could only just contain specific changes to client information within the same topic. This then enforces us to think of this topic as a general topic for ETL which may contain an event to process a new PriveXML result or process tenant data change.\\nMore so, the topics for each tenant must be tenant specific due to necessary policies and requirements to create seperation of data on physical levels.\\n","tokens":160,"id":0,"Predictions":"Decision (A).nThe PriveXML results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read\/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.n Decision (A).nThe PriveXML results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read\/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.n"}
{"File Name":"cdh-adrs\/0011-pentaho.md","Context":"## Context\\nWe currently have certain specific feeds being parsed using pentaho, these are legacy feeds which transform data feed files into PriveXMl format. These feed parsers will require migration into new CDH ETL feeds parsers based on defined standard library.\\n","Decision":"Due to limited resources and time constraints, we will defer the feed migration from pentaho to the CDH ETL till adequate human resources are available, more so an indept understanding of exactly how the parsers work is required, which will require talking with Christopher on this.\\n","tokens":52,"id":1,"Predictions":"Decision (A) The feeds will be parsed using pentaho.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.nB) The feeds will be parsed using CDH ETL feeds.n"}
{"File Name":"cdh-adrs\/0009-etl_feed_synchronization_and_delivery.md","Context":"## Context\\nCurrently, we move files around in S3 from Archive folders into bulkupload folders which is cumbersome and error prone,\\nmore so this is being done to allow us trigger data feed processing from the dev admin.\\nWe need a means of reducing the flow steps to go from files to processing and reduce surface level access for the ETL team.\\n","Decision":"- ETL will have a dedicated database tables which will serve the need to both register and retrieve data feed files in the\\nmost optimized manner necessary on a per feed basis. This database tables will be appropriately updated by the ETL service\\nbased on update events from either the uploader service, or the windows-specific uploader jars.\\nThe following operations then becomes possible:\\n1. Ability to query for specific files based on upload date (the date it was uploaded to S3).\\n2. Ability to query for a specific batch of files related to specific data feeds.\\n3. Ability to get data files specific to a given owner and\/or provider.\\n4. Ability to get data files specific to a given provider.\\n- Due to we creating a database to optimize a query to retrieve and manage data feed files, then we require a way to keep\\nthe ETL database tables up to date with new files from uploader service. Therefore, the ETL system will listen for events coming\\nfrom both a lambda function which will be called by S3 when new files are added to the specific bucket, and the new uploader\\nservice which will house necessary logic for retrieving such data feed files from their sources. Once all uploading logic\\nhave being migrated to the uploader, we will de-commission the lambda function and directly have the uploader service inform the\\nETL service as regards new files.\\nSee\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":73,"id":2,"Predictions":"Decision Makers should consider reducing the flow steps to go from files to processing.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cdh-adrs\/0004-cdh-tenants.md","Context":"## Context\\nMonolith will undergo segmentation where we plan to pull out the Client Data Hub out of the monolith as a separate service for better scaling and API management. The functionality of the client data hub to provided tenant, transaction and position related data will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](.\/etl.md).\\nThe client data hub provides the following services:\\n1. Delivery of tenant information (Accounts, Company, and Client data).\\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\\n4. Processing of reconciliation request on tenant accounts and positions.\\nThis requires us to consider migration procedures for moving existing data from the monolith into the new CDH and ETL related databases.\\n","Decision":"Create a specific services (ETL and CDH) where an ETL service and CDH service will be responsible for the processing of delivered PriveXML for consumption and delivery of client transactions + positions into data tables. Both services will communicate across a pubsub event bus.\\nThe following is expected:\\n- CDH will expose an API by which it will handle all tenant and transaction related requests from monolith.\\n- CDH will listen on specified event topic on message bus from which all tenant update requests will be received as events\\n- CDH will publish to the monolith events on updated tenant data by which monolith will update it's records.\\n- CDH will have all tenant data moved from monolith into it's control (to be discussed in CDH Tech).\\n- CDH will consume all PriveXML events to update it's records of transactions + tickers + positions.\\n![CDH Architecture](..\/assets\/images\/aab_workshop\/aab_5.jpg)\\n","tokens":183,"id":3,"Predictions":"Decision (A). The client data hub will remain in the monolith. The client data hub will be moved to an [ETL Service].nThe client data hub will remain in the monolith. The client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service].nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an [ETL Service.nThe client data hub will be moved to an"}
{"File Name":"cdh-adrs\/0006-etl_cdh_communication.md","Context":"## Context\\nWe wish to segment into separate processes where the data feed files processing is handled by the ETL service and the CDH service is reponsible for consuming these produced output which then are materialized into records which is used in response to request to the CDH service. This means ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\\n","Decision":"We have chosen an event based communication where the CDH and ETL service communicate results between each other over an event queue based on specified topics (deployed onsite within geozone of CDH and ETL services).\\n![Event Queue](..\/assets\/images\/workflows\/image3.png)\\n","tokens":81,"id":4,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cdh-adrs\/0005-etl-monolith-migration.md","Context":"## Context\\nCurrently, the parsing process for datafeeds is still part of the existing monolith codebase, which brings it's own share of problems with scaling and feed parsing managemnent. Monolith also has a hard dependency on AWS S3 which must be broken out to allow us specfically move these dependencies outside as they are not relatively core to the ETL processing architecture but are generally how we organize and move input into desired place for access and delivery.\\nA hard requirement for the ETL service is the ability to ensure processed datafeed consistently have relational information with their tenants as specific feeds\\nhave specific constraints on how they are accessed and stored.\\nAnother hard requirements is to organize how data feed files are accessed and retrieved from S3, this currently has issues due to the need to directly access S3, and move files and directories into the bulkupload directories for processing by the monolith. Whilst the alternative of storing both file, metadata and file binary into the database simplifies these access issues, we create more problems in the management of the database files (without using FileStream optimization in SQLServer), increasing cost of backup and replication.\\n","Decision":"ETL will be moved into an external service of it's own with the following responsibilities:\\n- Embodiment of all parser logic.\\n- Delivery of agreed parser format (currently PriveXML) into message queues.\\n- Standardized library for parsing delivery and logic.\\n- Standardized database tables for data feed file delivery and access.\\n- Standardized database tables for tenant data (Company, Client, Accounts).\\n- Creates tenant specific events for delivery for tenant specific datafeed.\\nAs regards data feed file access problem\\n- Manage synchronization of uploaded files events into database from uploader service.\\n","tokens":233,"id":5,"Predictions":"The monolith must be re-engineered to meet the ETL service's requirements.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A.nAnswer: A."}
{"File Name":"cdh-adrs\/0001-feed-parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":6,"Predictions":"Decision (A) The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n c). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n c). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n"}
{"File Name":"cdh-adrs\/0003-uploaders.md","Context":"## Context\\nMonolith and a few deployed jars handling delivery of data feed files into S3 for the existing parsing processes powering the monolith client data hub system, this increases cost on the monolith's systems which require more vertical scaling of resources to manage, more so, due to the monolith lock, any fix or update is locked to the monolith release SDLC.\\nConsidering these functions serve to move files from source to destination we need to migrate them as external services to both CDH and the monolith, these are then bundled into a single service responsible for the delivery of new data feed files into the S3 Archives and CDH feed data stores.\\n","Decision":"Migration of all monolith related uploading logic into external service which is responsible for the timely retreival, delivery and storage of data feed files from their respective sources. The service is responsible for ensuring the ETL service database is always up to date, by deliverying events on file additions into the archive storage regardless of what storage is being used by event delivery.\\n![Data Feed Delivery](..\/assets\/images\/workflows\/image8.png)\\n","tokens":135,"id":7,"Predictions":"The monolith will be able to migrate the data feed files to CDH and the monolith will be able to migrate the data feed files to CDH.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"Predictions":"Decision 1). The logic within CDH and ETL related services will be specialized and implement across feeds. 2). The logic within CDH and ETL related services will be generic and implement across feeds. 3) The logic within CDH and ETL related services will be generic and implement across feeds. 4) The logic within CDH and ETL related services will be generic and implement across feeds. 5) The logic within CDH and ETL related services will be generic and implement across feeds. 6) The logic within CDH and ETL related services will be generic and implement across feeds. 7) The logic within CDH and ETL related services will be generic and implement across feeds. 8) The logic within CDH and ETL related services will be generic and implement across feeds. 9) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds. 10) The logic within CDH and ETL related services will be generic and implement across feeds."}
{"File Name":"cdh-adrs\/0008-etl_feed_parsers.md","Context":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","Decision":"1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n","tokens":43,"id":9,"Predictions":"Decision (A) The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n c). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n c). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n b). The data feed files for processing will be a clear way for handling data feed files for processing.n"}
{"File Name":"gatemint-sdk\/adr-024-coin-metadata.md","Context":"## Context\\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps\\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\\napplication.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\\noperations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\\nas a result the total amount of asset types increases. We propose to have `x\/bank` additionally keep\\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\\nUX and remove the requirement for making any assumptions on the unit of denomination.\\n","Decision":"The `x\/bank` module will be updated to store and index metadata by `denom`, specifically the \"base\" or\\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\\nMetadata may also include a non-zero length list of denominations. Each entry containts the name of\\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\\nand on exchanges. The values in those fields link to an entry in the list of denominations.\\nThe list in `denom_units` and the `display` entry may be changed via governance.\\nAs a result, we can define the type as follows:\\n```protobuf\\nmessage DenomUnit {\\nstring denom    = 1;\\nuint32 exponent = 2;\\nrepeated string aliases = 3;\\n}\\nmessage Metadata {\\nstring description = 1;\\nrepeated DenomUnit denom_units = 2;\\nstring base = 3;\\nstring display = 4;\\n}\\n```\\nAs an example, the ATOM's metadata can be defined as follows:\\n```json\\n{\\n\"description\": \"The native staking token of the Cosmos Hub.\",\\n\"denom_units\": [\\n{\\n\"denom\": \"uatom\",\\n\"exponent\": 0,\\n\"aliases\": [\\n\"microatom\"\\n],\\n},\\n{\\n\"denom\": \"matom\",\\n\"exponent\": 3,\\n\"aliases\": [\\n\"milliatom\"\\n]\\n},\\n{\\n\"denom\": \"atom\",\\n\"exponent\": 6,\\n}\\n],\\n\"base\": \"uatom\",\\n\"display\": \"atom\",\\n}\\n```\\nGiven the above metadata, a client may infer the following things:\\n- 4.3atom = 4.3 * (10^6) = 4,300,000uatom\\n- The string \"atom\" can be used as a display name in a list of tokens.\\n- The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\\nan explicit decision to choose a different representation.\\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\\nas the base framework for this already exists in the Cosmos SDK.\\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x\/bank` module which is also\\nindexed by the base `denom`.\\n```go\\ntype GenesisState struct {\\nSendEnabled   bool        `json:\"send_enabled\" yaml:\"send_enabled\"`\\nBalances      []Balance   `json:\"balances\" yaml:\"balances\"`\\nSupply        sdk.Coins   `json:\"supply\" yaml:\"supply\"`\\nDenomMetadata []Metadata  `json:\"denom_metadata\" yaml:\"denom_metadata\"`\\n}\\n```\\n","tokens":336,"id":11,"Predictions":"We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution:"}
{"File Name":"gatemint-sdk\/adr-007-specialization-groups.md","Context":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the conception of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","Decision":"A specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n- Membership Admittance\\n- Membership Acceptance\\n- Membership Revocation\\n- (probably) Without Penalty\\n- member steps down (self-Revocation)\\n- replaced by new member from governance\\n- (probably) With Penalty\\n- due to breach of soft-agreement (determined through governance)\\n- due to breach of hard-agreement (determined by code)\\n- Execution of Duties\\n- Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n- Compensation\\n- Group compensation (further distribution decided by the specialization group)\\n- Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admittance to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possiblities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n","tokens":173,"id":12,"Predictions":"Decision (A).nThe idea of a decentralized Computer Emergency Response Team (dCERT) was first conceived of in order to fulfill the use case of thencreation of a decentralized Computer Emergency Response Team (dCERT), whosenmembers would be elected by a governing community and would fulfill the role ofncoordinating the community under emergency situations. This thinkingncan be further abstracted into the conception of \"blockchain specializationngroups\".nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nThe decision to create a decentralized Computer Emergency Response Team (dCERT) was made based on the use case of thencreation of a decentralized Computer Emergency Response Team (dCERT), whosenmembers would be elected by a governing community and would fulfill the role ofncoordinating the community under emergency situations. This thinkingncan be further abstracted into the conception of \"blockchain specializationngroups\".nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nThe decision to create a decentralized Computer Emergency Response Team (dCERT) was made based on the use case of thencreation of a decentralized Computer Emergency Response Team (dCERT), whosenmembers would be elected by a governing community and would fulfill the role ofncoordinating the community under emergency situations."}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"Predictions":"We are seeking a solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"gatemint-sdk\/adr-026-ibc-client-recovery-mechanisms.md","Context":"## Context\\n### Summary\\nAt launch, IBC will be a novel protocol, without an experienced user-base. At the protocol layer, it is not possible to distinguish between client expiry or misbehaviour due to genuine faults (Byzantine behavior) and client expiry or misbehaviour due to user mistakes (failing to update a client, or accidentally double-signing). In the base IBC protocol and ICS 20 fungible token transfer implementation, if a client can no longer be updated, funds in that channel will be permanently locked and can no longer be transferred. To the degree that it is safe to do so, it would be preferable to provide users with a recovery mechanism which can be utilised in these exceptional cases.\\n### Exceptional cases\\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\\n1. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client\\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\\n### Security model\\nTwo-thirds of the validator set (the quorum for governance, module participation) can already sign arbitrary data, so allowing governance to manually force-update a client with a new header after a delay period does not substantially alter the security model.\\n","Decision":"We elect not to deal with chains which have actually halted, which is necessarily Byzantine behaviour and in which case token recovery is not likely possible anyways (in-flight packets cannot be timed-out, but the relative impact of that is minor).\\n1. Require Tendermint light clients (ICS 07) to be created with the following additional flags\\n1. `allow_governance_override_after_expiry` (boolean, default false)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional internal query functions\\n1. `Expired() boolean`, which returns whether or not the client has passed the trusting period since the last update (in which case no headers can be validated)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional state mutation functions\\n1. `Unfreeze()`, which unfreezes a light client after misbehaviour and clears any frozen height previously set\\n1. Require Tendermint light clients (ICS 07) & solo machine clients (ICS 06) to be created with the following additional flags\\n1. `allow_governance_override_after_misbehaviour` (boolean, default false)\\n1. Add a new governance proposal type, `ClientUpdateProposal`, in the `x\/ibc` module\\n1. Extend the base `Proposal` with a client identifier (`string`) and a header (`bytes`, encoded in a client-type-specific format)\\n1. If this governance proposal passes, the client is updated with the provided header, if and only if:\\n1. `allow_governance_override_after_expiry` is true and the client has expired (`Expired()` returns true)\\n1. `allow_governance_override_after_misbehaviour` is true and the client has been frozen (`Frozen()` returns true)\\n1. In this case, additionally, the client is unfrozen by calling `Unfreeze()`\\nNote additionally that the header submitted by governance must be new enough that it will be possible to update the light client after the new header is inserted into the client state (which will only happen after the governance proposal has passed).\\nThis ADR does not address planned upgrades, which are handled separately as per the [specification](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-007-tendermint-client#upgrades).\\n","tokens":370,"id":15,"Predictions":"Decision based on the following: The state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. The state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated. The client cannot distinguish these two cases. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The client cannot be updated. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated."}
{"File Name":"gatemint-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ARD 020](.\/adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ARD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```proto\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/gogo\/protobuf)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```proto\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work direcly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":16,"Predictions":"Decision (A).n[ADR] is a continuation of the motivation, design, and context established in [ADR 019](.\/adr-019-protobuf-state-encoding.md) and [ADR 020](.\/adr-019-protobuf-transaction-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nThis ADR continues from [ARD 020](.\/adr-020-protobuf-transaction-encoding.md)nto specify the encoding of queries.nAnswer:A.nThe ADR is a continuation of the motivation, design, and context established in [ADR 019](.\/adr-019-protobuf-state-encoding.md) and [ADR 020](.\/adr-019-protobuf-transaction-encoding.md)nto specify the encoding of queries.nAnswer:A.nThe ADR is a continuation of the motivation, design, and context established in [ADR 019](.\/adr-019-protobuf-state-encoding.md) and [ADR 020](.\/adr-019-protobuf-transaction-encoding.md)nto specify the encoding of queries.nAnswer:A."}
{"File Name":"gatemint-sdk\/adr-009-evidence-module.md","Context":"## Context\\nIn order to support building highly secure, robust and interoperable blockchain\\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\\nevidence can be submitted, evaluated and verified resulting in some agreed upon\\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any\\n[IBC](https:\/\/github.com\/cosmos\/ics\/blob\/master\/ibc\/2_IBC_ARCHITECTURE.md) or\\ncross-chain validation protocol implementation in order to support the ability\\nfor any misbehavior to be relayed back from a collateralized chain to a primary\\nchain so that the equivocating validator(s) can be slashed.\\n","Decision":"We will implement an evidence module in the Cosmos SDK supporting the following\\nfunctionality:\\n- Provide developers with the abstractions and interfaces necessary to define\\ncustom evidence messages, message handlers, and methods to slash and penalize\\naccordingly for misbehavior.\\n- Support the ability to route evidence messages to handlers in any module to\\ndetermine the validity of submitted misbehavior.\\n- Support the ability, through governance, to modify slashing penalties of any\\nevidence type.\\n- Querier implementation to support querying params, evidence types, params, and\\nall submitted valid misbehavior.\\n### Types\\nFirst, we define the `Evidence` interface type. The `x\/evidence` module may implement\\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\\nIn addition, other modules may implement their own `Evidence` types in a similar\\nmanner in which governance is extensible. It is important to note any concrete\\ntype implementing the `Evidence` interface may include arbitrary fields such as\\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\\nWhen submitting evidence to the `x\/evidence` module, the concrete type must provide\\nthe validator's consensus address, which should be known by the `x\/slashing`\\nmodule (assuming the infraction is valid), the height at which the infraction\\noccurred and the validator's power at same height in which the infraction occurred.\\n```go\\ntype Evidence interface {\\nRoute() string\\nType() string\\nString() string\\nHash() HexBytes\\nValidateBasic() error\\n\/\/ The consensus address of the malicious validator at time of infraction\\nGetConsensusAddress() ConsAddress\\n\/\/ Height at which the infraction occurred\\nGetHeight() int64\\n\/\/ The total power of the malicious validator at time of infraction\\nGetValidatorPower() int64\\n\/\/ The total validator set power at time of infraction\\nGetTotalPower() int64\\n}\\n```\\n### Routing & Handling\\nEach `Evidence` type must map to a specific unique route and be registered with\\nthe `x\/evidence` module. It accomplishes this through the `Router` implementation.\\n```go\\ntype Router interface {\\nAddRoute(r string, h Handler) Router\\nHasRoute(r string) bool\\nGetRoute(path string) Handler\\nSeal()\\n}\\n```\\nUpon successful routing through the `x\/evidence` module, the `Evidence` type\\nis passed through a `Handler`. This `Handler` is responsible for executing all\\ncorresponding business logic necessary for verifying the evidence as valid. In\\naddition, the `Handler` may execute any necessary slashing and potential jailing.\\nSince slashing fractions will typically result from some form of static functions,\\nallow the `Handler` to do this provides the greatest flexibility. An example could\\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\\nby governance. The `Evidence` type should provide all the external information\\nnecessary in order for the `Handler` to make the necessary state transitions.\\nIf no error is returned, the `Evidence` is considered valid.\\n```go\\ntype Handler func(Context, Evidence) error\\n```\\n### Submission\\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\\nhandled by the `x\/evidence` module's `SubmitEvidence`.\\n```go\\ntype MsgSubmitEvidence struct {\\nEvidence\\n}\\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\\nreturn err.Result()\\n}\\n\/\/ emit events...\\nreturn Result{\\n\/\/ ...\\n}\\n}\\n```\\nThe `x\/evidence` module's keeper is responsible for matching the `Evidence` against\\nthe module's router and invoking the corresponding `Handler` which may include\\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\\n```go\\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\\nhandler := keeper.router.GetRoute(evidence.Route())\\nif err := handler(ctx, evidence); err != nil {\\nreturn ErrInvalidEvidence(keeper.codespace, err)\\n}\\nkeeper.setEvidence(ctx, evidence)\\nreturn nil\\n}\\n```\\n### Genesis\\nFinally, we need to represent the genesis state of the `x\/evidence` module. The\\nmodule only needs a list of all submitted valid infractions and any necessary params\\nfor which the module needs in order to handle submitted evidence. The `x\/evidence`\\nmodule will naturally define and route native evidence types for which it'll most\\nlikely need slashing penalty constants for.\\n```go\\ntype GenesisState struct {\\nParams       Params\\nInfractions  []Evidence\\n}\\n```\\n","tokens":176,"id":17,"Predictions":"Decision (A). The Cosmos SDK should be able to provide a mechanism in which arbitrarynevidence can be submitted, evaluated and verified resulting in some agreed uponnpenalty for any misbehavior committed by a validator, such as equivocation (double-voting),nsigning when unbonded, signing an incorrect state transition (in the future), etc.n Furthermore, such a mechanism is paramount for anyn[IBC](https:\/\/github.com\/cosmos\/ics\/blob\/master\/ibc\/2_IBC_ARCHITECTURE.md) orncross-chain validation protocol implementation in order to support the abilitynfor any misbehavior to be relayed back from a collateralized chain to a primarynchain so that the equivocating validator(s) can be slashed.nIBC_ARCHITECTURE.md.nIBC_ARCHITECTURE.md.nIBC_ARCHITECTURE.md.nIBC_ARCHITECTURE.md.nIBC_ARCHITECTURE.md.nIBC_ARCHITECTURE.md.nIBC_ARCHITECTURE.md.nIBC_ARCHITECTURE.md.n"}
{"File Name":"gatemint-sdk\/adr-013-metrics.md","Context":"## Context\\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\\nBy default, when telemetry is enabled, the application should track and expose metrics that are\\nstored in-memory. The operator may choose to enable additional sinks, where we support only\\n[Prometheus](https:\/\/prometheus.io\/) for now, as it's battle-tested, simple to setup, open source,\\nand is rich with ecosystem tooling.\\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\\nmetrics may be added or removed at will and without much friction. To do this, we will use the\\n[go-metrics](https:\/\/github.com\/armon\/go-metrics) library.\\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\\nwill be exposed via `\/metrics?format={text|prometheus}` via the API server.\\n","Decision":"We will add an additional configuration block to `app.toml` that defines telemetry settings:\\n```toml\\n###############################################################################\\n###                         Telemetry Configuration                         ###\\n###############################################################################\\n[telemetry]\\n# Prefixed with keys to separate services\\nservice-name = {{ .Telemetry.ServiceName }}\\n# Enabled enables the application telemetry functionality. When enabled,\\n# an in-memory sink is also enabled by default. Operators may also enabled\\n# other sinks such as Prometheus.\\nenabled = {{ .Telemetry.Enabled }}\\n# Enable prefixing gauge values with hostname\\nenable-hostname = {{ .Telemetry.EnableHostname }}\\n# Enable adding hostname to labels\\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\\n# Enable adding service to labels\\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\\n```\\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\\n```go\\n\/\/ Metrics defines a wrapper around application telemetry functionality. It allows\\n\/\/ metrics to be gathered at any point in time. When creating a Metrics object,\\n\/\/ internally, a global metrics is registered with a set of sinks as configured\\n\/\/ by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\\n\/\/ dump of formatted recent metrics will be sent to STDERR.\\ntype Metrics struct {\\nmemSink           *metrics.InmemSink\\nprometheusEnabled bool\\n}\\n\/\/ Gather collects all registered metrics and returns a GatherResponse where the\\n\/\/ metrics are encoded depending on the type. Metrics are either encoded via\\n\/\/ Prometheus or JSON if in-memory.\\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\\nswitch format {\\ncase FormatPrometheus:\\nreturn m.gatherPrometheus()\\ncase FormatText:\\nreturn m.gatherGeneric()\\ncase FormatDefault:\\nreturn m.gatherGeneric()\\ndefault:\\nreturn GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\\n}\\n}\\n```\\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\\nhandler accordingly.\\n```go\\nfunc (s *Server) Start(cfg config.Config) error {\\n\/\/ ...\\nif cfg.Telemetry.Enabled {\\nm, err := telemetry.New(cfg.Telemetry)\\nif err != nil {\\nreturn err\\n}\\ns.metrics = m\\ns.registerMetrics()\\n}\\n\/\/ ...\\n}\\nfunc (s *Server) registerMetrics() {\\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\\nformat := strings.TrimSpace(r.FormValue(\"format\"))\\ngr, err := s.metrics.Gather(format)\\nif err != nil {\\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\\nreturn\\n}\\nw.Header().Set(\"Content-Type\", gr.ContentType)\\n_, _ = w.Write(gr.Metrics)\\n}\\ns.Router.HandleFunc(\"\/metrics\", metricsHandler).Methods(\"GET\")\\n}\\n```\\nApplication developers may track counters, gauges, summaries, and key\/value metrics. There is no\\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\\n```go\\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\\ndefer metrics.MeasureSince(time.Now(), \"MintCoins\")\\n\/\/ ...\\n}\\n```\\n","tokens":229,"id":18,"Predictions":"Decision (A) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.n"}
{"File Name":"gatemint-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the SDK documentation. Current documentation includes a lot of non-related SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n- All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n- All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the SDK github repo as follows:\\n```\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n- `README`: Landing page of the docs.\\n- `intro`: Introductory material. Goal is to have a short explainer of the SDK and then channel people to the resource they need. The [sdk-tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n- `concepts`: Contains high-level explanations of the abstractions of the SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n- `clients`: Contains specs and info about the various SDK clients.\\n- `spec`: Contains specs of modules, and others.\\n- `modules`: Contains links to `godocs` and the spec of the modules.\\n- `architecture`: Contains architecture-related docs like the present one.\\n- `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n- `README`\\n- `intro`\\n- `concepts`\\n- `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":113,"id":19,"Predictions":"Decision (A) The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure.n- The SDK documentation should be a scalable structure."}
{"File Name":"gatemint-sdk\/adr-003-dynamic-capability-store.md","Context":"## Context\\nFull implementation of the [IBC specification](https:\/\/github.com\/cosmos\/ics) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\\nas described in [ICS 5](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\\nobject-capability keys at this time.\\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L132))\\nand passed to Keepers as fixed arguments ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L160)). Keepers cannot create or store capability keys during transaction execution \u2014 although they could call `NewKVStoreKey` and take the memory address\\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional \u2014 were this not the case, the keys would be predictable and couldn't serve as object capabilities).\\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\\nThis ADR proposes such an interface & mechanism.\\n","Decision":"The SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\\ninvoked.\\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\\nchain is running.\\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\\nThe persistent `KVStore` tracks which capability is owned by which modules.\\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\\na reverse mapping that map from module name, capability name to the capability index.\\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\\ngo-map to retrieve the capability at the original memory location.\\nThe `CapabilityKeeper` will define the following types & functions:\\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\\na name. A `String()` method is provided for debugging.\\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\\n```golang\\ntype Capability struct {\\nindex uint64\\n}\\n```\\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\\n```golang\\ntype CapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleNames   map[string]interface{}\\nsealed        bool\\n}\\n```\\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\\npassed by other modules.\\n```golang\\ntype ScopedCapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleName    string\\n}\\n```\\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\\nIt MUST be called before `InitialiseAndSeal`.\\n```golang\\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\\nif k.sealed {\\npanic(\"cannot scope to module via a sealed capability keeper\")\\n}\\nif _, ok := k.scopedModules[moduleName]; ok {\\npanic(fmt.Sprintf(\"cannot create multiple scoped keepers for the same module name: %s\", moduleName))\\n}\\nk.scopedModules[moduleName] = struct{}{}\\nreturn ScopedKeeper{\\ncdc:      k.cdc,\\nstoreKey: k.storeKey,\\nmemKey:   k.memKey,\\ncapMap:   k.capMap,\\nmodule:   moduleName,\\n}\\n}\\n```\\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\\ncreation of any new `ScopedCapabilityKeeper`s.\\n```golang\\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\\nif ck.sealed {\\npanic(\"capability keeper is sealed\")\\n}\\npersistentStore := ctx.KVStore(ck.persistentKey)\\nmap := ctx.KVStore(ck.memKey)\\n\/\/ initialise memory store for all names in persistent store\\nfor index, value := range persistentStore.Iter() {\\ncapability = &CapabilityKey{index: index}\\nfor moduleAndCapability := range value {\\nmoduleName, capabilityName := moduleAndCapability.Split(\"\/\")\\nmemStore.Set(moduleName + \"\/fwd\/\" + capability, capabilityName)\\nmemStore.Set(moduleName + \"\/rev\/\" + capabilityName, index)\\nck.capMap[index] = capability\\n}\\n}\\nck.sealed = true\\n}\\n```\\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\\nreference. The newly created capability is automatically persisted; the calling module need not\\ncall `ClaimCapability`.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\\n\/\/ check name not taken in memory store\\nif capStore.Get(\"rev\/\" + name) != nil {\\nreturn nil, errors.New(\"name already taken\")\\n}\\n\/\/ fetch the current index\\nindex := persistentStore.Get(\"index\")\\n\/\/ create a new capability\\ncapability := &CapabilityKey{index: index}\\n\/\/ set persistent store\\npersistentStore.Set(index, Set.singleton(sck.moduleName + \"\/\" + name))\\n\/\/ update the index\\nindex++\\npersistentStore.Set(\"index\", index)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to index\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, index)\\n\/\/ set the in-memory mapping from index to capability pointer\\ncapMap[index] = capability\\n\/\/ return the newly created capability\\nreturn capability\\n}\\n```\\n`AuthenticateCapability` can be called by any module to check that a capability\\ndoes in fact correspond to a particular name (the name can be untrusted user input)\\nwith which the calling module previously associated it.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\\n\/\/ return whether forward mapping in memory store matches name\\nreturn memStore.Get(sck.moduleName + \"\/fwd\/\" + capability) === name\\n}\\n```\\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\\nso that future `GetCapability` calls will succeed.\\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\\nthey will all own it.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to capability\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.add(sck.moduleName + \"\/\" + name)\\npersistentStore.Set(capability.Index(), owners)\\n}\\n```\\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\\nThe module is not allowed to retrieve capabilities which it does not own.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\\n\/\/ fetch the index of capability using reverse mapping in memstore\\nindex := memStore.Get(sck.moduleName + \"\/rev\/\" + name)\\n\/\/ fetch capability from go-map using index\\ncapability := capMap[index]\\n\/\/ return the capability\\nreturn capability\\n}\\n```\\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\\nmore owners exist, the capability will be deleted globally.\\n```golang\\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\nname := capStore.Get(sck.moduleName + \"\/fwd\/\" + capability)\\nif name == nil {\\nreturn error(\"capability not owned by module\")\\n}\\n\/\/ delete forward mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ delete reverse mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.remove(sck.moduleName + \"\/\" + name)\\nif owners.size() > 0 {\\n\/\/ there are still other owners, keep the capability around\\npersistentStore.Set(capability.Index(), owners)\\n} else {\\n\/\/ no more owners, delete the capability\\npersistentStore.Delete(capability.Index())\\ndelete(capMap[capability.Index()])\\n}\\n}\\n```\\n### Usage patterns\\n#### Initialisation\\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\\n```golang\\nck := NewCapabilityKeeper(persistentKey, memoryKey)\\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(\"mod1\"), ....)\\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(\"mod2\"), ....)\\n\/\/ other initialisation logic ...\\n\/\/ load initial state...\\nck.InitialiseAndSeal(initialContext)\\n```\\n#### Creating, passing, claiming and using capabilities\\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\\nModule 1 would have the following code:\\n```golang\\ncapability := scopedCapabilityKeeper.NewCapability(ctx, \"resourceABC\")\\nmod2Keeper.SomeFunction(ctx, capability, args...)\\n```\\n`SomeFunction`, running in module 2, could then claim the capability:\\n```golang\\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\\nk.sck.ClaimCapability(ctx, capability, \"resourceABC\")\\n\/\/ other logic...\\n}\\n```\\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\\n```golang\\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\\ncapability := k.sck.GetCapability(ctx, name)\\nmod1.UseResource(ctx, capability, \"resourceABC\")\\n}\\n```\\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\\n```golang\\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\\nif !k.sck.AuthenticateCapability(name, capability) {\\nreturn errors.New(\"unauthenticated\")\\n}\\n\/\/ do something with the resource\\n}\\n```\\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\\n","tokens":434,"id":20,"Predictions":"Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nAt present, the Cosmos SDK does not have the ability to do this.nKeepers cannot create or store capability keys during transaction execution \u2014 although they could call NewKVStoreKey and take the memory addressnof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional \u2014 were this not the case, the keys would be predictable and couldn't serve as object capabilities).nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A)."}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"Predictions":"Decision (A).nnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nSpecifically, the client-side migration path primarily includes tx generation andnsigning, message construction and routing, in addition to CLI & REST handlers andnbusiness logic (i.e. queriers).nWith this in mind, we will tackle the migration path via two main areas, txs andnquerying, however, this ADR solely focuses on transactions.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"gatemint-sdk\/adr-018-extendable-voting-period.md","Context":"## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n","Decision":"We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params:\\n- The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the the default voting period that all governance proposal voting periods start with.\\n- There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the guage the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n","tokens":75,"id":22,"Predictions":"Decision (A) The time period for the voting of governance proposals is the same as the time period for non-contentious proposals.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"gatemint-sdk\/adr-017-historical-header-module.md","Context":"## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n","Decision":"The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\\n```golang\\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper, req abci.RequestBeginBlock) abci.ResponseBeginBlock {\\ninfo := HistoricalInfo{\\nHeader: ctx.BlockHeader(),\\nValSet: keeper.StakingKeeper.GetAllValidators(ctx), \/\/ note that this must be stored in a canonical order\\n}\\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\\nn := keeper.GetParamRecentHeadersToStore()\\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\\n\/\/ continue handling request\\n}\\n```\\nAlternatively, the application MAY store only the hash of the validator set.\\nThe application MUST make these past `n` committed headers available for querying by SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x\/staking` or `x\/ibc`).\\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\\n","tokens":65,"id":23,"Predictions":"Decision 1). The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 2). The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 3) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 4) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 5) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 6) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 7) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 8) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 9) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specificationn"}
{"File Name":"gatemint-sdk\/adr-019-protobuf-state-encoding.md","Context":"## Context\\nCurrently, the Cosmos SDK utilizes [go-amino](https:\/\/github.com\/tendermint\/go-amino\/) for binary\\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\\nFrom the Amino docs:\\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\\n> support. See the [Proto3 spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3) for more\\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\\n>\\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\\nAmino also aims to have the following goals (not a complete list):\\n- Binary bytes must be decode-able with a schema.\\n- Schema must be upgradeable.\\n- The encoder and decoder logic must be reasonably simple.\\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\\nclients written in various languages while providing virtually little in the way of true backwards\\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1<\/sup>. This is\\nlargely reflected in the performance of simulations and application transaction throughput.\\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\\n- Language agnostic\\n- Platform agnostic\\n- Rich client support and thriving ecosystem\\n- High performance\\n- Minimal encoded message size\\n- Codegen-based over reflection-based\\n- Supports backward and forward compatibility\\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\\nmade to address client-side encoding.\\n","Decision":"We will adopt [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) for serializing\\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\\nwill provide three concrete implementations of the `Marshaler` interface: `AminoCodec`, `ProtoCodec`,\\nand `HybridCodec`.\\n- `AminoCodec`: Uses Amino for both binary and JSON encoding.\\n- `ProtoCodec`: Uses Protobuf for or both binary and JSON encoding.\\n- `HybridCodec`: Uses Amino for JSON encoding and Protobuf for binary encoding.\\nUntil the client migration landscape is fully understood and designed, modules will use a `HybridCodec`\\nas the concrete codec it accepts and\/or extends. This means that all client JSON encoding, including\\ngenesis state, will still use Amino. The ultimate goal will be to replace Amino JSON encoding with\\nProtbuf encoding and thus have modules accept and\/or extend `ProtoCodec`.\\n### Module Codecs\\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\\n`Marshaler` that will be a `HybridCodec`. This migration is simple as things will just work as-is.\\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\\n[gogoprotobuf](https:\/\/github.com\/gogo\/protobuf) Value types.\\nExample:\\n```go\\nts, err := gogotypes.TimestampProto(completionTime)\\nif err != nil {\\n\/\/ ...\\n}\\nbz := cdc.MustMarshalBinaryBare(ts)\\n```\\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\\nto the module and will contain method contracts that know how to serialize the needed interfaces.\\nExample:\\n```go\\n\/\/ x\/auth\/types\/codec.go\\ntype Codec interface {\\ncodec.Marshaler\\nMarshalAccount(acc exported.Account) ([]byte, error)\\nUnmarshalAccount(bz []byte) (exported.Account, error)\\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\\n}\\n```\\n### Usage of `Any` to encode interfaces\\nIn general, module-level .proto files should define messages which encode interfaces\\nusing [`google.protobuf.Any`](https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/master\/src\/google\/protobuf\/any.proto).\\nAfter [extension discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030),\\nthis was chosen as the preferred alternative to application-level `oneof`s\\nas in our original protobuf design. The arguments in favor of `Any` can be\\nsummarized as follows:\\n* `Any` provides a simpler, more consistent client UX for dealing with\\ninterfaces than app-level `oneof`s that will need to be coordinated more\\ncarefully across applications. Creating a generic transaction\\nsigning library using `oneof`s may be cumbersome and critical logic may need\\nto be reimplemented for each chain\\n* `Any` provides more resistance against human error than `oneof`\\n* `Any` is generally simpler to implement for both modules and apps\\nThe main counter-argument to using `Any` centers around its additional space\\nand possibly performance overhead. The space overhead could be dealt with using\\ncompression at the persistence layer in the future and the performance impact\\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\\nwith user experience as the higher order concern.\\nNote, that given the SDK's decision to adopt the `Codec` interfaces described\\nabove, apps can still choose to use `oneof` to encode state and transactions\\nbut it is not the recommended approach. If apps do choose to use `oneof`s\\ninstead of `Any` they will likely lose compatibility with client apps that\\nsupport multiple chains. Thus developers should think carefully about whether\\nthey care more about what is possibly a pre-mature optimization or end-user\\nand client developer UX.\\n### Safe usage of `Any`\\nBy default, the [gogo protobuf implementation of `Any`](https:\/\/godoc.org\/github.com\/gogo\/protobuf\/types)\\nuses [global type registration]( https:\/\/github.com\/gogo\/protobuf\/blob\/master\/proto\/properties.go#L540)\\nto decode values packed in `Any` into concrete\\ngo types. This introduces a vulnerability where any malicious module\\nin the dependency tree could registry a type with the global protobuf registry\\nand cause it to be loaded and unmarshaled by a transaction that referenced\\nit in the `type_url` field.\\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\\nvalues into concrete types through the `InterfaceRegistry` interface which\\nbears some similarity to type registration with Amino:\\n```go\\ntype InterfaceRegistry interface {\\n\/\/ RegisterInterface associates protoName as the public name for the\\n\/\/ interface passed in as iface\\n\/\/ Ex:\\n\/\/   registry.RegisterInterface(\"cosmos_sdk.Msg\", (*sdk.Msg)(nil))\\nRegisterInterface(protoName string, iface interface{})\\n\/\/ RegisterImplementations registers impls as a concrete implementations of\\n\/\/ the interface iface\\n\/\/ Ex:\\n\/\/  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\\nRegisterImplementations(iface interface{}, impls ...proto.Message)\\n}\\n```\\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\\nto communicate the list of concrete types that satisfy an interface to clients.\\nIn .proto files:\\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\\nmay be used via code generation, reflection &\/or static linting.\\nThe same struct that implements `InterfaceRegistry` will also implement an\\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\\n```go\\ntype InterfaceUnpacker interface {\\n\/\/ UnpackAny unpacks the value in any to the interface pointer passed in as\\n\/\/ iface. Note that the type in any must have been registered with\\n\/\/ RegisterImplementations as a concrete type for that interface\\n\/\/ Ex:\\n\/\/    var msg sdk.Msg\\n\/\/    err := ctx.UnpackAny(any, &msg)\\n\/\/    ...\\nUnpackAny(any *Any, iface interface{}) error\\n}\\n```\\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\\nusage of `Any`, it just introduces a security and introspection layer for\\ngolang usage.\\n`InterfaceRegistry` will be a member of `ProtoCodec` and `HybridCodec` as\\ndescribed above. In order for modules to register interface types, app modules\\ncan optionally implement the following interface:\\n```go\\ntype InterfaceModule interface {\\nRegisterInterfaceTypes(InterfaceRegistry)\\n}\\n```\\nThe module manager will include a method to call `RegisterInterfaceTypes` on\\nevery module that implements it in order to populate the `InterfaceRegistry`.\\n### Using `Any` to encode state\\nThe SDK will provide support methods `MarshalAny` and `UnmarshalAny` to allow\\neasy encoding of state to `Any` in `Codec` implementations. Ex:\\n```go\\nimport \"github.com\/cosmos\/cosmos-sdk\/codec\"\\nfunc (c *Codec) MarshalEvidence(evidenceI eviexported.Evidence) ([]byte, error) {\\nreturn codec.MarshalAny(evidenceI)\\n}\\nfunc (c *Codec) UnmarshalEvidence(bz []byte) (eviexported.Evidence, error) {\\nvar evi eviexported.Evidence\\nerr := codec.UnmarshalAny(c.interfaceContext, &evi, bz)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn evi, nil\\n}\\n```\\n### Using `Any` in `sdk.Msg`s\\nA similar concept is to be applied for messages that contain interfaces fields.\\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\\nan interface:\\n```protobuf\\n\/\/ x\/evidence\/types\/types.proto\\nmessage MsgSubmitEvidence {\\nbytes submitter = 1\\n[\\n(gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"\\n];\\ngoogle.protobuf.Any evidence = 2;\\n}\\n```\\nNote that in order to unpack the evidence from `Any` we do need a reference to\\n`InterfaceRegistry`. In order to reference evidence in methods like\\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\\ninterfaces before they're needed.\\n### Unpacking Interfaces\\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\\ninterfaces wrapped in `Any` before they're needed, we create an interface\\nthat `sdk.Msg`s and other types can implement:\\n```go\\ntype UnpackInterfacesMessage interface {\\nUnpackInterfaces(InterfaceUnpacker) error\\n}\\n```\\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\\nstruct itself with a public getter `GetCachedValue() interface{}`.\\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\\nand stored in `cachedValue` for reference later.\\nThen unpacked interface values can safely be used in any code afterwards\\nwithout knowledge of the `InterfaceRegistry`\\nand messages can introduce a simple getter to cast the cached value to the\\ncorrect interface type.\\nThis has the added benefit that unmarshaling of `Any` values only happens once\\nduring initial deserialization rather than every time the value is read. Also,\\nwhen `Any` values are first packed (for instance in a call to\\n`NewMsgSubmitEvidence`), the original interface value is cached so that\\nunmarshaling isn't needed to read it again.\\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\\n`GetEvidence` as follows:\\n```go\\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\\nvar evi eviexported.Evidence\\nreturn ctx.UnpackAny(msg.Evidence, *evi)\\n}\\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\\n}\\n```\\n### Amino Compatibility\\nOur custom implementation of `Any` can be used transparently with Amino if used\\nwith the proper codec instance. What this means is that interfaces packed within\\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\\nhave been registered properly with Amino).\\nIn order for this functionality to work:\\n- **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\\nnow a wrapper which properly handles `Any`**\\n- **all new code should use `Marshaler` which is compatible with both amino and\\nprotobuf**\\n- Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\\n### Why Wasn't X Chosen Instead\\nFor a more complete comparison to alternative protocols, see [here](https:\/\/codeburst.io\/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\\n### Cap'n Proto\\nWhile [Cap\u2019n Proto](https:\/\/capnproto.org\/) does seem like an advantageous alternative to Protobuf\\ndue to it's native support for interfaces\/generics and built in canonicalization, it does lack the\\nrich client ecosystem compared to Protobuf and is a bit less mature.\\n### FlatBuffers\\n[FlatBuffers](https:\/\/google.github.io\/flatbuffers\/) is also a potentially viable alternative, with the\\nprimary difference being that FlatBuffers does not need a parsing\/unpacking step to a secondary\\nrepresentation before you can access data, often coupled with per-object memory allocation.\\nHowever, it would require great efforts into research and full understanding the scope of the migration\\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\\nuntrusted inputs.\\n","tokens":439,"id":24,"Predictions":"Decision based on the following: The Cosmos SDK uses [go-amino](https:\/\/github.com\/tendermint\/go-amino\/) for binarynand JSON object encoding over the wire bringing parity between logical objects and persistence objects.nThe goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and decoder logic must be reasonably simple.nThe encoder and de"}
{"File Name":"gatemint-sdk\/adr-011-generalize-genesis-accounts.md","Context":"## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","Decision":"In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"github.com\/cosmos\/cosmos-sdk\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n- Type registration on `auth`'s codec (shown above)\\n- A `Validate` method for each `Account` concrete type\\n","tokens":111,"id":25,"Predictions":"Decision (A) The SDK should be used for custom account types.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"gatemint-sdk\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase chanel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistent layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":337,"id":26,"Predictions":"Decision based on the following: We want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.nWe want to keep the IBC relatednstate modification to be preserved even the application side state changenreverts.n"}
{"File Name":"gatemint-sdk\/adr-008-dCERT-group.md","Context":"## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n","Decision":"The dCERT group is proposed to include an implementation of a `SpecializationGroup`\\nas defined in [ADR 007](.\/adr-007-specialization-groups.md). This will include the\\nimplementation of:\\n- continuous voting\\n- slashing due to breach of soft contract\\n- revoking a member due to breach of soft contract\\n- emergency disband of the entire dCERT group (ex. for colluding maliciously)\\n- compensation stipend from the community pool or other means decided by\\ngovernance\\nThis system necessitates the following new parameters:\\n- blockly stipend allowance per dCERT member\\n- maximum number of dCERT members\\n- required staked slashable tokens for each dCERT member\\n- quorum for suspending a particular member\\n- proposal wager for disbanding the dCERT group\\n- stabilization period for dCERT member transition\\n- circuit break dCERT privileges enabled\\nThese parameters are expected to be implemented through the param keeper such\\nthat governance may change them at any given point.\\n### Continuous Voting Electionator\\nAn `Electionator` object is to be implemented as continuous voting and with the\\nfollowing specifications:\\n- All delegation addresses may submit votes at any point which updates their\\npreferred representation on the dCERT group.\\n- Preferred representation may be arbitrarily split between addresses (ex. 50%\\nto John, 25% to Sally, 25% to Carol)\\n- In order for a new member to be added to the dCERT group they must\\nsend a transaction accepting their admission at which point the validity of\\ntheir admission is to be confirmed.\\n- A sequence number is assigned when a member is added to dCERT group.\\nIf a member leaves the dCERT group and then enters back, a new sequence number\\nis assigned.\\n- Addresses which control the greatest amount of preferred-representation are\\neligible to join the dCERT group (up the _maximum number of dCERT members_).\\nIf the dCERT group is already full and new member is admitted, the existing\\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\\n- In the split situation where the dCERT group is full but a vying candidate\\nhas the same amount of vote as an existing dCERT member, the existing\\nmember should maintain its position.\\n- In the split situation where somebody must be kicked out but the two\\naddresses with the smallest number of votes have the same number of votes,\\nthe address with the smallest sequence number maintains its position.\\n- A stabilization period can be optionally included to reduce the\\n\"flip-flopping\" of the dCERT membership tail members. If a stabilization\\nperiod is provided which is greater than 0, when members are kicked due to\\ninsufficient support, a queue entry is created which documents which member is\\nto replace which other member. While this entry is in the queue, no new entries\\nto kick that same dCERT member can be made. When the entry matures at the\\nduration of the  stabilization period, the new member is instantiated, and old\\nmember kicked.\\n### Staking\/Slashing\\nAll members of the dCERT group must stake tokens _specifically_ to maintain\\neligibility as a dCERT member. These tokens can be staked directly by the vying\\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\\nbenefits for doing so). This staking mechanism should use the existing global\\nunbonding time of tokens staked for network validator security. A dCERT member\\ncan _only be_ a member if it has the required tokens staked under this\\nmechanism. If those tokens are unbonded then the dCERT member must be\\nautomatically kicked from the group.\\nSlashing of a particular dCERT member due to soft-contract breach should be\\nperformed by governance on a per member basis based on the magnitude of the\\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\\nby the dCERT group prior to being slashed by governance.\\nMembership suspension by the dCERT group takes place through a voting procedure\\nby the dCERT group members. After this suspension has taken place, a governance\\nproposal to slash the dCERT member must be submitted, if the proposal is not\\napproved by the time the rescinding member has completed unbonding their\\ntokens, then the tokens are no longer staked and unable to be slashed.\\nAdditionally in the case of an emergency situation of a colluding and malicious\\ndCERT group, the community needs the capability to disband the entire dCERT\\ngroup and likely fully slash them. This could be achieved though a special new\\nproposal type (implemented as a general governance proposal) which would halt\\nthe functionality of the dCERT group until the proposal was concluded. This\\nspecial proposal type would likely need to also have a fairly large wager which\\ncould be slashed if the proposal creator was malicious. The reason a large\\nwager should be required is because as soon as the proposal is made, the\\ncapability of the dCERT group to halt message routes is put on temporarily\\nsuspended, meaning that a malicious actor who created such a proposal could\\nthen potentially exploit a bug during this period of time, with no dCERT group\\ncapable of shutting down the exploitable message routes.\\n### dCERT membership transactions\\nActive dCERT members\\n- change of the description of the dCERT group\\n- circuit break a message route\\n- vote to suspend a dCERT member.\\nHere circuit-breaking refers to the capability to disable a groups of messages,\\nThis could for instance mean: \"disable all staking-delegation messages\", or\\n\"disable all distribution messages\". This could be accomplished by verifying\\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\\n`baseapp\/baseapp.go`).\\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\\nmeaning that no capability to unbreak a message route on a live chain is\\nrequired.\\nNote also, that if there was a problem with governance voting (for instance a\\ncapability to vote many times) then governance would be broken and should be\\nhalted with this mechanism, it would be then up to the validator set to\\ncoordinate and hard-fork upgrade to a patched version of the software where\\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\\nthey should all be severely slashed.\\n","tokens":292,"id":27,"Predictions":"Decision (A). dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.n"}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"Predictions":"Decision 1). The delegators should be able to delegate to large validators. 2). The delegators should be able to delegate to large validators. 3) The delegators should be able to delegate to large validators. 4) The delegators should be able to delegate to large validators. 5) The delegators should be able to delegate to large validators. 6) The delegators should be able to delegate to large validators. 7) The delegators should be able to delegate to large validators. 8) The delegators should be able to delegate to large validators. 9) The delegators should be able to delegate to large validators. 10) The delegators should be able to delegate to large validators. 10) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators."}
{"File Name":"gatemint-sdk\/adr-012-state-accessors.md","Context":"## Context\\nSDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\\nmediocre.\\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\\nwhich are repetitive and hard to maintain.\\nSecond, this makes it harder to align with the object capability theorem: the right to access the\\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\\nFinally, because the getter\/setter functions are defined as methods of a module's `Keeper`, the reviewers\\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\\nThere is no static way to know which part of the state that the function is accessing (and which is not).\\n","Decision":"We will define a type named `Value`:\\n```go\\ntype Value struct {\\nm   Mapping\\nkey []byte\\n}\\n```\\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\\nspace it will access and `Value.key` defines the exact key for the reference.\\nWe will define a type named `Mapping`:\\n```go\\ntype Mapping struct {\\nstoreKey sdk.StoreKey\\ncdc      *codec.LegacyAmino\\nprefix   []byte\\n}\\n```\\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\\nWe will define the following core methods for the `Value` type:\\n```go\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Value) Get(ctx Context, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\\n\/\/ Get stored data as raw byte slice\\nfunc (Value) GetRaw(ctx Context) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Value) Set(ctx Context, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Value) Exists(ctx Context) bool {}\\n\/\/ Delete a raw value value\\nfunc (Value) Delete(ctx Context) {}\\n```\\nWe will define the following core methods for the `Mapping` type:\\n```go\\n\/\/ Constructs key-value pair reference corresponding to the key argument in the Mapping space\\nfunc (Mapping) Value(key []byte) Value {}\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\\n\/\/ Get stored data as raw byte slice\\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\\n\/\/ Delete a raw value value\\nfunc (Mapping) Delete(ctx Context, key []byte) {}\\n```\\nEach method of the `Mapping` type that is passed the arugments `ctx`, `key`, and `args...` will proxy\\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\\nIn addition, we will define and provide a common set of types derived from the `Value` type:\\n```go\\ntype Boolean struct { Value }\\ntype Enum struct { Value }\\ntype Integer struct { Value; enc IntEncoding }\\ntype String struct { Value }\\n\/\/ ...\\n```\\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\\nin core methods are replaced by explicit return types.\\nFinally, we will define a family of types derived from the `Mapping` type:\\n```go\\ntype Indexer struct {\\nm   Mapping\\nenc IntEncoding\\n}\\n```\\nWhere the `key` argument in core method is typed.\\nSome of the properties of the accessor types are:\\n- State access happens only when a function which takes a `Context` as an argument is invoked\\n- Accessor type structs give rights to access the state only that the struct is referring, no other\\n- Marshalling\/Unmarshalling happens implicitly within the core methods\\n","tokens":248,"id":30,"Predictions":"Decision (A) The state access function should be able to access the state.nB) The state access function should be able to access the state.nC) The state access function should be able to access the state.nD) The state access function should be able to access the state.nAnswer:A.B. The state access function should be able to access the state.nB. The state access function should be able to access the state.nC. The state access function should be able to access the state.nD. The state access function should be able to access the state.nAnswer:A.B. The state access function should be able to access the state.nB. The state access function should be able to access the state.nC. The state access function should be able to access the state.nD. The state access function should be able to access the state.nAnswer:A.B. The state access function should be able to access the state.nB. The state access function should be able to access the state.nB. The state access function should be able to access the state.nAnswer:A.B. The state access function should be able to access the state.nB. The state access function should be able to access the state.nB. The state access function should be able to access the state.nB. The state access function should be able to access the state.nAnswer:A."}
{"File Name":"gatemint-sdk\/adr-023-protobuf-naming.md","Context":"## Context\\nProtocol Buffers provide a basic [style guide](https:\/\/developers.google.com\/protocol-buffers\/docs\/style)\\nand [Buf](https:\/\/buf.build\/docs\/style-guide) builds upon that. To the\\nextent possible, we want to follow industry accepted guidelines and wisdom for\\nthe effective usage of protobuf, deviating from those only when there is clear\\nrationale for our use case.\\n### Adoption of `Any`\\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\\ninterface types (as opposed to `oneof`) makes package naming a central part\\nof the encoding as fully-qualified message names now appear in encoded\\nmessages.\\n### Current Directory Organization\\nThus far we have mostly followed [Buf's](https:\/\/buf.build) [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout)\\nwhich although being convenient for developing code comes with the warning\\nfrom Buf that:\\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\\n### Adoption of gRPC Queries\\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\\npath. In the future, gRPC queries may be allowed from within persistent scripts\\nby technologies such as CosmWasm and these query routes would be stored within\\nscript binaries.\\n","Decision":"The goal of this ADR is to provide thoughtful naming conventions that:\\n* encourage a good user experience for when users interact directly with\\n.proto files and fully-qualified protobuf names\\n* balance conciseness against the possibility of either over-optimizing (making\\nnames too short and cryptic) or under-optimizing (just accepting bloated names\\nwith lots of redundant information)\\nThese guidelines are meant to act as a style guide for both the SDK and\\nthird-party modules.\\nAs a starting point, we should adopt all of the [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\ncheckers in [Buf's](https:\/\/buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout),\\nexcept:\\n* [PACKAGE_VERSION_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n* [SERVICE_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#service_suffix)\\nFurther guidelines to be described below.\\n### Principles\\n#### Concise and Descriptive Names\\nNames should be descriptive enough to convey their meaning and distinguish\\nthem from other names.\\nGiven that we are using fully-qualifed names within\\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\\nkeep names concise, without going overboard. The general rule of thumb should\\nbe if a shorter name would convey more or else the same thing, pick the shorter\\nname.\\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\\nSuch conciseness makes names both more pleasant to work with and take up less\\nspace within transactions and on the wire.\\nWe should also resist the temptation to over-optimize, by making names\\ncryptically short with abbreviations. For instance, we shouldn't try to\\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\\nThe goal is to make names **_concise but not cryptic_**.\\n#### Names are for Clients First\\nPackage and type names should be chosen for the benefit of users, not\\nnecessarily because of legacy concerns related to the go code-base.\\n#### Plan for Longevity\\nIn the interests of long-term support, we should plan on the names we do\\nchoose to be in usage for a long time, so now is the opportunity to make\\nthe best choices for the future.\\n### Versioning\\n#### Don't Allow Breaking Changes in Stable Packages\\nAlways use a breaking change detector such as [Buf](https:\/\/buf.build) to prevent\\nbreaking changes in stable (non-alpha or beta) packages. Breaking changes can\\nbreak smart contracts\/persistent scripts and generally provide a bad UX for\\nclients. With protobuf, there should usually be ways to extend existing\\nfunctionality instead of just breaking it.\\n#### Omit v1 suffix\\nInstead of using [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix),\\nwe can omit `v1` for packages that don't actually have a second version. This\\nallows for more concise names for common use cases like `cosmos.bank.Send`.\\nPackages that do have a second or third version can indicate that with `.v2`\\nor `.v3`.\\n#### Use `alpha` or `beta` to Denote Non-stable Packages\\n[Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n(ex. `v1alpha1`) _should_ be used for non-stable packages. These packages should\\nlikely be excluded from breaking change detection and _should_ generally\\nbe blocked from usage by smart contracts\/persistent scripts to prevent them\\nfrom breaking. The SDK _should_ mark any packages as alpha or beta where the\\nAPI is likely to change significantly in the near future.\\n### Package Naming\\n#### Adopt a short, unique top-level package name\\nTop-level packages should adopt a short name that is known to not collide with\\nother names in common usage within the Cosmos ecosystem. In the near future, a\\nregistry should be created to reserve and index top-level package names used\\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\\n[ICS](https:\/\/github.com\/cosmos\/ics) specifications could consider a\\nshort top-level package like `ics23` based upon the standard number.\\n#### Limit sub-package depth\\nSub-package depth should be increased with caution. Generally a single\\nsub-package is needed for a module or a library. Even though `x` or `modules`\\nis used in source code to denote modules, this is often unnecessary for .proto\\nfiles as modules are the primary thing sub-packages are used for. Only items which\\nare known to be used infrequently should have deep sub-package depths.\\nFor the Cosmos SDK, it is recommended that that we simply write `cosmos.bank`,\\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\\ntypes can go straight in the `cosmos` package or we can introduce a\\n`cosmos.base` package if needed. Note that this naming _will not_ change\\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\\n`x\/bank`.\\n### Message Naming\\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\\ntypes which are used in transactions will retain the `Msg` prefix as that provides\\nhelpful context.\\n### Service and RPC Naming\\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\\nimplement a gRPC query service. We should consider the principle of conciseness\\nfor query service and RPC names as these may be called from persistent script\\nmodules such as CosmWasm. Also, users may use these query paths from tools like\\n[gRPCurl](https:\/\/github.com\/fullstorydev\/grpcurl). As an example, we can shorten\\n`\/cosmos_sdk.x.bank.v1.QueryService\/QueryBalance` to\\n`\/cosmos.bank.Query\/Balance` without losing much useful information.\\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`\/\\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\\non the `Query` service, the request and response types would be `QueryBalanceRequest`\\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\\nand `BalanceResponse`.\\n#### Use just `Query` for the query service\\nInstead of [Buf's default service suffix recommendation](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6033),\\nwe should simply use the shorter `Query` for query services.\\nFor other types of gRPC services, we should consider sticking with Buf's\\ndefault recommendation.\\n#### Omit `Get` and `Query` from query service RPC names\\n`Get` and `Query` should be omitted from `Query` service names because they are\\nredundant in the fully-qualified name. For instance, `\/cosmos.bank.Query\/QueryBalance`\\njust says `Query` twice without any new information.\\n","tokens":330,"id":31,"Predictions":"Decision (A) The following are the steps that should be taken to implement gRPC queries:n### Adoption of Anynn### Adoption of google.protobuf.Anyn### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries. The full gRPC service path thus becomes a key part of ABCI querynpath.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md]"}
{"File Name":"gatemint-sdk\/adr-027-deterministic-protobuf-serialization.md","Context":"## Context\\n[Protobuf](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3)\\nseralization is not unique (i.e. there exist a practically unlimited number of\\nvalid binary representations for a protobuf document)<sup>1<\/sup>. For signature\\nverification in Cosmos SDK, signer and verifier need to agree on the same\\nserialization of a SignDoc as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization. This document describes a deterministic serialization scheme for\\na subset of protobuf documents, that covers this use case but can be reused in\\nother cases as well.\\n","Decision":"The following encoding scheme is proposed to be used by other ADRs.\\n### Scope\\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\\nserialization, such that every protobuf parser can parse it.\\nNo maps are supported in version 1 due to the complexity of defining a\\nderterministic serialization. This might change in future. Implementations must\\nreject documents containing maps as invalid input.\\n### Serialization rules\\nThe serialization is based on the\\n[protobuf 3 encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding)\\nwith the following additions:\\n1. Fields must be serialized only once in ascending order\\n2. Extra fields or any extra data must not be added\\n3. [Default values](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#default)\\nmust be omitted\\n4. `repeated` fields of scalar numeric types must use\\n[packed encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding#packed)\\nby default.\\n5. Variant encoding of integers must not be longer than needed.\\nWhile rule number 1. and 2. should be pretty straight forward and describe the\\ndefault behaviour of all protobuf encoders the author is aware of, the 3rd rule\\nis more interesting. After a protobuf 3 deserialization you cannot differentiate\\nbetween unset fields and fields set to the default value<sup>2<\/sup>. At\\nserialization level however, it is possible to set the fields with an empty\\nvalue or omitting them entirely. This is a significant difference to e.g. JSON\\nwhere a property can be empty (`\"\"`, `0`), `null` or undefined, leading to 3\\ndifferent documents.\\nOmitting fields set to default values is valid because the parser must assign\\nthe default value to fields missing in the serialization<sup>3<\/sup>. For scalar\\ntypes, omitting defaults is required by the spec<sup>4<\/sup>. For `repeated`\\nfields, not serializing them is the only way to express empty lists. Enums must\\nhave a first element of numeric value 0, which is the default<sup>5<\/sup>. And\\nmessage fields default to unset<sup>6<\/sup>.\\nOmitting defaults allows for some amount of forward compatibility: users of\\nnewer versions of a protobuf schema produce the same serialization as users of\\nolder versions as long as newly added fields are not used (i.e. set to their\\ndefault value).\\n### Implementation\\nThere are three main implementation strategies, ordered from the least to the\\nmost custom development:\\n- **Use a protobuf serializer that follows the above rules by default.** E.g.\\n[gogoproto](https:\/\/pkg.go.dev\/github.com\/gogo\/protobuf\/gogoproto) is known to\\nbe compliant by in most cases, but not when certain annotations such as\\n`nullable = false` are used. It might also be an option to configure an\\nexisting serializer accordingly.\\n- **Normalize default values before encoding them.** If your serializer follows\\nrule 1. and 2. and allows you to explicitly unset fields for serialization,\\nyou can normalize default values to unset. This can be done when working with\\n[protobuf.js](https:\/\/www.npmjs.com\/package\/protobufjs):\\n```js\\nconst bytes = SignDoc.encode({\\nbodyBytes: body.length > 0 ? body : null, \/\/ normalize empty bytes to unset\\nauthInfoBytes: authInfo.length > 0 ? authInfo : null, \/\/ normalize empty bytes to unset\\nchainId: chainId || null, \/\/ normalize \"\" to unset\\naccountNumber: accountNumber || null, \/\/ normalize 0 to unset\\naccountSequence: accountSequence || null, \/\/ normalize 0 to unset\\n}).finish();\\n```\\n- **Use a hand-written serializer for the types you need.** If none of the above\\nways works for you, you can write a serializer yourself. For SignDoc this\\nwould look something like this in Go, building on existing protobuf utilities:\\n```go\\nif !signDoc.body_bytes.empty() {\\nbuf.WriteUVarInt64(0xA) \/\/ wire type and field number for body_bytes\\nbuf.WriteUVarInt64(signDoc.body_bytes.length())\\nbuf.WriteBytes(signDoc.body_bytes)\\n}\\nif !signDoc.auth_info.empty() {\\nbuf.WriteUVarInt64(0x12) \/\/ wire type and field number for auth_info\\nbuf.WriteUVarInt64(signDoc.auth_info.length())\\nbuf.WriteBytes(signDoc.auth_info)\\n}\\nif !signDoc.chain_id.empty() {\\nbuf.WriteUVarInt64(0x1a) \/\/ wire type and field number for chain_id\\nbuf.WriteUVarInt64(signDoc.chain_id.length())\\nbuf.WriteBytes(signDoc.chain_id)\\n}\\nif signDoc.account_number != 0 {\\nbuf.WriteUVarInt64(0x20) \/\/ wire type and field number for account_number\\nbuf.WriteUVarInt(signDoc.account_number)\\n}\\nif signDoc.account_sequence != 0 {\\nbuf.WriteUVarInt64(0x28) \/\/ wire type and field number for account_sequence\\nbuf.WriteUVarInt(signDoc.account_sequence)\\n}\\n```\\n### Test vectors\\nGiven the protobuf definition `Article.proto`\\n```protobuf\\npackage blog;\\nsyntax = \"proto3\";\\nenum Type {\\nUNSPECIFIED = 0;\\nIMAGES = 1;\\nNEWS = 2;\\n};\\nenum Review {\\nUNSPECIFIED = 0;\\nACCEPTED = 1;\\nREJECTED = 2;\\n};\\nmessage Article {\\nstring title = 1;\\nstring description = 2;\\nuint64 created = 3;\\nuint64 updated = 4;\\nbool public = 5;\\nbool promoted = 6;\\nType type = 7;\\nReview review = 8;\\nrepeated string comments = 9;\\nrepeated string backlinks = 10;\\n};\\n```\\nserializing the values\\n```yaml\\ntitle: \"The world needs change \ud83c\udf33\"\\ndescription: \"\"\\ncreated: 1596806111080\\nupdated: 0\\npublic: true\\npromoted: false\\ntype: Type.NEWS\\nreview: Review.UNSPECIFIED\\ncomments: [\"Nice one\", \"Thank you\"]\\nbacklinks: []\\n```\\nmust result in the serialization\\n```\\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\\n```\\nWhen inspecting the serialized document, you see that every second field is\\nomitted:\\n```\\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\\n1: \"The world needs change \\360\\237\\214\\263\"\\n3: 1596806111080\\n5: 1\\n7: 2\\n9: \"Nice one\"\\n9: \"Thank you\"\\n```\\n","tokens":135,"id":32,"Predictions":"Decision (A) The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting thenserialization.n ## Solution The signer and verifier need to agree on the samenserialization of a SignDoc as defined inn[ADR-020](.\/adr-020-protobuf-transaction-encoding.md"}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"Predictions":"Decision (A) The user should be able to withdraw the coins from the account.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"gatemint-sdk\/adr-022-custom-panic-handling.md","Context":"## Context\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#Recovery-middleware)).\\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\\nin order to customize it.\\n","Decision":"### Design\\n#### Overview\\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\\ncan be found [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6053).\\n#### Implementation details\\n##### Recovery handler\\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\\n`recover()` from the `builtin` package.\\n```go\\ntype RecoveryHandler func(recoveryObj interface{}) error\\n```\\nHandler should type assert (or other methods) an object to define if object should be handled.\\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\\nAn example:\\n```go\\nfunc exampleErrHandler(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(error)\\nif !ok { return nil }\\nif someSpecificError.Is(err) {\\npanic(customPanicMsg)\\n} else {\\nreturn nil\\n}\\n}\\n```\\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\\n##### Recovery middleware\\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\\nchain processing.\\n```go\\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\\nif err := handler(recoveryObj); err != nil {\\nreturn nil, err\\n}\\nreturn next, nil\\n}\\n}\\n```\\nFunction receives a `recoveryObj` object and returns:\\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\\n`OutOfGas` middleware example:\\n```go\\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\\nif !ok { return nil }\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\\n\"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\\n),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, next)\\n}\\n```\\n`Default` middleware example:\\n```go\\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, nil)\\n}\\n```\\n##### Recovery processing\\nBasic chain of middlewares processing would look like:\\n```go\\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\\nif middleware == nil { return nil }\\nnext, err := middleware(recoveryObj)\\nif err != nil { return err }\\nif next == nil { return nil }\\nreturn processRecovery(recoveryObj, next)\\n}\\n```\\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\\n`default` handler which must return an `error`.\\n##### BaseApp changes\\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\\n```go\\ntype BaseApp struct {\\n\/\/ ...\\nrunTxRecoveryMiddleware recoveryMiddleware\\n}\\nfunc NewBaseApp(...) {\\n\/\/ ...\\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\\n}\\nfunc (app *BaseApp) runTx(...) {\\n\/\/ ...\\ndefer func() {\\nif r := recover(); r != nil {\\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\\nerr, result = processRecovery(r, recoveryMW), nil\\n}\\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\\n}()\\n\/\/ ...\\n}\\n```\\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\\n```go\\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\\nfor _, h := range handlers {\\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\\n}\\n}\\n```\\nThis method would prepend handlers to an existing chain.\\n","tokens":337,"id":34,"Predictions":"Decision (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (B) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (C) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (D) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (B) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (C) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (D) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery."}
{"File Name":"gatemint-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n- create new random consensus key.\\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n- start validating with new consensus key.\\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n- consensus key mapping information management strategy\\n- store history of each key mapping changes in the kvstore.\\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n- the state machine does not need any historical mapping information which is past more than unbonding period.\\n- key rotation costs related to LCD and IBC\\n- LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n- limits\\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\\n- parameters can be decided by governance and stored in genesis file.\\n- key rotation fee\\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n- evidence module\\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n- abci.ValidatorUpdate\\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n- new genesis parameters in `staking` module\\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n- checks if the signing account has enough balance to pay `KeyRotationFee`\\n- pays `KeyRotationFee` to community fund\\n- overwrites `NewPubKey` in `validator.ConsPubKey`\\n- deletes old `ValidatorByConsAddr`\\n- `SetValidatorByConsAddr` for `NewPubKey`\\n- Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n- Note : All above features shall be implemented in `staking` module.\\n","tokens":173,"id":35,"Predictions":"Decision based on the following: We suggest a simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, so from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.nWe consider that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept.n"}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"Predictions":"Decision MakersnN\/An ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnAn ## Decision MakersnA"}
{"File Name":"Marain.Tenancy\/0001-synthetic-root-tenant.md","Context":"## Context\\nCorvus Tenancy, which provides the underpinnings of the Marain Tenancy service, has always had a concept of a root tenant (dating from its earlier, pre-open-source incarnation). This ADR captures aspects of this root tenant that are non-obvious. (We learned that it was non-obvious because some code has been written that was unaware of the special status of the root tenant.)\\n","Decision":"There is a special tenant known as the Root Tenant. It has a well-known id, `f26450ab1668784bb327951c8b08f347`. It is special in three respects:\\n* tenants are hierarchical, and the root tenant forms the root of that hierarchy\\n* the tenanted storage mechanisms will all fall back to the root tenant to find default connection settings if the tenant being used has not defined tenant-specific settings\\n* within Marain services, the root tenant is always represented by a special in-memory instance of the `RootTenant` type, whereas all other tenants are managed by the tenancy service\\nThat third item is there to support the second: because each service puts its own `RootTenant` into the DI service collection, as a singleton, it becomes possible for the service to attach whatever service-specific fallback settings it requires. We describe the root tenant as \"synthetic\" because each service creates its own object to represent the root tenant, whereas all other objects representing tenants are obtained via the `Marain.Tenancy` service, typically through the `ClientTenantProvider`.\\nWe contemplated separating out the first two concerns (which might enable us not to need the third characteristic above) because it has been a source of confusion in the past. However, for the time being we are planning to keep it this way because that alternative approach would require us to introduce an extra mechanism to support these kinds of defaults.\\n","tokens":88,"id":37,"Predictions":"Decision (A) The root tenant should be a non-obvious tenant.nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nbsp;nb"}
{"File Name":"Marain.Tenancy\/0002-separation-of-read-and-modify.md","Context":"## Context\\nOur initial design for tenancy in Corvus (which necessarily affected Marain.Tenancy) comingled read and write behaviour. The model was similar to the .NET Entity Framework: if you wanted to modify a tenant, you would first fetch an object representing that tenant, then make changes to that object, and then invoke an operation indicating that you wanted those changes to be written back.\\nWe made various changes to the Property Bag system that tenancy uses to store tenant properties to disassociate the API from any particular JSON serialization framework. We had previously forced a dependency on Json.NET, but we wanted to be able to move onto `System.Text.Json`, so we wanted to introduce a Property Bag abstraction that was independent of serialization mechanism (although still with a presumption that it must be possible for the properties to be serialized as JSON).\\nOne of the basic principles of efficient JSON parsing in the new world is that you don't build an object model representing the JSON unless you really need to. Ideally, you leave the JSON in its raw UTF-8 state, referred to via one or more `IMemory<byte>` values, and extract what data you need only as you need it. This can dramatically reduce GC pressure, particularly in cases where most of the data in question is not used most of the time. However, this model does not fit well with the \"modifiable entities\" approach to updates. If anything is free to modify the properties at any time, this implies an ability to edit or regenerate the JSON.\\nIn practice, modification of tenant properties is the exception, not the rule. Most Marain services will only ever fetch tenant properties. Only the Marain.Tenancy service should normally directly edit these properties. So the \"modifiable entities\" approach is not really necessary, and causes problems for migration to allocation-efficient strategies.\\n","Decision":"Since `Corvus.Json.Abstractions` separates out read and update operations for `IPropertyBag`, and `Corvus.Tenancy` therefore does the same (since it uses property bags), Marain.Tenancy will follow suit.\\nThe web API presented by Marain.Tenancy for modifying tenants uses JSON Patch. So instead of this procedure:\\n* fetch a serialized representation of an ITenant from the web API\\n* modify that representation to reflect the changes you wish to make\\n* PUT that serialized representation of an ITenant back to the web API\\nwe now use this procedure instead:\\n* send a PATCH request in describing the changes required in JSON Patch format\\nFor example, to rename a tenant, you would send this PATCH to the Marain.Tenancy service, using the URL representing the tenant (the same URL from which you would fetch the tenant if reading) with an `application\/json-patch+json` content type:\\n```json\\n[{\\n\"path\": \"\/name\",\\n\"op\": \"replace\",\\n\"value\": \"NewTenantName\"\\n}]\\n```\\nJSON Patch supports multiple changes in a single request, e.g.:\\n```json\\n[\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/StorageConfiguration__corvustenancy\",\\n\"value\": {\\n\"AccountName\": \"mardevtenancy\",\\n\"Container\": null,\\n\"KeyVaultName\": \"mardevkv\",\\n\"AccountKeySecretName\": \"mardevtenancystore\",\\n\"DisableTenantIdPrefix\": false\\n}\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__bar\",\\n\"value\": \"Some string\"\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__spong\",\\n\"value\": 42\\n}\\n]\\n```\\nThe `op` can be set to `remove` to delete properties.\\nClients will not typically build these PATCH requests themselves, because the `ClientTenantStore` type contains the relevant code. `ClientTenantStore` provides an implementation of `ITenantStore` that works by using the web API provided by Marain.Tenancy. So in practice, updating the name of a tenant is as simple as:\\n```csharp\\nawait tenantStore.UpdateTenantAsync(tenantId, name: \"NewTenantName\");\\n```\\nAdding or changing a property looks like this:\\nawait tenantStore.UpdateTenantAsync(\\ntenantId,\\npropertiesToSetOrAdd: new Dictionary<string, object>()\\n{\\n{ \"StorageConfiguration__corvustenancy\", myStorageConfig },\\n{ \"SomeOtherSetting\": 42 },\\n});\\n","tokens":373,"id":39,"Predictions":"Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (B) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (C) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (D) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (B) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (C) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (D) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (B) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (A) The model should be able to move onto System.Text.Json, or System.Text.Json.n Decision (B) The model should be able to move onto System.Text.Json, or System.Text.Json, or System.Text.Json.n Decision (C) The model should be able to move onto System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json, or System.Text.Json,"}
{"File Name":"Menes\/0001-menes-exceptions-have-specialized-constructors.md","Context":"## Context\\nSwitching on the C# 8.0 nullable references feature for Menes has revealed some ambiguities around whether certain properties of exceptions are meant to be nullable.\\nIn many cases, the only reason for ambiguity is that we have followed a pattern of defining various \"standard constructors\", such as default constructors, exception-message-only constructors, and deserializing constructors.\\n","Decision":"Menes exceptions will not have any of these standard exceptions except in cases where there are no required properties (e.g., the exception's type tells you everything you need to know).\\nProperties that always have non-null values in practice will declare this formally by having non-nullable types.\\nWe will remove all deserializing constructors, and remove the `[Serializable]` attribute from all exceptions that have them. This has been motivated by the use of nullable references, because deserializing constructors cause some challenges there, but this is a distinct issue. Menes exceptions are all designed for use within a Menes-based service. Menes is designed to implement service boundaries, and by definition, if we ever attempt to throw a Menes-defined exception across a process boundary, we've made a mistake.\\n","tokens":77,"id":40,"Predictions":"Decision Makers should consider the following: a. The exceptions feature should be used to indicate that the exceptions are nullable. b. The exceptions feature should be used to indicate that the exceptions are nullable. c. The exceptions feature should be used to indicate that the exceptions are nullable. d. The exceptions feature should be used to indicate that the exceptions are nullable. e. The exceptions feature should be used to indicate that the exceptions are nullable. f. The exceptions feature should be used to indicate that the exceptions are nullable. g. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that the exceptions are nullable. h. The exceptions feature should be used to indicate that"}
{"File Name":"Menes\/0002-multitargeting-.net-standard-2.0-and-2.1.md","Context":"## Context\\nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the `System.Diagnostics.CodeAnalysis` namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.\\nThese attributes are not available in `netstandard2.0`. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the `Nullable` NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.\\nThe problem is that you don't want to use this workaround unless you have to. Newer versions of .NET Core and .NET Standard have these attributes, so it's just a waste of space to define your own.\\n","Decision":"Menes will target both .NET Standard 2.0 and .NET Standard 2.1. The .NET Standard 2.0 version brings its own copies of the attributes, the .NET Standard 2.1 version relies on the ones built into the framework.\\n","tokens":167,"id":41,"Predictions":"nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the System.Diagnostics.CodeAnalysis namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.nThese attributes are not available in netstandard2.0. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the Nullable NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.nThe problem is that you don't want to use this workaround unless you have to. Newer versions of.NET Core and.NET Standard have these attributes, so it's just a waste of space to define your own.n Decision nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the System.Diagnostics.CodeAnalysis namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.nThese attributes are not available in netstandard2.0. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the Nullable NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.nThe problem is that you don't want to use this workaround unless you have to. Newer versions of.NET Core and.NET Standard have these attributes, so it's just a waste of space to define your own.n"}
{"File Name":"Menes\/0003-allowing-implicit-object-type.md","Context":"## Context\\nIn real-world OpenAPI schema, we have discovered that people sometimes omit the `type: object` from their object definitions. We believe that this *is* valid Open API schema.\\n```yaml\\nPet:\\nrequired:\\n- id\\n- name\\nproperties:\\nid:\\ntype: integer\\nformat: int64\\nname:\\ntype: string\\ntag:\\ntype: string\\n```\\nHowever, there are other scenarios where you are *not* expected to supply the `type` property. Specifically, the `anyOf`, `oneOf`, `allOf` cases.\\n```yaml\\nsomeEntity:\\nanyOf:\\n- type: string\\n- type: object\\n- type: array\\n- type: boolean\\n- type: integer\\n- type: number\\n```\\n","Decision":"Menes will support these semantics. We have updated our schema validation to support this by translating the missing `type` element into the internal schema type `None`, rather than translating to `Object`.\\n","tokens":170,"id":42,"Predictions":"Decision : We believe that this schema is valid Open API schema.nyamlnPet:nrequired:n- idn- namenproperties:nid:ntype: integernformat: int64nname:ntype: stringntag:ntype: stringnnHowever, there are other scenarios where you are *not* expected to supply the type property. Specifically, the anyOf, oneOf, allOf cases.nyamlnsomeEntity:nanyOf:n- type: stringn- type: objectn- type: integern- type: numbernnn nyamlnsomeEntity:nanyOf:n- type: integern- type: numbernnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modiapersonoversikt\/0002-selvstendig-visittkort-som-library.md","Context":"## Context\\nModiapersonoversikt (denne frontenden) blir utviklet som en selvstendig frontend som p\u00e5 sikt skal erstatte frontenden i dagens modiabrukerdialog. For \u00e5 kunne levere fortl\u00f8pende ny funksjonalitet til saksbehandlerene, \u00f8nsker vi \u00e5 levere ofte og sm\u00e5tt.\\n","Decision":"Visittkortet dras inn som en enkeltst\u00e5ende react-komponent inn til modiabrukerdialog.\\n","tokens":81,"id":43,"Predictions":"Decision                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 "}
{"File Name":"modiapersonoversikt\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":44,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"bunyan-logger\/0001-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":45,"Predictions":"GitHub Actions are the best choice for this project.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"mokka\/0003-upgrade-to-spring-boot-2-2-1.md","Context":"## Context\\nCurrently used: 2.1.0\\nThe latest stable version of Spring Boot is 2.2.1:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-2.2-Release-Notes\\nMore frequent but smaller upgrades are recommended.\\n","Decision":"Spring Boot will be upgraded to 2.2.1.\\nAccording to release notes no migration needed on Mokka side.\\n","tokens":63,"id":46,"Predictions":"Decision (A) To upgrade the Spring Boot to 2.2.1.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"mokka\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":48,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"Predictions":"Decision Makers should consider the following options: (i) transferring configuration between Istio components during runtime; (ii) transferring configuration between Istio components during runtime; (iii) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime; (iv) transferring configuration between Istio components during runtime;"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"Predictions":"Decision 12nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"copilot\/0004-check-in-copilot-dependencies-that-were-already-vendored.md","Context":"## Context\\nPrevious packaging of Copilot in istio release relied on the fact that you would\\nbe building copilot on the local machine (bosh pre-packaging).  This meant that\\nyou could reliably fetch all of your dependencies using dep (which was included\\nas a blob in the release).\\nWhen we moved to get rid of pre-packaging and instead do all packaging on a bosh\\nvm (just known as packaging) we ended up missing one key external dependency for\\ndep to work (git). Including git as part of release would have meant adding\\nanother blob and packaging step just for git.\\n","Decision":"We removed the .gitignore of the vendor directory and checked-in all of the\\nsource code that dep was placing in that directory at build time.\\n","tokens":128,"id":51,"Predictions":"Decision (A). The packaging step should be included in the release.nB. The packaging step should be included in the release.nC. The packaging step should be included in the release.nD. The packaging step should be included in the release.nA. The packaging step should be included in the release.nB. The packaging step should be included in the release.nC. The packaging step should be included in the release.nD. The packaging step should be included in the release.nA. The packaging step should be included in the release.nB. The packaging step should be included in the release.nC. The packaging step should be included in the release.nD. The packaging step should be included in the release.nA. The packaging step should be included in the release.nB. The packaging step should be included in the release.nC. The packaging step should be included in the release.nD. The packaging step should be included in the release.nA. The packaging step should be included in the release.nB. The packaging step should be included in the release.nC. The packaging step should be included in the release.nD. The packaging step should be included in the release.nA. The packaging step should be included in the release.nB. The packaging step should be included in the release.nC. The packaging step should be included in the release.nD. The packaging step should be included in the release.nA. The packaging step should be included in the release.nB. The packaging step should be included in the release.n"}
{"File Name":"copilot\/0005-enable-grpc-gzip-compression-between-copilot-and-route-syncer.md","Context":"## Context\\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\\n","Decision":"We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\\n","tokens":44,"id":52,"Predictions":"Decision (A) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (B) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (C) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot.n  Decision (D) The GRPC default message size of 4 mb currently causing a"}
{"File Name":"copilot\/0002-use-event-streaming-model-for-diego-actuallrp-syncing.md","Context":"## Context\\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\\nacross all diego cells at a specified time interval (at the time of writing 10\\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\\nimpact the performance of the BBS (large response sets coming back).\\n","Decision":"We want to use the [Event package](https:\/\/github.com\/cloudfoundry\/bbs\/blob\/master\/doc\/events.md)\\nto get the event stream for each ActualLRP. We will also use a bulk sync every\\n60 seconds to catch any events that were missed.\\n","tokens":75,"id":53,"Predictions":"Decision (A) The diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnnnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds).nnThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a specified time interval (at the time of writing 10nseconds.nThe diego ActualLRP syncing model as currently implemented will fetch all LRPsnacross all diego cells at a"}
{"File Name":"copilot\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":54,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ELEN4010\/ADR Maps API choice.md","Context":"## Context\\nFor the project, we need an API for a map: interface, search, marker placement, satellite or road map imagery. There are several options for maps, will be a primary mode of interacting with the site.\\n* Google maps\\n* OpenLayers\\n* TomTom\\n* MapBox\\n* HERE\\n* Mapfit\\nMain factor are cost, and ease of use (documentation for the API)\\nGoogle maps are highly customizable in style and appearance, and configerable for marker placement, information windows, and interface\/controls.\\n","Decision":"Upon examining the options, Google Maps was considered the most mature, easy-to-use and well-supported option. The API has excellent documentation and example code. The interface will be familiar to the majority of site users.\\n","tokens":112,"id":55,"Predictions":"Decision Makers should consider the following options: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit: Mapfit"}
{"File Name":"ELEN4010\/ADR Testing.md","Context":"## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n","Decision":"The testing framework chosen for automated testing is Jest. This framework is used because:\\n- It has a simple installation and configuration process for Node.js\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\n","tokens":156,"id":56,"Predictions":"Decision (A). The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nC. The website should be redesigned.nD. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nA. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.nB. The website should be redesigned.n"}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"Predictions":"Decision - 121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212"}
{"File Name":"ELEN4010\/ADR Email invites.md","Context":"## Context\\nIn order to join a group, a potential member must be invited (it's not correct to add a person to a group without asking permission.) A person invited can either be a member of the website (have an account) or be a new user (no account registered yet). In order to cover both of these scenarios, and to avoid the website being a \"walled garden\" with a tiny set of users, and to encourage potential future growt, a mechanism to invite users could be an email, sent by an existing member, to any valid email address with an invitation to join. This could be in the form of a token with a payload, or more simply, an extra table in the DB, linking the invited person's email to a trip ID.\\n","Decision":"The mechanism of an external invitation with a specific link requires the ability to send an email (prefereably attractively  styled and clearly phrased, to avoid being rejected as unsolicited or junk email). The node module 'nodemailer' was selected as appropriate, for its wide support, mature development and ease of use, and 0 dependecies.\\n","tokens":158,"id":58,"Predictions":"Decision (A). The proposed mechanism would be a token with a payload.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ELEN4010\/ADR Reordering Destinations.md","Context":"## Context\\nA list of destinations should be reorderable, not fixed\\n","Decision":"A trip is made up of a list of destinations. This list should be able to be reordered, on the main site or the mobile version of the site. Draggable would be the best, but a button for moving an extry up and down will also work.\\n","tokens":15,"id":59,"Predictions":"Decision based on the information in the record.nA list of destinations should be reorderable, not fixednnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ELEN4010\/ADR Entering destinations for a trip.md","Context":"## Context\\nDestinations need to be entered into a trip somehow. The two most obvious choices seem to be by typing (some kind of auto-completion feature) or by clicking directly on a map, to set markers. These paradigms are the dominant ones in most existing APIs and site\/map websites.\\n","Decision":"We will aim to support both autocomplete AND clicking on the map. This would be the most convenient for users of the site.\\n","tokens":63,"id":60,"Predictions":"Decide whether to use typing or clicking to set markers.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"Predictions":"Decision (A) The Google Login API should be used as an alternative method for users to login.nbr>br>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.nbr>The Google Login API should be used as an alternative method for users to login.n"}
{"File Name":"ELEN4010\/ADR GitHub Project Board as KanBan.md","Context":"## Context\\nA SCRUM-based agile devlopment workflow would benefit from a central KANBAN board to keep track of userstories that have been written, are in progress, and are complete. This will help identify the sprint backlog, and the current focus of the sprint. Labels could be used to indicate size\/priority\/difficuly or value to the project, to help calculate the sprint velocity and determine what can get done inside a single sprint.\\n","Decision":"Using the GitHib Project page with a single project for the repo, and using Issues labelled as User Stories, with columns for \"To Do\", \"In progress\", and \"Completed\".  We can leverage some of the automatic rules in Git to help automate some of the completetion of tasks ties to Milestones for each sprint:\\nhttps:\/\/github.com\/witseie-elen4010\/2019-005-project\/projects\/1\\n","tokens":94,"id":62,"Predictions":"Decision Makers should use KANBAN to track userstories.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ELEN4010\/ADR Trunk-Based Development.md","Context":"## Context\\nTo perform Continual Integration and development, with weekly releases, it would be convenient and useful to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac\/front-end. It is difficult to automate these front-end interface tests, and there may be factors not present in a localhost\/express server that only become apparent in an online scanario.\\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\\n","Decision":"**master** will be the release branch\\n**development** will be the main development\/test branch. This will also be made into the \"default\" branch for all pull requests, to avoid accidentaly PR into master\\n**feature** branches must be made off development, with unique names. All pull requests for completed features to be made into \"development\".\\n* All PRs must be reviewed by at least two developers to merge into \"development\"\\n* All PRs must be reviewed by at the three other developers to merge into \"master\"\\n* All PRs must pass all tests (Jest, Travis, and Coveralls) in order to be considered valid for a merge\\n* Stale reviews will be automatically dismissed if a new commit is pushed to the same branch\\n* Accepted PRs for completed features (User Stories) should be deleted after sucessfully merging\\n","tokens":172,"id":63,"Predictions":"Decision (A) To have a testing branch as well. (B) To have a testing branch as well. (C) To have a testing branch as well. (D) To have a testing branch as well. (A) To have a testing branch as well. (A) To have a testing branch as well. (B) To have a testing branch as well. (C) To have a testing branch as well. (D) To have a testing branch as well. (A) To have a testing branch as well. (A) To have a testing branch as well. (B) To have a testing branch as well. (C) To have a testing branch as well. (D) To have a testing branch as well. (A) To have a testing branch as well. (A) To have a testing branch as well. (B) To have a testing branch as well. (C) To have a testing branch as well. (D) To have a testing branch as well. (A) To have a testing branch as well. (A) To have a testing branch as well. (B) To have a testing branch as well. (C) To have a testing branch as well. (D) To have a testing branch as well. (A) To have a testing branch as well. (A) To have a testing branch as well."}
{"File Name":"ELEN4010\/ADR Folder structure.md","Context":"## Context\\nHaving a fixed structure for a project has may advantages, limiting spread of files across multiple folders and contraining locations to known places. THere is an advantage is letting a folder strucute emerge oganically, but also a large risk, as things can break when low-level file locations change, necesitating logs of bug fixing and refactoring. Having a rigid initial structure canb lead to later restrictions, or imposed complexity.\\n","Decision":"The following folder strucure is adopted:\\n.\\n\u251c\u2500\u2500 app\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 controllers\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 public\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 css\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 img\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 js\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 views\\n\u251c\u2500\u2500 docs\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 adr\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 misc\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project_artifacts\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\\n\u251c\u2500\u2500 node_modules\\n\u251c\u2500\u2500 test\\n\u2514\u2500\u2500 local_only\\n**Update** Removed folders originally specified that were found to not be required during project development: 'log' and 'utility'\\n","tokens":90,"id":64,"Predictions":"Decision Makers should consider the following options: a fixed structure for a project with a fixed structure will be more efficient and efficient, and a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient. Having a fixed structure for a project with a fixed structure will be more efficient and efficient."}
{"File Name":"infection\/0002-@covers-annotations.md","Context":"### Context\\nPHPUnit offers a range of `@covers` annotations with the possible to enforce a strict mode or to\\nenforce them. The question is when should those annotations be enforced and\/or if we need to enable\\nanother settings as well?\\n### Decision\\nSince we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\Bridge\\PhpUnit\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n","Decision":"Since we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\Bridge\\PhpUnit\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n","tokens":361,"id":65,"Predictions":"We decide to leverage the [symfony\/phpunit-bridge][phpunit-bridge] in phpunit.xml.dist in order to avoid to require then@covers annotations whilst still benefit from it.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]nsetting general_phpdoc_annotation_remove has been refused.nFor this reason, the proposal to remove the n@covers annotations via the [PHP-CS-Fixer][php-cs-fixer]"}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"Predictions":"Decision : Keep the usage of self over self in the codebase.n### DecisionnSince there is no clear benefits of adopting self over self and given the context of its usage,nthe decision is to keep the usage of self over self in the codebase.n### StatusnAccepted ([#1061][1061])n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.htmln[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061n ## Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n### Decision Makers: n###"}
{"File Name":"infection\/0001-inheritdoc.md","Context":"### Context\\nUsing `@inheritdoc` was done inconsistently across the codebase so the decision of whether we use it\\nsystematically or remove it systematically had to be done.\\nA number of points:\\n- [PHPDoc][phpdoc-inheritance] provides inheritance of the docblocks by default when appropriate\\n- Static analysers such as PHPStan or Psalm can do without at the time of writing\\nAlso it has a very limited value.\\n### Decision\\nDo not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\\nbe removed when submitting pull requests.\\n### Status\\nAccepted ([#860][860])\\n[phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n[860]: https:\/\/github.com\/infection\/infection\/issues\/860\\n","Decision":"Do not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\\nbe removed when submitting pull requests.\\n### Status\\nAccepted ([#860][860])\\n[phpdoc-inheritance]: https:\/\/docs.phpdoc.org\/guides\/inheritance.html\\n[860]: https:\/\/github.com\/infection\/infection\/issues\/860\\n","tokens":175,"id":67,"Predictions":"Nonnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"Predictions":"Decision nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is necessary, the decision taken is to leverage the expectException*() API when possible.nAs recommended by [Sebastian Bergmann][sebastian-bergmann] inn[this article][phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit-exception-best-practices]n[phpunit"}
{"File Name":"dapr\/API-002-actor-api-design.md","Context":"## Context\\nGiven Dapr is going out with language specific Actor SDKs, we formally introduced an Actor API into Dapr to make Actors are first-class citizen in Dapr. The goal of this review was to ensure Dapr can provide strong support of Service Fabric stateful actors programming model so that we can offer a migration path to the majority of existing actor users.\\n","Decision":"### Dapr\\n* A separate Actor interface is defined.\\n* Actors should support multiple reminders and timers.\\n* Actor state access methods are encapsulated in the Actor interface itself.\\n* Actor interface shall support updating a group of key-value states in a single operation.\\n* Actor interface shall support deletion of an actor. If the actor is activated when the method is called, the in-flight transaction is allowed to complete, then the actor is deactivated, deleted, with associated state removed.\\n### Non-Dapr\\n* Transaction across multiple API calls is left for future versions, if proven necessary. Due to single-threaded guarantee, such transaction scope might be unnecessary. However, if developer expects an Actor code to behave atomically (in an implied transaction scope), we may have to implement this.\\n","tokens":75,"id":69,"Predictions":"Decision (A) The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr.nbr>The Actor API should be used in Dapr."}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"Predictions":"Decision based on the information in this document.nAnswer:A.B.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C."}
{"File Name":"dapr\/API-006-universal-namespace.md","Context":"## Context\\nFor cloud-edge hybrid scenarios and multie-region deployment scenarios, we need the ability to facilitate communications cross clusters. Specifically, it's desirable to have services scoped by cluster names so that a service in one cluster can address and invoke services on another trusted cluster through fully qualified names in a universal namespace, such as cluster1.serviceb.\\n","Decision":"We should consider adding universal namespace capabilities to Dapr.\\n","tokens":70,"id":71,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"Predictions":"Decision Authenticode signing of binaries.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"Predictions":"The minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java).nThe minimum Java version supported by Dapr's Java SDK should be Java 11 or Java 8 (or the latest version of Java)."}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"Predictions":"We decided to use Docker repositories to store our images.nWe chose Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe chose Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.nWe decided to use Docker repositories because they support multiple repositories along with versioning of images and different architectures.n"}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"Predictions":"Decision (A) The API change is needed to account for the requested type of operation.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/API-001-state-store-api-design.md","Context":"## Context\\nWe reviewed storage API design for completeness and consistency.\\n","Decision":"* All requests\/responses use a single parameter that represents the request\/response object. This allows us to extend\/update request\/response object without changing the API.\\n* Add Delete() method\\n* Support bulk operations: BulkDelete() and BulkSet(). All operations in the bulk are expected to be completed within a single transaction scope.\\n* Support a generic BulkOperation() method, which is carried out as a single transaction.\\n* Transaction across multiple API requests is postponed to future versions.\\n* Actor state operations are moved to a new Actor interface. Please see [API-002-actor-api-design](.\/API-002-actor-api-design.md).\\n","tokens":14,"id":76,"Predictions":"Decision Makers should consider the storage API design as a completeness and consistency.nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>n"}
{"File Name":"dapr\/API-012-content-type.md","Context":"## Context\\nNot adding content-type to state store, pubsub and bindings.\\n","Decision":"* We will not add content-type since it is a persisted metadata and it can cause problems such as:\\n* Long term support since metadata persisted previously would need to be supported indefinitely.\\n* Added requirement for components to implement, leading to potentially hacky implementations to persist metadata side-by-side with data.\\nOriginal issue and discussion: https:\/\/github.com\/dapr\/dapr\/issues\/2026\\n","tokens":17,"id":77,"Predictions":"Decision Makers should have added content-type to state store, pubsub and bindings.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"Predictions":"Decision i.e., we need to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.nbr>We need to decide if we want to couple CLI versions with runtime versions.n"}
{"File Name":"dapr\/API-010-appcallback-versioning.md","Context":"## Context\\nThere was a proposal to introducing versioning for HTTP App Callbacks. The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR\\n","Decision":"- Introducing versioning to app callback APIs would require changes to the user applications which is not feasible\\n- There would be no way for DAPR runtime to find out the app callback version before hand\\nWe decided not to introduce such a versioning scheme on the app callback APIs. Post v1.0, if required, the versioning could be implemented inside the payload but not on the API itself. A missing version in the payload could imply v1.0.\\n","tokens":47,"id":79,"Predictions":"Decision (A) The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/API-008-multi-state-store-api-design.md","Context":"## Context\\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\\nto support multi state store with no backward compatibility.\\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\\nv1.0\/state\/storeA\/\\nv1.0\/state\/storeB\/\\nEarlier this breaking change, the API is v1.0\/state\/`<key>`\\nWe have reviewed multi storage API design for completeness and consistency.\\n","Decision":"*  New state store API is v1.0\/state\/`<store-name>`\/\\n*  If user is using actors and like to persist the state then user must provide actorStateStore: true in the configuration yaml.\\nIf the attribute is not specified or multiple actor state stores are configured, Dapr runtime will log warning.\\nThe actor API to save the state will fail in both these scenarios where actorStore is not specified or multiple actor stores\\nare specified.\\n*  It is noted that after this breaking change, actor state store has to be specified unlike earlier where first state store is picked up by default.\\n* It is noted that this breaking change will also require a CLI change to generate the state store YAML for redis with actorStateStore.\\n* To provide multiple stores, user has to provide separate YAML for each store and giving unique name for the store.\\n* It is noted that the param's keyPrefix represents state key prefix, it's value included ${appid} is the microservice appid, ${name} is the CRDs component's unique name, ${none} is non key prefix and the custom key prefix\\nFor example, below are the 2 sample yaml files in which redis store is used as actor state store while mongodb store is not used as actor state store.\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore1  # Required. This is the unique name of the store.\\nspec:\\ntype: state.redis\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: actorStateStore  # Optional. default: false\\nvalue : true\\n```\\n```\\napiVersion: dapr.io\/v1alpha1\\nkind: Component\\nmetadata:\\nname: myStore2 # Required. This is the unique name of the store.\\nspec:\\ntype: state.mongodb\\nmetadata:\\n- name: keyPrefix\\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\\n- name: <KEY>\\nvalue: <VALUE>\\n- name: <KEY>\\nvalue: <VALUE>\\n```\\nSo with the above example, the state APIs will be : v1.0\/state\/myStore1\/`<key>`\\nand v1.0\/state\/myStore2\/`<key>`\\n","tokens":115,"id":80,"Predictions":"We decided to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in APInto support multi state store with no backward compatibility.nWith this change, the state API allows the app to target a specific state store by store-name, for example:nv1.0\/state\/storeA\/nv1.0\/state\/storeB\/nEarlier this breaking change, the API is v1.0\/state\/storeA\/nEarlier this breaking change, the API is v1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n12nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n2nThe state API is v1.0\/state\/storeA\/nv1.0\/state\/storeB\/nWe have reviewed multi storage API design for completeness and consistency.n Decision n"}
{"File Name":"dapr\/ENG-002-Dapr-Release.md","Context":"## Context\\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\\n","Decision":"### Integration build release\\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\\n### Official build release\\n#### Pre-release build\\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\\n**Pre-release process**\\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\\n```\\n$ git tag \"v0.1.0-alpha.0\" -m \"v0.1.0-alpha.0\"\\n$ git push --tags\\n```\\n3. CI creates new build and push the images with only version tag\\n4. Test and validate the functionalities with the specific version\\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\\n7. Repeat from 4 to 6 until all bugs are fixed\\n#### Release the stable version to users\\nOnce all bugs are fixed, we will create the release note under [.\/docs\/release_notes](https:\/\/github.com\/dapr\/dapr\/tree\/master\/docs\/release_notes) and run CI release manually in order to deliver the stable version to users.\\n### Release Patch version\\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\\n","tokens":27,"id":81,"Predictions":"Decision Object: To release new dapr binaries and the corresponding configurations without any blockers to users.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/API-005-state-store-behavior.md","Context":"## Context\\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\\n","Decision":"### Concurrency model\\n* Dapr supports two flavors of optimistic concurrency: first-write wins and last-write wins. First-write wins is implemented through ETag.\\n* User code can express concurrency intention with a *config* annotation attached to a request. See **Config annotation** for details.\\n* Future version of Dapr may support call throttling through application channel.\\n* We'll choose last-write wins as the default.\\n### Consistency model\\n* Dapr supports both eventual consistency and strong consistency.\\n* Actors always use strong consistency.\\n* We'll choose eventual consistency as default for services other than actors.\\n### Actor Transaction\\n* Dapr-compatible Actor state stores shall support ACID transaction.\\n* Dapr doesn't mandate specific transaction isolation level at this point. However, when deemed necessary, we can easily add those to **Config annotation** as needed.\\n### Config annotation\\n* User payload can contain an optional **config** annotation\/element that expresses various constraints and policies to be applied to the call, including:\\n* Concurrency model: first-write or last-write\\n* Consistency model: strong or eventual\\n* Retry policies:\\n* Interval\\n* Pattern: linear, expotential\\n* Circuit-breaker Timeout (before an open circuit-breaker is reset)\\n### State store configuration probe\\n* An Dapr-compatible state store shall provide an endpoint that answers to configuration probe and returns (among others):\\n* Supported concurrency model\\n* Supported consistency model\\n* A state store instance shall return the specific configuration of the current instance.\\n* It's considered out of scope to require state store to dynamically apply new configurations.\\n### Dapr\\n* Update state store API spec to reflect above decisions\\n* Create backlog of issues to implement above decisions\\n","tokens":62,"id":82,"Predictions":"We need to define component behaviors in the spec and make sure those are implemented in our implementation.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/ARC-003-grpc-protobuf-coding-convention.md","Context":"## Context\\nWe have defined gRPC services and protobuf messages without convention, which results in the duplicated protobuf definitions and inconsistent names of services and messages. Thus, this record defines the minimum-level coding convention for Protobuf message to improve the quality of grpc\/protobuf message definitions.\\n","Decision":"* Use `google.protobuf.Any` data field only if the message field conveys serialized protobuf message with type url. Otherwise, use the explicit data type or protobuf message.\\n* Use `Request` suffix for gRPC request message name and `Response` suffix for gRPC response message name\\n* Do not use `Client` and `Service` suffix for gRPC service name e.g. (x) DaprClient, DaprService\\n* Avoid the duplicated protobuf message definitions by defining the messages in shared proto\\n* Define and use enum type if field accepts only predefined values.\\n","tokens":56,"id":83,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"Predictions":"Decision i.e., we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.nbr>We need to decide if we need to introduce different manifest types.n"}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"Predictions":"Decision (A) To allow the self hosted configuration to be reinstalled.nbr>nbr>nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/ARC-004-http-server.md","Context":"## Context\\nGo community has the multiple http server implementations, such as go net\/http, fasthttp, gin, to serve HTTP Restful API. This decision records describes which http server implementation uses in Dapr.\\n","Decision":"* Use [fasthttp server](https:\/\/github.com\/valyala\/fasthttp) implementation because fasthttp offers [the best performance and lowest resource usages](https:\/\/github.com\/valyala\/fasthttp#http-server-performance-comparison-with-nethttp) for the existing HTTP 1.1 server\\n* Use [fasthttpadaptor](https:\/\/godoc.org\/github.com\/valyala\/fasthttp\/fasthttpadaptor) if you need to convert fasthttp request context to net\/http context.\\n","tokens":44,"id":86,"Predictions":"The following HTTP server implementations are used in Dapr: Go net\/http, fasthttp, gin, and go net\/http.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/ARC-001-refactor-for-modularity-and-testability.md","Context":"## Context\\nAs we keep building up Dapr features, it becomes apparent that we need to refactor the existing code base to reinforce component modularity. This will improve testability and maintainability in long run. And this refactor also lays the foundation of opening up extensible points (such as Bindings) to the community.\\n","Decision":"### Dapr\\n* Formally separate hosting and API implementations. Hosting provides communication protocols (HTTP\/gRPC) as different access heads to the same Dapr API implementation.\\n* Ensure consistency between gRPC and HTTP interface.\\n* Separate binding implementations to a separate repository.\\n* Use smart defaults for configurable parameters.\\n* Rename Dapr runtime binary from **dapr** to **daprd**.\\n### Non-Dapr\\n* We may consider allowing Dapr to dynamically load bindings during runtime. However, we are not going to implement this unless it's justified by customer asks.\\n* A unified configuration file that includes paths to individual configuration files.\\n* Provide a Discovery building block with hopefully pluggable discovery mechanisms (such as a custom DNS).\\n","tokens":66,"id":87,"Predictions":"We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the existing code base to reinforce component modularity.nbr>We need to refactor the code base to reinforce component modularity.n"}
{"File Name":"dapr\/SDK-001-releases.md","Context":"## Context\\nDapr exposes APIs for building blocks which can be invoked over http\/gRPC by the user code. Making raw http\/gRPC calls from user code works but it doesn't provide a good strongly typed experience for developers.\\n","Decision":"* Dapr provides language specific SDKs for developers for C#, Java, Javascript, Python, Go, Rust, C++. There may be others in the future\\n- For the current release, the SDKs are auto-generated from the Dapr proto specifications using gRPC tools.\\n- In future releases, we will work on creating and releasing strongly typed SDKs for the languages, which are wrappers on top of the auto-generated gRPC SDKs (e.g. C# SDK shipped for state management APIs with the 0.1.0 release.) This is the preferred approach. Creating purely handcrafted SDKs is discouraged.\\n* For Actors, language specific SDKs are written as Actor specific handcrafted code is preferred since this greatly simplifies the user experience. e.g. The C# Actor SDK shipped with the 0.1.0 release.\\n","tokens":47,"id":88,"Predictions":"Decision (A) The APIs for building blocks can be invoked over http\/gRPC by the user code.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/API-003-messaging-api-names.md","Context":"## Context\\nOur existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.\\n","Decision":"### Dapr\\n* All messaging APIs are grouped under a **messaging** namespace\/package.\\n* We define three distinct messaging interfaces:\\n- **direct**\\nOne-to-one messaging between two parties: a sender sending message to a recipient.\\n- **broadcast**\\nOne-to-many messaging: a sender sending message to a list of recipients.\\n- **pub-sub**\\nMessaging through pub-sub: a publisher publishing to a topic, to which subscribers subscribe.\\n* We distinguish message and direct invocation. For messaging, we guarantee at-least-once delivery. For direct invocation, we provide best-attempt delivery.\\n","tokens":30,"id":89,"Predictions":"Decision (A).nThe existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dapr\/API-011-state-store-api-parity.md","Context":"## Context\\nWe reviewed parity of state store APIs .\\n","Decision":"* GetState APIs continue to have Single Key Get and Bulk Get APIs behaviour as current 0.10.0 version.\\n* SaveState API will continue to have one SaveState API endpoint. If user wants to save single key, same save state API will be used\\nfor passing single item in the bulk set.\\nPotential issues arises if following new single key save state API is introduced:\\n`Post : state\/{storeName}\/{key}`\\nThis will conflict with\\n- State Transaction API, if the key is \"transaction\"\\n- GetBulkState API, if the key is \"bulk\"\\nSo the decision is to continue the Save State API behaviour as current 0.10.0 version.\\n* Bulk Delete API might come in future versions based on the scenarios.\\n","tokens":13,"id":90,"Predictions":"Decision Makers should consider the following: a. State store APIs should be rated as parity of state store APIs. b. State store APIs should be rated as parity of state store APIs. c. State store APIs should be rated as parity of state store APIs. d. State store APIs should be rated as parity of state store APIs. e. State store APIs should be rated as parity of state store APIs. f. State store APIs should be rated as parity of state store APIs. g. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs. h. State store APIs should be rated as parity of state store APIs."}
{"File Name":"dapr\/API-007-tracing-endpoint.md","Context":"## Context\\nWe now support distributed tracing across Dapr sidecars, and we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code. However, it's up to the user code to configure and implement proper tracing themselves.\\n","Decision":"We should consider adding a tracing endpoint that user code can call in to log traces and telemetries.\\n","tokens":53,"id":91,"Predictions":"Decision (A) The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.nbr>The tracing process should be based on the user code's knowledge of the tracing process.n"}
{"File Name":"nicoprj\/adr-001.md","Context":"## Context\\nWhile we were considering how to ship the CSSK, the question of how it\\nshould be organized came up several times. The crux of the issue was\\nwhether the more reusable parts (for example, the event dispatch bits)\\nshould be separated into a library, or whether it should all just live\\nin one big pile of code. Currently, the CSSK is organized in the\\nlatter manner.\\nSeparating the infrastructure code out into a separate library would\\nmake it more obvious which parts of the code are intended to be edited\\nby the developer. Right now, it\u2019s not immediately obvious where to\\nmake changes, and what bits are there to support the development\\nexperience.\\nSeparating the infrastructure code out into a separate library would\\nalso make it far more inconvenient to make changes to it. Developers\\nwould need to update the lib, possibly creating their own fork, and\\nthen ensure that the app code was using the appropriate version. This\\nis extremely likely to happen because the infrastructure code is\\nneither comprehensive nor mature.\\n","Decision":"We have decided to keep it all together, but clearly separate the\\n\u201clibrary\u201d code from the \u201capp\u201d code by using separate directories for\\nthose two types of code: \u201clib\u201d and \u201capp\u201d.\\n","tokens":219,"id":92,"Predictions":"The CSSK should be organized in a library.nSeparating the infrastructure code out into a separate library wouldnmake it more obvious which parts of the code are intended to be editednby the developer.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it. Developersnwould need to update the lib, possibly creating their own fork, andnthen ensure that the app code was using the appropriate version.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it.nSeparating the infrastructure code out into a separate library wouldnmake it far more inconvenient to make changes to it.n"}
{"File Name":"nucleus\/0002-push-data-changes-to-github.md","Context":"## Context\\nThe API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separate from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.\\n","Decision":"We've decided to go with pushing changes directly to GitHub via the GitHub API and using an async worker system to do it. The async system chosen was [Spinach][].\\n","tokens":149,"id":93,"Predictions":"The API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separately from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.n ## Decision Makers n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          "}
{"File Name":"nucleus\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":94,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"CopperBend\/D002_unify_command_flow_for_player_and_creatures.md","Context":"## Context\\nThe 'core mechanic' of this game is the player being in control of allied plants.  There may be other cases of player control of other in-game entities as dev continues, but this is core.\\nGiving creatures and the player character pluggable command\/control is more dev overhead.  As a spare time project, any choice for extra complexity deserves extra scrutiny.  On the same wave, though, the outcomes which can justify the extra effort are a larger set:  Not just important functionality, but significant learning and straight pleasure in craftsmanship can be enough reason.\\nThis offers options for reuse.\\nIf our guy is terrified, the player's InputCommandSource can be temporarily replaced with a FleeingCommandSource (or some such).  When the player is in charge of other entities, they get an InputCommandSource themselves, and will naturally be controlled by the player when their moments to act arrive in the schedule.\\n","Decision":"I'll do this.  It feels correct, and it trends toward the sort of wins and frustrations I'm hoping for in this project.  That is, where my decisions, rather than the guts of frameworks, are what I'm wrestling most often.\\n","tokens":191,"id":95,"Predictions":"Decision Makers should consider the following options: a. The player should be able to control the plant in the game. b. The player should be able to control the plant in the game. c. The player should be able to control the plant in the game. d. The player should be able to control the plant in the game. e. The player should be able to control the plant in the game. f. The player should be able to control the plant in the game. g. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game. h. The player should be able to control the plant in the game."}
{"File Name":"CopperBend\/D001_keep_ADRs_in_git_and_freely_reformat.md","Context":"## Context\\nI feel that ADRs in some form may be very beneficial at Envisage, but two things make me reluctant to push the practice right now.  First, ignorance--I've never done this, so I can't talk personally about the experience.  Second, the enthusiasm of first encounter.  Both of these are addressed by actually doing it for a while.\\nCopper Bend is a learning\/entertainment project.  Trying new techniques and technologies is a primary goal.\\nThe Envisage products are very different from this one.  What works well here won't necessarily cross over.\\n","Decision":"I will avoid tooling, and simply handcraft the documents.  I'll keep them in the main project repo.  I'll review periodically, trimming the present-but-useless and adding the missing-yet-promising.\\nI won't worry about synchronizing the format between ADR docs.\\n","tokens":125,"id":96,"Predictions":"Is Envisage a good place to practice ADRs?nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"Predictions":"The simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"react\/adr-XXX-file-structure.md","Context":"## Context\\nComponents might be more grokable if they were structured consistently. This ADR proposes conventions\\n","Decision":"TL;DR:\\n```\\nprimer-react\/\\n\u251c\u2500 src\/\\n\u2502  \u251c\u2500 Breadcrumbs\/\\n\u2502  \u2502  \u251c\u2500 index.ts                    \/\/ Just re-exporting?\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.tsx             \/\/ Primary component\\n\u2502  \u2502  \u251c\u2500 BreadcrumbsItem.tsx         \/\/ Subcomponent (include parent component name to increase findability in most IDEs)\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.mdx             \/\/ Documentation. Always .mdx, not .md\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.stories.tsx\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.test.tsx        \/\/ Unit tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.types.test.tsx  \/\/ Type tests\\n\u2502  \u2502  \u251c\u2500 Breadcrumbs.yml             \/\/ Component metadata (Possible future)\\n\u2502  \u2502  \u2514\u2500 __snapshots__\/\\n\u2506  \u2506\\n```\\n### Rules\\n- Every component should have its own PascalCased directory directly under `src\/`\\n- Subcomponents should be properties of the exported parent component (e.g., `Breadcrumbs.Item`)\\n- Replacements of existing components should use an incrementing number (e.g., `Breadcrumbs2` rather than `NewBreadcrumbs`)\\n","tokens":22,"id":98,"Predictions":"Decision Makers should consider the following: -Answer: A. -Answer: B. -Answer: C. -Answer: D. -Answer: B. -Answer: C. -Answer: D. -Answer: B. -Answer: C. -Answer: D. -Answer: B. -Answer: C. -Answer: D. -Answer: C. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D. -Answer: D."}
{"File Name":"react\/adr-003-prop-norms.md","Context":"## Context\\nOur component prop APIs have, at times been a bit of a mess. We've seen:\\n- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)\\n- Explicit plans to change some of those (e.g., the deprecation of Styled System props)\\n- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)\\nThis ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.\\n","Decision":"### \ud83d\udfe2 `sx`\\nAll components that ultimately render to the DOM should accept an `sx` prop.\\nThe `sx` prop (of type `SystemStyleObject`) should generally set styles for the root HTML element rendered by the component. An exception would be components like `<Dialog>`, whose outermost HTML element is a backdrop. In that case, it would be appropriate for `sx` styles to apply to child of the backdrop that is more likely to need styling overrides.\\n### \ud83d\udfe2 `ref`\\nAll components that ultimately render to the DOM should accept a `ref` prop. That `ref` prop should most often be passed to the root HTMLElement rendered by the component, although occasionally a different descendent node may make more sense.\\nSee also: [Discussion on `ref` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/131)\\n### \ud83d\udfe1 `as`\\nOnly components with a clear need for polymorphism should accept an `as` prop. Reasonable cases include:\\n- Components that need functionality from the component passed to the `as` prop, like a `<Button>` that renders a React Router link.\\n- Components whose accessibility are improved by using semantically appropriate HTML elements, like an ActionList\\nWhen a Primer component user passes an `as` prop to a component, it should be done in a way that is consistent with the component\u2019s intended use. In some situations we can enforce that with a narrowed type for our `as` prop.\\nSee also: [Discussion on `as` props (internal)](https:\/\/github.com\/github\/primer\/discussions\/130)\\n### \ud83d\udfe1 DOM props: Limited\\nAll components that accept an `as` prop should accept props en masse for the element specified by the `as` prop (excluding props of the same name already used by the component). _Additionally_, some other elements that do _not_ accept an `as` prop should accept the props for their root HTML element when those props are fundamental to the component\u2019s function (e.g., `<TextInput>` should accept DOM props for its underlying `<input>`).\\n### \ud83d\udd34 Styled System props\\nComponents should not accept Styled System props (except our utility components: `Box` and `Text`)\\n_Reasoning:_ Utility components are meant to provide a convenient API for writing styles (including styles that reference theme and other context managed within Primer). Non-utility components implement specific design patterns where additional styling is available for exceptional cases.\\nSee also: [Discussion on the deprecation of styled-system props (internal)](https:\/\/github.com\/github\/primer\/discussions\/132)\\n### \ud83d\udd34 `theme`\\nComponents should not accept a `theme` prop (with the exception of `ThemeProvider`).\\n_Reasoning:_ The `theme` prop doesn't enable anything that can't be done with `<ThemeProvider>`, and promotes the anti-pattern of per-component theme overrides.\\n### `children`\\nI'm intentionally withholding advocacy about `children` prop types because I expect that topic will be covered by a future ADR.\\n### Sequencing\\n1. Deprecate remaining unwanted Styled System props (should be done? Let's verify.)\\n1. Release an eslint rule to disallow Styled System props\\n1. Release an eslint rule to disallow `theme`\\n1. Migrate all usage within PRC\\n1. Assist GitHub projects with migration\\n1. Remove support for unwanted props\\n1. Update docs to reflect the standards in this ADR\\n","tokens":120,"id":100,"Predictions":"Decision Makers should use the ADR to identify the best way to implement prop APIs.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"react\/adr-001-typescript.md","Context":"## Context\\nPrimer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https:\/\/github.com\/primer\/react\/commit\/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https:\/\/www.geeksforgeeks.org\/typescript-ambients-declaration\/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.\\n`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https:\/\/github.com\/primer\/react\/issues\/906) [TypeScript](https:\/\/github.com\/primer\/react\/issues\/540) [bug](https:\/\/github.com\/primer\/react\/issues\/520) [reports](https:\/\/github.com\/primer\/react\/issues\/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.\\n","Decision":"We will rewrite Primer React components in TypeScript.\\n","tokens":284,"id":101,"Predictions":"We'll continue to maintain the type definitions.nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nb"}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"Predictions":"We need to define unique field names for each field which needs to be analyzed under a specific locale.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kitsune\/0003-es-aaq-documents.md","Context":"## Context\\nAs we are re-implementing our search in ElasticSearch v7,\\nwe must re-implement Ask a Question (AAQ) search.\\nThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\\nwhich we must continue to be able to do in the redesigned Search:\\nsearching for an AAQ thread as a unit.\\nThere are other secondary use-cases which we may want to support when storing AAQ documents in ES.\\nA non-exhaustive list of these are:\\n-   Searching within AAQ threads\\n-   Searching within questions and their solutions\\n-   Aggregating answers to create contribution data\\nWe also want search to be _fast_,\\nso should model our data to avoid nested fields and parent-child relationships,\\nand use de-normalization wherever possible:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/tune-for-search-speed.html#_document_modeling\\n","Decision":"We will model our data in ES based on what makes most sense for our expected use-cases,\\nand what will make those fast and efficient,\\nrather than feeling like we must have a 1:1 copy of our data structure in our database.\\nIn this vein, we will use a structure of two document \"types\" within one index,\\n`QuestionDocument` and `AnswerDocument`,\\nwhere a `QuestionDocument` will exist for each `Question` which exists in the database,\\nand an `AnswerDocument` will exist for each `Answer` which exists in the database.\\n`AnswerDocument` will be a subclass of `QuestionDocument` so will inherit all of its fields,\\nand we will set the value of those fields to the value of the `Question` associated with its `Answer`.\\nFor instance, if in database:\\n```\\nanswer.created => 2020-10-27\\nanswer.question.created => 2020-11-01\\n```\\nin elastic:\\n```\\nanswer_document.created => 2020-10-27\\nanswer_document.question_created => 2020-11-01\\n```\\n`QuestionDocument` will also have an `answer_content` field,\\nwhich contains the content of all a Question's Answers.\\nWe will set this to null in the `AnswerDocument`.\\n","tokens":205,"id":103,"Predictions":"Decision based on the information in this document.nAnswer:A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.B.C.A.C.A.B.C.A.C.A.B.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C.A.C."}
{"File Name":"kitsune\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":104,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Nosedive\/0006-use-of-migratus-library.md","Context":"## Context\\nDirect form the [jdbc documentation](http:\/\/clojure-doc.org\/articles\/ecosystem\/java_jdbc\/home.html)\\nAnother common need with SQL is for database migration libraries. Some of the more popular options are:\\n* Drift\\n* Migratus\\n* Ragtime\\n","Decision":"After a quick read of the documentation, and see that all solutions are similar, I select Migratus, by the comodity of have a lein plugin\\n","tokens":60,"id":105,"Predictions":"Decision Makers can use the following libraries:n* Driftn* Migratusn* Ragtimen* Driftn* Migratusn* Ragtimen* Driftn* Driftn* Migratusn* Ragtimen* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn* Driftn"}
{"File Name":"Nosedive\/0004-start-with-console-app.md","Context":"## Context\\nTe console appication it is the more siple app that come to my mind.\\n","Decision":"Creates a console app and send the data via parameters\\n","tokens":21,"id":106,"Predictions":"Decision Makers would like to have a console.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Nosedive\/0009-use-mount-and-clojure-tools-namespace.md","Context":"## Context\\nIn clojure a normal work flow is use the repl. The problem is that when you reload the appliction the states die.\\n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.\\nDepending on how application state is managed during development, the above three superpowers can either stay, go somewhat, or go completely.\\n","Decision":"Use mount libray and clojure tools space.\\nThe decision of mount over component is made afer review bouth solutions. My feeling is\\n* Mount is more clojure dialect oriented\\n* Mount use namespace and component\\nRecords, this made that the compliler control the dependencies\\n* Mount is [less contagious](https:\/\/engineering.riotgames.com\/news\/taxonomy-tech-debt)\\n","tokens":100,"id":107,"Predictions":"Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Depending on how application state is managed during development, the above three superpowers can either stay, go somewhat, or go completely.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.n Decision : The problem is that when you reload the appliction the states die.n"}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"Predictions":"Decision Makers should consider the following: nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe architecture of the cloud is a complex process, and the architecture of the cloud is a complex process.nThe decision maker should consider the following:"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"Predictions":"Decision Object: [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows versionnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Nosedive\/0002-use-of-clojure.md","Context":"## Context\\nAs developer, I have a good understanding of clujure language, but not the ecosystem. How persist in db, create api's, secure them, etc.\\n","Decision":"This will be a pet project, the goal is learning clojure ecosystem, not the aplication itself.\\n","tokens":37,"id":110,"Predictions":"Is this the right decision?nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Nosedive\/0007-uso-of-tools-cli.md","Context":"## Context\\nWe need working with command line arguments.\\n","Decision":"Use of [tools.cli](https:\/\/github.com\/clojure\/tools.cli)\\n","tokens":12,"id":111,"Predictions":"Decision Makers should consider working with command line arguments.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Nosedive\/0008-migrating-by-code.md","Context":"## Context\\nWe are testing how deploy in diferente databases per envirtoment, and don't find the way of do it using the pluggin\\n","Decision":"Develop the migration part in the code\\n","tokens":31,"id":112,"Predictions":"We will not deploy the pluggin.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database.nWe will deploy the pluggin using the same database."}
{"File Name":"Nosedive\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":113,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"Predictions":"I'd like to change the default settings.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"Predictions":"Decision (A) To allow zsh to be the default shell on MacOS Catalina.n (B) To allow zsh to be the default shell on MacOS Catalina.n (C) To allow zsh to be the default shell on MacOS Catalina.n (D) To allow zsh to be the default shell on MacOS Catalina.n (A) To allow zsh to be the default shell on MacOS Catalina.n (A) To allow zsh to be the default shell on MacOS Catalina.n (B) To allow zsh to be the default shell on MacOS Catalina.n (C) To allow zsh to be the default shell on MacOS Catalina.n (D) To allow zsh to be the default shell on MacOS Catalina.n (A) To allow zsh to be the default shell on MacOS Catalina.n (A) To allow zsh to be the default shell on MacOS Catalina.n (B) To allow zsh to be the default shell on MacOS Catalina.n (C) To allow zsh to be the default shell on MacOS Catalina.n (D) To allow zsh to be the default shell on MacOS Catalina.n"}
{"File Name":"dotfiles\/0005-use-coc-in-place-of-ycm.md","Context":"## Context\\nYCM has been a mostly positive experience over the years of using it, however,\\nthere have been many occasions where time has been spent fixing issues. The\\ntime has come to look at using an alternative.\\n","Decision":"The decision is to use [CoC](https:\/\/github.com\/neoclide\/coc.nvim).\\n","tokens":48,"id":116,"Predictions":"The alternative would be a better choice.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"Predictions":"GitHub Actions are a good way to introduce GitHub to the community.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dpul\/0003-synchronization-via-rabbitmq.md","Context":"## Context\\nWe want Pomegranate to be a separate application from Figgy, but need some way\\nfor Figgy to tell Pomegranate about new resources so that when something is\\nmarked Complete in Figgy or taken down that it's reflected in Pomegranate.\\n","Decision":"Figgy will send create\/update\/delete messages to a fanout RabbitMQ Exchange.\\nPomegranate will register a durable queue which listens to that exchange and\\nprocess messages using [Sneakers](https:\/\/github.com\/jondot\/sneakers).\\nThe message will contain the following information:\\n* Collection slugs the object is a member of\\n* Manifest URL of the object\\n* change event (create \/ update \/ delete)\\n","tokens":60,"id":119,"Predictions":"Decision Makers will consider the following options: a). The Pomegranate application will be a separate application from Figgy. b). The Pomegranate application will be a separate application from Figgy. c). The Pomegranate application will be a separate application from Figgy. d). The Pomegranate application will be a separate application from Figgy. e. The Pomegranate application will be a separate application from Figgy. f. The Pomegranate application will be a separate application from Figgy. g. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h. The Pomegranate application will be a separate application from Figgy. h."}
{"File Name":"dpul\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described at https:\/\/adr.github.io\/\\n","tokens":16,"id":120,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dpul\/0002-build-on-spotlight.md","Context":"## Context\\nPUDL has [collection landing\\npages](http:\/\/pudl.princeton.edu\/collections\/pudl0058) because it's a digital collections site which\\nstaff pushed content into. PUDL required all items be in a single collection,\\nand couldn't provide good search-across. Figgy is a staff back-end, so we need some sort of\\napplication to provide that functionality.\\nCurators also had a history of either requesting or creating ad-hoc websites to showcase\\ntheir material or accompany on-site exhibits. We wanted instead to provide a CMS for\\nthem to create those experiences based on material they curate which wouldn't\\ncreate metadata silos and increased maintenance.\\nFurther, curators often had different use cases about how metadata should\\ndisplay in different contexts. For example, items with the same title in the catalog may need to be differentiated in an exhibit. It was important that certain fields display a certain\\nway, but be cataloged according to best practices.\\nSpotlight is an exhibit building platform that provides controlled CMS\\nfunctionality and is built on Blacklight similar to our new catalog. It allows\\nfor local overrides of fields.\\n","Decision":"We will use Spotlight to fulfill both the requirements of a Collection Landing\\nPage as well as exhibits. Curators of collections will generate the collection\\npages. Staff who wish to generate exhibits will be able to manage membership in\\nFiggy but have the tools to create those exhibits in Pomegranate.\\n","tokens":244,"id":121,"Predictions":"We decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits.nWe decided to use Spotlight to catalog the exhibits."}
{"File Name":"dpul\/0007-indexing-visibility.md","Context":"## Context\\nFiggy resources may have any of the following visibilities:\\n- Open (public)\\n- Princeton (netid)\\n- On Campus (ip)\\n- Reading Room\\n- Private\\nFor each of these we need a policy regarding whether it will be indexed in DPUL.\\nWe used to index only public \/ complete items. But to support the music reserves\\ncollection we need pages that would have a viewer for logged-in institutional\\nusers only.\\n","Decision":"Resources with the following visibilities should index into DPUL:\\n- Open (public)\\n- Princeton (netid)\\n- On Campus (ip)\\nThis is implemented with a token authentication mechanism in `iiif_resource#def\\nurl`\\n","tokens":98,"id":122,"Predictions":"We need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.nWe need to make sure that the library's resources are indexed in DPUL.n"}
{"File Name":"dpul\/0006-exhibits-are-collections.md","Context":"## Context\\nSpotlight uses the term \"Exhibits\" because its primary use case is generating\\nmultiple exhibit sites. However, as per\\n[ADR #2](.\/0002-build-on-spotlight.md) we want to use it as landing pages for\\ncollections.\\nThe term \"Collection\" was used everywhere else, including PUDL, so we decided to\\nuse that instead. Originally reported in\\n[#89](https:\/\/github.com\/pulibrary\/pomegranate\/issues\/89).\\n","Decision":"* Use \"Collection\" in the UI everywhere \"Exhibit\" is mentioned in Spotlight.\\n","tokens":104,"id":123,"Predictions":"We decided to use the term \"Collection\" instead of \"Exhibits\" because it was used everywhere else.n Decision n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12n12"}
{"File Name":"dpul\/0005-resource-per-collection.md","Context":"## Context\\nWe needed to be able to display a resource in more than one collection, because\\nin Figgy a resource can be a member of multiple collections.\\nAt the time of this decision, one IIIFResource could only be a member of one Exhibit.\\n","Decision":"We create one IIIFResource per Collection of which it is a member.\\nWe map each IIIFResource to one SolrDocument.\\n","tokens":55,"id":124,"Predictions":"Decision i.e., the resource could be a member of more than one collection.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"dpul\/0004-index-from-iiif-manifests.md","Context":"## Context\\nObjects in Pomegranate need to get their metadata from Figgy, where they are\\nadministered. Spotlight ships with a IIIF-based indexer. Figgy already produces\\nIIIF manifests to support viewing the objects. However the metadata bucket\\ndoesn't contain rich enough metadata for pomegranate use cases.\\nFiggy (plum, at the time) didn't have an API at the time this decision was made. Manifests were the\\nonly way to get data out. Today Figgy has a graphql API.\\n","Decision":"We will use the IIIF Manifests to pull data from Figgy into Pomegranate. The\\nManifest gives us the manifest url (used for presenting a viewer), the thumbnail\\niiif image url, and the jsonld metadata location (via seeAlso).\\n","tokens":115,"id":125,"Predictions":"Decision (A) The metadata bucket should be expanded to support pomegranate use cases.nB) The metadata bucket should be expanded to support pomegranate use cases.nC) The metadata bucket should be expanded to support pomegranate use cases.nD) The metadata bucket should be expanded to support pomegranate use cases.nAnswer:A.nB.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.n.nB.n.nB.n.nB.n.nB.n.nnB.n.nnB.n.nnB.n.nnB.n.nnB.n.nnB.n.n.nnB.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n"}
{"File Name":"kosmos\/0001-app-db.md","Context":"# Context\\nI'm currently working on migrating from redux store to re-frame. It's a good point to define data\\nstructure and API to access it.\\n# Decision\\nApplication DB be a hashmap with the following keys:\\n- `nodes` to store hashmap where the key is the node's id and value is a node.\\n- `root` to store a root node id\\n- `loc` to store focused node id\\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\\nApplication should define the following event handlers to modify nodes:\\n- `[:nodes\/make node]` create a node at given loc.\\n- `[:nodes\/append-child child]` adds a child as a bottom child.\\n- `[:nodes\/insert-child child]` adds a child as a top child.\\n- `[:nodes\/insert-up child]` adds a sibling to up of loc.\\n- `[:nodes\/insert-down child]` adds a sibling to down of loc.\\n- `[:nodes\/remove]` removes node at loc; lot will be moved to the preceding node.\\n- `[:nodes\/replace node]` replaces node at loc.\\nFollowing event handler to modify loc:\\n- `[:loc\/root]` moves loc to root.\\n- `[:loc\/down]` moves loc down.\\n- `[:loc\/up]` moves loc up.\\n- `[:loc\/bottom]` moves loc to bottom.\\n- `[:loc\/top]` moves loc to top.\\n- `[:loc\/right]` moves loc right.\\n- `[:loc\/left]` moves loc left.\\nFollowing event handler to modify root:\\n- `[:root\/change root]` change root to a new value.\\n","Decision":"Application DB be a hashmap with the following keys:\\n- `nodes` to store hashmap where the key is the node's id and value is a node.\\n- `root` to store a root node id\\n- `loc` to store focused node id\\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\\nApplication should define the following event handlers to modify nodes:\\n- `[:nodes\/make node]` create a node at given loc.\\n- `[:nodes\/append-child child]` adds a child as a bottom child.\\n- `[:nodes\/insert-child child]` adds a child as a top child.\\n- `[:nodes\/insert-up child]` adds a sibling to up of loc.\\n- `[:nodes\/insert-down child]` adds a sibling to down of loc.\\n- `[:nodes\/remove]` removes node at loc; lot will be moved to the preceding node.\\n- `[:nodes\/replace node]` replaces node at loc.\\nFollowing event handler to modify loc:\\n- `[:loc\/root]` moves loc to root.\\n- `[:loc\/down]` moves loc down.\\n- `[:loc\/up]` moves loc up.\\n- `[:loc\/bottom]` moves loc to bottom.\\n- `[:loc\/top]` moves loc to top.\\n- `[:loc\/right]` moves loc right.\\n- `[:loc\/left]` moves loc left.\\nFollowing event handler to modify root:\\n- `[:root\/change root]` change root to a new value.\\n","tokens":379,"id":126,"Predictions":"[:root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] [root] "}
{"File Name":"dddsample-pelargir\/0002-isolate-components-into-submodules.md","Context":"## Context\\nI want to be able to mix and match different implementations\\nof the ddd sample application, without needing to pull in\\na large collection of unnecessary dependencies.\\nI want all of the code to be together in one place; which\\nis to say, I want to treat the entire project as a mono-repo.\\nI can't be bothered to maven install\/maven deploy each\\nlittle piece to propagate the necessary changes between\\nisolated libraries.\\n","Decision":"Use a maven reactor project to track the dependencies between\\ndifferent libraries within the project\\n","tokens":97,"id":127,"Predictions":"Decision (A).nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.nIt is important that the project be able to mix and match different implementationsnof the ddd sample application, without needing to pull in a large collection of unnecessary dependencies.n"}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"Predictions":"Decision (A).nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.nIt is not possible to install the citerus library into mynrepositories all the time.n"}
{"File Name":"dddsample-pelargir\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":129,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"accessibility-monitoring\/ADR005-database-public-sector-websites.md","Context":"## Context\\nWe currently have a \"public sector domains\" table (along with an \"organisations\" table containing the owners of the domains).\\nHowever, many organisations have websites that are not on their own domain but in a subfolder of another domain. Moreover, many _services_ reside in subfolders or other logical locations.\\nEach of these needs to be treated as a separate \"website\" in the context of accessibility monitoring.\\nThus, we either change the \"public sector domains\" table to include all websites, not just unique domains, OR we create a new table specifically for public sector websites and initially copy over those domains that we can confirm are definitely websites.\\nThe domains table is useful in and of itself as a repository of registered domains. We should keep and maintain this as-is.\\n","Decision":"We will create and populate a new table in the PubSecDomains schema that contains:\\n* url\\n* name of the service\\n* the site's title from its HTML <head><title> element, where given\\n* the site's description from its <head><meta name=\"description\"> element, where given\\n* last_updated timestamp\\n* the website's sector (foreign key to sectors table)\\n* many-to-many join to the existing Organisations table (one site can, surprisingly, come under the auspices of more than one organisation; obviously one organisation can have multiple websites for their various services)\\n","tokens":162,"id":130,"Predictions":"We should keep the \"public sector domains\" table as-is.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"accessibility-monitoring\/ADR001-record-architectural-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":131,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"accessibility-monitoring\/ADR003-tool-choices.md","Context":"## Context\\nThe choice of tools that will be built and used by the accessibility monitoring team are influenced by:\\n* Suitability to the task\\n* Cost and ongoing-cost (open-source preference)\\n* The [GDS Way](https:\/\/gds-way.cloudapps.digital\/).\\nAs defined in ADR002, we need to meet a number of requirements:\\n1. Maintaining an organised list of public sector websites\\n1. Picking websites to test from that list\\n1. Triaging each site\\n1. Prioritising sites for testing\\n1. Tracking the progress of testing on each website\\n1. Creating a report for the completed test\\n1. Sending the report to the site's owner\\n1. Managing and recording interaction with the site's owner\\n","Decision":"We will...\\n### Use Zendesk\\nRationale:\\n* GDS have a license for Zendesk.\\n* It has an extensive, well-documented API.\\n* There is a lot of experience in GDS of general usage, and a fair amount in using the API.\\n* We have a sandbox Zendesk environment.\\nThis will be the driver of testing work. Tickets will be created in Zendesk (manually or automatically) representing websites to test.\\nThey be prioritised in Zendesk and then be assigned to \/ picked up by Accessibility Officers.\\nZendesk will also handle communication and follow-up with the site owner.\\nThis satisfies items 5, 7 and 8 above.\\n### Use Postgres\\n* A relational database is best suited to the requirements for both a public sector domains database and the testing records\\n* It is open-source\\n* It is well supported and documented\\n* It is available as a \"plug-and-play\" service on [GOV.UK PaaS](https:\/\/www.cloud.service.gov.uk\/) (see below)\\nThis satisfies item 1 and facilitates items 2, 5 and 6.\\n### Use GOV.UK Platform-as-a-Service\\n* Meets GDS' [cloud-first policy](https:\/\/www.gov.uk\/guidance\/government-cloud-first-policy)\\n* VERY well supported with an extremely well-experienced and skilled team within GDS.\\n* Supports all of the chosen technologies\\n### Use Deque Axe\\n[Deque Axe](https:\/\/github.com\/dequelabs\/axe-core) is a 3rd-party, open-source tool that tests the web page that you give it against a set of predefined rules.\\n* It is open-source\\n* It can be run from the command-line and so can probably be automated.\\n* It produces results in a machine-readable format (JSON)\\nThis satisfies item 3.\\n### Use the GOV.UK Design System for the front-end\\nThe [GOV.UK Design System](https:\/\/design-system.service.gov.uk\/) is a set of components, styles and patterns created by the GOV.UK team after extensive user-research.\\nNot only does this serve as a ready-made template engine, but we know that it scores extremely highly on accessibility.\\n### Programming languages\\n* Use node.js for Axe-integration as it is well suited to asynchronous http calls.\\n* Use node.js (and nunjucks)for front-end code as that's what GOV.UK Design System is written in.\\n* Use Python 3 for Zendesk integration as there is a well-maintained open-source Python library.\\n### (Under review) Use PowerMapper SortSite\\n[SortSite](https:\/\/www.powermapper.com\/products\/sortsite\/) by PowerMapper is a 3rd-party commercial product that is widely used in the testing of websites, and has a thorough accessibility checking feature. It also crawls a website and catalogues its pages, a function that Axe does not perform and would be useful to us.\\nHowever, it is not open-source and only runs in a Windows or MacOS environment, so would not be easy to integrate into an automated process.\\nThere is a web service whereby a domain is submitted to it and it returns an HTML report, but this is not a machine-friendly format and would require screen-scraping and parsing - quite a major task, and one that's prone to breaking if\/when SortSite changes its output format.\\nThe site-mapping functionality of SortSite could be achieved with an existing open-source python or node library.\\n","tokens":159,"id":132,"Predictions":"Decision 1). The accessibility monitoring team will use the [GDS Way] to build and use the tools. 2). The accessibility monitoring team will use the [GDS Way] to build and use the tools. 3) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 4) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 5) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 6) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 7) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 8) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 9) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 10) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 10) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 10) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 10) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 10) The accessibility monitoring team will use the [GDS Way] to build and use the tools. 10) The accessibility monitoring team will use the [GDS Way] to build and use the tools."}
{"File Name":"accessibility-monitoring\/ADR002-build-some-things.md","Context":"## Context\\nThe accessibility monitoring team require a means of:\\n* Picking websites to test (from a random list weighted by a variety of criteria, from complaints passed to us, from previously-tested sites (to test changes) and sites specifically chosen by guiding bodies)\\n* Collecting and maintaining a list of public sector websites from which to pick\\n* Triaging each site (check it is live, check for accessibility statement(s), measure its size in pages etc)\\n* Prioritising sites for testing\\n* Tracking the progress of testing on each website\\n* Creating a report for the completed test\\n* Sending the report to the site's owner\\n* Managing and recording interaction with the site's owner\\nWhilst there are a number of accessibility testing solutions available, they are all geared towards testing of one's own site, and require (or at least, assume) access to the source code and deployment system. Testing of multiple sites by a 3rd party from outside the sites' domains is not a common requirement for vendors!\\nSimilarly, whilst the reports generated by some of the testing tools are very good, they are not up to GDS standards for content and, ironically, accessibility. Furthermore, GDS have no control over the content of the generated reports.\\nWe therefore need a combination of database administration, semi-automatic accessibility testing and a Customer Relationship \/ Support Ticket Management system, all able to move data between them in a usable fashion.\\n","Decision":"We will use 3rd-party tools where they provide part of the functionality that is required and can be made to communicate easily with each other.\\nWe will write whatever code is necessary to form the \"glue\" that binds them together and to automate whatever can be (and is worth) automating.\\n","tokens":294,"id":134,"Predictions":"Decision (A). The accessibility monitoring team will focus on the following: a). The accessibility monitoring team will focus on the following: b). The accessibility monitoring team will focus on the following: c). The accessibility monitoring team will focus on the following: d). The accessibility monitoring team will focus on the following: e). The accessibility monitoring team will focus on the following: f). The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: h). The accessibility monitoring team will focus on the following: iii. The accessibility monitoring team will focus on the following: b). The accessibility monitoring team will focus on the following: c). The accessibility monitoring team will focus on the following: d). The accessibility monitoring team will focus on the following: e). The accessibility monitoring team will focus on the following: f). The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: iii. The accessibility monitoring team will focus on the following: b). The accessibility monitoring team will focus on the following: c). The accessibility monitoring team will focus on the following: d). The accessibility monitoring team will focus on the following: e). The accessibility monitoring team will focus on the following: f). The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: iii. The accessibility monitoring team will focus on the following: b). The accessibility monitoring team will focus on the following: c). The accessibility monitoring team will focus on the following: d). The accessibility monitoring team will focus on the following: e). The accessibility monitoring team will focus on the following: f). The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: b). The accessibility monitoring team will focus on the following: c). The accessibility monitoring team will focus on the following: d). The accessibility monitoring team will focus on the following: e). The accessibility monitoring team will focus on the following: f). The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: g). The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on the following: iv. The accessibility monitoring team will focus on"}
{"File Name":"documents\/0002-move-to-clang-format-5.md","Context":"## Context\\nCurrent version of clang-format is aging an becoming unavailable.\\n","Decision":"We will move to clang-format 5 and not the bleeding edge version. Visual Studio 2017 will ship with clang-format 5 so it makes sense to standardize\\non this version across the project.\\n","tokens":15,"id":135,"Predictions":"The current version of clang-format is aging and becoming unavailable.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"documents\/0003-move-to-visual-studio-2017.md","Context":"## Context\\nVisual Studio 2017 has been released for roughly a year and includes improvements to C++ standards conformance. It is C++14\\nfeature complete and has many C++17 features.\\n","Decision":"As part of the maintenance cycle after release 3.13 we will move the Windows compilers forward to\\nVisual Studio 2017 in a step towards requiring C++14 as a minimum standard.\\n","tokens":42,"id":136,"Predictions":"It is likely that Visual Studio 2017 will be released later this year.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"documents\/0004-move-to-devtoolset-7-on-rhel7.md","Context":"## Context\\nRed Hat 7's default compiler is gcc 4.8. This is not C++14 compliant and prevents access to more modern C++ features.\\n","Decision":"As part of the maintenance cycle after release 3.13 we will move Red Hat 7 compilers over to use\\ngcc 7 as part of the [devtoolset-7](https:\/\/www.softwarecollections.org\/en\/scls\/rhscl\/devtoolset-7\/) tools provided by Red Hat.\\n","tokens":35,"id":137,"Predictions":"The compiler is not compatible with Red Hat 7n.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the future architectural decisions made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"Predictions":"Decision was made to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"Predictions":"Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and -r that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others. ## Decision ## Scott will add a parameter called --require and"}
{"File Name":"openmrs-java-client\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":140,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"lcarsde\/use-of-colors.md","Context":"## Context and Problem Statement\\nWhat colors shall be used for buttons, statistics and inactive elements considering usability. This concerns first and\\nforemost the tool applications.\\n","Decision":"Buttons or any usable elements should use the colors #ff9900, #9999ff, #cc6666 and #cc6699 as background and black text.\\nDisplayed text should preferably be colored #ff9900 on black background. Other text colors may be used after consideration.\\nGraphs, diagrams and other status and statistical displays should use the colors #ffcc99, #9999cc, #9999ff and for warning purposes #ff9966, #ff9900 and #cc6666.\\nDesign elements should use the colors #cc99cc, #9999cc and #ff9966. These colors are less \"strong\" and don't take as much attention.\\n[1]: http:\/\/mrbsdomain.com\/gallery\/var\/albums\/repository\/lcars_colors.gif\\n","tokens":33,"id":141,"Predictions":"Decision (A) The color scheme should be consistent with the tool applications.nB) The color scheme should be consistent with the tool applications.nC) The color scheme should be consistent with the tool applications.nD) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nB) The color scheme should be consistent with the tool applications.nC) The color scheme should be consistent with the tool applications.nD) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications.nAnswer (A) The color scheme should be consistent with the tool applications."}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"Predictions":"Decision (A).nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.n Decision (A).nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.n Decision (A).nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.n Decision (A).nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.n"}
{"File Name":"lcarsde\/language-for-wm.md","Context":"## Context and Problem Statement\\nWhat language shall be used for the development of the window manager?\\n## Decision Drivers <!-- optional -->\\n* I want to learn Kotlin\\n* I want to learn Python\\n* I don't want to take care of the need to update an API layer between the window manager code and the\\n* Libraries for display management and tooling in that area is usually written in C\\n","Decision":"* I want to learn Kotlin\\n* I want to learn Python\\n* I don't want to take care of the need to update an API layer between the window manager code and the\\n* Libraries for display management and tooling in that area is usually written in C\\nChosen option: \"Kotlin\/Native\", because it compiles to fast native code and C-APIs can be used directly.\\n### Positive Consequences <!-- optional -->\\n* Can use C-APIs directly (converted automatically by tooling)\\n* Compiles to fast native application\\n* No extra API layer\\n### Negative Consequences <!-- optional -->\\n* Needs separate compiling on every machine type\\n","tokens":83,"id":143,"Predictions":"Decision Makers!-- optional -->n* I want to learn Kotlinn* I want to learn Pythonn* I don't want to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area is usually written in Cn ## Decision Makers!-- optional -->n* I want to learn Kotlinn* I want to learn Pythonn* I don't want to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area is usually written in Cn ## Decision Makers!-- optional -->n* I want to learn Kotlinn* I want to learn Pythonn* I don't want to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area is usually written in Cn ## Decision Makers!-- optional -->n* I want to learn Kotlinn* I want to learn Pythonn* I don't want to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area is usually written in Cn ## Decision Makers!-- optional -->n* I want to learn Kotlinn* I want to learn Pythonn* I don't want to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area is usually written in Cn ## Decision Makers!-- optional -->n* I want to learn Kotlinn* I want to learn Pythonn* I don't want to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area is usually written in Cn ## Decision Makers!-- optional -->n* I want to learn Kotlinn* I want to learn Pythonn* I don't want to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area is usually written in Cn ## Decision Makers!-- optional -->n*"}
{"File Name":"lcarsde\/ipc-between-wm-and-menu.md","Context":"## Context and Problem Statement\\nThe application menu needs to know the currently active windows, shall display the name and enable the user to select or close them. Therefor it is necessary to exchange the corresponding information with the window manager.\\n## Decision Drivers <!-- optional -->\\n* Fast, the application menu entry information should be updated \"instantly\" when windows open, close or get renamed\\n* No message should be lost\\n","Decision":"* Fast, the application menu entry information should be updated \"instantly\" when windows open, close or get renamed\\n* No message should be lost\\nChosen option: \"Message queues\", because this approach is a faster than shared storage approaches and doesn't need to handle locking. Message queues are asynchronous (the producer doesn't need to wait for the consumer) and messages are queued and not lost.\\n### Positive Consequences <!-- optional -->\\n* Easy to use and flexible\\n* Library support in Python\\n### Negative Consequences <!-- optional -->\\n* Manual implementation in Kotlin necessary (but straight forward)\\n","tokens":86,"id":144,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"lcarsde\/identification-of-special-tool-windows.md","Context":"## Context and Problem Statement\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\n","Decision":"Chosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\n","tokens":48,"id":145,"Predictions":"Decision (A) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.nB) To allow the status bar and the side bar menu to be placed in the UI.n"}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"Predictions":"The code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.n## Decisionn[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_of code style conventions.n ## Decision.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"PactSwift\/ADR-001-Language_choice.md","Context":"# Context\\niOS applications can be written in Objective-C or Swift. Objective-C offers greater interaction with C++ code but is considered a legacy language choice in the iOS developer community. The `pact-consumer-swift` framework was built to support Objective-C as well, but it's proven to become a bigger challenge supporting both with newer Xcode and Swift versions.\\n# Decision\\nThe framework is written in Swift.\\n# Consequences\\n","Decision":"The framework is written in Swift.\\n# Consequences\\n","tokens":90,"id":148,"Predictions":"The pact-consumer-swift framework was built to support Objective-C as well, but it's proven to become a bigger challenge supporting both with newer Xcode and Swift versions.n# DecisionnThe framework is written in Swift.n# Consequencesn ## Decision nThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Swift.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-C.n# DecisionnThe framework is written in Objective-"}
{"File Name":"PactSwift\/ADR-004-Dependencies-management.md","Context":"# Context\\nAlmost all software we write depends on some other code, library or development tool which allows us to build what we want faster. Although this project attempts to avoid bringing in 3rd party dependencies, there are is functionality already written that is critical to this projects success.\\n# Decision\\nThe main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation\/pact-reference\/rust](https:\/\/github.com\/pact-foundation\/pact-reference\/tree\/main\/rust\/pact_mock_server_ffi).\\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and\/or interact with different services (interacting with Pact Mock Server, etc.).\\nMatt's [CwlPreconditionTesting](https:\/\/github.com\/mattgallagher\/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `.\/Carthage\/Checkouts\/CwlPreconditionTesting\/*` added into the project itself). For SPM it is defined as a dependency in `.\/PactSwiftServices\/Package.swift`.\\n# Consequences\\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\\n# Follow-up (September 30, 2020)\\nAll 3rd party dependencies have been successfully removed from this project\/framework.\\n","Decision":"The main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation\/pact-reference\/rust](https:\/\/github.com\/pact-foundation\/pact-reference\/tree\/main\/rust\/pact_mock_server_ffi).\\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and\/or interact with different services (interacting with Pact Mock Server, etc.).\\nMatt's [CwlPreconditionTesting](https:\/\/github.com\/mattgallagher\/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `.\/Carthage\/Checkouts\/CwlPreconditionTesting\/*` added into the project itself). For SPM it is defined as a dependency in `.\/PactSwiftServices\/Package.swift`.\\n# Consequences\\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\\n# Follow-up (September 30, 2020)\\nAll 3rd party dependencies have been successfully removed from this project\/framework.\\n","tokens":494,"id":149,"Predictions":"The main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This project attempts to avoid bringing in 3rd party dependencies, there is is functionality already written that is critical to this project's success. Although this project attempts to avoid bringing in 3rd party dependencies, there is is functionality already written that is critical to this project's success.n# DecisionnThe main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation\/pact-reference\/rust](https:\/\/github.com\/pact-foundation\/pact-reference\/tree\/main\/rust\/pact_mock_server_ffi).nThe binary framework(s) that are built using cargo lipo --release command are added into the Xcode project.nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required.nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and\/or interact with different services (interacting with Pact Mock Server, etc.).nMatt's [CwlPreconditionTesting](https:\/\/github.com\/mattgallagher\/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files.\/Carthage\/Checkouts\/CwlPreconditionTesting\/* added into the project itself)..."}
{"File Name":"PactSwift\/ADR-005-Project-Structure.md","Context":"## Context\\n`PactSwift` takes advantage of Mock Server FFI binaries built from shared Rust code. These are generally large binary files when it comes to iOS and macOS platforms and we are limited with hosting them in the GitHub repo. The FFI also follows it's own source and changes are available independently to changes to `PactSwift`'s functionality. Separating the responsibilities would be welcomed.\\nFurthermore, the pain of managing multiple binaries with the same name but each with its specific architecture slice could be reduced by generating an `XCFramework` using an automated script and kept from the framework user. These can blow up to more than 100Mb each (the fat binary with all slices for iOS platform blew up to more than 300MB). Using `XCFramework` we can shed off a lot of the statically linked code. Mock Server FFI (`MockServer.swift`) is the only part of `PactSwift` package that depends on binaries being built for specific architectures and run platforms. With removal of binaries from the main `PactSwift` project, we should be able to avoid managing them, mixing them up (as they are all named the same), discarding them at `git add` and `commit` steps and rebuilding them at next `PactSwift` build\/test cycle.\\n","Decision":"- Mock Server FFI interface and implementation to be split into it's own Swift Package called `PactSwiftMockServer` and distributed as a binary (`XCFramework`) when on Apple platforms and as a source package when used on Linux platforms.\\n- Utilities used by both the main `PactSwift` and `PactSwiftMockServer` packages are split into one package called `PactSwiftToolbox`.\\n- Where it makes sense the dependencies' versions should be exact. If exact version is not set for a valid reason then `.upToMinor()` must be used to avoid breaking changes when releasing packages in isolation.\\n- Scripts to automate the release processes will be provided within the projects' scripts folders.\\n","tokens":268,"id":150,"Predictions":"We should remove binaries from the main PactSwift project.nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>n"}
{"File Name":"eslint-config-nhsuk\/0001-replace-istanbul-with-nyc-for-code-coverage.md","Context":"## Context\\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\\nnyc is one of the suggested replacements. Additional information is available\\non [npm](https:\/\/www.npmjs.com\/package\/istanbul). This leaves the package open\\nto security flaws that will not be patched. Features available in the latest\\nversions of node will not be supported.\\n","Decision":"The decision is to migrate from istanbul to nyc.\\n","tokens":81,"id":151,"Predictions":"Decision (A) The package should be unsupported.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"Predictions":"GitHub Actions are the best choice for this project.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"AYCHAIN.github.io\/adr-001-tools-and-methodologies.md","Context":"## Context\\nLearning Machine handles a Blockcerts verifier in multiple repository and with different ways of deploying. This is costly and hard to maintain.\\nDecision has been made to unify the verifier into one sole component and repository, with scalibility and maintainability at heart.\\n","Decision":"#### JS\\nWe decided to use Polymer 3.0 as previous versions of the verifier were already written with Polymer. Also because Web Components seem like a promising technology that could open a interesting future for the usage of the component.\\nState of the application will be handled by Redux, as demonstrated in the [example project of Polymer](https:\/\/github.com\/Polymer\/pwa-starter-kit).\\n#### CSS\\nWe decided to use ITCSS for its interesting way to handle CSS scalability and maintainability.\\nMore information about this methodology can be found here:\\nhttps:\/\/github.com\/sky-uk\/css\\nhttps:\/\/www.xfive.co\/blog\/itcss-scalable-maintainable-css-architecture\/\\n#### Testing\\nWe are using the tools provided by the Polymer Project, hence [WCT](https:\/\/github.com\/Polymer\/tools\/tree\/master\/packages\/web-component-tester). We also test for accessibility.\\n#### Accessibility\\nThe Web Component needs to be WCAG2.0 AA compliant.\\n","tokens":55,"id":153,"Predictions":"Decision was made to unify the verifier into one sole component and repository.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"AYCHAIN.github.io\/adr-003-store-connector-container-component.md","Context":"## Context\\n[pwa-starter-kit](https:\/\/github.com\/Polymer\/pwa-starter-kit) is an example of a Redux application working with Polymer 3. While it serves its purpose, the separation of concerns is not entirely clear between the state and the views, which means that in the example views have too much knowledge of what provides and modfies the state.\\nThis is potentially dangerous for scalability, and bloats the view code with decisions that shouldn't be of its concerns.\\nReact provides an architectural pattern to handle this abstraction, in the shape of `Containers`. Containers are wrapper around the view component, which connect (via the `react-redux` helper function of the same name) the view and the state (to read it, and to action it).\\nThis pattern is interesting because from an architectural standpoint every responsibility is properly distributed. The view receives props, may they be data or callback functions, and utilizes them, the container selects where to retrieve the data or callback functions, and the state solely stores the data and provide mechanisms to modify it.\\n","Decision":"We decided to keep this approach for a Polymer project too. Because the out-of-the-box tools do not provide that abstraction, we implemented our own architectural approach to fit this need.\\nA visual representation of the intent is as follows:\\n![Chain of connection: State > Connector > Container > Component](https:\/\/user-images.githubusercontent.com\/12797962\/41294972-a254d432-6e59-11e8-8e08-214c43772173.png)\\n- The state handles the live data of the application.\\n- The connector has an instantiation of the store, but is also a metaview, which enables communication with the methods of the store, without exposing it elsewhere.\\n- The container is an instance of the connector, which allows defining which part of the state and which actions we would like to bind to its wrapped component.\\n- And the wrapped component is just a regular component which expects some props, but does not know nor care where they come from.\\n","tokens":219,"id":154,"Predictions":"Decision Makers should be able to make a decision on the state of the state.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"AYCHAIN.github.io\/adr-004-domain.md","Context":"## Context\\nRedux provides a good mechanism to handle the state and its mutations, but not so much to handle the business logic.\\nHowever, like always, to keep a good segregation of concerns, we want to handle all business related logic, as well as all external communication of the application isolated.\\n","Decision":"To do so, we introduce the concept of domain.\\nThe domain is the part responsible for everything that's not the view concern, nor the state concern.\\nThe view concern is to render data.\\nThe state concern is to store and modify data.\\nThe rest belongs to the domain.\\nA visual representation is as follows:\\n![Domain connection: outer world > domain > state > view](https:\/\/user-images.githubusercontent.com\/12797962\/41419432-4c70fd4e-6ff2-11e8-8acd-a18367087193.png)\\nEach method of the next inner layer only knows about the layer above it.\\nThis domain is only inspired from Domain Driven Design and aims at being a simpler implementation of the approach.\\n","tokens":61,"id":155,"Predictions":"Decision Makers should consider Redux as a good mechanism to handle the state and its mutations.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"AYCHAIN.github.io\/adr-006-handling-plural-i18n.md","Context":"## Context\\nSometimes labels need to handle pluralization. While it could be just as easy as adding a `s` at the end of the word in English (it is not), French or Spanish, other languages have a variation of their plural form that require a better handling.\\n","Decision":"We chose to follow the second approach which adds less overhead to the bundle.\\nIt comes at the cost of having the contributor to add the plural version of the string they want translated. But this explicit approach also reduces the risk of error and inconsistency.\\n","tokens":57,"id":156,"Predictions":"Decision (A) The pluralization of words is not a problem for labels.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"Predictions":"Decision (A) To maintain component level styles but enable shared styles. (B) To maintain component level styles but enable shared styles. (C) To maintain component level styles but enable shared styles. (D) To maintain component level styles but enable shared styles. (E) To maintain component level styles but enable shared styles. (F) To maintain component level styles but enable shared styles. (G) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles. (H) To maintain component level styles but enable shared styles."}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"Predictions":"The substeps list should be animated between showing and hiding the substeps of a step. The height of the list should be fixed and working with little work.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"Predictions":"Kotlin is now fully supported for Android by Google.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\\nI need a way to document design decisions.\\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"Predictions":"Is this the right way to document design decisions?nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"MoviesAndroid\/0003 The Movie Db.md","Context":"## Context and Problem Statement\\nThis app shows how i would build an Android app. Therefore the app architecture is the goal itself,\\nthe features this app has, are just a way to show an app architecture.\\n## Decision Drivers\\n* Availability\\n* Realism\\n* Content creation\\n","Decision":"* Availability\\n* Realism\\n* Content creation\\nChosen option: \"The Movie Db\", because it is more realistic when the app consumes a real service.\\nPositive Consequences:\\n* It is realistic\\nNegative consequences:\\n* When it's down the app cannot consume it\\n","tokens":59,"id":161,"Predictions":"The app architecture is the goal itself.n## Decision Driversn* Availabilityn* Realismn* Content creationn## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n## Decision n"}
{"File Name":"disco-poc-vue\/0002-use-heroku-static-buildpack.md","Context":"## Context\\nWe need to decide how to deploy our Vue app in Heroku. We can achieve this either\\nby using a Node.js web server, or by deploying the app as a static site with\\nthe [Heroku static buildpack](https:\/\/github.com\/heroku\/heroku-buildpack-static).\\n","Decision":"We will use the static buildpack to deploy, as our app will essentially be a\\nstatic frontend backed by TIMDEX. This is consistent with [Vue's deployment\\nguidelines](https:\/\/cli.vuejs.org\/guide\/deployment.html#general-guidelines)\\non deployments.\\n","tokens":63,"id":162,"Predictions":"We need to deploy the app as a static site in Heroku. We can achieve this either by using a Node.js web server, or by deploying the app as a static site with the [Heroku static buildpack](https:\/\/github.com\/heroku\/heroku-buildpack-static).n Decision 0A.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"disco-poc-vue\/0004-use-yarn.md","Context":"## Context\\nNPM is causing confusion as to why lock files are changing in local\\nenvironments when no changes have been made. We have found explanations and\\nworkarounds, but it feels like the type of unexpected default behavior that will\\nlead to frustration as new developers join the project.\\nYarn is an alternative package manager that seems to have a more expected set\\nof default behaviors while maintaining compatibility in case we need to revert.\\n","Decision":"We will use Yarn instead of NPM for this project.\\n","tokens":90,"id":163,"Predictions":"Decision (A). The NPM will be a better choice for the project.nB. The NPM will be a better choice for the project.nC. The NPM will be a better choice for the project.nD. The NPM will be a better choice for the project.nA. The NPM will be a better choice for the project.nB. The NPM will be a better choice for the project.nC. The NPM will be a better choice for the project.nD. The NPM will be a better choice for the project.nA. The NPM will be a better choice for the project.nB. The NPM will be a better choice for the project.nC. The NPM will be a better choice for the project.nD. The NPM will be a better choice for the project.nA. The NPM will be a better choice for the project.nB. The NPM will be a better choice for the project.nC. The NPM will be a better choice for the project.nD. The NPM will be a better choice for the project.nA. The NPM will be a better choice for the project.nB. The NPM will be a better choice for the project.nC. The NPM will be a better choice for the project.nD. The NPM will be a better choice for the project.nA. The NPM will be a better choice for the project.nB. The NPM will be a better choice for the project.n"}
{"File Name":"disco-poc-vue\/0003-use-vuejs-3.md","Context":"## Context\\nWe reviewed various options for building our front end and decided Vue.js was\\nthe best fit. However, Vue is preparing for a major version upgrade. We could\\nchoose to use the current stable version (2) and migrate to the new version (3)\\nwhen it is released or start with the release candidates for the new version.\\n","Decision":"We will use Vue.js 3 releases candidates.\\n","tokens":72,"id":164,"Predictions":"We would migrate to the current stable version (2) and migrate to the new version (3).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR0048_add_resources_table.md","Context":"## Context\\nThe GOV.UK PaaS billing system receives a series of events from Cloud Foundry notifying, for each tenant, whether services or resources have been created, renamed, or deleted.\\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services\/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\\nIf there is a problem with a tenant's bill it was very difficult to find the source of the problem.\\n","Decision":"We need to have a persistent record of the services or resources each tenant has provisioned with a dates indicating when the service or resource was\/is being used. This persistent record is in the new `resources` table.\\nThe reason for this is that there is no need to regenerate historical records of services or resources provisioned for tenants each time billing is run each month since this information does not change. Furthermore, recording this information for each month makes it difficult for us to calculate bills between any two dates and times.\\nThe `resources` table also acts as an audit point within GOV.UK PaaS billing. It makes investigation of discrepancies in tenant bills easier to investigate. Anyone supporting GOV.UK PaaS billing can first look at the contents of `resources` and see whether the discrepancy arose in the population of `resources` or afterwards in the actual calculation of the bill.\\n","tokens":167,"id":166,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR040-bosh-access-without-socks.html.md","Context":"## Context\\nTo comply with [Payment Card Industry (PCI) Requirements](https:\/\/www.pcisecuritystandards.org\/pci_security\/maintaining_payment_security) we will remove the use of shared credentials anywhere in the platform.\\nWe will use Google Single Sign On (SSO) to avoid credential sharing.\\nTo enable SSO we will expose some of the APIs on the BOSH instance without using a SOCKS5 proxy. This is due to the SOCKS5 proxy not being compatible with our VPN, which in turn creates a support risk.\\nWe discussed proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)\\n","Decision":"We will remove the reliance on SOCKS5 or SSH tunnels for the User Account and Authentication Service (UAA) API endpoint so that we can remove the use of shared credentials.\\nWith these in place we are unable to complete an SSO journey from one of our existing IDPs as the BOSH instance is not browser accessible.\\nThe current method will be replaced with SSO in order to obtain a UAA token. This token is then used to interact with the rest of the APIs on the BOSH instance.\\n","tokens":188,"id":168,"Predictions":"We will remove the use of shared credentials anywhere in the platform.nWe will use Google Single Sign On (SSO) to avoid credential sharing.nWe discussed proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR006-rds-broker.md","Context":"Context\\n=======\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n- [18F](https:\/\/github.com\/18F\/rds-service-broker)\\nDecision\\n========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n","Decision":"========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n","tokens":183,"id":169,"Predictions":"We will continue to use RDS as a service broker.n ## Decision - Final.nn======nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.html.md","Context":"## Context\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\n","Decision":"* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\n","tokens":87,"id":172,"Predictions":"Decision (A) The user should be allowed to access the network. (B) The user should be allowed to access the network. (C) The user should be allowed to access the network. (D) The user should be allowed to access the network. (A) The user should be allowed to access the network. (A) The user should be allowed to access the network. (B) The user should be allowed to access the network. (C) The user should be allowed to access the network. (D) The user should be allowed to access the network. (A) The user should be allowed to access the network. (A) The user should be allowed to access the network. (B) The user should be allowed to access the network. (C) The user should be allowed to access the network. (D) The user should be allowed to access the network. (A) The user should be allowed to access the network. (A) The user should be allowed to access the network. (B) The user should be allowed to access the network. (C) The user should be allowed to access the network. (D) The user should be allowed to access the network. (A) The user should be allowed to access the network. (A) The user should be allowed to access the network. (B) The user should be allowed to access the network. (C) The user should be allowed to access the network. (D) The user should be allowed to access the network."}
{"File Name":"paas-team-manual\/ADR033-redirect-http-for-applications.html.md","Context":"## Context\\nIn [ADR032](\/architecture_decision_records\/ADR032-ssl-only-for-applications-and-cf-endpoints) we decided that\\nwe would only support https for applications on the PaaS, and that we would\\ndrop plain http connections (port 80).\\nSince then, we've observed this causing confusion for users on numerous\\noccasions where they think their app isn't working after being pushed.\\nThe situation is improved with the inclusion of the `cloudapps.digital` domain\\nin the [HSTS preload](https:\/\/hstspreload.org\/?domain=cloudapps.digital) list,\\nbut this only helps users with recent versions of modern browsers.\\nAs a result of the continued confusion for users we should revisit the decision\\nfrom ADR443.\\nThere are a number of things that could be done to address this:\\n### Update the CF CLI to include the scheme\\nCurrently, the CF CLI outputs the fully-qualified hostname of the app after\\npushing, but doesn't include the scheme. This has caused confusion for users\\nwhen this is copy\/pasted into browsers, and then times out.\\nGetting the CLI to include the scheme here will help with the specific case of\\nusers getting confused immediately after pushing an app.\\nIt's unclear how much work this involves, as currently information about\\nwhether a route is http or https doesn't appear to be modeled in CloudFoundry\\nanywhere.\\nIf this involves changes to the CLI, there's no guarantee that users will\\nupgrade their CLI.\\nAdditionally, there is some debate about how effective this change would be. It\\nwill probably fix some cases, but won't cover everything.\\n### Redirect http to https\\nAdd an endpoint that listens to all http requests on cloudapps.digital and\\nredirects them to the corresponding https URL.\\nThere's a risk with this that a service could link to the http version of a\\npage by mistake and not notice due to the redirect. We can mitigate this be\\nhaving the redirect strip the path and query when redirecting so that it always\\nredirects to the base URL.\\nThere's another risk that a misconfiguration could allow non-encrypted traffic\\nthrough to applications. This would need to be mitigated by having acceptance\\ntests to cover this.\\n","Decision":"We will redirect http traffic to the corresponding root https endpoint.\\nWe will continue to maintain HSTS preload lists for our production domains.\\n","tokens":472,"id":173,"Predictions":"We should revisit the decision from ADR443 to support https for applications on the PaaS, and that we would drop plain http connections (port 80).nAs a result of the continued confusion for users we should revisit the decision from ADR032 to ADR043.nThere are a number of things that could be done to address this.n### Update the CF CLI to include the scheme.nCurrently, the CF CLI outputs the fully-qualified hostname of the app after pushing, but doesn't include the scheme. This has caused confusion for usersnwhen this is copied\/pasted into browsers, and then times out.nGetting the CLI to include the scheme here will help with the specific case ofnusers getting confused immediately after pushing an app.nIt's unclear how much work this involves, as currently information aboutnwhether a route is http or https doesn't appear to be modeled in CloudFoundrynanywhere.nIf this involves changes to the CLI, there's no guarantee that users willnupgrade their CLI.nAdd an endpoint that listens to all http requests on cloudapps.digital andnredirects them to the corresponding https URL.nThere's a risk with this that a service could link to the http version of anpage by mistake and not notice due to the redirect. We can mitigate this by having the redirect strip the path and query when redirecting so that it alwaysnredirects to the base URL.n"}
{"File Name":"paas-team-manual\/ADR042-isolation-segments.html.md","Context":"## Context\\nGOV.UK PaaS would like to be able to isolate specific tenant apps and tasks to\\ndifferent pools of virtual machines (VMs).\\nGOV.UK PaaS would like to be able to prevent specific tenant apps and tasks\\nfrom egressing to the internet.\\nApps running inside the separate pools of VMs should be able to discover and\\naccess other apps running within the platform, providing that the correct Cloud\\nFoundry Network Policies have been created.\\nApps running in the shared pools of VMs should be able to discover and access\\napps running inside an isolation segment, providing that the correct Cloud\\nFoundry Network Policies have been created.\\n","Decision":"GOV.UK PaaS will implement egress-restricted isolation segments.\\nIsolation segments will be configured by a GOV.UK PaaS developer, in a similar\\nmanner to VPC peering connections.\\nIsolation segments will have the following variable properties:\\n- Number of instances (e.g. 1, 2, 3, 6)\\n- Instance type (e.g. small\/large - maps to an AWS instance type + disk sizing)\\n- Whether egress to the internet is restricted\\nWe will use IPTables rules to achieve egress restriction.\\n### Isolation segments\\nCloud Foundry supports separating apps and tasks for specific Organizations\\nand Spaces via a feature called\\n[Isolation Segments](https:\/\/docs.cloudfoundry.org\/adminguide\/isolation-segments.html).\\nAn Isolation Segment is a group of Diego cells with separate placement tags,\\nwhich map to the isolation segment name.\\nIsolation segments will be implemented as new instance groups defined in the\\nBOSH deployment manifest, with additional placement tags. A placement tag\\ncorresponds to an isolation group name.\\nFor example, an instance group with the placement tags:\\n- `fast-cpu`\\n- `fast-network`\\nenables us to run the following commands successfully:\\n- `cf create-isolation-segment fast-cpu`\\n- `cf create-isolation-segment fast-network`\\nwhich creates two isolation segments.\\nThese isolation segments can be shared such that a segment can be:\\n- used by only a single organization or space\\n- shared by multiple organizations and spaces\\n### Egress restrictions\\nContainer-to-container networking within Cloud Foundry is implemented via a\\nVirtual Extensible Local Area Network\\n([VXLAN](https:\/\/tools.ietf.org\/html\/rfc7348)).\\nEach container is assigned a virtual IP address inside the subnet 10.255\/16\\n[Silk](https:\/\/github.com\/cloudfoundry\/silk)\\nand VXLAN create\/update\/delete\\n[IPTables](https:\/\/linux.die.net\/man\/8\/iptables) rules\\nvia the Container Network Interface\\n([CNI](https:\/\/github.com\/containernetworking\/cni)),\\nto ensure containers can talk to each other.\\nIPTables is an interface to control networking within the Linux kernel.\\nExisting network traffic restrictions are defined by Silk and VXLAN as\\ndescribed above. We can configure extra IPTables rules with higher precedence\\nto create tighter restrictions than currently exist.\\nIPTables can be used to prevent unauthorised egress via REJECT rules,\\ndepending on the destination IP address. This can be done, either:\\n- In the global INPUT or FORWARD chains, with source IP qualifier to ensure only container traffic is affected\\n- In each container\u2019s \u201cnetout\u201d chain\\nImplementing such IPTables rules allows us to block traffic from an IP address\\nwithin 10.255.0.0\/16 (apps) to an address outside 10.0.0.0\/8 (outside the VPC).\\nThis would have the effect of preventing app traffic egressing from the\\nplatform.\\nWe would apply these IPTables rules to running apps and tasks, but not staging\\napps.  This will allow staging apps to communicate with the outside world (e.g.\\nfor downloading dependencies).\\n","tokens":140,"id":174,"Predictions":"Decision based on the information in the record.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR001-manifest-management.md","Context":"Context\\n=======\\nThe alpha initially took the approach of starting with a vanilla set of cloud\\nfoundry manifests, and merging new values into it using spiff. This became\\ndifficult to reason about, and cf-release was forked because it was easier than\\noverriding necessary values using spiff. However, the confusing spiff hierarchy\\nremained.\\nDecision\\n========\\nWe will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n","Decision":"========\\nWe will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n","tokens":209,"id":175,"Predictions":"We will create our own set of manifests based on those in cf-release.nWe will modify these as required.nWe will name the files with a numeric prefix and rely on shell globbing tondetermine the merge order rather than listing the merge order in thenbuild-manifest script.n## Decision - We will need to define an upgrade path for our cloud foundry manifests - makingnsure we pull in the required values from the new release manifests into ours.n## Decision - We will create our own set of manifests based on those in cf-release.nWe will modify these as required.nWe will name the files with a numeric prefix and rely on shell globbing tondetermine the merge order rather than listing the merge order in thenbuild-manifest script.n## Decision - We will need to define an upgrade path for our cloud foundry manifests - makingnsure we pull in the required values from the new release manifests into ours.n## Decision - We will create our own set of manifests based on those in cf-release.nWe will modify these as required.nWe will name the files with a numeric prefix and rely on shell globbing tondetermine the merge order rather than listing the merge order in thenbuild-manifest script.n## Decision - We will need to define an upgrade path for our cloud foundry manifests - makingnsure we pull in the required values from the new release manifests into ours.n## Decision - We will create our own set of manifests based on those in cf-release.nWe will name the files with a numeric prefix and rely on shell globbing tondetermine the merge order rather than listing the merge order in thenbuild-manifest script.n## Decision - We will need to define an upgrade path for our cloud foundry manifests - makingnsure we pull in the required values from the new release manifests into ours.n"}
{"File Name":"paas-team-manual\/ADR038-audit-logs-in-splunk.html.md","Context":"## Context\\n### Audit events\\nThe GOV.UK PaaS has components which expose events which can be used for\\nauditing. For example:\\n- BOSH director kernel audit logs\\n- BOSH director Credhub security events\\n- BOSH director UAA events\\n- Cloud Foundry UAA events\\n- Cloud Foundry Cloud Controller security events\\nThe BOSH director and BOSH managed instances store these logs in\\n`\/var\/vcap\/sys\/log\/`.\\n### Logging service\\nThe Cyber Security team run a centralised log ingestion system called the\\nCentral Security Logging Service (CSLS).\\nThis service runs in AWS and uses [CloudWatch log group subscription\\nfilters](https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/logs\/SubscriptionFilters.html)\\nto ingest logs, which are then sent to Splunk for indexing.\\n","Decision":"We will use Terraform to create log groups in our existing pipelines:\\n- `create-bosh-concourse`\\n- `create-cloudfoundry`\\nWe will store audit logs in CloudWatch for 18 months (545 days).\\nWe will use Terraform to create log group subscription filters which will send\\nlogs to CSLS.\\nTerraform failing to create log group subscription filters should not block the\\nexecution of the pipeline, so our pipelines are not coupled to CSLS.\\nWe will run a fork of the\\n[awslogs-boshrelease](https:\/\/github.com\/alphagov\/paas-awslogs-boshrelease),\\non all instances that have relevant audit and security event logs, to send logs\\nfrom the instances to CloudWatch.\\nThe CloudWatch log groups will have subscription filters that will send logs\\nto CSLS so that logs are indexed in Splunk.\\n","tokens":176,"id":176,"Predictions":"Decision (A) The GOV.UK PaaS must be re-architected to support audit events.n### Audit eventsnThe GOV.UK PaaS has components which expose events which can be used for auditing. For example:n- BOSH director kernel audit logsn- BOSH director Credhub security eventsn- BOSH director UAA eventsn- Cloud Foundry UAA eventsn- Cloud Foundry Cloud Controller security eventsnThe BOSH director and BOSH managed instances store these logs inn\/var\/vcap\/sys\/log\/.n### Logging servicenThe Cyber Security team run a centralised log ingestion system called thenCentral Security Logging Service (CSLS).nThis service runs in AWS and uses [CloudWatch log group subscriptionnfilters](https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/logs\/SubscriptionFilters.html)nto ingest logs, which are then sent to Splunk for indexing.n### Decision (A) The GOV.UK PaaS must be re-architected to support audit events.n### Audit eventsnThe GOV.UK PaaS has components which expose events which can be used for auditing.n### Logging servicenThe Cyber Security team run a centralised log ingestion system called thenCentral Security Logging Service (CSLS).nThe Cyber Security team run a centralised log ingestion system called thenCentral Security Logging Service (CSLS).nThe Cyber Security team run a centralised log ingestion system called thenCentral Security Logging Service (CSLS).nThe Logging service runs in AWS and uses [CloudWatch log group subscriptionnfilters](https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/logs\/SubscriptionFilters.html)nto ingest logs, which are then sent to Splunk for indexing.n"}
{"File Name":"paas-team-manual\/ADR043-new-product-pages-language-and-framework.html.md","Context":"## Context\\n[PaaS product pages] have been reviewed and a number of accessibility issues\\nhave been identified. To resolve those we, would need to make an upgrade and\\nand review if any additional changes are needed to align with the GOV.UK Design System.\\nAs those pages are built in Ruby and in [ADR024] we've made the decision\\nto develop our user-facing applications on Node, it's a good opportunity to\\nlook at rebuilding the product pages.\\nWe've discussed user needs and it emerged that:\\n* anyone in the team (developer and non-developer) should be able to update pages\\nwith less effort\\n* pages should be performant for end users\\n* pages should be rendered by the server\\n* keeping pages up to date with GOV.UK Design System releases should be quicker and easier\\n* alignment of technologies for our user-facing web products should provide better\\ndeveloper experience and give us the option to have shared component libraries\\nWith the above in mind we researched options. Our admin interface is built in React,\\nso we narrowed the scope to React-based static site generators.\\nWe ended up comparing two: [NextJS] with static page export and [GatsbyJS]\\nwhich exports static pages by default.\\nFor page content we agreed that writing pages in [Markdown] is a good option,\\nso we tested both with [MDX] which can also embed React components inside content pages.\\n[NextJS] and [GatsbyJS] have different approaches to development and there are minor\\nperformance differences between them.\\nOur use case for now is narrow enough, and with the primary need of anyone in the team\\nbeing able to update pages, [NextJS] marginally gets more votes as Gatsby cannot be installed and run on\\nnon-developer machines.\\n","Decision":"We will use [NextJS] together with [MDX] to author PaaS product pages content in\\n[Markdown] and deliver them to users as static pages.\\n","tokens":375,"id":177,"Predictions":"We will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [NextJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update the product pages.nWe will use [GatsbyJS] to update"}
{"File Name":"paas-team-manual\/ADR014-hsts-preload-using-api-gateway.html.md","Context":"## Context\\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).\\nTo add our domains to [HSTS preload lists](https:\/\/hstspreload.appspot.com\/), there are these requirements:\\n1. Serve a valid certificate.\\n2. Redirect from HTTP to HTTPS on the same host.\\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\\n4. Serve an HSTS header on the base domain for HTTPS requests:\\nWe need an endpoint to provide these requirements.\\nOur Cloud Foundry app endpoint already [serves the\\nright HSTS Security header with HAProxy](..\/ADR008-haproxy-for-request-rewriting)\\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\\nWe can implement a second ELB to listening on HTTP and HTTPS and use\\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\\nBut this increases our dependency on the HAProxy service.\\nWe must serve from the root domain (or apex domain), but it is not allowed to\\nserve [CNAME records in the root\/apex domain](http:\/\/serverfault.com\/questions\/613829\/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\\nan issue when serving the service using ELB or CloudFront.\\n","Decision":"* We will implement a basic [AWS API Gateway](https:\/\/aws.amazon.com\/api-gateway\/)\\nwith a default [MOCK response](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2015\/09\/introducing-mock-integration-generate-api-responses-from-api-gateway-directly\/)\\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\\ncontent of the response is irrelevant, it can be a 302.\\nA [Custom Domain Name](http:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-custom-domains.html),\\nwhich creates a [AWS Cloud Front distribution](http:\/\/docs.aws.amazon.com\/AmazonCloudFront\/latest\/DeveloperGuide\/distribution-overview.html),\\nwill provide public access to this API.\\n* We will use [AWS Route 53 `ALIAS` resource record](http:\/\/docs.aws.amazon.com\/Route53\/latest\/APIReference\/CreateAliasRRSAPI.html)\\nto [serve the IPs of the AWS Cloud Front distribution as A records](http:\/\/docs.aws.amazon.com\/Route53\/latest\/DeveloperGuide\/routing-to-cloudfront-distribution.html).\\n","tokens":341,"id":178,"Predictions":"We will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](..\/ADR032-ssl-only-for-applications-and-cf-endpoints).nTo add our domains to [HSTS preload lists](https:\/\/hstspreload.appspot.com\/), there are these requirements:n1. Serve a valid certificate.n2. Redirect from HTTP to HTTPS on the same host.n3. Serve all subdomains over HTTPS (actually checks for www.domain.com)n4. Serve an HSTS header on the base domain for HTTPS requests:nWe can implement a second ELB to listening on HTTP and HTTPS and usenHAProxy to do the HTTP to HTTPS redirect and serve the right header.nBut this increases our dependency on the HAProxy service.nWe must serve from the root domain (or apex domain), but it is not allowed to serve [CNAME records in the root\/apex domain](http:\/\/serverfault.com\/questions\/613829\/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain).n## Decision Makers:n"}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"Predictions":"We implemented automatic certificate rotation.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR021-cell-capacity-assignment-2.html.md","Context":"## Context\\nWe've been running with the cell provisioning policy in ADR017 since February.\\nWe haven't ever run out of cell capacity, but we've observed that there's\\nexcess capacity that we're paying for.\\nAt the time that we wrote ADR017 we had fewer tenants and an individual\\ntenant's quota was a much greater proportion of total memory size. In other\\nwords a single tenant could conceivably use a greater proportion of our excess\\ncapacity.\\nCells are still deployed across 3 AZs.\\nWe still don't have a way to autoscale the number of cells to meet demand, so\\nwe need to ensure that we have surplus capacity for when we're not around.\\nCells are almost completely uncontended; we're not experiencing CPU or disk I\/O\\ncontention and not all the cell memory is being used.\\nOver the 3 month period from 1st August - 1st November\\n- Usable memory (free + cached + buffered) is running between 77% and 92% of total cell memory\\n- The maximum increase in memory usage over an exponentially smoothed average from a week previously was 36%\\n- We're running at about 10% of our total container capacity\\n- container usage has peaked at about 20% above the previous weeks\\n- Average CPU usage is about 10%. We see daily peaks of 80%\\n- reps think that about 50% of the capacity of cells is used\\n- the largest amount that rep's allocated memory increased week on week was 55%\\n","Decision":"Our objectives are:\\nState | Expected behaviour\\n------|-------------------\\n# All cells operational | Enough capacity to allow some but not all tenants to scale up to their full quota. The amount of excess capacity required should be enough to accommodate the fluctuations we can expect over a 3 day period (weekend + reaction time)\\n# While CF being deployed | As above: enough capacity to allow some tenants to scale up to their full quota\\n# One availability zone failed\/degraded | Enough capacity to maintain steady state app usage. Not guaranteed to be able to scale apps up.\\n# More than one AZ failed | The system is not expected to have sufficient capacity to host all running apps.\\nTo achieve this we need to start basing our capacity planning on current memory\\noccupied by processes on cells, rather than the sum of all quotas given to\\nusers. We will define alerts for capacity planning purposes, the in-hours\\nsupport person is expected to respond by adjusting the number of cells.\\nWe want to ensure that cells have some headroom above a smoothed\\naverage:\\n- to allow some headroom for increases in the memory consumed by apps.\\n- to allow buffering and caching to occur and not adversely impact application\\nperformance.\\nFrom our data analysis (see context) the amount of memory consumed by apps\\ncan reach about 36% over a week-ago's smoothed average. We round up to 40% to\\ninclude buffering\/caching.\\nIf an AZ fails, we need enough capacity remaining to host all our apps. The\\nfailed AZ's apps are evenly divided amongst the surviving AZs. Because we have\\ntwo remaining AZs, each surviving AZ will have 1.5x as many apps running.\\nBecause we want 40% headroom, we'll want 1.4 (headroom) x 1.5 (evacuated apps)\\ncurrent usage. This is about 2x actual memory consumed by processes on cells.\\nTherefore we need to start alerting when the memory occupied by processes on\\ncells is above 50%, when suitably smoothed to avoid noise \/ small spikes\\ncausing frequent alarms.\\nCPU usage is assumed to be a linear relation of memory usage and we will have a\\nsimilar alert defined when it exceeds 50% on cells.\\nIn addition to wanting the cells to not run short on memory, we also want\\ntenants to be able to scale apps up and down when all AZs are functional. In\\norder to ensure this, we need to allow for a ~50% increase in requested memory,\\nwhich means alerting when all the reps have a cumulative remaining capacity of\\n~33%, when smoothed to avoid false alarms.\\nWe also need enough container capacity to allow tenants to scale apps up and\\ndown and deploy new apps. We should alert when we're using > 80% of the sum of\\nour reps' container capacity. Again, this should be smoothed to ensure that\\nshort lived fluctuations in usage don't cause unnecessary alerts.\\nIt is likely that patterns such as the fluctuation in memory use over a week\\nmay change over time. We should review this decision after 6 months.\\n","tokens":327,"id":181,"Predictions":"Decision (A). The cell provisioning policy in ADR017 is not being implemented.nWe're not experiencing CPU or disk I\/O or not experiencing CPU or disk I\/O.nWe're running at about 10% of our total container capacity.nWe're running at about 20% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 20% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 20% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total container capacity.nWe're running at about 10% of our total cell memory.nWe're running at about 10% of our total"}
{"File Name":"paas-team-manual\/ADR005-pingdom-healthchecks.md","Context":"Context\\n=======\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\nDecision\\n========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","Decision":"========\\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\nStatus\\n======\\nProposed\\nConsequences\\n============\\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\\n### Positive\\n* We are now able to test accessibility using the staging environment\\n* We are now able to use Pingdom to assert not just application health, but routing as well.\\n* We have maintained consistency between staging and production\\n### Negative\\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\\n* We would no longer have healthchecks via Pingdom for our CI environment.\\n","tokens":449,"id":182,"Predictions":"Decision based on the following: a. We would make the staging environment accessible to the outside world and production. b. We would make the staging environment accessible to the outside world and production. c. We would make the staging environment accessible to the outside world and production. d. We would make the staging environment accessible to the outside world and production. e. We would make the staging environment accessible to the outside world and production. f. We would make the staging environment accessible to the outside world and production. g. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the staging environment accessible to the outside world and production. h. We would make the testing environment accessible to the outside world and production. h. We would make the testing environment accessible to the outside world and production. h. We would make the testing environment accessible"}
{"File Name":"paas-team-manual\/ADR011-security-group-structure.html.md","Context":"## Context\\nIn https:\/\/www.pivotaltracker.com\/story\/show\/115252309 we described a pattern\\nfor defining security groups between services and the VMs that connect to\\nthem. This involved creating a client security group for each service that\\nthe service allowed connections from. This client security group was then\\napplied to the VMs that needed to be able to connect.\\nThis has the problem that in some cases we would need to add several client\\nsecurity groups to a given VM, and we are at risk of running into the AWS\\nlimits (by default it's a max of 5 groups per interface).\\n","Decision":"We will use an alternative approach where a security group is defined for each\\ncomponent (or component group) in the system. Services then define rules\\nallowing connections from the components that need access. There are some\\ninstances where a component will be both a client of other services and a\\nprovider of a service. We will use the same security group for both of these\\nroles.\\nThis approach has a number of advantages:\\n* The number of security groups applied to a VM is driven by the number of\\ncomponents on that VM, not the number of things that they need to connect to.\\nThe number of components is likely to be small, and we're better able to\\ncontrol this that we can control the number of things a  component talks to.\\n* It's easier to see what components connect to a given service by looking at\\nthe rules for that service's security group.\\n* When moving a component between VMs it's much clearer which security groups\\nneed to move with it.\\n","tokens":129,"id":183,"Predictions":"Decision (A).nThe security groups should be based on the AWS security group rules.nB. The security groups should be based on the AWS security group rules.nC. The security groups should be based on the AWS security group rules.nD. The security groups should be based on the AWS security group rules.nA. The security groups should be based on the AWS security group rules.nB. The security groups should be based on the AWS security group rules.nC. The security groups should be based on the AWS security group rules.nD. The security groups should be based on the AWS security group rules.nA. The security groups should be based on the AWS security group rules.nB. The security groups should be based on the AWS security group rules.nC. The security groups should be based on the AWS security group rules.nD. The security groups should be based on the AWS security group rules.nA. The security groups should be based on the AWS security group rules.nB. The security groups should be based on the AWS security group rules.nC. The security groups should be based on the AWS security group rules.nD. The security groups should be based on the AWS security group rules.nA. The security groups should be based on the AWS security group rules.nB. The security groups should be based on the AWS security group rules.nC. The security groups should be based on the AWS security group rules.nD. The security groups should be based on the AWS security group rules."}
{"File Name":"paas-team-manual\/ADR024-web-app-language-and-framework-selection-2.html.md","Context":"## Context\\nWe are starting to develop a number of user-facing applications with web\\ninterfaces that need to be GOV.UK branded. In order to keep things consistent\\nwe want to pick a single programming language to write these in.\\nWe've previously chosen Ruby on Rails as our desired framework in the previous\\n[ADR022]. And whether or not it was a right choice, we decided to revisit and\\nreconsider that choice.\\nThe reason for that, is simply the direction GDS is heading at. It would appear\\nthat old components will be soon deprecated and the Frontend Community has no\\ndesire to support gems in the future. Saying that, we could have remain with\\nRails and delegate the component management to NPM. This would however increase\\nthe amount of possible maintenance work we would need to undertake, due to the\\nuse of [nunjucks] by the Design System team.\\nWe questioned the need of running Rails application for something that\\nessentially is a templating system for existing data and API.\\nAfter some more research, discussion with the head of the Frontend Community,\\nmembers of the team and some others in GDS, the better choice would be Node for\\nthe following reasons:\\n* It's the way Frontend Community is heading at\\n* It will be easier to rotate\/onboard Frontend Developers\\n* The initial applications are to be simple (thin layer between API calls and\\nHTML parsing)\\n* It's light and essentially is JavaScript\\n* It supports [nunjucks] which will help us in maintenance\\n","Decision":"We will use Node to create new user-facing applications that render a web\\ninterface for our service but will not be used to implement any significant\\n\u2018application logic\u2019.\\n","tokens":322,"id":184,"Predictions":"Decision (A) The frontend community should remain with Rails and delegate the component management to NPM. (B) The frontend community should remain with Ruby on Rails and delegate the component management to NPM. (C) The frontend community should remain with Rails and delegate the component management to NPM. (D) The frontend community should remain with Rails and delegate the component management to NPM. (E) The frontend community should remain with Rails and delegate the component management to NPM. (A) The frontend community should remain with Rails and delegate the component management to NPM. (A) The frontend community should remain with Rails and delegate the component management to NPM. (A) The frontend community should remain with Rails and delegate the component management to NPM. (B) The frontend community should remain with Rails and delegate the component management to NPM. (C) The frontend community should remain with Rails and delegate the component management to NPM. (D) The frontend community should remain with Rails and delegate the component management to NPM. (A) The frontend community should remain with Rails and delegate the component management to NPM. (A) The frontend community should remain with Rails and delegate the component management to NPM. (A) The frontend community should remain with Rails and delegate the component management to NPM."}
{"File Name":"paas-team-manual\/ADR006-rds-broker.html.md","Context":"## Context\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n- [18F](https:\/\/github.com\/18F\/rds-service-broker)\\n","Decision":"We will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\n","tokens":85,"id":186,"Predictions":"Decision (A). The RDS service broker will be used for the first iteration of the RDS architecture. The RDS service broker will be used for the second iteration of the RDS architecture. The RDS service broker will be used for the third iteration of the RDS architecture. The RDS service broker will be used for the fourth iteration of the RDS architecture. The RDS service broker will be used for the fifth iteration of the RDS architecture. The RDS service broker will be used for the sixth iteration of the RDS architecture. The RDS service broker will be used for the seventh iteration of the RDS architecture. The RDS service broker will be used for the eighth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture. The RDS service broker will be used for the ninth iteration of the RDS architecture."}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"Predictions":"Decision (A). The non-production environments are not worth the time.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR045-aws-waf.html.md","Context":"## Context\\nGOV.UK PaaS uses [AWS Shield Advanced](https:\/\/aws.amazon.com\/shield\/features\/#AWS_Shield_Advanced) as well as AWS WAF to protect from DDoS attacks.\\nHowever the mitigations are not automatic and we have access to the AWS DDoS Response Team\\n(DRT) who are experts in mitigating these types of attack.\\n```\\nShield Advanced detects web application layer vectors, like web request floods and\\nlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web\\napplication layer vectors, you must employ AWS WAF rules or the DRT must employ the\\nrules on your behalf.\\n```\\nIn order to be functional they require access to our AWS WAF logs in order to identify what\\nthe attack is and where is is coming from, and API access to the WAF in order to apply the\\nmitigating rules.\\nTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger\\nthe [emergency engagement Lambda](https:\/\/s3.amazonaws.com\/aws-shield-lambda\/ShieldEngagementLambda.pdf)\\n","Decision":"We will grant access to the AWS DRT to read from restricted S3 buckets\\n","tokens":243,"id":188,"Predictions":"Decision (A). PaaS will use [AWS Shield Advanced] to protect from DDoS attacks.nnnnShield Advanced detects web application layer vectors, like web request floods andnlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web application layer vectors, you must employ AWS WAF rules or the DRT must employ the WAF on your behalf.nnIn order to be functional they will set up CloudWatch alarms on our WAF rules in order to trigger [emergency engagement Lambda](https:\/\/s3.amazonaws.com\/aws-shield-lambda\/ShieldEngagementLambda.pdf)n ## Decision Makers are required to make decisions about the mitigations.nnnnShield Advanced detects web application layer vectors, like web request floods andnlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web application layer vectors, you must employ AWS WAF rules or the DRT must employ the WAF on your behalf.nnIn order to be functional they require access to our AWS WAF logs in order to identify whatnthe attack is and where is is coming from, and API access to the WAF in order to apply thenmitigating rules.nnTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger [emergency engagement Lambda](https:\/\/s3.amazonaws.com\/aws-shield-lambda\/ShieldEngagementLambda.pdf)n"}
{"File Name":"paas-team-manual\/ADR035-do-not-use-haproxy-use-aws-alb.html.md","Context":"## Context\\nIn ADR008 and ADR014 we decided to use HAProxy, for three reasons:\\n- Writing HSTS header if they are not present in the upstream request\\n- Implementing HTTP -> HTTPS redirect\\n- Custom health check for Gorouter\\nThese problems have since been fixed:\\n- Header rewriting was implemented in [v0.183.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.183.0)\\n- HTTP healthchecking was implemented in [v0.139.0](https:\/\/github.com\/cloudfoundry\/routing-release\/releases\/tag\/0.139.0)\\n- HTTP -> HTTPS redirect can be done using AWS ALBs\\nWe currently use multiple ELBs (classic) which we want to replace with ALBs.\\nWe want to use ALBs because:\\n- ELBs are deprecated in terraform and cause crashes\\n- ALBs can give us more metrics in CloudWatch\\n- ALBs have better support for X-Forwarded-For\\n- ALBs support fixed-response which can be used for HTTP -> HTTPS rewriting\\nHAProxy adds significant complexity to our routing deployment and maintenance:\\n- Proxy Protocol is non-standard and hard to understand\\n- HTTP -> HTTPS rewriting is hard to understand\\n- HAProxy config is rarely touched\\n- We have to maintain our own HAProxy BOSH release\\n- HAProxy duplicates the number of logs we receive because every platform request is written twice\\n- HAProxy adds an extra network hop for every request\\n","Decision":"- Replace ELBs with ALBs\\n- Use ALB fixed-response to redirect HTTP -> HTTPS\\n- Use Gorouter directly for:\\n- TLS termination\\n- HSTS header rewriting\\n- Healthchecking the router instance\\n- Remove HAProxy\\n","tokens":315,"id":189,"Predictions":"Decision based on ADR014 and ADR014nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the routing protocol.nAssignment a new name for the new name for the routing protocol.nAssignment a new name for the new name for the routing protocol.nAssignment a new name for the new name for the routing protocol.nAssignment a new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name for the new name"}
{"File Name":"paas-team-manual\/ADR029-aiven-project-structure.html.md","Context":"## Context\\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\\nThe PaaS has several environments which will need to use Aiven. These\\nenvironments should be isolated from each other so that changes made in testing\\nand development environments do do not affect production users.\\nAiven provide a \"Project\" abstraction where a user can be a member of several\\nprojects. API tokens are user specific. By creating one user per project it's\\npossible to scope API tokens to a project.\\n","Decision":"We'll use separate projects for separate environments, initially using the\\nfollowing Aiven projects:\\n* ci-testing (for the CI environment for the elasticsearch broker itself)\\n* paas-cf-dev\\n* paas-cf-staging\\n* paas-cf-prod\\nFor staging and prod we will use separate API tokens within the same project to\\nseparate credentials between the London and Ireland regions.\\nWe will have the following per-project users to hold API tokens:\\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\\nThe credentials for the ci and dev users will be stored in the\\n`paas-credentials` passwordstore. staging and prod will be stored in\\n`paas-credentials-high`.\\nMembers of the PaaS team will each have their own user which will have access\\nall of the projects for management purposes.\\n","tokens":100,"id":191,"Predictions":"Decision (A) The Aiven environment should be isolated from each other. (B) The Aiven environment should be isolated from each other. (C) The Aiven environment should be isolated from each other. (D) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (B) The Aiven environment should be isolated from each other. (C) The Aiven environment should be isolated from each other. (D) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other. (B) The Aiven environment should be isolated from each other. (C) The Aiven environment should be isolated from each other. (D) The Aiven environment should be isolated from each other. (A) The Aiven environment should be isolated from each other."}
{"File Name":"paas-team-manual\/ADR032-ssl-only-for-applications-and-cf-endpoints.html.md","Context":"## Context\\nNote: This has been superceeded. See [Status](#status) below.\\nIt is expected for the government websites to be secure and keep the user\\ninteractions private. Because that we want to enforce all communications to\\nany application and to the platform endpoints to use only and always HTTPS,\\nas [it is described in the Gov Service Manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nWhen a user inputs a website name without specifying the\\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\\nEven if the server always redirect HTTP to HTTPS, an initial\\nunprotected request including user information will be transferred\\nin clear: full URL with domain, parameter, [cookies without secure flag](https:\/\/en.wikipedia.org\/wiki\/HTTP_cookie#Secure_and_HttpOnly)\\nor browser meta-information.\\n[HTTP Strict Transport Security](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security)\\nmitigates this issue by instructing modern browsers that support it to\\nalways connect using HTTPS.\\nThis is also a [requirement in the service manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nThere is still a potential initial unprotected HTTP request that might happen\\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\\nTo solve this issue, the root domain can be added to\\n[HSTS preload list](https:\/\/hstspreload.appspot.com\/) which will be used by most\\ncommon browsers.\\nCurrently the only way to avoid any clear text HTTP interaction is closing or\\ndropping any attempt to connect to the port 80 at TCP level.\\nAlthough not all application deployed on the PaaS will be \"services\"\\nas in the service manual meaning, we must not allow HTTP to make\\nit easier to service owners to comply with this requirements.\\n","Decision":"We will only open port 443 (HTTPS) and drop\/reject any TCP connection to TCP port 80 (HTTP).\\nWe will implement and maintain HSTS preload lists for our production domains.\\n","tokens":391,"id":192,"Predictions":"Decision (A) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.nB) To allow HTTP to make it easier to service owners to comply with this requirement.n"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.md","Context":"Context\\n=======\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\nDecision\\n========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n","Decision":"========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n","tokens":213,"id":193,"Predictions":"Decision ## Decision ## The pool resource is not a workaround for flow between jobs.nWe will not use the pool resource for flow between jobs - instead we will use the semver resource.nStatusn======nAcceptednConsequencesn=======nThis was an investigation into a different approach, so no consequencesn ## Decision ## The semver resource is not a workaround for flow between jobs.nWe will not use the pool resource for flow between jobs - instead we will use the semver resource.nStatusn=======nAcceptednConsequencesn========nThis was an investigation into a different approach, so no consequencesn ## Decision ## Decision ## The semver resource is not a workaround for flow between jobs.nStatusn=======nAcceptednConsequencesn========nThis was an investigation into a different approach, so no consequencesn ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ## Decision ##"}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"Predictions":"Decision (A).nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.nThis is a form of end-to-end encryption.nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).nThis is a form of end-to-end encryption.nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) allegesnthat the IPSec release is no longer maintained.n Decision (A).nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.nThis is a form of end-to-end encryption.nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) allegesnthat the IPSec release is no longer maintained.n Decision (A).nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).nThis is a form of end-to-end encryption.n Decision (A).nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).nThis is a form of end-to-end encryption.n Decision (A)."}
{"File Name":"paas-team-manual\/ADR008-haproxy-for-request-rewriting.html.md","Context":"## Context\\nWe want to serve [HSTS\\nheaders](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security) for all\\nHTTPS requests to the apps domains, but it will safeguard existing users from\\nbeing MITMed over insecure connections and it will improve the user experience\\nwhen they select a hostname that doesn't have a protocol.\\n(Note that without pre-loading in browsers this won't help first time users,\\nbut that is out of context)\\nWe want to leave open the option of able to override these headers from\\nthe tenant application if they wish.\\nThis feature requires conditionally process and modify the request headers.\\nThere are several possible implementations:\\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\\nand add the header if required, by:\\n* Supporting the specific HSTS headers, and allowing configure some\\nsort of behaviour and default value.\\n* Allow inject any additional header if they are missing.\\nBut [current `gorouter` implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)\\ndoes not support any of these features, which require being added.\\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\\nthe go-routers and after the ELB.\\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\\nAll the commercial CDN have the capacity to add additionally headers\\nconditionally.\\n4. AWS ELB: They do not support this logic and will not in the short term.\\nIn consequence they cannot be used to solve this problem.\\n","Decision":"We do not want to add any additional logic in the CDN, as they will\\nbe an optional part of the platform and we will try to keep as simple\\nas possible.\\nWe consider that the optional solution would be implement this logic in\\nthe `gorouter`, but that requires some development effort and a PR being merged\\nupstream.\\nBecause that we will implement, in the short term, the second option: a proxy\\nin front of the `gorouter`.\\n* We will implement [HAproxy](http:\/\/www.haproxy.org\/) in front of the go router.\\n* Ha-proxy is the default LB solution for the official CF distribution.\\n* It is really powerful and has good support.\\n* Enough features to cover our needs.\\n* It will be setup colocated with the `gorouter`, proxying directly to\\nlocalhost.\\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\\nis OK as the two services are colocated in the same VM.\\n* We will reuse the code from [official haproxy job from cf-release](https:\/\/github.com\/cloudfoundry\/cf-release\/tree\/master\/jobs\/haproxy),\\nalthough we will have to fork it to add additional settings in the\\nhaproxy configuration.\\nFuture work:\\n* We will implement and propose a PR to add logic in go-router to allow\\ndefine additional headers.\\n","tokens":367,"id":195,"Predictions":"Decision based on the information in this passage.nWe want to serve [HSTSnheaders](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security) for all HTTPS requests to the apps domains, but it will safeguard existing users fromnbeing MITMed over insecure connections and it will improve the user experiencenwhen they select a hostname that doesn't have a protocol.nWe want to leave open the option of able to override these headers fromnthe tenant application if they wish.nBut [current gorouter implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)ndoes not support any of these features, which require being added.nBut [current gorouter implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)ndoes not support any of these features, which require being added.nBut [current gorouter implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)ndoes not support any of these features, which require being added.nBut [current gorouter implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)ndoes not support any of these features, which require being added.nBut [current gorouter implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)ndoes not support any of these features, which require being added.nBut [current gorouter implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)ndoes not support any of these features, which require being added.n"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.html.md","Context":"## Context\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\n","Decision":"We will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\n","tokens":303,"id":197,"Predictions":"We need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is becausen1 Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.n1 Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.n1 Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.n1 Decision n"}
{"File Name":"paas-team-manual\/ADR004-domain-naming-scheme.html.md","Context":"## Context\\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\\n![pipeline image](..\/images\/pipeline.jpg)\\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\\n### Naming considerations\\nA number of aspects were considered as part of the naming process.\\n* Clear sense of purpose\\n* Clear distinction between Production and other Environments\\n* No overly technical names (for example, hosting\/paas\/scalable-elastic-public-government-hosting)\\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\\n","Decision":"For _non_ production environments we will be using the following domains:\\n* [environment name].cloudpipeline.digital\\n* [app name].[environment name].cloudpipelineapps.digital\\nFor our production environment we will be using the following domains:\\n* cloud.service.gov.uk\\n* [app name].cloudapps.digital\\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\\n","tokens":247,"id":198,"Predictions":"Decision (A).n### Name considerationsnA number of aspects were considered as part of the naming process.n### Name considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Name considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n### Naming considerationsnA number of aspects were considered as part of the naming process.n"}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"Predictions":"The service plans should be renamed to reflect the new service plans.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR0049_calculate_bills.md","Context":"## Context\\nTenants provision a wide variety of services and AWS\/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services\/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry.\\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services\/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\\nIn the GOV.UK PaaS billing rewrite, this has been changed. We want to calculate bills for variable time periods and also to forecast bills for the future (for the web-based billing calculator). However, the method to calculate the actual bill always needs to be the same.\\n","Decision":"The code to calculate the bill has been decoupled from the code used to calculate the bill. This is so we can use exactly the same code for calculation of all bills, whether these bills are for tenants or for prospective tenants (using the billing calculator).\\nThe approach taken is:\\n1. Populate database temporary table with what is being billed (which resources over which time interval, including the future). This can be in a stored function or embedded SQL. This is the code entry point into billing.\\n2. Call a stored function to calculate the bill ([`calculate_bill`](https:\/\/github.com\/alphagov\/paas-billing\/blob\/main\/billing-db\/sprocs\/calculate_bill.sql)) only using the contents of the temporary table populated in step 1 above. No parameters are passed into ([`calculate_bill`](https:\/\/github.com\/alphagov\/paas-billing\/blob\/main\/billing-db\/sprocs\/calculate_bill.sql)).\\n","tokens":229,"id":201,"Predictions":"We need to calculate bills for variable time periods and forecast bills for the future (for the web-based billing calculator).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR022-web_app_language_and_framework_selection.html.md","Context":"## Context\\nWe are starting to develop a number of user-facing applications with web\\ninterfaces that need to be styled to look like GOV.UK etc. In order to keep\\nthings consistent we want to pick a single programming language and framework\\nto write these in.\\nWe've previously used [Sinatra][] for this, but ran into\\nissues with its default configuration which isn't secure, leading to an XSS\\nvulnerability. We therefore want to choose something that comes with secure\\ndefaults, and makes it easier to avoid this sort of issue.\\nRequirements:\\n* Must be well supported by [govuk_template][] and [govuk_frontend_toolkit][] (as\\nwell as the future [govuk-frontend][] project)\\n* Must be understood broadly by members of the team.\\n* Must be understood broadly by members of the frontend developer community within GDS.\\nAfter dicsussion with the head of the frontend community and members of the\\nteam, the choice seems to be Rails for the following reasons:\\n* Most of www.gov.uk is written in Rails, as is the Verify frontend, and\\ntherefore is well known within the frontend developer community.\\n* It's well supported by the frontend toolkits (both projects are available as\\ngems that provide a Rails engine). Given the wide use of Rails with GDS, the\\nfuture [govuk-frontend][] project is likely to support it.\\n* It's the framework that's most familiar to our team.\\n* It is opinionated, and comes with secure defaults making it much easier to\\ncreate a secure web app.\\n","Decision":"We will use Ruby and Rails to create new user-facing applications with web frontends.\\n","tokens":337,"id":202,"Predictions":"We chose Rails because it is opinionated and comes with secure defaults.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to use Rails for the frontend development.nWe want to"}
{"File Name":"paas-team-manual\/ADR047-postgres-extensions-approach.html.md","Context":"## Context\\nUp to the release of postgres 13 plans to tenants, the policy for choosing\\nthe set of postgres extensions we allow for a certain major version was ad-hoc.\\nA mixture of \"allow everything\", simply copying the list from the previous\\npostgres version and not actually trying any of these extensions had lead\\nto an extensions list (published at https:\/\/docs.cloud.service.gov.uk\/deploying_services\/postgresql\/#add-or-remove-extensions-for-a-postgresql-service-instance)\\nwith:\\n- misnamed entries\\n- entries removed from earlier postgres versions\\n- missing newly-offered extensions\\n- extensions that could never be used without superuser access\\n- extensions that could never be used with the VPC restrictions we have\\nin place for our RDS instances.\\nSome of these extensions had been omitted from the list in the documentation,\\nbut it was unclear how these decisions had been made and where we stood on\\neach extension.\\nThis is confusing for tenants and could lead them down a path of trying to\\nuse an extension which will never work, or even start designing a system\\nthat relies on functionality in a listed extension only to find it unusable.\\n","Decision":"For postgres 13 onwards, offer a selection of extensions limited to those\\nwe know can be successfully enabled and think are feasible to use given\\nthe limitations of our platform.\\nMaintain a document (initially a spreadsheet https:\/\/docs.google.com\/spreadsheets\/d\/100qBo3Q2mfY70ek9fNWbpEsS4HzjOarF3q1a_hPR1uU\/edit?usp=sharing)\\ntracking our conclusions on each extension by postgres major version.\\nWhen preparing to offer a new postgres major version to tenants, a new\\nsheet should be copied from the previous release's sheet and adjusted\\naccording to the new list of extensions in the RDS documentation. New\\nentries should be researched and decided upon whether they are feasible\\nto use without superuser privileges and from within our VPC.\\nUsing a dev environment with the new postgres available and a script\\nsuch as the one found in\\nhttps:\/\/github.com\/alphagov\/paas-rds-broker\/tree\/main\/scripts (or preferably\\nan improved one), each extension remaining in the \"final list\" should be\\nchecked in turn to ensure it can be enabled and then disabled. This may\\ninvolve discovering (through trial and error) which extensions have\\ndependencies on others. It's likely to be the same as the previous release,\\nso that's a good starting point. This is good information to maintain as it\\nwould come in useful if we ever decided to make the rdsbroker smarter and able\\nto auto-load dependencies.\\nThis should also reveal whether any new extensions require\\n`shared_preload_libraries`. Any that do should have appropriate additions\\nmade to https:\/\/github.com\/alphagov\/paas-rds-broker\/blob\/main\/rdsbroker\/supported_extensions.go .\\nBeyond making sure an extension loads, actually trying out the functionality\\nof each extension is beyond the time committment we're willing to give\\nthis for now. We may update the documentation to note the semi-supported\\nnature of postgres extensions on our platform.\\n","tokens":243,"id":203,"Predictions":"Decision (A). The policy for choosing postgres extensions was ad-hoc.nAnswer:A. [A]. [B]. [C]. [D]. [A]. [B]. [C]. [D]. [A]. [B]. [C]. [D]. [A]. [B]. [C]. [D]. [A]. [B]. [C]. [D]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [C]. [D]. [A]. [B]. [A]. [B]. [B]. [B]. [A]. [B]. [B]. [B]. [B]. [B]. [C]. [D]. [A]."}
{"File Name":"paas-team-manual\/ADR041-bosh-access-with-mtls.html.md","Context":"## Context\\nIn [ADR040 BOSH access without SOCKS](..\/ADR040-bosh-access-without-socks\/) we removed the requirement for using a SOCKS5 proxy or SSH tunnel to access the User Account and Authentication Service (UAA).\\nWe are moving towards a [zero trust network model](https:\/\/www.ncsc.gov.uk\/blog-post\/zero-trust-architecture-design-principles) and as part of this, we are removing the IP allow lists that have been in place.\\nWe discussed the proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https:\/\/docs.google.com\/document\/d\/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps\/edit#heading=h.xscqoqxlc072)\\n","Decision":"We will remove the reliance on IP allow lists for all services on the BOSH instance.\\nMutual TLS will replace the allow lists.\\nCyber prefer this method, as it give a much stronger authentication to the platform. This is due to authenticating both the individual and the machine that are accessing critical services.\\n","tokens":180,"id":204,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR009-x-forwarded-headers.md","Context":"Context\\n=======\\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this:\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\nDecision\\n========\\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe http traffic between ELB and HAProxy will not be encrypted.\\n","Decision":"========\\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe http traffic between ELB and HAProxy will not be encrypted.\\n","tokens":329,"id":205,"Predictions":"We will disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.n ## Decision - No.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR013-building-bosh-releases.html.md","Context":"## Context\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse-ci.org\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\n","Decision":"We will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\n","tokens":239,"id":206,"Predictions":"Decision (A).nBuilding Bosh ReleasesnWe investigated different approaches to creating bosh releases, in particularn* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)nThe work on these spikes was recorded innhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731n)n### Building Bosh ReleasesnWe investigated different approaches to creating bosh releases, in particularn* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)nThe work on these spikes was recorded innhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731n"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"Predictions":"Decision (A) To store state in a database or k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n* To store state in a k\/v store.n"}
{"File Name":"paas-team-manual\/ADR039-aiven-metrics-for-users.html.md","Context":"## Context\\nWe offer our users the following backing services through [Aiven](https:\/\/aiven.io):\\n- Elasticsearch\\n- InfluxDB (private beta)\\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\\n- debug and respond to usage and service performance changes\\n- understand the operational characteristics of their applications and services\\n- make better capacity planning and budgeting decisions\\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\\n- shipping logs to an Elasticsearch\/Rsyslog\\n- sending metrics to Datadog\\n- sending metrics to Aiven Postgres\/InfluxDB\\n- exposing metrics in Prometheus exposition format\\nWe currently run Prometheus for monitoring the platform, using the [Prometheus BOSH release](https:\/\/github.com\/bosh-prometheus\/prometheus-boshrelease) and have confidence and experience using it.\\nWe will need to think about Prometheus failover. If we load balance Prometheus without sticky sessions, the metrics Prometheus reports will be erratic, as different instances report different metrics.\\n","Decision":"We will use Prometheus to scrape Aiven-provided services.\\nWe will deploy new Prometheus in the Cloud Foundry BOSH deployment using the Prometheus BOSH release. This will reduce blast radius - tenant usage of metrics will not affect our ability to operate and monitor the platform using Prometheus.\\nWe will need to automate the following tasks:\\n1. Service discovery: make sure Prometheus has an updated list of Aiven services to scrape. We must colocate this automation with the Prometheus instance.\\n2. Service integration: make sure every eligible Aiven-provided service uses the Aiven service integration for Prometheus.\\n","tokens":229,"id":208,"Predictions":"We need to consider Prometheus failover.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR027-pipeline-locking.html.md","Context":"## Context\\nIn our Concourse pipelines a locking mechanism was required to prevent concurrent deploys. Concurrent deploys cause\\nproblems because:\\n* one deployment may make changes which break another\\n* we have availability tests which tell us if a specific change causes app or API downtime\\n* Bosh has its own locking mechanisms, which mean concurrent deploys will fail.\\n* concurrent testing puts extra load on the platform and Concourse\\n","Decision":"We decided to implement locking using the Concourse pool resource, AWS CodeCommit, and Terraform.\\nThe Concourse pool resource was chosen because it is the Concourse-native solution for locking. This created a\\ndependency on having a Git repository for the lock, as this is how the pool resource is implemented. AWS CodeCommit was\\nchosen over Github for several reasons:\\n* client libraries for interacting with the necessary APIs were much more mature for AWS and this particular use case.\\nIt saved a lot of work.\\n* Github would have required managing users and SSH keys, or tokens. AWS could use the existing instance profile of the\\nConcourse VM.\\n* Github would have meant the repository containing the locks would have been public.\\nTerraform was required to allow our pipelines (the Concourse instance profile) to manage AWS CodeCommit. We used a\\npattern whereby we allow the creation of IAM users under a specific name prefix, and allow adding these users to a\\npredefined IAM group. The permissions of said group are defined in a private repository in Terraform configuration\\nwhich is ran manually. This limits the permissions the Concourse instance profile can grant to users.\\n","tokens":87,"id":209,"Predictions":"Decision (A). The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is necessary. The locking mechanism is not necessary. The locking mechanism is not necessary. The locking mechanism is necessary. The locking mechanism is not necessary. The locking mechanism is necessary. The locking mechanism is not necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The locking mechanism is necessary. The"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"Predictions":"Decision (A).nThe pool resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe pool resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe pool resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe pool resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe pool resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe semver resource should be used to control flownthrough jobs.nThe"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"Predictions":"a). You should use pre-generated keys. b). You should use temporary credentials. c). You should use temporary credentials. d). You should use temporary credentials. e). You should use temporary credentials. e). You should use temporary credentials. f). You should use temporary credentials. g). You should use temporary credentials. h). You should use temporary credentials. h). You should use temporary credentials. h). You should use temporary credentials. ii. You should use temporary credentials. j. You should use temporary credentials. iii. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. iv. You should use temporary credentials. v. You should use temporary credentials. iv. You should use temporary credentials. v. You should use temporary credentials. iv. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials. v. You should use temporary credentials."}
{"File Name":"paas-team-manual\/ADR005-pingdom-healthchecks.html.md","Context":"## Context\\nWe wanted to open up access to tenant applications in our production environment.\\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\\n* [#116104189 - set up Pingdom](https:\/\/www.pivotaltracker.com\/story\/show\/116104189)\\n* [#115347323 - allow public access to tenant applications](https:\/\/www.pivotaltracker.com\/story\/show\/115347323)\\n","Decision":"It was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\\n","tokens":212,"id":212,"Predictions":"Decision (A).nThe Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world.nThe decision to allow public access to tenant applications is a final decision."}
{"File Name":"paas-team-manual\/ADR012-haproxy-healthcheck.md","Context":"Context\\n=======\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nWe were investigating how to avoid downtime to deployed applications when we\\nmade changes to the platform. We had discovered that short outages occurred\\nwhen the gorouter was taken out of service.\\ngorouter has drain functionality to allow upstream load balancers to gracefully\\ntake instances of the gorouter out of service before any requests start to\\nfail.\\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor\/1.1`\\nstart failing with HTTP 503. User requests are allowed to continue.\\nTwo things prevented us from using drain mode:\\n- we had the ELB configured to use TCP healthchecks, not HTTP\\n- the ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\nDecision\\n========\\nWe decided to:\\n- submit a change upstream to [allow the gorouter to recognise ELB\\nhealthchecks](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/138)\\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\\nwhich appends the `User-Agent: HTTP-Monitor\/1.1` that gorouter expects\\n- enable HTTP healthchecks on the ELB\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n- There was less downtime for deployed applications during a deploy.\\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\\n","Decision":"========\\nWe decided to:\\n- submit a change upstream to [allow the gorouter to recognise ELB\\nhealthchecks](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/138)\\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\\nwhich appends the `User-Agent: HTTP-Monitor\/1.1` that gorouter expects\\n- enable HTTP healthchecks on the ELB\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n- There was less downtime for deployed applications during a deploy.\\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\\n","tokens":437,"id":214,"Predictions":"Decision ## Final.n======nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"Predictions":"Decision ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n ## Final decision - Implementation of the security measures.n"}
{"File Name":"paas-team-manual\/ADR001-manifest-management.html.md","Context":"## Context\\nThe alpha initially took the approach of starting with a vanilla set of cloud\\nfoundry manifests, and merging new values into it using spiff. This became\\ndifficult to reason about, and cf-release was forked because it was easier than\\noverriding necessary values using spiff. However, the confusing spiff hierarchy\\nremained.\\n","Decision":"We will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\n","tokens":74,"id":217,"Predictions":"Decisions were made to merge the values into a vanilla set of cloudnfoundry manifests.nnnThe alpha initially took the approach of starting with a vanilla set of cloudnfoundry manifests, and merging new values into it using spiff. This becamendifficult to reason about, and cf-release was forked because it was easier thannoverriding necessary values using spiff. However, the confusing spiff hierarchynremained.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR012-haproxy-healthcheck.html.md","Context":"## Context\\nStories: [#123490171](https:\/\/www.pivotaltracker.com\/story\/show\/123490171) & [#121933113](https:\/\/www.pivotaltracker.com\/story\/show\/121933113)\\nWe were investigating how to avoid downtime to deployed applications when we\\nmade changes to the platform. We had discovered that short outages occurred\\nwhen the gorouter was taken out of service.\\ngorouter has drain functionality to allow upstream load balancers to gracefully\\ntake instances of the gorouter out of service before any requests start to\\nfail.\\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor\/1.1`\\nstart failing with HTTP 503. User requests are allowed to continue.\\nTwo things prevented us from using drain mode:\\n- we had the ELB configured to use TCP healthchecks, not HTTP\\n- the ELB sends HTTP healthcheck requests with `User-Agent:\\nELB-HealthChecker\/1.0`, which means they were not recognised as healthcheck\\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\\ndraining gorouter out of service.\\nThe result was a very small amount of downtime for deployed apps that received\\nrequests during a short window of < 1 second.\\n","Decision":"We decided to:\\n- submit a change upstream to [allow the gorouter to recognise ELB\\nhealthchecks](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/138)\\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\\nwhich appends the `User-Agent: HTTP-Monitor\/1.1` that gorouter expects\\n- enable HTTP healthchecks on the ELB\\n","tokens":289,"id":218,"Predictions":"Decision (A).nThe ELB should not have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nC. The ELB should have taken the gorouter out of service.nD. The ELB should have taken the gorouter out of service.nA. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nC. The ELB should have taken the gorouter out of service.nD. The ELB should have taken the gorouter out of service.nA. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nC. The ELB should have taken the gorouter out of service.nD. The ELB should have taken the gorouter out of service.nA. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nC. The ELB should have taken the gorouter out of service.nD. The ELB should have taken the gorouter out of service.nA. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nC. The ELB should have taken the gorouter out of service.nD. The ELB should have taken the gorouter out of service.nA. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nC. The ELB should have taken the gorouter out of service.nD. The ELB should have taken the gorouter out of service.nA. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.nB. The ELB should have taken the gorouter out of service.n"}
{"File Name":"paas-team-manual\/ADR013-building-bosh-releases.md","Context":"Context\\n=======\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse.ci\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\nDecision\\n========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n","Decision":"========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n","tokens":353,"id":219,"Predictions":"We will migrate all our Bosh release builds to their own build pipelines.n### Building Bosh ReleasesnWe researched different approaches to creating bosh releases, in particularn* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)nThe work on these spikes was recorded innhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731nDecisionn========nWe must gradually migrate all our Bosh release builds to their own build pipelines.nStatusn=======nAcceptednConsequencesn========nWe may have to add additional config in projects we fork to allow us to create final builds.n## Decisionn=======nWe must gradually migrate all our Bosh release builds to their own build pipelines.nStatusn=======nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.nStatusn=======nWe may have to add additional config in projects we fork to allow us to create final builds.n"}
{"File Name":"paas-team-manual\/ADR031-services-core-pipeline-separate-jobs.html.md","Context":"# Context\\nWe have a single pipeline `create-cloudfoundry` which creates a Cloud Foundry\\ndeployment, and also deploys additional services to the platform.\\nThese services include:\\n- PaaS Accounts\\n- PaaS Admin\\n- PaaS Billing\\n- PaaS Metrics\\nWhich are core components to our platform, but not to Cloud Foundry.\\nCurrently these services are unnecessarily coupled in a couple of places:\\n- The `post-deploy` job\\n- The `custom-acceptance-tests` job\\nUnnecessarily coupling has resulted in flakey app tests blocking CVE\\nremediation from reaching production.\\n# Decision\\nMove, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n","Decision":"Move, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n","tokens":221,"id":220,"Predictions":"Move, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samenncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samennncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samennncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samennncreate-cloudfoundry pipeline) such that they do not impede progress ofndeployment to the core platform.n# DecisionnMove, where possible, PaaS services into their own jobs (within the samennnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR028-move-logs-to-logit.html.md","Context":"## Context\\nThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.\\nIt was paused because some IA issues with Logit were not resolved. At one point\\nRE recommended that PaaS should host our own logstash as this part of the\\nservice was not widely available by market Elastic SaaS providers. The PaaS\\nteam was also considering to use Elasticsearch on AWS with our accounts.\\n","Decision":"An updated conversation with the RE tool team has confirmed that the IA issues\\nhad been resolved, and that GDS can continue to use Logit for now.\\nIt is a GDS strategy to use a consistent logging solution. Hence, we should\\ncontinue our migration of platform logs to logit, including our logstash\\nfilters.\\nThere is considerably less maintenance work for us if we use Logit's logstash\\nfilter rather than hosting the bosh release one. In the future if GDS choose to\\nuse another vendor that do not have a hosted logstash solution, they would need\\nto provide a migration strategy for all the current logstash users.\\n","tokens":91,"id":222,"Predictions":"Decision (A).nnThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.nIt was paused because some IA issues with Logit were not resolved. At one pointnRE recommended that PaaS should host our own logstash as this part of thenservice was not widely available by market Elastic SaaS providers. The PaaSnteam was also considering to use Elasticsearch on AWS with our accounts.nnnnnThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.nIt was paused because some IA issues with Logit were not resolved. At one pointnRE recommended that PaaS should host our own logstash as this part of thenservice was not widely available by market Elastic SaaS providers. The PaaSnteam was also considering to use Elasticsearch on AWS with our accounts.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"Predictions":"Decision (A) To allow the new user to modify the table ownership. (B) To allow the new user to modify the table ownership. (C) To allow the new user to modify the table ownership. (D) To allow the new user to modify the table ownership. (A) To allow the new user to modify the table ownership. (B) To allow the new user to modify the table ownership. (C) To allow the new user to modify the table ownership. (D) To allow the new user to modify the table ownership. (A) To allow the new user to modify the table ownership. (A) To allow the new user to modify the table ownership. (B) To allow the new user to modify the table ownership. (C) To allow the new user to modify the table ownership. (D) To allow the new user to modify the table ownership. (A) To allow the new user to modify the table ownership. (A) To allow the new user to modify the table ownership. (B) To allow the new user to modify the table ownership. (C) To allow the new user to modify the table ownership. (D) To allow the new user to modify the table ownership. (A) To allow the new user to modify the table ownership. (A) To allow the new user to modify the table ownership. (B) To allow the new user to modify the table ownership. (C) To allow the new user to modify the table ownership. (D) To allow the new user to modify the table ownership."}
{"File Name":"paas-team-manual\/ADR009-x-forwarded-headers.html.md","Context":"## Context\\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL\/TCP mode ELB is not able to set any `X-Forwarded` header.\\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http:\/\/serverfault.com\/questions\/775010\/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\\nThere are two options to workaround this:\\n* Use a more complex configuration of HAProxy with two frontends\/listeners chained\\n* Disable SSL between ELB and HAProxy\\n","Decision":"We have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\\nWe don't think this has any significant increase in risk because:\\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\\n* the inner interface of the ELB is on an internal network in our VPC\\n","tokens":213,"id":225,"Predictions":"Disable SSL between ELB and HAProxy.n1: Disable SSL between ELB and HAProxy.n2: Disable SSL between ELB and HAProxy.n3: Disable SSL between ELB and HAProxy.n4: Disable SSL between ELB and HAProxy.n5: Disable SSL between ELB and HAProxy.n6: Disable SSL between ELB and HAProxy.n7: Disable SSL between ELB and HAProxy.n8: Disable SSL between ELB and HAProxy.n9: Disable SSL between ELB and HAProxy.n9: Disable SSL between ELB and HAProxy.n10: Disable SSL between ELB and HAProxy.n11: Disable SSL between ELB and HAProxy.n12: Disable SSL between ELB and HAProxy.n13: Disable SSL between ELB and HAProxy.n14: Disable SSL between ELB and HAProxy.n15: Disable SSL between ELB and HAProxy.n16: Disable SSL between ELB and HAProxy.n17: Disable SSL between ELB and HAProxy.n18: Disable SSL between ELB and HAProxy.n19: Disable SSL between ELB and HAProxy.n20: Disable SSL between ELB and HAProxy.n20: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n23: Disable SSL between ELB and HAProxy.n19: Disable SSL between ELB and HAProxy.n20: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n22: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL between ELB and HAProxy.n21: Disable SSL"}
{"File Name":"paas-team-manual\/ADR034-continuously-deploy-platform-cf-apps.html.md","Context":"## Context\\nAs part of GOV.UK PaaS we have a number of applications that we consider to be\\nan essential part of the platform deployed as cloud foundry apps. These include:\\n* paas-admin\\n* paas-billing\\n* paas-accounts\\nPrior to July 2019 the versions of these applications were pinned in paas-cf,\\nand they were only deployed along with the whole platform. This had a couple of\\nnegative consequences:\\n* Following a change to an app's source and a passing build an additional\\nmanual change to paas-cf was required to deploy it\\n* Deploying apps required waiting for unrelated slow bits of the pipeline to\\ncomplete (e.g. terraform apply, bosh deploy, cf acceptance tests), despite\\nthe fact that a change to an app could not possibly affect or be affected by\\nthese steps\\nThere were also some advantages of specifying the version of applications\\nexplicitly:\\n* There's an record of which versions of the applications work with which\\nversions of the platform.\\n* Merges to paas-cf require a signed merge commit to be deployed, so we can be\\nexplicit about the set of people who are allowed to deploy changes\\n","Decision":"We will update the pipeline to:\\n* Require GPG signatures on git commits before deploying applications in staging and production\\n* Automatically deploy every build of the master branch of the applications to staging\\n* In staging: tag the application repo when a deployment passes its acceptance tests\\n* In production: automatically deploy any build which has been tagged by staging\\n","tokens":255,"id":228,"Predictions":"Decision based on the information in the record. (i) The application versions of the platform are not compatible with the existing versions of the platform. (ii) The application versions of the platform are not compatible with the existing versions of the platform. (iii) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platform are not compatible with the existing versions of the platform. (iv) The application versions of the platforms are not compatible with the existing versions of the platform. (iv) The application versions of the platforms are not compatible with the existing versions of the platform."}
{"File Name":"paas-team-manual\/ADR026-DNS-layout-for-UK-hosting.html.md","Context":"## Context\\nWe are moving AWS hosting from Ireland to London. This ADR contains the decisions of the DNS names we will use for apps and system components that will be hosted in London.\\n","Decision":"We will use the following domain patterns for the London hosting:\\n* `(system_component).london.(system_domain)`\\n* `(app_name).london.(app_domain)`\\nWhere:\\n* (system_component) -- api, uaa, doppler, ssh, etc.\\n* (system_domain) -- _cloud.service.gov.uk_, _staging.cloudpipeline.digital_\\n* (app_domain) -- _cloudapps.digital_, _staging.cloudpipelineapps.digital_\\nThe reasons are:\\n* We should re-use the (system_component) first part to minimise the changes to the Cloud Foundry manifests.\\n* We should re-use the (system_domain) and (app_domain) last part, because these domains are assigned to GOV.UK PaaS as the public interface.\\n* The domain part `london` is preferrable to `uk`, because AWS may provide multiple-region hosting within the UK in the future.\\nThe domain structure for the dev and CI environments won't change. For the dev environments we will create a flag to choose where to create the deployment.\\n### Examples\\n#### Production\\n|Ireland|London|\\n|----|------|\\n|# _api.cloud.service.gov.uk_|_api.london.cloud.service.gov.uk_|\\n|# _sample-app.cloudapps.digital_|_sample-app.london.cloudapps.digital_|\\n#### Staging\\n|Ireland|London|\\n|----|------|\\n|# _api.staging.cloudpipeline.digital_|_api.london.staging.cloudpipeline.digital_|\\n|# _sample-app.staging.cloudpipelineapps.digital_|_sample-app.london.staging.cloudpipelineapps.digital_|\\n","tokens":39,"id":229,"Predictions":"We will use the following DNS names: -a, -b, -c, -d, -e, -f, -g, -h, -i, -j, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -l, -i, -i, -l, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i, -i"}
{"File Name":"paas-team-manual\/ADR030-single-staging-london.html.md","Context":"# Context\\nWe run a Cloud Foundry platform in London (AWS eu-west-2 region) to allow GOV.UK PaaS tenants to host their applications and services in the UK. This is driven by the hosting needs of some of the existing GOV.UK PaaS tenants.\\nIn addition we need to manage the use of Amazon Web Service resources to reduce our running costs. Having staging environments in both Ireland and the UK increases the total infrastructural costs, which is not justified by the benefit of the additional tested cases.\\nThe London region is newer than the Ireland and currently offers a subset of services compared to the Ireland region. Hence, having the staging environment in London should allow us to capture the cases that may cause failure due to the unavailability of services.\\nTherefore, the risks of running a single staging environment are offset by the cost savings and simplification of deployment pipelines.\\n# Decision\\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n","Decision":"We will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\\n# Status\\nAccepted\\n# Consequences\\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\\n","tokens":253,"id":230,"Predictions":"We will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# StatusnAcceptedn# ConsequencesnWe will have one staging environment for testing both London and Ireland production environment.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the Ireland staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment from the AWS London (eu-west-2) platform.n# DecisionnWe will remove the London staging environment"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"Predictions":"We will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.n ## Decision n=======nStatusn======nAcceptednConsequencesn========nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.n ## Decision n========nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.n ## Decision n========nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.n ## Decision n========nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.n ## Decision n========nWe will maintain at least 50% of total org reserved capacity available when a zone fails."}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"Predictions":"Decision nAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:A"}
{"File Name":"Maud\/0008-package-structure.md","Context":"## Context\\nWe try to structure our package in logical sub-units but we want to maintain a\\nconsistent public interface.\\n","Decision":"We allow for arbitrarily nested sub-packages but export important classes and\\nfunctions to the top level thus exposing a public interface. Our unit tests\\nshould reflect this package structure.\\n","tokens":26,"id":234,"Predictions":"We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.n"}
{"File Name":"Maud\/0007-unit-tests.md","Context":"## Context\\nWe need to make a decision on the testing framework for our project.\\n","Decision":"We will make use of pytest. It is a de facto standard in the Python community\\nand has unrivaled power.\\n","tokens":18,"id":235,"Predictions":"Decision based on the information in the record.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0017-posterior-predictive-sampling.md","Context":"## Context\\nAfter sampling from the posterior distribution users may want the option\\nof validating their results against experimental data or to predict\\ncellular behaviour using a trained kinetic model.\\n","Decision":"Posterior predictive samples will be implemented using the posterior\\ndraws for kinetic and thermodynamic parameters. The boundary conditions\\nfrom the predicted experiments will be sampled from their marginal distributions.\\nThe number of samples will be limited to the number of samples from the posterior\\ndistribution.\\nIn order to define what is part of the training set and prediction set a new\\nfile will be introduced called experiment_metadata. A toml file where the training\\nprediction split will be defined under the headers [\"training\"], and [\"prediction\"]\\nrespectively.\\nA new stan file will be made where there is no model block (minimal if required).\\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\\nfor their respective generative distributions.\\nlog_probabilities should be calculated if measurements are also included for the\\nprediction experiments.\\n","tokens":36,"id":236,"Predictions":"Decision Makers should consider using a trained kinetic model to validate their results.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0006-code-testing.md","Context":"## Context\\nSetting up different testing environments and configurations can be a painful\\nand error prone process.\\n","Decision":"We use tox to define, configure, and run different test scenarios.\\n","tokens":21,"id":237,"Predictions":"Decision Makers should be able to identify the best way to set up testing environments and configurations.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0003-python-3-6-only.md","Context":"## Context\\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\\nto natively support f-strings which are sweet.\\n","Decision":"We make an early decision to only support Python 3.6 and above.\\n","tokens":38,"id":238,"Predictions":"Decision (A) The f-strings in Python 3.6 are sweet.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"Predictions":"We will continue to use the adjoint ODE solver.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution.nWe will consider the alternative solution."}
{"File Name":"Maud\/0014-phosphorylation-as-mass-action.md","Context":"## Context\\nPhosphorylation is the process which deactivates metabolic enzymes.\\nThe process is often conducted using kinases, which phosphorylate enzymes,\\nand phosphotases, which dephosphorylate the enzyme. This is a major metabolic\\nregulator and is essential for the accurate simulation of metabolic networks.\\n","Decision":"To mininimise the number of parameters and reducing sampling time,\\nthe linear rate law was selected (see [1] for a review). The linear mechanism\\nis approximately correct when the following assumptions are satisfied:\\n* [Kinase] and [Phosphotase] << [Metabolic Enzyme],\\n* Rapid equilibrium binding between phosphorylation enzymes and metabolic enzyme,\\n* [Kinase] << dissociation constant for Kinase,\\n* [Phosphotase] << dissociation constant for Phosphotse,\\n* Competitive binding of phosphotases and kinases is negligable,\\n* The ATP\/ADP ratio remains approximately constant,\\n* Phosphorylation and Dephosphorylation is an irreversible process.\\nUsing these assumptions the steady state phosphorylated concentration is defined as:\\n`fraction_phosphorylated = alpha \/ (alpha + beta)`.\\nWhere, alpha and beta correspond to the phosphorylation and dephosphorylation\\nrates, which are linear with respect to the kinase and phosphotase concentrations.\\n```\\nalpha = kcat * [Kinase]\\nbeta = kcat * [Phosphatase]\\n```\\nThe activity of the metabolic enzyme is proportional to the dephosphorylated\\namount. To avoid situations where the kinase and phosphatase have the opposite\\nimpact on the target enzymem, Maud will only refer to them as activating\\nand inhibiting enzymes.\\n","tokens":66,"id":240,"Predictions":"Decision Makers should consider the following: nThe process is often conducted using kinases, which phosphorylate enzymes,nand phosphotases, which dephosphorylate the enzyme.nThe process is essential for the accurate simulation of metabolic networks.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"Predictions":"Decision Makers should consider the following options: a). They should consider the following options: b). They should consider the following options: c). They should consider the following options: d). They should consider the following options: e). They should consider the following options: f). They should consider the following options: g). They should consider the following options: h). They should consider the following options: ii. They should consider the following options: iii. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options."}
{"File Name":"Maud\/0016-validating-measured-fluxes.md","Context":"## Context\\nDefining measurements for independent fluxes isn't always clear.\\nThis can occur when you measure more fluxes than there are degrees\\nof freedom in a network.\\nAn example would be this simplified network:\\nA -> B -> C\\nwhere reaction 1 and reaction 2 are dependent, implying that\\nno additional information is achieved by including both.\\nAnother issue is knowing when you do not have enough fluxes\\nmeasured, resulting in an underdetermined system. Due to the\\nBayesian implementation of Maud, these systems are still theoretically\\nresolvable. However, supplementing as much information\\nas possible will likely be beneficial.\\n","Decision":"Identifying underdetermined systems is acomplished by first calculating\\nthe null space of the matrix. This gives the number of degrees of freedom\\nof the system as well. Then we calculate the reduced row echelon form of\\nthe transpose of the null space. The resulting matrix represents the\\nindependent flux pathways through the network as rows. If you take the\\nmeasured subset of reactions and there is a row containing no non-zero\\nentries then the system is not fully described using the current measurements.\\nDetermining if the system is overspecified is achieved by comparing the\\nnumber of measurements to the degrees of freedom. If the number of measurements\\nis larger than the degrees of freedom then the system is overdetermined.\\nIt is possible to both have an underdetermined system which is overspecified\\nby having multiple measurements on dependent paths. It is also possible to\\nrecieve the warning that the system is overspecified by independent measurements.\\nFor instance, a linear pathway where the influx and efflux are both measured.\\nThis is still valid as they are independent measurements.\\n","tokens":136,"id":242,"Predictions":"Decision nAnAnswer:AnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAn"}
{"File Name":"Maud\/0012-unbabalanced_mets-enzymes-as-priors.md","Context":"## Context\\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\\nmodel. Experimental conditions are defined with respect to these values and drains,\\nwhich are already defined as priors. Therefore, our prior knowledge about the\\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\\nthe likelihood evaluations to the prior information.\\nThe benefit of treating priors in this way is that we define a prior on the phenotype\\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\\nare still considered using weakly informative priors (read: within biologically relevant\\nboundaries).\\n","Decision":"Unbalanced metabolites and enzyme concentrations can also be considered as prior distributions\\nrather than likelihood evaluations.\\n","tokens":148,"id":243,"Predictions":"Decision based on the prior knowledge of the boundary conditions.n1n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2"}
{"File Name":"Maud\/0009-gpl.md","Context":"## Context\\nWe need to choose an appropriate license before making Maud public. The main\\ncontenders are the Apache license, which would allow others to use Maud in\\nproprietary software, and the GPL v3 license, which would not.\\n","Decision":"We will use the GPL license but may change to a more permissive license in a\\nfuture release, depending on the circumstances at the time.\\n","tokens":52,"id":244,"Predictions":"Decision (A) The Apache license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.nB) The GPL v3 license should be used for Maud.n"}
{"File Name":"Maud\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":245,"Predictions":"Decision Makers should be able to make the code more stable.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0013-single-simulation-study.md","Context":"## Context\\nWe need a way to run simulation studies in order to verify that our models work\\nand to answer questions about how informative particular measurements and\\npriors are.\\n","Decision":"We will initially make a python script that implements a single simulation\\nstudy, targeting the model at `ecoli_small_experiments.toml`. Based on how\\nthat goes, we can then think about how to make a more general system for\\nrunning arbitrary simulation studies.\\nIt's better to creep towards a simulation study system like this rather than\\njump straight to the general tool because it's quite a big job, and we don't\\nreally know exactly what the general tool will look like yet.\\n","tokens":36,"id":246,"Predictions":"Decision Makers should consider using simulation studies to verify that their models work.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0010-modular-rate-law-only.md","Context":"## Context\\nThe `code_generation.py` module is probably the ugliest part of Maud,\\nparticularly the function `create_fluxes_function`. It could be cleaned up a\\nbit by removing non-modular enzyme mechanisms.\\nNon-modular mechanisms aren't used in any of the models we are currently\\ntesting, but for historical reasons the logic in `code_generation.py` basically\\nassumes that they are the default. The modular rate law is kind of an\\nadd-on. This change would let us move towards switching this around, having the\\nmodular rate law as the default and working in other mechanisms as and when\\nnecessary.\\n","Decision":"We remove non-modular rate laws from the `code_generation.py` module.\\n","tokens":133,"id":247,"Predictions":"Decision (A).nThe code_generation.py module is probably the ugliest part of Maud,nparticularly the function create_fluxes_function. It could be cleaned up anbit by removing non-modular enzyme mechanisms.nNon-modular mechanisms aren't used in any of the models we are currently testing, but for historical reasons the logic in code_generation.py basicallynassumes that they are the default. The modular rate law is kind of annadd-on. This change would let us move towards switching this around, having thenmodular rate law as the default and working in other mechanisms as and whennnecessary.nA Decision (A).nThe code_generation.py module is probably the ugliest part of Maud,nparticularly the function create_fluxes_function. It could be cleaned up anbit by removing non-modular enzyme mechanisms.nNon-modular mechanisms aren't used in any of the models we are currently testing, but for historical reasons the logic in code_generation.py basicallynassumes that they are the default. The modular rate law is kind of annadd-on. This change would let us move towards switching this around, having thenmodular rate law as the default and working in other mechanisms as and whennnecessary.n"}
{"File Name":"Maud\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":248,"Predictions":"We will standardize on some best-in-class tools.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Maud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":249,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"scientific-thesis-template\/0005-use-pdflatex-as-default.md","Context":"## Context and Problem Statement\\nWhich latex compiler to use?\\n* pdflatex\\n* xelatex\\n* lualatex\\n","Decision":"Chosen option: \"pdflatex\", because compiles faster and correct ligatures are required at the final typesetting step only.\\n### Positive Consequences\\n* Faster compile time\\n### Negative Consequences\\n* Using lualatex just before publication might lead to a different layout and additional effort\\n* lualatex's power might not be used at all\\n","tokens":30,"id":250,"Predictions":"Decision (A) The following latex compilers are to be used: pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* xelatexn* lualatexn.n* pdflatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatexn* xelatex"}
{"File Name":"scientific-thesis-template\/0004-use-lualatex-for-correct-ligatures.md","Context":"## Context and Problem Statement\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nSee [english.stackexchange.com](https:\/\/english.stackexchange.com\/q\/50660\/66058) for a long discusisson.\\nSee a German rant on wrong ligatures: <https:\/\/web.archive.org\/web\/20150425155310\/http:\/\/www.mengensatz.de\/blog\/?p=79>.\\nFor instance, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\n### More readings\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures: <https:\/\/tex.stackexchange.com\/questions\/tagged\/ligatures>\\n* Disable ligatures in headings: <https:\/\/tex.stackexchange.com\/q\/198743\/9075>\\n","Decision":"Chosen option: \"lualatex\", because comes out best (see below).\\n","tokens":237,"id":251,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"scientific-thesis-template\/0003-use-minted-for-code-highlithing.md","Context":"## Context and Problem Statement\\nSource code needs to be highlighted\\n","Decision":"Chosen option: \"minted\", because\\n1. Listings is inferior because it doesn't utilize a full lexer\\n2. Minted offers integration with [pygments](http:\/\/pygments.org\/) and is actively maintained.\\n3. The person who took over minted development, [evaluated all alternatives](https:\/\/tex.stackexchange.com\/a\/103471\/9075) and came to the conclusion that minted is the most promising solution.\\nPositive Consequences:\\n* Source is highlighted properly\\nNegative consequences:\\n* Users need to install python and pygments\\n","tokens":13,"id":252,"Predictions":"Decision Makers should have highlighted the source code.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"offender-management-architecture-decisions\/0005-allocation-api-owns-allocation-data.md","Context":"## Context\\nAllocation of Prison Offender Managers is a new process being introduced by\\nOMiC, and it's different from Offender Supervisor allocation (data for which\\nis mostly stored in per-prison spreadsheets). The data which will be created\\nby this process does not live anywhere yet.\\nWe've heard that some prisons are starting to use the new terminology and have\\nstarted to store staff names as POMs against prisoners in NOMIS (although they\\nare not actually using the OMiC model yet). There is no support in NOMIS for\\nthe tiering data or anything else needed to support the allocation process.\\nWe have learned from our allocation discovery that staff around the prison\\nneed to know who is responsible for a prisoner, but there's no indication that\\nstoring that data in NOMIS is a good way of meeting that need.\\nWe don't know much yet about the user needs around handover of responsibility\\nfrom prison to probation. It's possible that there will be a stronger need to\\nstore some allocation data in Delius (for example) so we may need to revisit\\nthis decision in future.\\nHMPPS needs to reduce its dependence on NOMIS over time, and building an\\necosystem of APIs around new datastores will enable data to be used in other\\nservices without it all needing to be in one central database. The keyworker\\nAPI is an existing example of this pattern.\\n","Decision":"The allocation API will store all data created by the allocation process in its\\nown database.\\nWe will not also write allocation data back to NOMIS or any other system.\\n","tokens":297,"id":253,"Predictions":"Decision (A).nAllocation of Prison Offender Managers is a new process being introduced bynOMiC, and it's different from Offender Supervisor allocation (data for whichnis mostly stored in per-prison spreadsheets).nWe've heard that some prisons are starting to use the new terminology and havenstarted to store staff names as POMs against prisoners in NOMIS (although theynare not actually using the OMiC model yet).nWe have learned from our allocation discovery that staff around the prisonnneed to know who is responsible for a prisoner, but there's no indication thatnstoring that data in NOMIS is a good way of meeting that need.nWe have learned from our allocation discovery that staff around the prisonnneed to know who is responsible for a prisoner, but there's no indication thatnstoring that data in NOMIS is a good way of meeting that need.nWe may need to revisit this decision in future.nHMPPS needs to reduce its dependence on NOMIS over time, and building annecosystem of APIs around new datastores will enable data to be used in othernservices without it all needing to be in one central database.nWe may need to revisit this decision in future.nHMPPS needs to reduce its dependence on NOMIS over time, and building annecosystem of APIs around new datastores will enable data to be used in othernservices without it all needing to be in one central database.n"}
{"File Name":"offender-management-architecture-decisions\/0016-use-elite2-api.md","Context":"## Context\\nOur main source of data on prisoners and prison staff is NOMIS. The\\nallocation tool currently uses the Custody API, as decided in [ADR 0006](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0006-use-the-custody-api-to-access-nomis-data.md), to retrieve\\ndata on both offenders and staff.\\nThere are still four APIs into NOMIS providing general data access, with varying\\napproaches to presenting the data and authentication. We still do not want to add\\nto this duplication.\\nAlthough it has been agreed by the HMPPS technical community that we would\\nlike to move all clients to use the Custody API in preference to the other APIs,\\ninitial use of the Custody API has raised some issues. Problems exist\\nwith the locality of data, N+1 API requests, and data that is unavailable\\nthrough the currently published endpoints. These issues are intrinsic to\\nthe current design of the Custody API, and it is unlikely that they will\\nbe resolved or mitigated in the short to medium term. The scale of the work\\nrequired makes it unrealistic that we would be able to deliver this in a\\nrealistic timescale.\\nThe Elite2 API, is under more active development, is entirely owned by the\\nSheffield team and provides endpoints more focused on the need of clients.\\nMost of the endpoints available work on the caseload of the connecting\\nclient token, but also support the provision of more specific parameters\\nto handle alternate needs. Although we will not be authenticating with\\nthe token of the eventual end user, enough flexibility exists for us to\\nobtain the data we require from Elite2.\\nElite2 provides functionality closer to what we currently require\\nand its design encompasses the need for extra API endpoints for specific\\nservices. Moving to Elite2 is a pragmatic, and tactical approach to\\nresolving the issues around the Custody API to allow us to deliver the\\nallocation component of MOiC. This decision doesn't invalidate the\\noverall agreed strategy of moving HMPPS services to the Custody API\\nover time, but it highlights that more coordinated work is needed to\\nachieve that than we are able to take on ourselves.\\n","Decision":"We will use the Elite2 API for access to the data we require from NOMIS.\\nWe will work with the team in Sheffield on development of the Elite2 API to\\nadd support for accessing the data we need, in the structure that we need it.\\n","tokens":471,"id":255,"Predictions":"Decision based on the present design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n2 Decision based on the current design of the Custody API and the need for additional API endpoints for specific services.n"}
{"File Name":"offender-management-architecture-decisions\/0011-use-nomis-oauth-server2-for-allocation-api-authentication.md","Context":"## Context\\nWe need to protect the allocation API with authentication, but we'd rather not\\nhave to come up with an approach to do that ourselves from scratch.\\nThe new [NOMIS OAuth2 server](https:\/\/github.com\/ministryofjustice\/nomis-oauth2-server)\\nis already being used in production for authentication on almost all of the\\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\\nauthenticate with the Custody API, and the other services which may need to use\\nthe allocation API are very likely to already be using this authentication\\nmethod for the other APIs they use.\\nClients can use one token (of a particular grant type) to authenticate with all\\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\\nthose services - they don't have to work with multiple different authentication\\napproaches.\\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\\nservices can verify the integrity and authenticity of tokens presented by\\nclients using the corresponding public key.\\nWe've decided that the allocation manager will be entirely responsible for user\\naccess control and will call other APIs directly, and the allocation API will\\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\\nThat means that the allocation API doesn't need to know which user it's\\nreturning data for, and we can use a system-to-system approach to\\nauthentication.\\nWe don't know of any other shared approaches to API authentication which are\\nused in the prison space.\\n","Decision":"We will use the NOMIS OAuth2 server for authentication on the allocation API.\\nWe will use the client credentials OAuth2 grant type for authentication on the\\nallocation API.\\nWe will verify signatures on presented tokens in the allocation API.\\nWe will respect expiration times on presented tokens in the allocation API.\\n","tokens":337,"id":257,"Predictions":"Decision (A). The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nC. The allocation manager will be responsible for user access control.nD. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nC. The allocation manager will be responsible for user access control.nD. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nC. The allocation manager will be responsible for user access control.nD. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nC. The allocation manager will be responsible for user access control.nD. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nC. The allocation manager will be responsible for user access control.nD. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nC. The allocation manager will be responsible for user access control.nD. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nA. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be responsible for user access control.nB. The allocation manager will be"}
{"File Name":"offender-management-architecture-decisions\/0017-database-migrations.md","Context":"## Context\\nWe have reached a point where we are going to start reading and writing data to our Allocation API application database and will need to manage database migrations when adding tables, columns and so forth.  This ADR decision outlines the agreed upon approach that the team will take for managing these migrations.\\n","Decision":"We will use blue-green deployments split into the following steps (using adding column as an example):\\n* Add a database migration that inserts the new column\\n* Update the application so that all new data gets written to new column\\n* Run a task to copy all the data from the old column to the new column\\n* Update the application so that it reads from the new column\\n* Add a database migration that removes the old column\\nWe will also ensure that any migrations include 'up' and 'down' methods, rather than just 'change' to avoid any situations where Rails doesn't know how to handle the inverse of the up or down.\\n","tokens":61,"id":258,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"offender-management-architecture-decisions\/0010-allocation-api-has-less-responsibility.md","Context":"## Context\\nWhen we decided to start off with separate API and user-facing applications\\n(see [ADR 0004](0004-separate-api-and-user-facing-applications.md)) we thought\\nthat we would need to build more than one service as part of this work, to\\nmeet different needs of different users. That may still be true in general, but\\nfrom what we've learned in the Handover discovery it's looking more likely at\\nthe moment that the Handover phase of our work will involve expanding on\\nAllocations rather than being an entirely separate journey and service.\\nWe were also intending that the allocation manager would be a fairly minimal\\nuser-facing application, which would fetch all the data it needed from the\\nallocation API and then render pages based directly on it.\\nWe've been looking at how we should set up authentication on our two\\napplications and have realised that that split of responsibilities between the\\nallocation manager and API could lead to some confusions:\\n- Calling the Custody API from the allocation API means that the allocation API\\ncan be seen as sitting in two different layers depending on the context:\\neither above the Custody API from the point of view of the allocation manager,\\nbut on the same level as the Custody API from the point of view of other\\nservices which need to use allocation data. This lack of clarity raises\\nquestions around which OAuth2 grant type to use for authentication on the\\nallocation API.\\n- Responsibility for access control may be split between the allocation\\nmanager and API. The user-facing application would need to know what the user\\nshould have access to in order to show the right options, but the API may\\nalso need to know what the user should have access to in order to fetch the\\nright data from other APIs.\\nWe still think there's value in having a separate allocation API, but it would\\nbe easier to understand if it were a more RESTful interface to the data it\\nstores.\\n","Decision":"The allocation API will be a simpler interface to the data held in its database.\\nThe allocation API will not call other APIs to return data from them to the\\nallocation manager.\\nThe allocation manager will call the allocation API and other APIs (Custody,\\nDelius etc) directly.\\nThe allocation manager will be entirely responsible for access control for its\\nusers.\\n","tokens":405,"id":260,"Predictions":"Decision (A). The allocation manager should be a minimal user-facing application.nWe still think there's value in having a separate allocation API, but it wouldnbe easier to understand if it were a more RESTful interface to the data itnstores.nWe still think there's value in having a separate allocation API, but it wouldnbe easier to understand if it were a more RESTful interface to the data itnstores.nWe still think there's value in having a separate allocation API, but it wouldnbe easier to understand if it were a more RESTful interface to the data itnstores.nWe still think there's value in having a separate allocation API, but it wouldnbe easier to understand if it were a more RESTful interface to the data itnstores.nWe still think there's value in having a separate allocation API, but it wouldnbe easier to understand if it were a more RESTful interface to the data itnstores.nWe still think there's value in having a separate allocation API, but it wouldnbe easier to understand if it were a more RESTful interface to the data itnstores.nWe still think there's value in having a separate allocation API, but it wouldnbe easier to understand if it were a more RESTful interface to the data itnstores.n"}
{"File Name":"offender-management-architecture-decisions\/0004-separate-api-and-user-facing-applications.md","Context":"## Context\\nWe are moving into beta on allocations, which is the first area of our work on\\nManage Offenders in Custody. The team is now also in discovery on handover, and\\nthere are more areas which we know very little about at this stage which we\\nintend to look at in future.\\nWe anticipate that we will build more than one product as part of this work, to\\nmeet different needs of different users. That means that we're very likely to\\nbuild more than one application as part of this larger service area in future.\\nWe need to decide now whether we should start out by building one application\\nfor allocations, or follow the common pattern of separating an API layer from\\npresentation.\\n### Microservices\\nThere are many advantages to a microservices architecture, but also some costs.\\nIf we start off with more than one application, the decisions we make early on\\nabout their responsibilities may take more work to change later on, as we learn\\nmore.\\nOn the other hand, if we start off with one application and decide later that\\nwe want to split it into several, that work is likely to be substantial. Given\\nthe timescales involved in OMiC, it may be hard to prioritise paying off that\\ntechnical debt when it becomes a burden.\\nWe anticipate that other services will need to use allocation data, and so we\\nare likely to need to make it accessible via APIs at some stage anyway. Those\\nservices may need to access allocation data in different ways from our\\nfrontend, so more work may be needed then to support them even if we do have\\nan API from the start.\\n### Operating microservices\\nMicroservices can introduce more operational complexity by moving function\\ncalls to network calls and by introducing more moving parts which need to be\\ndeployed. However we expect the number of microservices to remain relatively\\nsmall so do not anticipate this complexity to be significant for us.\\nOur choice of the Kubernetes-based Cloud Platform for hosting (see [ADR 0002](0002-use-cloud-platform-for-hosting.md))\\nshould make it easier to scale small services efficiently and independently.\\nIt also gives us easy access to tooling designed to support many small\\nservices.\\nWe expect our decision on how far we should follow a microservices approach to\\nhave minimal impact on our hosting costs.\\n","Decision":"We will start off with two applications for allocation:\\n- an allocation API which will call other APIs (to read data from NOMIS, and\\nin future Delius and OASys)\\n- an allocation frontend which will call the allocation API and serve\\nprogressively enhanced HTML to users (see [ADR 0003](0003-use-progressive-enhancement.md))\\n","tokens":491,"id":261,"Predictions":"We should start by building one application for allocations, or follow the common pattern of separating an API layer from presentation.n### MicroservicesnWe anticipate that we will build more than one application as part of this work, tonmeet different needs of different users.nWenintend to look at in future.nWe need to decide now whether we should start out by building one application for allocations, or follow the common pattern of separating an API layer fromnpresentation.n### Operating microservicesnWe expect the number of microservices to remain relatively relatively small so do not anticipate this complexity to be significant for us.nWe expect our decision on how far we should follow a microservices approach to have minimal impact on our hosting costs.n### Decision nA Decision nImportant: We are moving into beta on allocations, which is the first area of our work on Manage Offenders in Custody. The team is now also in discovery on handover, andnthere are more areas which we know very little about at this stage which wenintend to look at in future.nWe need to decide now whether we should start out by building one application for allocations, or follow the common pattern of separating an API layer fromnpresentation.n### MicroservicesnThere are many advantages to a microservices architecture, but also some costs.nIf we start off with more than one application and decide later thatnwe want to split it into several, that work is likely to be substantial. Given the timescales involved in OMiC, it may be hard to prioritise paying off thatntechnical debt when it becomes a burden.nWe anticipate that other services will need to use allocation data, and so wenare likely to need to make it accessible via APIs at some stage anyway.nWe anticipate that the number of microservices to remain relatively relatively small so do not anticipate this complexity to be significant for us.nWe expect our decision on how far we should follow a microservices approach to have minimal impact on our hosting costs.n### Decision n"}
{"File Name":"offender-management-architecture-decisions\/0018-testing-tls-mutual-auth-for-delius-api-access.md","Context":"## Context\\nAs documented in [ADR 0014](0014-access-the-delius-api-via-ndh.md) we are\\ngoing to access the Delius API via the NOMIS data hub using TLS mutual auth,\\nmanaged on our side by a sidecar container.\\nWe need to find a way of testing our auth setup - in particular that it only\\nallows access to the Delius API from our allocation manager. The NDH only\\nexists in production, so we need to decide how to test this setup and\\ndeployment of the sidecar in our other environments (at the moment we only\\nhave staging and production). We want staging to be as similar as possible to\\nproduction so that we're testing changes in a realistic environment before\\npushing them to production.\\n","Decision":"We will set up a separate namespace on the cloud platform for a Delius staging\\nenvironment.\\nWe will deploy the NDH-side mutual auth container into that environment, along\\nwith the Delius API with a dataset constructed to match T3 NOMIS data.\\nInitially our main concern is testing the mutual auth setup so instead of the\\nDelius API we could deploy a simple HTTP server if that's easier in the short\\nterm.\\nWe will deploy the sidecar in our staging environment and connect to our Delius\\nstaging environment from it.\\n","tokens":164,"id":262,"Predictions":"Decision (A) To test the setup of the Delius API via the NOMIS data hub. (B) To test the setup of the Delius API via the NOMIS data hub. (C) To test the setup of the Delius API via the NOMIS data hub. (D) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (B) To test the setup of the Delius API via the NOMIS data hub. (C) To test the setup of the Delius API via the NOMIS data hub. (D) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (A) To test the setup of the Delius API via the NOMIS data hub. (B) To test the setup of the Delius API via the NOMIS data hub."}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"Predictions":"Decision (A).nThe service manual states that teams should use progressive enhancementnwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancementnThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive"}
{"File Name":"offender-management-architecture-decisions\/0008-use-rails.md","Context":"## Context\\nWe have already decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\\nThe team are already very familiar with Rails and it is widely used within MOJ.\\n","Decision":"We will use Rails as our web framework for our new applications.\\n","tokens":61,"id":266,"Predictions":"Decision (A) The team should use Ruby for their new applications. (B) The team should use Rails for their new applications. (C) The team should use Rails for their new applications. (D) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (B) The team should use Rails for their new applications. (C) The team should use Rails for their new applications. (D) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (B) The team should use Rails for their new applications. (C) The team should use Rails for their new applications. (D) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (B) The team should use Rails for their new applications. (C) The team should use Rails for their new applications. (D) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (A) The team should use Rails for their new applications. (B) The team should use Rails for their new applications. (C) The team should use Rails for their new applications."}
{"File Name":"offender-management-architecture-decisions\/0006-use-the-custody-api-to-access-nomis-data.md","Context":"## Context\\nOur main source of data on prisoners and prison staff which we need for\\nallocations is NOMIS.\\nThere are now four APIs into NOMIS providing general data access, with varying\\napproaches to presenting the data and authentication. We do not want to add to\\nthis duplication.\\nThe APIs which have been developed more recently are more under the control of\\nHMPPS than the earlier ones (which were developed by a supplier). That gives us\\nmore flexibility around how we work with them and makes it possible to get\\nchanges into production more quickly. Using one of the newer APIs should mean\\nthat we are less blocked by delays around API changes than we have been on our\\nwork on Visit someone in prison.\\nIt has been agreed by the HMPPS technical community that we would like to move\\nall clients to use the Custody API in preference to the other APIs over time.\\nAlthough that work has not yet been prioritised, using the Custody API for new\\napplications will reduce the work needed in future to align our API usage.\\nThe Custody API has been designed to give a more direct view of the data in\\nNOMIS than the previous APIs have been - earlier approaches have favoured\\nimplementing specific endpoints to meet the needs of service teams rather than\\ngiving a more comprehensive view of all the data.\\n","Decision":"We will use the Custody API to access the NOMIS data we need.\\nWe will work with the team in Sheffield on development of the Custody API to\\nadd support for accessing the data we need.\\n","tokens":277,"id":267,"Predictions":"Decision (A). The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nC. The Custody API should be used for new applications.nD. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nC. The Custody API should be used for new applications.nD. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nC. The Custody API should be used for new applications.nD. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nC. The Custody API should be used for new applications.nD. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nC. The Custody API should be used for new applications.nD. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nC. The Custody API should be used for new applications.nD. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.nA. The Custody API should be used for new applications.nB. The Custody API should be used for new applications.n"}
{"File Name":"offender-management-architecture-decisions\/0009-use-circleci-for-ci-and-deployment.md","Context":"## Context\\nWe need to choose tooling for various continuous integration and deployment\\ntasks:\\n- running automated tests on branches\/pull requests\\n- running security and code quality checks on branches\/pull requests\\n- building Docker images\\n- deploying applications to our environments\\nJenkins and CircleCI are commonly used at MOJ for these tasks, and the team\\nhave experience with both of them, as well as other options such as Travis.\\nThe Cloud Platform team operate a Jenkins server but are keen to move services\\naway from it, especially anything which doesn't use Template Deploy.\\nThe LAA fee calculator is built and deployed using CircleCI with the new cloud\\nplatform, so we'd have an example to follow if we also chose CircleCI. That\\nteam didn't evaluate a lot of options when they made that decision, but it's\\nworking well so far for them. Our applications are a little more complex than\\ntheirs since we have a database, but we don't expect to be doing anything\\nunusual in our build and deploy pipelines.\\nThere's also some [documentation](https:\/\/ministryofjustice.github.io\/cloud-platform-user-docs\/02-deploying-an-app\/004-use-circleci-to-upgrade-app\/)\\non using CircleCI with Helm for continuous deployment to the new cloud\\nplatform - we may not want to follow it exactly, but it covers some useful\\ntopics.\\nOur team haven't been practising continuous deployment (that is, deploying\\nevery change automatically to production) on Visit someone in prison - deploys\\nto production for that service need to be triggered by a human. We'd like to\\nkeep that option open for this service, though.\\nIt's important to us that our build and deployment configuration is managed in\\ncode. We've decided to start out with two applications (see [ADR 0004](0004-separate-api-and-user-facing-applications.md))\\nand want to be able to easily make and reproduce changes to builds and\\ndeployments for both.\\nWe're keen to get started quickly - we'd rather not spend time assessing lots\\nof tooling options at this stage. Our needs will evolve anyway, so it's fine\\nto pick something that works for us now and revisit that decision later on if\\nwe need to.\\n","Decision":"We will use CircleCI for continuous integration.\\nWe will use CircleCI for deploying.\\n","tokens":482,"id":269,"Predictions":"We want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Visit someone in prison.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Helm for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.nWe want to use CircleCI with Visit someone in prison for continuous deployment.n"}
{"File Name":"offender-management-architecture-decisions\/0020-unify-api-and-presentation-layer.md","Context":"## Context\\nPreviously, it was decided in [ADR 0004](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0004-separate-api-and-user-facing-applications) that we would separate data and presentation concerns.  This however was reversed by [ADR 0010](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0010-allocation-api-has-less-responsibility) which meant there was an overlap of data concerns with some data access from the presentation layer, and some via the Allocation API.\\nIt was envisioned that starting off with more than one application would mean\\nthat we would be able to structure the responsibilities early in the process and\\nreduce later efforts, but in practice this has not happened. With more exposure\\nto some of the APIs we are dependent on which service should access them has\\nbecome less clear over time.  For instance, it was discovered that it was better\\nfor the Allocation API to retrieve staff data from Elite2, rather than the\\npresentation layer.\\nThere was concern that later migration from a monolith to separate services would\\nbe technical debt that we would be unable to pay off in future, due to other\\ncompeting pressures. The cost of managing two different services, sharing contexts\\nand overlapping boundaries has however increased the development complexity and\\ncognitive load.\\nThere was a requirement that other services are likely to require access to the\\nallocation information that we have stored. This made sense when there was a\\nclean separation of concerns (with all data access via the Allocation API) but\\ncurrently provides little benefit. Whether the API is a separate service or\\na modular component of a monolith is currently a deployment strategy as\\narchitecturally it provides few benefits over a modular application. It is\\nentirely possible to provide an API via a modular monolith.\\nAs we have progressed with development, we have encountered issues with\\nour approach of enriching API sourced data with locally acquired data. Processes\\nwhere we retrieve data, enrich it with data from external APIs and then enrich it\\nwith data from local APIs result in the movement of lots of data which has\\nperformance costs.  Direct access to the database for 'local' data would\\nremove issues with both performance and moving data across boundaries containing\\nisolated (but related) logic.\\n","Decision":"We will integrate the existing Allocation API into the Allocation Manager, and make\\nthe public api available at \/api.\\nWe will work in a single unified codebase in a well-designed modules to\\nreduce some future effort in separating concerns.\\n","tokens":498,"id":270,"Predictions":null}
{"File Name":"offender-management-architecture-decisions\/0002-use-cloud-platform-for-hosting.md","Context":"## Context\\nThe team are familiar with the [Template Deploy](https:\/\/github.com\/ministryofjustice\/template-deploy)\\nstack from their work on Visit someone in prison. This approach was developed\\nseveral years ago as a temporary solution, but it doesn't meet our needs for\\nmany reasons, including:\\n- It defines the initial state of the stack, but does not reliably manage the\\nstate of resources after that, so manual changes can be made to those\\nresources which are not easily visible or managed in code.\\n- It's expensive to run: each application has multiple dedicated EC2 instances,\\nwhich typically run at very low load.\\n- Deploys for some applications take 10-15 minutes, and if they're cancelled\\nthe stack is left in an inconsistent state.\\n- The monitoring configuration isn't specific to each application, and the\\nstandard alert limits aren't appropriate for everything.\\n- Few people understand how it works.\\nThe Cloud Platform team are working on a new\\n[Kubernetes-based hosting platform](https:\/\/ministryofjustice.github.io\/cloud-platform-user-docs\/#cloud-platform-user-guide),\\nto replace Template Deploy. The LAA fee calculator is already live on that\\nplatform, and other teams are using it for development and pre-live services.\\nThis platform is MOJ D&T's strategic hosting choice.\\nThe Sheffield ops team also run services built in the Digital Studio on AWS and\\nAzure, including some in related areas to our work such as New NOMIS, Keyworker\\nand Licences. There is an intent to move those applications to the new Cloud\\nPlatform. Our team aren't familiar with those stacks.\\n","Decision":"We will use the Cloud Platform for hosting our applications.\\n","tokens":341,"id":271,"Predictions":"Decision (A). The [Template Deploy](https:\/\/github.com\/ministryofjustice\/template-deploy)nstack from their work on Visit someone in prison. This approach was developed several years ago as a temporary solution, but it doesn't meet our needs fornmany reasons, including:n- It defines the initial state of the stack, but does not reliably manage thenstate of resources after that, so manual changes can be made to thosenresources which are not easily visible or managed in code.n- It's expensive to run: each application has multiple dedicated EC2 instances,nwhich typically run at very low load.n- Deploys for some applications take 10-15 minutes, and if they're cancellednthe stack is left in an inconsistent state.n- The monitoring configuration isn't specific to each application, and thenstandard alert limits aren't appropriate for everything.n- Few people understand how it works.nThe Cloud Platform team are working on a new cloud platform. The Cloud Platform team also run services built in the Digital Studio on AWS and Azure, including some in related areas to our work such as New NOMIS, Keyworkernand Licences. There is an intent to move those applications to the new Cloud Platform. Our team aren't familiar with those stacks."}
{"File Name":"offender-management-architecture-decisions\/0001-record-architecture-decisions.md","Context":"## Context\\nDuring our work on Manage Offenders in Custody, we will have to make\\narchitectural decisions around both design and scope of the work.\\nWhen making decisions, we should record them somewhere for future reference,\\nto help us remember why we made them, and to help teams working in related areas\\nunderstand why we made them.\\nWe should make our decisions public so that other teams can find them more\\neasily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).\\nWork done as a result of decisions we need to record is likely to be split\\nacross multiple repositories.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAn architecture decision record is a short text file describing a single\\ndecision.\\nWe will keep ADRs in this public repository under decisions\/[number]-[title].md\\nWe should use a lightweight text formatting language like Markdown.\\nADRs will be numbered sequentially and monotonically. Numbers will not be\\nreused.\\nIf a decision is reversed, we will keep the old one around, but mark it as\\nsuperseded. (It's still relevant to know that it was the decision, but is no\\nlonger the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest:\\n**Title** These documents have names that are short noun phrases. For example,\\n\"ADR 1: Record architectural decisions\" or \"ADR 9: Use Docker for deployment\"\\n**Status** A decision may be \"proposed\" if it's still under discussion, or\\n\"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it\\nmay be marked as \"deprecated\" or \"superseded\" with a reference to its\\nreplacement.\\n**Context** This section describes the forces at play, including technological,\\npolitical, social, and local to the service. These forces are probably in\\ntension, and should be called out as such. The language in this section is\\nvalue-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated\\nin full sentences, with active voice. \"We will ...\"\\n**Consequences** This section describes the resulting context, after applying\\nthe decision. All consequences should be listed here, not just the \"positive\"\\nones. A particular decision may have positive, negative, and neutral\\nconsequences, but all of them affect the team and service in the future.\\nThe whole document should be one or two pages long. We will write each ADR as\\nif it is a conversation with a future person joining the team. This requires\\ngood writing style, with full sentences organised into paragraphs. Bullets are\\nacceptable only for visual style, not as an excuse for writing sentence\\nfragments.\\n[adr-tools](https:\/\/github.com\/npryce\/adr-tools) can help us work with our ADRs\\nconsistently.\\nWe will link to these ADRs from other documentation where relevant.\\n","tokens":153,"id":272,"Predictions":"We should make our decisions public so that other teams can find them more easily.nWe should make our decisions public so that other teams can find them more easily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).nWe should make our decisions public so that other teams can find them more easily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).nWe should make our decisions public so that other teams can find them more easily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).nWe should make our decisions public so that other teams can find them more easily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).nWe should make our decisions public so that other teams can find them more easily, and because [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).n"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"Predictions":"Decision : The most common containers should be supported.n# Considered Optionsn* [pdfbox](https:\/\/pdfbox.apache.org\/) libraryn* [imagemagick](https:\/\/www.imagemagick.org\/) external commandn* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external commandn* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external commandnThere are no screenshots here, because it doesn't make sense sincenthey all look the same on the screen. Instead we look at the filesnpropertiesn.n**Input File**nThe input files are:nn$ identify input\/*ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000nn# Considered Optionsn* [pdfbox](https:\/\/pdfbox.apache.org\/) libraryn* [imagemagick](https:\/\/www.imagemagick.org\/) external commandn* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external commandn* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external commandnThere are no screenshots here, because it doesn't make sense sincenthey all look the same on the screen. Instead we look at the filesnpropertiesn.n**Input File**nThe input files are:nn$ identify input\/*ninput\/jfif.jpg JPEG 2480x3514 +0+0 8-bit sRGB 240229B 0.000u 0:00.000ninput\/letter-en.jpg JPEG 1695x2378 +0+0 8-bit Gray 256c 467341B 0.000u 0:00.000ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000ninput\/letter-en.tiff TIFF 1695x2378 +0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000nn# Decision : The most common containers should be supported.n# Considered Optionsn* [pdfbox](https:\/\/pdfbox.apache.org\/) libraryn* [imagemagick](https:\/\/www.imagemagick.org\/) external commandn* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external commandn* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external commandnThere are no screenshots here, because it doesn't make sense sincenthey all look the same on the screen. Instead we look at the filesnpropertiesn.n**Input File**nThe input files are:nn$ identify input\/*ninput\/jfif.jpg"}
{"File Name":"docspell\/0002_component_interaction.md","Context":"# Context and Problem Statement\\nThere are multiple web applications with their rest servers and there\\nare multiple document processors. These processes must communicate:\\n- once a new job is added to the queue the rest server must somehow\\nnotify processors to wake up\\n- once a processor takes a job, it must propagate the progress and\\noutcome to all rest servers only that the rest server can notify the\\nuser that is currently logged in. Since it's not known which\\nrest-server the user is using right now, all must be notified.\\n# Considered Options\\n1. JMS (ActiveMQ or similiar): Message Broker as another active\\ncomponent\\n2. Akka: using a cluster\\n3. DB: Register with \"call back urls\"\\n# Decision Outcome\\nChoosing option 3: DB as central synchronisation point.\\nThe reason is that this is the simplest solution and doesn't require\\nexternal libraries or more processes. The other options seem too big\\nof a weapon for the task at hand. They are both large components\\nitself and require more knowledge to use them efficiently.\\nIt works roughly like this:\\n- rest servers and processors register at the database on startup each\\nwith a unique call-back url\\n- and deregister on shutdown\\n- each component has db access\\n- rest servers can list all processors and vice versa\\n","Decision":"Choosing option 3: DB as central synchronisation point.\\nThe reason is that this is the simplest solution and doesn't require\\nexternal libraries or more processes. The other options seem too big\\nof a weapon for the task at hand. They are both large components\\nitself and require more knowledge to use them efficiently.\\nIt works roughly like this:\\n- rest servers and processors register at the database on startup each\\nwith a unique call-back url\\n- and deregister on shutdown\\n- each component has db access\\n- rest servers can list all processors and vice versa\\n","tokens":276,"id":274,"Predictions":"DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: JMS (ActiveMQ or similiar)n# Decision Makingn# Choosing option 2: Akkan# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: JMS (ActiveMQ or similiar)n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 1: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 2: DB as central synchronisation point.n# Decision Makingn# Choosing option 3: DB as central synchronisation point.n# Decision Making"}
{"File Name":"docspell\/0014_fulltext_search_engine.md","Context":"# Context and Problem Statement\\nTo allow searching the documents contents efficiently, a separate\\nindex is necessary. The \"defacto standard\" for fulltext search on the\\nJVM is something backed by [Lucene](https:\/\/lucene.apache.org).\\nAnother option is to use a RDBMS that supports fulltext search.\\nThis adds another component to the mix, which increases the complexity\\nof the setup and the software. Since docspell works great without this\\nfeature, it shouldn't have a huge impact on the application, i.e. if\\nthe fulltext search component is down or broken, docspell should still\\nwork (just the fulltext search is then not working).\\n# Considered Options\\n* [Apache SOLR](https:\/\/solr.apache.org)\\n* [ElasticSearch](https:\/\/www.elastic.co\/elasticsearch\/)\\n* [PostgreSQL](https:\/\/www.postgresql.org\/docs\/12\/textsearch.html)\\n* All of them or a subset\\n# Decision Outcome\\nIf docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\nSOLR. This enables all users to use the fulltext search feature. As a\\nlater step, an implementation based on PostgreSQL and\/or ElasticSearch\\ncould be provided, too.\\n","Decision":"If docspell is running on PostgreSQL, it would be nice to also use it\\nfor fulltext search to save the cost of running another component. But\\nI don't want to lock the database to PostgreSQL *only* because of the\\nfulltext search feature.\\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\\npart of Lucene and therefore lives in the Apache ecosystem. I would\\nchoose SOLR over ElasticSearch, because I used it before.\\nThe last option (supporting all) is interesting, since it would enable\\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\\nthe database for docspell.\\nIn a first step, identify what docspell needs from a fulltext search\\ncomponent and create this interface and an implementation for Apache\\nSOLR. This enables all users to use the fulltext search feature. As a\\nlater step, an implementation based on PostgreSQL and\/or ElasticSearch\\ncould be provided, too.\\n","tokens":416,"id":275,"Predictions":"docspell should be able to use PostgreSQL for fulltext search.n# Decision Makingnnn#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:An#nAnswer:AnAnswer:An"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"Predictions":"The main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n# Decision MakingnThe main components of the application should be arranged in a manner that is consistent with the goal of the application.n"}
{"File Name":"docspell\/0015_convert_pdf_files.md","Context":"# Context and Problem Statement\\nSome PDFs contain only images (when coming from a scanner) and\\ntherefore one is not able to click into the pdf and select text for\\ncopy&paste. Also it is not searchable in a PDF viewer. These are\\nreally shortcomings that can be fixed, especially when there is\\nalready OCR build in.\\nFor images, this works already as tesseract is used to create the PDF\\nfiles. Tesseract creates the files with an additional text layer\\ncontaining the OCRed text.\\n# Considered Options\\n* [ocrmypdf](https:\/\/github.com\/jbarlow83\/OCRmyPDF) OCRmyPDF adds an\\nOCR text layer to scanned PDF files, allowing them to be searched\\n","Decision":"Add ocrmypdf as an optional conversion from PDF to PDF. Ocrmypdf is\\ndistributed under the GPL-3 license.\\n","tokens":154,"id":277,"Predictions":"Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A). OCRmyPDF adds annOCR text layer to scanned PDF files, allowing them to be searchedn ## Decision (A)."}
{"File Name":"docspell\/0003_encryption.md","Context":"# Context and Problem Statement\\nSince docspell may store important documents, it should be possible to\\nencrypt them on the server. It should be (almost) transparent to the\\nuser, for example, a user must be able to login and download a file in\\nclear form. That is, the server must also decrypt them.\\nThen all users of a collective should have access to the files. This\\nrequires to share the key among users of a collective.\\nBut, even when files are encrypted, the associated meta data is not!\\nSo especially access to the database would allow to see tags,\\nassociated persons and correspondents of documents.\\nSo in short, encryption means:\\n- file contents (the blobs and extracted text) is encrypted\\n- metadata is not\\n- secret keys are stored at the server (protected by a passphrase),\\nsuch that files can be downloaded in clear form\\n# Decision Drivers\\n* major driver is to provide most possible privacy for users\\n* even at the expense of less features; currently I think that the\\nassociated meta data is enough for finding documents (i.e. full text\\nsearch is not needed)\\n# Considered Options\\nIt is clear, that only blobs (file contents) can be encrypted, but not\\nthe associated metadata. And the extracted text must be encrypted,\\ntoo, obviously.\\n","Decision":"* major driver is to provide most possible privacy for users\\n* even at the expense of less features; currently I think that the\\nassociated meta data is enough for finding documents (i.e. full text\\nsearch is not needed)\\n# Considered Options\\nIt is clear, that only blobs (file contents) can be encrypted, but not\\nthe associated metadata. And the extracted text must be encrypted,\\ntoo, obviously.\\nNo encryption, because of its complexity.\\nFor now, this tool is only meant for \"self deployment\" and personal\\nuse. If this changes or there is enough time, this decision should be\\nreconsidered.\\n","tokens":279,"id":278,"Predictions":"Decision nAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:AnAnswer:A"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"Predictions":"Decision (A) The current job table should be a separate table for periodic tasks. (B) The current job table should be a separate table for periodic tasks. (C) The current job table should be a separate table for periodic tasks. (D) The current job table should be a separate table for periodic tasks. (A) The current job table should be a separate table for periodic tasks. (A) The current job table should be a separate table for periodic tasks. (B) The current job table should be a separate table for periodic tasks. (C) The current job table should be a separate table for periodic tasks. (D) The current job table should be a separate table for periodic tasks. (A) The current job table should be a separate table for periodic tasks. (A) The current job table should be a separate table for periodic tasks. (B) The current job table should be a separate table for periodic tasks. (C) The current job table should be a separate table for periodic tasks. (D) The current job table should be a separate table for periodic tasks. (A) The current job table should be a separate table for periodic tasks. (A) The current job table should be a separate table for periodic tasks."}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"Predictions":"I go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n- The UI is written with Elm, it's probably good to adopt theirnstylen ## Decision MakernI go with the epoch time. Every timestamp\/date-time values isntransfered as Unix timestamp.nReasons:n"}
{"File Name":"docspell\/0006_more-file-types.md","Context":"# Context and Problem Statement\\nDocspell currently only supports PDF files. This has simplified early\\ndevelopment and design a lot and so helped with starting the project.\\nHandling pdf files is usually easy (to view, to extract text, print\\netc).\\nThe pdf format has been chosen, because PDFs files are very common and\\ncan be viewed with many tools on many systems (i.e. non-proprietary\\ntools). Docspell also is a document archive and from this perspective,\\nit is important that documents can be viewed in 10 years and more. The\\nhope is, that the PDF format is best suited for this. Therefore all\\ndocuments in Docspell must be accessible as PDF. The trivial solution\\nto this requirement is to only allow PDF files.\\nSupport for more document types, must then take care of the following:\\n- extracting text\\n- converting into pdf\\n- access original file\\nText should be extracted from the source file, in case conversion is\\nnot lossless. Since Docspell can already extract text from PDF files\\nusing OCR, text can also be extracted from the converted file as a\\nfallback.\\nThe original file must always be accessible. The main reason is that\\nall uploaded data should be accessible without any modification. And\\nsince the conversion may not always create best results, the original\\nfile should be kept.\\n# Decision Drivers\\nPeople expect that software like Docspell support the most common\\ndocument types, like all the \u201coffice documents\u201d (`docx`, `rtf`, `odt`,\\n`xlsx`, \u2026) and images. For many people it is more common to create\\nthose files instead of PDF. Some (older) scanners may not be able to\\nscan into PDF files but only to image files.\\n# Considered Options\\nThis ADR does not evaluate different options. It rather documents why\\nthis feature is realized and the thoughts that lead to how it is\\nimplemented.\\n# Realization\\n","Decision":"People expect that software like Docspell support the most common\\ndocument types, like all the \u201coffice documents\u201d (`docx`, `rtf`, `odt`,\\n`xlsx`, \u2026) and images. For many people it is more common to create\\nthose files instead of PDF. Some (older) scanners may not be able to\\nscan into PDF files but only to image files.\\n# Considered Options\\nThis ADR does not evaluate different options. It rather documents why\\nthis feature is realized and the thoughts that lead to how it is\\nimplemented.\\n# Realization\\n","tokens":410,"id":281,"Predictions":"The main reason is that documents can be viewed in 10 years and more. Thenhope is, that the PDF format is best suited for this. Therefore allndocuments in Docspell must be accessible as PDF. The trivial solutionnto this requirement is to only allow PDF files.nSupport for more document types, must then take care of the following:n- extracting textn- converting into pdfn- access original filenText should be extracted from the source file, in case conversion isnnot lossless. Since Docspell can already extract text from PDF filesnusing OCR, text can also be extracted from the converted file as anfallback.nThe original file should always be accessible. The main reason is thatnall uploaded data should be accessible without any modification. Andnsince the conversion may not always create best results, the originalnfile should be kept.n# Considered OptionsnPeople expect that software like Docspell support the most commonndocument types, like all the \u201coffice documents\u201d (docx, rtf, odt,nxlsx,...) and images. For many people it is more common to createnthose files instead of PDF. Some (older) scanners may not be able tonscan into PDF files but only to image files.n# Considered OptionsnThis ADR does not evaluate different options. It rather documents whynthis feature is realized and the thoughts that lead to how it isnimplemented.n# Realizationn"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"docspell\/0013_archive_files.md","Context":"# Context and Problem Statement\\nDocspell should have support for files that contain the actual files\\nthat matter, like zip files and other such things. It should extract\\nits contents automatcially.\\nSince docspell should never drop or modify user data, the archive file\\nmust be present in the database. And it must be possible to download\\nthe file unmodified.\\nOn the other hand, files in there need to be text analysed and\\nconverted to pdf files.\\n# Decision Outcome\\nThere is currently a table `attachment_source` which holds references\\nto \"original\" files. These are the files as uploaded by the user,\\nbefore converted to pdf. Archive files add a subtlety to this: in case\\nof an archive, an `attachment_source` is the original (non-archive)\\nfile inside an archive.\\nThe archive file itself will be stored in a separate table `attachment_archive`.\\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\\n- `attachment_source`: report.jpg\\n- `attachment`: report.pdf\\n- `attachment_archive`: files.zip\\nArchive may contain other archives. Then the inner archives will not\\nbe saved. The archive file is extracted recursively, until there is no\\nknown archive file found.\\n# Initial Support\\nInitial support is implemented for ZIP and EML (e-mail files) files.\\n","Decision":"There is currently a table `attachment_source` which holds references\\nto \"original\" files. These are the files as uploaded by the user,\\nbefore converted to pdf. Archive files add a subtlety to this: in case\\nof an archive, an `attachment_source` is the original (non-archive)\\nfile inside an archive.\\nThe archive file itself will be stored in a separate table `attachment_archive`.\\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\\n- `attachment_source`: report.jpg\\n- `attachment`: report.pdf\\n- `attachment_archive`: files.zip\\nArchive may contain other archives. Then the inner archives will not\\nbe saved. The archive file is extracted recursively, until there is no\\nknown archive file found.\\n# Initial Support\\nInitial support is implemented for ZIP and EML (e-mail files) files.\\n","tokens":285,"id":283,"Predictions":"Decision nThe archive file must be present in the database.nThe archive file must be unmodified.nThe archive file must be stored in a separate table attachment_source.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table attachment_archive.nThe archive file itself will be stored in a separate table "}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"Predictions":"Decision (A) The native view is better than the ndocx or odt document.nB) The native view is better than the ndocx or odt document.nC) The native view is better than the ndocx or odt document.nD) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nB) The native view is better than the ndocx or odt document.nC) The native view is better than the ndocx or odt document.nD) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or odt document.nAnswer (A) The native view is better than the ndocx or"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"Predictions":"Decision (A). The user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be able to add custom metadata to items.nThe user should be "}
{"File Name":"docspell\/0008_convert_plain_text.md","Context":"# Context and Problem Statement\\nHow can plain text and markdown documents be converted into a PDF\\nfiles?\\nRendering images is not important here, since the files must be self\\ncontained when uploaded to Docspell.\\nThe test file is the current documentation page of Docspell, found in\\n`microsite\/docs\/doc.md`.\\n```\\n---\\nlayout: docs\\nposition: 4\\ntitle: Documentation\\n---\\n# {page .title}\\nDocspell assists in organizing large amounts of PDF files that are\\n...\\n","Decision":"Java library \"flexmark\".\\nI think all results are great. It depends on the type of document and\\nwhat one expects to see. I guess that most people expect something\\nlike pandoc-html produces for the kind of files docspell is for (it is\\nnot for newspaper articles, where pandoc-latex would be best fit).\\nBut choosing pandoc means yet another external command to depend on.\\nAnd the results from flexmark are really good, too. One can fiddle\\nwith options and css to make it look better.\\nTo not introduce another external command, decision is to use flexmark\\nand then the already existing html->pdf conversion.\\n","tokens":112,"id":286,"Predictions":"Decision (A) The test file is the current documentation page of Docspell.nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationn---n# page.titlenDocspell assists in organizing large amounts of PDF files that aren...n ## Decision Making nThe test file is the current documentation page of Docspell, found innmicrosite\/docs\/doc.md.nn---nlayout: docsnposition: 4ntitle: Documentationnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"Predictions":"Is it possible to convert HTML documents into a PDF file that looks as much as the original?nIt would be nice to have a java-only solution. But if an external toolnhas a better outcome, then an external tool is fine, too.n# Considered Optionsn* [pandoc](https:\/\/pandoc.org\/) external commandn* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external commandn* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external commandnNative (firefox) view:n figure(file=\"example-html-native.jpg\")n# Considered Optionsn* [pandoc](https:\/\/pandoc.org\/) external commandn* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external commandn* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external commandnNative (firefox) view:n figure(file=\"example-html-native.jpg\")n# Decision Makern# Final Solutionn* [pandoc](https:\/\/pandoc.org\/) external commandn* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external commandn* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external commandnNative (firefox) view:n figure(file=\"example-html-native.jpg\")n# Decision Makern"}
{"File Name":"core\/0002-release-policy.md","Context":"## Context and Problem Statement\\nProject Thoth itself consists of many components all having their own release cycles and delivery artifacts such as\\ncontainer image or Python libraries.\\n","Decision":"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\\nproject\/product level while maintianing freedom of others to update.\\n### Positive Consequences <!-- optional -->\\n* users have a clear base line of versions, these versions have been tested with each other and have\\nundergone integration testing.\\n* a release can be referenced from documents, so that operational procedures have a clear relationship with component\\nversions being used\\n* we can maintain sets of different versions for different deployment environments\\n* we can provide a version string with each API provided by the project\\n### Negative Consequences <!-- optional -->\\n* A release might not contain the latest versions of components\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":33,"id":288,"Predictions":"Decision (A) The components of Project Thoth are not compatible with the existing architecture.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"core\/0003-decommision-qeb-hwt.md","Context":"## Context and Problem Statement\\n[Qeb-Hwt](https:\/\/github.com\/marketplace\/qeb-hwt) is a GitHub Application which adds `thamos advise` based output to\\nPull Requests as a check. This functionality could be integrated into https:\/\/github.com\/marketplace\/khebhut and\\ncomplexity and maintain costs.\\n## Decision Drivers <!-- optional -->\\n* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\n","Decision":"* cost of maintaining Qeb-Hwt code and app\\n* redundancy of infrastructure\\nChosen option: \"merge function into Khebhut\", because we can reduce the cost of maintaining our software infrastructure\\nby reducing redundancy.\\n","tokens":96,"id":289,"Predictions":"n* The cost of maintaining Qeb-Hwt code and appn* redundancy of infrastructurennnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"core\/0002-allow-dynamic-loading-of-yetibot-plugins-via-config.md","Context":"## Context\\nYetibot currently resides across two primary repos:\\n- github.com\/yetibot\/yetibot\\n- github.com\/yetibot\/yetibot-core\\nThese code bases continue to grow in size, and consist of a diverse range of\\nfeatures, many of which many users won't care to use.\\n","Decision":"Switching to a plugin system allows us to split up the code base into much more\\nfine grained, logical units. For example, we may split the `github` command into\\nits own plugin.\\nThe first plugin is [yetibot-kroki](https:\/\/github.com\/yetibot\/yetibot-kroki).\\nWe will continue to extract plugins from both of the above code bases.\\n","tokens":70,"id":290,"Predictions":"Decision (A) The code bases are too large to be used by users.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"core\/0004-naming-convention-images.md","Context":"## Context and Problem Statement\\nImage names are important for branding and let others identify easily a specific image they need. For example \"I want to work on computer vision project with Tensorflow, what stack and image should I use?\" Having a trusted well maintained source of images with clean naming convention can help on that.\\n","Decision":"Selected option: `ps-{application}` as it shows what our intention is: we want to provide a curated\/predictable software stack, it might be used by ODH or RHODS or others, it might use S2I or other technology. Moreover helps from pipeline creation point of view, because the length of repo name on quay can crate issues.\\n### Positive Consequences <!-- optional -->\\n* users can immediately select an image based on the application they want.\\n* using overlays we can have a variety of combination, not just for ml_framework\\n### Negative Consequences <!-- optional -->\\n* N\/A\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":64,"id":291,"Predictions":"Decision (A). The image name should be a clean naming convention.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"core\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":292,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"beadledom\/default_dependencies_primary.md","Context":"## **Context**:\\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\\n","Decision":"In order to do this passively, we changed the dependency class to default to primary unless otherwise specified.\\nWe did this because we don't know if our consumers rely on the previous behavior of meta\/health checking dependencies if they were of unspecified importance.\\n","tokens":51,"id":293,"Predictions":"Decision (A).nPrimary Health checks were not checking if dependencies were primary or not before checking their health.nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"opg-refunds\/0002-default-service-key-for-dynamo-db-sessions.md","Context":"## Context\\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\\n","Decision":"We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\\nTable names                                       |\\n--------------------------------------------------|\\nrefunds-sessions-front-<opg_stackname>            |\\nrefunds-sessions-caseworker-front-<opg_stackname> |\\n","tokens":48,"id":295,"Predictions":"Decision (A) The security of the frontend user sessions is compromised.nB) The security of the frontend user sessions is compromised.nC) The security of the frontend user sessions is compromised.nD) The security of the frontend user sessions is compromised.nAnswer:A.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.n"}
{"File Name":"wh_covid19_app\/ADR-0002 Embed content in the app.md","Context":"## Context\\nThe app primarily displays information for medical staff working in hospitals.  The information needs to come from somewhere and there are a few options:\\n### Download content from a CMS\/S3 bucket\/Website\\nWe could pull the information in the app from somewhere on the web.  Some kind of CMS, S3 bucket or website where the files are hosted would be the most obvious choices.\\nThis does make it easier to quickly the information in the app, but would require the build out of more infrastructure and architecture to support all this.\\nIt also could mean that if the internet connection goes down to a hospital in  a crisis any information that isn't already cached in the app would be unavailable to users.\\n### Embed the content in the app\\nThe alternative method is to embed all of the information in the app so that all of the information is included, after downloading the app.\\nThis is architecturally much simpler and means all the content will be available offline if the internet should go down.\\nSubsequently, a new version of the app needs to be released to update the information.\\n","Decision":"For the initial versions of the application, we will embed the content in the app.\\nOnce the situation stabilises, and we can provide support to more hospitals, we can re-visit the decision and decide if we want to download content.\\n","tokens":224,"id":296,"Predictions":"The app should be able to be downloaded from a CMS\/S3 bucket\/Website. The app should be able to be downloaded from a CMS\/S3 bucket\/Website. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline. The app should be able to be accessed offline."}
{"File Name":"wh_covid19_app\/ADR-0001 Use Flutter.md","Context":"## Context\\nFlutter is a framework for building apps provided by Google.  Google describes it as a:\\n> ... UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\\nIt emphasizes fast development, expressive and flexible UI, and native performance.\\nGregg Miller, a Senior Anesthesiologist at Western Health hospitals, reached out to us with a mobile app he had been developing to provide COVID-19 information to frontline medical staff.  The app that Gregg had developed in was with Flutter.  Gregg sought assistance and guidance from the GDG community to help him complete the app.  Considering time constraints, we needed to put together and ship an app to the hospital before the crisis swamped the staff, which at that time as predicted to occur in about two weeks.\\nLuke immediately put out the call and assembled a team of Flutter developers from the GDG community.  We performed an initial evaluation on the codebase and decided that it would be best to deliver a re-write rather than building on the existing codebase.\\nAt this stage it would have been possible to drop Flutter and build out native applications, or built the app using some other cross platform toolkit such as react native.\\n","Decision":"Time was of the essence - as we already had a team of Flutter developers ready to go we decided to continue on with using Flutter to build the app.  It was also likely that Flutters emphasis on fast development and an expressive UI would be helpful in quickly building out the app.\\n","tokens":254,"id":297,"Predictions":"We chose to build the app using Flutter instead of building on the existing codebase.n234567899999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999"}
{"File Name":"wh_covid19_app\/ADR-0003 Use Flutter beta channel.md","Context":"## Context\\nThere are three different versions of flutter that you can build out: Stable, Beta and Dev. Each version will come with various trade-offs.\\nCurrently, there has been a missed stable release, and Beta is required to build iOS correctly.\\n","Decision":"We are using the beta channel.  Once some of the fixes land in stable, we may decide to revisit this decision.\\n","tokens":52,"id":298,"Predictions":"Decision (A) The stable release will be a stable release. (B) The stable release will be a beta release. (C) The stable release will be a Dev release. (D) The Dev release will be a Dev release. (A) The Dev release will be a Dev release. (A) The Dev release will be a Dev release. (B) The Dev release will be a Dev release. (C) The Dev release will be a Dev release. (D) The Dev release will be a Dev release. (A) The Dev release will be a Dev release. (A) The Dev release will be a Dev release. (B) The Dev release will be a Dev release. (C) The Dev release will be a Dev release. (D) The Dev release will be a Dev release. (A) The Dev release will be a Dev release. (A) The Dev release will be a Dev release. (B) The Dev release will be a Dev release. (C) The Dev release will be a Dev release. (D) The Dev release will be a Dev release. (A) The Dev release will be a Dev release. (A) The Dev release will be a Dev release."}
{"File Name":"nso.aurora\/MartketPlace.md","Context":"## Context\\nDo we need a MarketPlace component?\\n","Decision":"It doesn't look like there is a requirement for a need of a MarketPlace component.  It's decided we will not have one for now.\\n","tokens":12,"id":299,"Predictions":"Decision based on the information in this passage.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/Notifier.md","Context":"## Context\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n","Decision":"A Notifier component is needed to send email and\/SMS to customers.\\n","tokens":38,"id":300,"Predictions":"Decision (A) The customer will be notified of the survey after their purchase.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/LoyaltyProgram.md","Context":"## Context\\nDo we need a Loyalty Program component?\\n","Decision":"It doesn't look like there is a requirement for a need of a Loyalty Program component.  It's decided we will not have one for now.\\n","tokens":13,"id":301,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"Predictions":"Decision based on the information in this passage.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/NumberOfDatabases.md","Context":"## Context\\nThe state and data are required for the following components: Inventory, Customer, Order Management, Rating Manager and Recommendation Manager.  We had a database for each of the component initially.  However, it can save on the number of distrubted transactions by storing state and data in one database.\\n","Decision":"It was agreed that one database would be sufficient.\\n","tokens":63,"id":303,"Predictions":"Saving on the number of distrubted transactions by storing state and data in one database would be beneficial.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/SegmentationEngine.md","Context":"## Context\\nThe purpose of Segmentation engine are:\\n- grouping of customers' goals\\n- targeted groups\\n- history\\n- purchasing behaviour\\n- eating habit, location\\n- managing coupons and promotions\\nWhat is the difference between Recommendation and Segmentation engines?\\n","Decision":"It was decided to not have it.\\n","tokens":54,"id":304,"Predictions":"Decision Makers can use Segmentation engine to group customers' goals.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/InventoryManager.md","Context":"## Context\\nShould we have an Inventory Manager and a Fridge Manager where the Inventory Manager would hold the food items and Fridge would manage the physical fridges?\\n","Decision":"Inventory Manager can manage the following:\\n- manage fridges\\n- manage food items including their pricing\\n- manage the number of items in each fridge\\n","tokens":34,"id":305,"Predictions":"Decision (A). The Inventory Manager should be the physical fridge manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the inventory manager. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Inventory Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges. The Fridge Manager should be the physical fridges."}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"Predictions":"Decision Makers will not be able to determine the number of messages from Order Management.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"Predictions":"Decision Makers: The messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/QueuingForNotification.md","Context":"## Context\\nWe shouldn't overwhelm the Notifier.  This would happen then we have many orders, surveys and recommendations to send to customers.\\n","Decision":"The decision is to introduce a queue for all these messages.\\n","tokens":30,"id":308,"Predictions":"We should send more orders, surveys and recommendations to customers.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/FaultTolerance.md","Context":"## Context\\nWe have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\\n","Decision":"The decision is to have the polling mechanism to regularly obtain Smartfridges' state\/inventory.\\n","tokens":31,"id":309,"Predictions":"We will have the last known inventory of Smartfridges.nbr>Nonnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nso.aurora\/Kiosks-Fridge-customer-vendor-management.md","Context":"## Context\\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\\n","Decision":"We will have separate components to manage kiosks (along with point of sale systems), fridges, customers and vendors\\n","tokens":30,"id":310,"Predictions":"Decision (A). The kiosks-fridge-customer-vendor-management should be a part of the kiosks-fridge-customer-vendor-management system.nNo.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system.nIt is not necessary to have a kiosks-fridge-customer-vendor-management system."}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"Predictions":"Decision Makers should consider a local machine.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hodder\/0003-use-circle-ci-for-ete-tests.md","Context":"## Context\\nWe want automated ETE tests on build to keep things consistent. We also want this to be free - because free is good. CircleCI provides a free tier and solid docker support.\\n","Decision":"Use CircleCI for automated ETE tests\\n","tokens":41,"id":312,"Predictions":"We will use CircleCI to test the ETE test.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"mediawiki-extensions-Wikidata.org\/001-including-query-service-lag-in-maxlaginfo.md","Context":"## Context\\nDue to WDQS having a lag on catching up with edits to Wikidata, and that lag being affected\\nby the size of the entities being edited not just the rate of those edits, high volume edits can\\ncreate strain on WDQS due to update processing of the dispatched update jobs,\\nmore than WDQS update capacity can handle in a timely manner, resulting in sluggish WDQS.\\nExample incident can be found here:\\nhttps:\/\/www.wikidata.org\/w\/index.php?title=Wikidata:Administrators%27_noticeboard&oldid=963260218#Edoderoobot_edit_rate\\n","Decision":"As part of [T221774](https:\/\/phabricator.wikimedia.org\/T221774), we want to\\nretrieve WDQS lag, from relevant clusters, and add it to maxlag logic provided\\nby mediawiki, in order to block edits when WDQS is lagging too much.\\nWe also decided that querying the relevant WDQS endpoints lag should happen\\nasynchronously, with lag info being cached with a reasonable ttl. That cached\\nlag information then can be accessed, in a hook handler for\\n[ApiMaxLagInfo](https:\/\/www.mediawiki.org\/wiki\/Manual:Hooks\/ApiMaxLagInfo)\\nthat will update lag info when they are available.\\nThe following visual diagram shows the elements of the design\\non conceptual level:\\n![](.\/assets\/QueryServiceLagHook_ConceptualDesign.png)\\n","tokens":126,"id":313,"Predictions":"Decision (A). WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nC. WDQS should be able to handle the high volume edits.nD. WDQS should be able to handle the high volume edits.nA. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nC. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nC. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nC. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nC. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nC. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.nB. WDQS should be able to handle the high volume edits.n"}
{"File Name":"seda-frontend\/0002-using-local-state-in-settings-module.md","Context":"## Context\\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\\n","Decision":"The structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer\/context approach in favour of `redux` complexity.\\n","tokens":133,"id":314,"Predictions":"Decision (A) To store all data in the global store.nB) To store all data in the global store.nC) To store all data in the global store.nD) To store all data in the global store.nAnswer:A.nB.nB.nC.nD.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nA.nB.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.n"}
{"File Name":"seda-frontend\/0003-application-configuration-setup.md","Context":"## Context\\nUp to the point of writing this ADR, the SIA application aims to service only the municipality of Amsterdam. However, because of the nature of the application, more municipalities have shown an interest in the functionality the application has to offer.\\nThis poses a challenge, because there is a lot in the application that is specific to the municipality of Amsterdam. To sum up, amongst others (in random order):\\n- Docker registry URLs\\n- URLs of API endpoints, machine learning service, Authz service and map server\\n- Nginx configuration\\n- HTML `<title \/>`\\n- [`Sentry` Package](https:\/\/www.npmjs.com\/package\/@sentry\/browser) dependency and configuration\\n- [`Matomo` Package](https:\/\/www.npmjs.com\/package\/@datapunt\/matomo-tracker-js) dependency and configuration\\n- PWA manifest\\n- Logo\\n- Favicon\\n- PWA icons\\n- Hardcoded strings\\n- Main menu items\\n- Theme configuration\\n- Maps settings\\n- [`amsterdam-stijl` Package](https:\/\/www.npmjs.com\/package\/amsterdam-stijl) Package dependency\\n- Package URL and description\\nAll of the above need to be configurable or should be taken out of the equation to be able to publish a white-label version of `signals-frontend`.\\n","Decision":"Taking the pros and cons, the application's architecture, the Datapunt infrastructure and upcoming client wishes into account, the decision is made to go for the [Server-side \/ container](#Server-side \/ container) option.\\nThe repository will contain default configuration options so that it can be run locally and still be deployed to (acc.)meldingen.amsterdam.nl without the application breaking.\\nConfiguration is injected in the `index.html` file so it can be read at run-time.\\n","tokens":274,"id":315,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"verify-event-store-schema\/0002-database-migrations-are-standalone-releases.md","Context":"## Context\\nDatabase schema migrations are often considered as part of an application change -\\n\"in order to do X we will need a new index Y\"\\nHowever, this does not work well with a zero-downtime approach, when we have\\na single replicated database - we can't deploy version 0.2 of the application\\nto some servers simultaneously with version 0.2 of the schema to their databases;\\nwe need to deploy changes to the database either before or after the\\ncorresponding application changes\\n","Decision":"Database schema changes should be made independently of application changes.\\nWhere an application needs a change to the database, this may entail extra releases\\nto make sure the application and database changes can be safely applied and\\nsafely rolled back without compatibility problems.\\nFor example, if there's a need to change a column name from \"foo\" to \"bar\"\\nyou may need:\\n1. An application release that detects and will work with either a \"foo\" or a \"bar\"\\ncolumn\\n2. A database schema release that renames the column from \"foo\" to \"bar\"\\n3. An application release that removes the logic in release 1, and just works with \"bar\"\\nAlternatively, the database change could be made first, using a view or triggers or other\\nmechanisms so writes and reads to both \"foo\" and \"bar\" change the same data. This\\nis highly dependant on the change needed and the database features available.\\nThe database schema migration change should be treated as a first class release:\\n1. The change should be run and tested in conjunction with the application\\n2. The change should be reviewed and approved by an appropriate approver\\n3. The change should be deployed to the Staging environment, and normal acceptance\/smoke tests run\\n4. The change should be deployed to Production\/DR\\n### A note on backups\\nDepending on the complexity of the change, you may wish to coordinate a backup snapshot\\nof the database before running a migration.  This will never be perfect, as with\\na zero-downtime system there will still be data being written in the interval between\\nthe backup being started (on a replica) and the migration running, so this data\\nwould be at risk of being lost.\\nMany database migrations however are totally safe and should not need a backup -\\nfor instance, adding an index or adding a column is a very low risk change.  Nightly\\nbackups should be enough to mitigate against any risk with this sort of change.\\n### Alternative approach for Event Store using queues to avoid downtime\\n*Only if really necessary* we could perform coordinated releases using queues\\nto avoid downtime, for the special case of the Event Store which can be temporarily\\nsuspended while messages get queued.\\nThe approach would be similar to:\\n1. Modify the event recorder Lambda timing so the event recorder does not run, or point it to a test queue instead\\n2. Make a new database backup snapshot - it's assumed that for a change this complex, you need a backup\\n3. Deploy the database change\\n4. Test the change\\n5. Re-enable the Lambda\\nNote that the SQS queue has a limit of 100,000 queued messages - at peak we have\\nhistorically received around 75,000 messages an hour.  So this technique is quite time constrained;\\nif anything goes wrong you only have a small amount of time to fix it.\\n","tokens":109,"id":316,"Predictions":"Decision (A) The application changes should be implemented in a single replicated database. (B) The application changes should be implemented in a single replicated database. (C) The application changes should be implemented in a single replicated database. (D) The application changes should be implemented in a single replicated database. (E) The application changes should be implemented in a single replicated database. (A) The application changes should be implemented in a single replicated database. (B) The application changes should be implemented in a single replicated database. (C) The application changes should be implemented in a single replicated database. (D) The application changes should be implemented in a single replicated database. (A) The application changes should be implemented in a single replicated database. (A) The application changes should be implemented in a single replicated database. (B) The application changes should be implemented in a single replicated database. (C) The application changes should be implemented in a single replicated database. (D) The application changes should be implemented in a single replicated database. (A) The application changes should be implemented in a single replicated database. (A) The application changes should be implemented in a single replicated database. (B) The application changes should be implemented in a single replicated database. (C) The application changes should be implemented in a single replicated database. (D) The application changes should be implemented in a single replicated database."}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"Predictions":"Decision (A).nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A."}
{"File Name":"verify-event-store-schema\/0001-flyway.md","Context":"## Context\\nWe need a way of running database migrations to the verify event store database.\\n","Decision":"We have chosen to use [Flyway](https:\/\/flywaydb.org) since the team has some experince with it and it seems like a simple and lightweight option.\\n","tokens":19,"id":318,"Predictions":"Decision Makers must decide if they want to migrate the database to the verify event store database.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"Predictions":"Decision based on the following: The aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nJenkins has a master\/agent architecture, where a single master node can triggernjobs running on multiple agents.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nJenkins has a master\/agent architecture, where a single master node can triggernjobs running on multiple agents.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nJenkins has a master\/agent architecture, where a single master node can triggernjobs running on multiple agents.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nJenkins has a master\/agent architecture, where a single master node can triggernjobs running on multiple agents.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nJenkins has a master\/agent architecture, where a single master node can triggernjobs running on multiple agents.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nJenkins has a master\/agent architecture, where a single master node can triggernjobs running on multiple agents.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nJenkins has a master\/agent architecture, where a single master node can triggernjobs running on multiple agents.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe aim of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is to allow teams to quickly create their own Jenkinsnplatform, running on AWS.nThe purpose of this project is"}
{"File Name":"re-build-systems\/0003-https-implementation.md","Context":"## Context\\nWe needed to implement HTTPS and TLS termination as part of the template we offer in our MVP.\\nWe identified 4 ways to get that done.\\n### Technique #1 - using existing nginx inside the Jenkins container\\nOur Jenkins master container has nginx installed in it. We can install the certificates and keys\\nthere. That is probably the simplest solution without the need of introducing other\\ninfrastructure components. Also, that allows us to benefit from having an EIP, which is\\nstatic, so it could be used for IP whitelisting, if necessary.\\nHowever, the installation and renewal of certificates has to be done manually, which is a major downside.\\n### Technique #2 - using ELB\\nWe introduce an ELB in front of the existing infrastructure, like so:\\nDNS resolution -> ELB -> Jenkins master EC2\\nCompared to #1, the major benefit is the ability to manage the provisioning of the certificate fully within AWS using [AWS Certificate Manager](https:\/\/docs.aws.amazon.com\/elasticloadbalancing\/latest\/classic\/ssl-server-cert.html).\\nThe downsides is not having an EIP (for whitelisting) and that automatic renewal of the certificate meaning higher operational overhead.\\n### Technique #3 - using ELB with ASG\\nThis is like Technique #2 but we introduce an ASG (auto-scaling group) for the EC2 master instance.\\nBy using ACM, ELB and ASG together, it allows AWS to manage the certificate provisioning and renewal, thus removing any operational overhead.\\n### Technique #4 - using a NLB\\nLike #2 but with a Network Load Balancer (NLB), like so:\\nDNS resolution -> EIP -> NLB -> ELB -> Jenkins master EC2\\nThe advantage is that we can have an EIP. However, we probably need Cloudwatch\\nto detect changes in the EIP which would trigger a Lambda to register the new EIP with the NLB, this is a suggested solution from AWS, but the downsides is that it is complex, expensive and difficult to manage.\\n","Decision":"After exploring and understanding all of the techniques and discussion within the team, option 3 was chosen.\\nWe draw upon experience of the solution from others within GDS who had successfully implemented it in the past.\\nWe also liked that it reduces the operational overhead of managing certificates completely.\\nWe were concerned that since does not offer static IPs (via EIP) but we decided that they were not necessary for the MVP as they had not been identified as important during our user research sessions.\\n","tokens":430,"id":320,"Predictions":"Decision (A) Implementing HTTPS and TLS termination as part of the template we offer in our MVP.nWe chose Technique #1 - using existing nginx inside the Jenkins container. We can install the certificates and keys there. We can install the certificates and keysnthere. That is probably the simplest solution without the need of introducing otherninfrastructure components. Also, that allows us to benefit from having an EIP, which is static, so it could be used for IP whitelisting, if necessary.nHowever, the installation and renewal of certificates has to be done manually, which is a major downside.n### Technique #3 - using ELB -> ELB -> Jenkins master EC2nCompared to #1, the major benefit is the ability to manage the provisioning of the certificate fully within AWS using [AWS Certificate Manager](https:\/\/docs.aws.amazon.com\/elasticloadbalancing\/latest\/classic\/ssl-server-cert.html).nThe downside is that we can have an EIP (for whitelisting) and that automatic renewal of the certificate meaning higher operational overhead.n### Technique #4 - using a NLBnLike #2 but with a Network Load Balancer (NLB), like so:nDNS resolution -> EIP -> NLB -> ELB -> Jenkins master EC2nThe advantage is that we can have an EIP. However, we probably need Cloudwatch to detect changes in the EIP which would trigger a Lambda to register the new EIP with the NLB, this is a suggested solution from AWS, but the downsides is that it is complex, expensive and difficult to manage.n"}
{"File Name":"re-build-systems\/0007-add-ability-to-import-custom-jobs.md","Context":"## Context\\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\\nWe have identified a number of ways to do this:\\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\\n2. Automatically create the jobs by scanning a Github account\\n3. [Jenkins Job Builder]\\n4. [Job DSL plugin]\\n### Option 1\\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\\n### Option 2\\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\\n### Option 3\\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\\n### Option 4\\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\\n","Decision":"We decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\\n","tokens":500,"id":321,"Predictions":"Decision based on the information in this PR.n### Option 1nThis option is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option as code would be quite involved.n### Option 2nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option as code would be quite involved.n### Option 3nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option as code would be quite involved.n### Option 4nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.n### Option 4nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.n### Option 5nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly."}
{"File Name":"re-build-systems\/0008-do-not-switch-from-efs-to-ebs.md","Context":"## Context\\nTo guide our work, we need to decide whether we use AWS\u2019 Elastic File System\\n(EFS) or Elastic Block Store (EBS) as the cloud storage service for our Jenkins\\nbuild.\\nThe decision is needed as it will impact on the team\u2019s work and may be affected\\nby wider GDS or Government strategy.\\n","Decision":"Based on the below pros and cons, we have decided to remain with EFS as the\\ncloud storage service, until such a time that there is a strategic need to\\nchange.\\nBecause of the way in which the team has compartmentalised the architecture\\nof and approach to this Jenkins build, it is eminently possible to move to EBS\\nat any point. It also means that further development of the system will not make\\nit more complicated to do so.\\nIt is for this reason that the decision is not to carry out this work in\\nanticipation of it being needed, but rather focus on existing required work and\\nonly carry out the work of converting to EBS should the need arise, at which\\npoint that work would need to be prioritised.\\n","tokens":70,"id":322,"Predictions":"We need to decide whether we should use AWS\u2019 Elastic File Systemn(EFS) or Elastic Block Storen(EBS) as the cloud storage service for our Jenkinsnbuild.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be affected by wider GDS or Government strategy.nThe decision is needed as it will impact on the team\u2019s work and may be"}
{"File Name":"re-build-systems\/0006-buildah-discovery.md","Context":"## Context\\nThere is a desire from users to be able to build docker images as an artifact from their jobs, this creates a security problem because of technical limitations within docker.  This ADR is the result of discovery work to discern whether there are alternative solutions to building docker images securely.  The discovery focussed on evaluating the software product called [Buildah].\\n## Technical Problem\\nAs the workers run within a docker container, they may need to build another docker image as an artifact.  To do so requires that the host servers docker socket be exposed to the worker container.  The docker daemon runs as root and has full control of the host server, meaning malicious code running within the workers docker container could trivially gain access to the host system by mounting (via -v) the host systems root.\\nAs Docker does not support RBAC to control and restrict access to its socket there is no way provided by docker to prevent this privilege escalation method occuring.\\nBuildah solves this problem by being able to build a docker container without the need to use docker and the vulnerable socket.\\n","Decision":"Buildah addresses the problems of docker container privilege escalation, is feature rich to support typical use cases (as it is feature compatible with docker), and provides us a means in which to provide docker image artifacts.\\n","tokens":222,"id":324,"Predictions":"Decision (A). [Buildah] is a software product.n#A.n#B.n#C.n#D.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#B.n#A.n#A.n#B.n#A.n#A.n#B.n#A.n#A.n#B.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A.n#A"}
{"File Name":"re-build-systems\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":325,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Brighter\/0002-use-a-single-threaded-message-pump.md","Context":"## Context\\nAny service activator pattern will have a message pump, which reads from a queue.\\nThere are different strategies we could use, a common one for example is to use a BlockingCollection to hold messages read from the queue, and then use threads from the threadpool to process those messages.\\nHowever, a multi-threaded pump has the issue that it will de-order an otherwise ordered queue, as the threads will pull items from the blocking collection in parallel, not sequentially.\\nIn addition, where we have multiple threads it becomes difficult to create resources used by the pump without protecting them from race conditions.\\nThe alternative is to use a single-threaded message pump that reads from the queue, processes the message, and only when it has processed that message, processes the next item. This prevents de-ordering of the queue, because items are read in sequence.\\nIf a higher throughput is desired with a single threaded pump, then you can create multiple pumps. In essence, this is the competing consumers pattern, each performer is its own message pump.\\nThe message pump performs the usual sequence of actions:\\n- GetMessage. Read Message From Queue\\n- Translate Message. Translate Message from Wire Format to Type\\n- Dispatch Message. Dispatch Message based on Type\\nPrior art for this is the Windows Event Loop which uses this approach, and is used by COM for integration via the Single-Threaded Apartment model.\\n","Decision":"Use a single-threaded message pump to preserve ordering and ensure sequential access to shared resources. Allow multiple pump instances for throughput.\\n","tokens":285,"id":326,"Predictions":"Decision (A) To use a single-threaded message pump.nB) To use a single-threaded message pump.nC) To use a single-threaded message pump.nD) To use a single-threaded message pump.nAnswer:A.nOptions:A.B.C.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.nA.B.C.D.A.A.nA.B.C.D.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A."}
{"File Name":"Brighter\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":327,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"Predictions":"We will use the first option to manage multiple environments.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"docnow-infra\/0003-use-terraform-enterprise-for-backend.md","Context":"## Context\\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and\/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\\n","Decision":"Use Terraform Cloud for teams\\n","tokens":80,"id":329,"Predictions":"Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"docnow-infra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":330,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openfido-auth-service\/0002-authentication.md","Context":"## Context\\nA couple of client projects need a simple authentication system. One of the\\nprojects already uses Flask and Postgres, while another is in the design phase.\\nIn the short term we want to have a minimal and functional authentication system\\nimplemented as soon as possible.\\nIn the very long term we hope that this implementation would be reused many\\ntimes and to have easily customized drop-in libraries for Flask projects.\\n","Decision":"Create a minimal reference authentication implementation that uses Flask and\\nPostgres. Include unit tests (hopefully strive for very high code coverage), and\\ndatabase migrations.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nDelay any features that aren't in our current project requirements. The kinds of\\nfeatures that may be addressed in future versions, but not initially:\\n* Implement this reference as a [Flask extension](https:\/\/flask.palletsprojects.com\/en\/1.1.x\/extensiondev\/), and possibly add to the PyPi public repositories.\\n* Create a React reference implementation.\\n* password resets\\n* email account verification\\n* permissions\\n* multi factor authentication\\n* OAuth\\n* Expiring tokens\\n* Revoking\/blacklisting tokens\\n* prevent re-use of old passwords\\n","tokens":87,"id":331,"Predictions":"Decision (A).nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A."}
{"File Name":"openfido-auth-service\/0003-deployment.md","Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- AWS_ACCESS_KEY_ID\\n- AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":332,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openfido-auth-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":333,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"Predictions":"Decision (A) Migrate to React Query\/Context API\/Hooks from Reduxn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn## Solutionn"}
{"File Name":"html-integrations\/004-Support-for-Froala editor-major-versions.md","Context":"## Context (Discussion)\\nWe want to give support to any new Froala major version. For this reason, we must decide whether to create a new plugin for the new version of Froala, or use an existing one.\\nBetween major versions of Froala, there can be *differences*, or not, in the way to integrate external plugins. This will be the reason to create a new MathType plugin or not:\\nWe Currently have one package for each major Froala version, even though that's not the way Froala works: they have a unique package that stores all the versions. Even if we decided to follow the conventions Froala does, we would have to rename or delete our current modules, and npm wouldn't allow it, meaning that once a name is used, it can't be used never again for a new package.\\n","Decision":"We will create and give the Froala MathType packages for any new major version, the following name: *@wiris\/mathtype-froalaX*, where X equals the oldest Froala major version. Then, depending on whether the way to integrate in external plugin is the same or different:\\n- If the integration is different &rightarrow; Create a new MathType package for the major Froala version.\\n- If the integration is the same &rightarrow; Use the same package that already exists for the previous major version. A good example is the situation we are in with the versions 3 and 4, where the integration is the same, so the package for both versions is the mathtype-froala3.\\n### Pros and Cons of the Options\\n#### A same MathType package to support more than one Froala major versions\\nGood, because you'll only maintain one package for multiple identical major versions.\\nGood, because it'll reduce DRI.\\nBad, because the package name is not self-explanatory.\\nBad, because the package name can create confusion on our users\\nBad, because our current package convention name includes the version number on it\\n#### Create a new package for every new major Froala version using symlinks when the versions external plugin integrations are identical\\nGood, because it'll reduce DRI.\\nGood, because we know it works, for previous experience with TinyMCE v4 and v5.\\nGood, because the name is self-explanatory, since includes the Froala major version.\\nBad, because, we'll have to maintain more than one package, at least.\\nBad, because the infrastructure is more complex.\\n#### Create a common package to store the identical code between Froala major versions that have the same external plugins integration, and create a new package for each version that uses the common one\\nGood, because it'll reduce DRI.\\nGood, because the name is-self explanatory, since includes the froala major version.\\nBad, because, we'll have to maintain more than one package, at least thee.\\n","tokens":173,"id":335,"Predictions":"We will create a new plugin for the new version of Froala. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin. We will use an existing plugin."}
{"File Name":"html-integrations\/003--use-shx-wrapper-for-shell-commands.md","Context":"## Context (Discussion)\\nWhen writing custom scripts in the package.json files, one must take into account that those commands\\nshould be compatible with both sh and CMD.exe. So, it is common to use third-party npm packages that\\nact as an API to mimic these tasks in both environments.\\nOne of the most common tasks we do in these scripts is file manipulation (deleting, copying, moving).\\n### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n**ShellJS:**\\nGood for writing long scripts, all in JS, running via NodeJS (e.g. node myScript.js).\\n**shx:**\\nGood for writing one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").\\n","Decision":"It is proposed to use [shx][shx-url], a wrapper for sh commands that works on Windows too, and behaves as the corresponding sh commands.\\nIt has more than 150K downloads per week, is well-maintaned by the community and it occupies less space than our current past alternatives: rimfaf and copyfiles.\\n","tokens":166,"id":336,"Predictions":"Decision (A) To write custom scripts in the package.json files, one must take into account that those commandsnshould be compatible with both sh and CMD.exe. So, it is common to use third-party npm packages thatnact as an API to mimic these tasks in both environments.n### Difference Between ShellJS and shxn**ShellJS:**nGood for writing long scripts, all in JS, running via NodeJS (e.g. node myScript.js).n**shx:**nGood for writing one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in npm package scripts (e.g. \"shx rm -rf out\/\").n### Decision (A) To write one-off commands in"}
{"File Name":"html-integrations\/001-Generate--RFC-compliant--UUIDs--Telemetry.md","Context":"## Context (Discussion)\\nIn the context of the Telemetry project, we want to generate\\nvalid and consisten UUIDs since they are needed to ensure the uniqueness, randomeness and its validity.\\nUUID identifiers have an specification defined as the RFC-4122 standard  [A Universally Unique IDentifier (UUID) URN Namespace](https:\/\/tools.ietf.org\/html\/rfc4122) on ietf.org.\\nWe don't want to reinvent the wheel and it seems unwise to write our own library to generate this UUIDs since there are third party solutions with good support, small and secure.\\n","Decision":"We'll use [uuid package] to generate RFC4122 version 4 UUIDs to use on the Telemetry implementation. The code of the [uuid project] is available at github.\\nTherefore, **uuid** becomes the first functional dependency of the 'MathType Web Integration JavaScript SDK', known as npm package as '@wiris\/mathtype-html-integration-devkit'.\\n* [uuid project](https:\/\/github.com\/uuidjs\/uuid)\\n* [uuid package](https:\/\/www.npmjs.com\/package\/uuid)\\n### Pros and Cons of the Options\\n#### Implement our own Javascript library for that\\n- Bad, because Javascript Math.random function is not very good.\\n- Bad, because we'll need to maintain it.\\n- Bad, because more work to the backlog.\\n- Bad, because we're reinventing the wheel.\\n#### Using a third party library like github.com\/uuidjs\/uuid\\n- Good, because supports RFC4122 version 1, 3, 4, and 5 UUIDs.\\n- Good, because its well maintained, no issues and widely used; (26.085.977 downloads\/week).\\n- Good, because solves our problem immediately.\\n- Good, because it's secure, small and cross-platform.\\n- Bad, because we're adding a dependency to our core library, and therefore, to all our Javascript plugins.\\n","tokens":127,"id":337,"Predictions":"We will use the RFC-4122 standard to generate UUIDs.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"html-integrations\/002-Longevity--senderId--sessionId--Telemetry.md","Context":"## Context (Discussion)\\nReading the Telemetry documentation proposed by the Data Science team, we have\\nencountered that the sender and session ids are not concretely defined for each\\nproduct.\\nIn the case of MathType web, there could be various interpretations, e.g.\\n- senderId: changes on page load.\\n- sessionId: changes on opening MathType.\\n","Decision":"We asked the Data Science team and finally settled on the following interpretation:\\n- senderId: ideally lasts for ever. In practice, should at least last in a same web session, across page loads.\\n- sessionId: changes on page load.\\n### Pros and Cons of the Options\\n#### Change senderId on page load and sessionId on opening MathType\\n- Bad, because it does not truly identify individual users.\\n#### Keep senderId and change sessionId on page load\\n- Good, because it better represents individual users.\\n- Bad, because it's hard to keep permanent data on the client's browser.\\n","tokens":80,"id":338,"Predictions":"Decision (A) The sender and session ids are not concretely defined for each product.nThe sender and session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are not concretely defined for each product.nThe session ids are"}
{"File Name":"youtube-playlist-manager\/0002-use-chrome-identity-launchwebauthflow-instead-of-chrome-identity-getauthtoken.md","Context":"## Context\\nA method to authorize against the youtube API to get the access token needs to be added to the application.\\n","Decision":"I decided to use the launchWebAuthFlow instead of the getAuthToken api to do the oauth2 authentication. This is\\nbecause getAuthToken can only authorize the google signed in in chrome. This prevents me from using a different account\\nfor youtube from my chrome account.\\n","tokens":25,"id":339,"Predictions":"Decision 12nA method to authorize against the youtube API to get the access token needs to be added to the application.n22n22 Decision 122nA method to authorize against the youtube API to get the access token needs to be added to the application.n222 Decision 122nA method to authorize against the youtube API to get the access token needs to be added to the application.n22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222222322222222222232222222223222222223222222223222222232222223222222322222322222322222322222322223222232222322223222232223222322232223222322232223222322322232232232232232232232232232232232232232232232232232232232232232232322323223232232323223232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323232323233232323323233233233233233233233233233233233233233233233323332333233333233333333333333333"}
{"File Name":"youtube-playlist-manager\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":340,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"midashboard-infrastructure\/0002-use-aws-bare-metal-rig-approach.md","Context":"## Context\\nWe need to create a riglet for our new bookit project so that we practice what we preach.\\n","Decision":"We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: Travis\\n","tokens":25,"id":341,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"midashboard-infrastructure\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":342,"Predictions":"We will continue to use the Pipleine.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"midashboard-infrastructure\/0004-use-cloudwatch-logs-for-log-aggregation.md","Context":"## Context\\nContext here...\\n","Decision":"Decision here...\\n","tokens":7,"id":343,"Predictions":"Decision 0 of the Architectural Decision Record is correct. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record. The record is a record of Architectural Decision Record."}
{"File Name":"midashboard-infrastructure\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":344,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"checkout\/0153-checkout-v2.md","Context":"## Context\\nThis ADR details the behavior for `actions\/checkout@v2`.\\nThe new action will be written in typescript. We are moving away from runner-plugin actions.\\nWe want to take this opportunity to make behavioral changes, from v1. This document is scoped to those differences.\\n","Decision":"### Inputs\\n```yaml\\nrepository:\\ndescription: 'Repository name with owner. For example, actions\/checkout'\\ndefault: ${{ github.repository }}\\nref:\\ndescription: >\\nThe branch, tag or SHA to checkout. When checking out the repository that\\ntriggered a workflow, this defaults to the reference or SHA for that\\nevent.  Otherwise, uses the default branch.\\ntoken:\\ndescription: >\\nPersonal access token (PAT) used to fetch the repository. The PAT is configured\\nwith the local git config, which enables your scripts to run authenticated git\\ncommands. The post-job step removes the PAT.\\nWe recommend using a service account with the least permissions necessary.\\nAlso when generating a new PAT, select the least scopes necessary.\\n[Learn more about creating and using encrypted secrets](https:\/\/help.github.com\/en\/actions\/automating-your-workflow-with-github-actions\/creating-and-using-encrypted-secrets)\\ndefault: ${{ github.token }}\\nssh-key:\\ndescription: >\\nSSH key used to fetch the repository. The SSH key is configured with the local\\ngit config, which enables your scripts to run authenticated git commands.\\nThe post-job step removes the SSH key.\\nWe recommend using a service account with the least permissions necessary.\\n[Learn more about creating and using\\nencrypted secrets](https:\/\/help.github.com\/en\/actions\/automating-your-workflow-with-github-actions\/creating-and-using-encrypted-secrets)\\nssh-known-hosts:\\ndescription: >\\nKnown hosts in addition to the user and global host key database. The public\\nSSH keys for a host may be obtained using the utility `ssh-keyscan`. For example,\\n`ssh-keyscan github.com`. The public key for github.com is always implicitly added.\\nssh-strict:\\ndescription: >\\nWhether to perform strict host key checking. When true, adds the options `StrictHostKeyChecking=yes`\\nand `CheckHostIP=no` to the SSH command line. Use the input `ssh-known-hosts` to\\nconfigure additional hosts.\\ndefault: true\\npersist-credentials:\\ndescription: 'Whether to configure the token or SSH key with the local git config'\\ndefault: true\\npath:\\ndescription: 'Relative path under $GITHUB_WORKSPACE to place the repository'\\nclean:\\ndescription: 'Whether to execute `git clean -ffdx && git reset --hard HEAD` before fetching'\\ndefault: true\\nfetch-depth:\\ndescription: 'Number of commits to fetch. 0 indicates all history for all tags and branches.'\\ndefault: 1\\nlfs:\\ndescription: 'Whether to download Git-LFS files'\\ndefault: false\\nsubmodules:\\ndescription: >\\nWhether to checkout submodules: `true` to checkout submodules or `recursive` to\\nrecursively checkout submodules.\\nWhen the `ssh-key` input is not provided, SSH URLs beginning with `git@github.com:` are\\nconverted to HTTPS.\\ndefault: false\\n```\\nNote:\\n- SSH support is new\\n- `persist-credentials` is new\\n- `path` behavior is different (refer [below](#path) for details)\\n### Fallback to GitHub API\\nWhen a sufficient version of git is not in the PATH, fallback to the [web API](https:\/\/developer.github.com\/v3\/repos\/contents\/#get-archive-link) to download a tarball\/zipball.\\nNote:\\n- LFS files are not included in the archive. Therefore fail if LFS is set to true.\\n- Submodules are also not included in the archive.\\n### Persist credentials\\nThe credentials will be persisted on disk. This will allow users to script authenticated git commands, like `git fetch`.\\nA post script will remove the credentials (cleanup for self-hosted).\\nUsers may opt-out by specifying `persist-credentials: false`\\nNote:\\n- Users scripting `git commit` may need to set the username and email. The service does not provide any reasonable default value. Users can add `git config user.name <NAME>` and `git config user.email <EMAIL>`. We will document this guidance.\\n#### PAT\\nWhen using the `${{github.token}}` or a PAT, the token will be persisted in the local git config. The config key `http.https:\/\/github.com\/.extraheader` enables an auth header to be specified on all authenticated commands `AUTHORIZATION: basic <BASE64_U:P>`.\\nNote:\\n- The auth header is scoped to all of github `http.https:\/\/github.com\/.extraheader`\\n- Additional public remotes also just work.\\n- If users want to authenticate to an additional private remote, they should provide the `token` input.\\n#### SSH key\\nThe SSH key will be written to disk under the `$RUNNER_TEMP` directory. The SSH key will\\nbe removed by the action's post-job hook. Additionally, RUNNER_TEMP is cleared by the\\nrunner between jobs.\\nThe SSH key must be written with strict file permissions. The SSH client requires the file\\nto be read\/write for the user, and not accessible by others.\\nThe user host key database (`~\/.ssh\/known_hosts`) will be copied to a unique file under\\n`$RUNNER_TEMP`. And values from the input `ssh-known-hosts` will be added to the file.\\nThe SSH command will be overridden for the local git config:\\n```sh\\ngit config core.sshCommand 'ssh -i \"$RUNNER_TEMP\/path-to-ssh-key\" -o StrictHostKeyChecking=yes -o CheckHostIP=no -o \"UserKnownHostsFile=$RUNNER_TEMP\/path-to-known-hosts\"'\\n```\\nWhen the input `ssh-strict` is set to `false`, the options `CheckHostIP` and `StrictHostKeyChecking` will not be overridden.\\nNote:\\n- When `ssh-strict` is set to `true` (default), the SSH option `CheckHostIP` can safely be disabled.\\nStrict host checking verifies the server's public key. Therefore, IP verification is unnecessary\\nand noisy. For example:\\n> Warning: Permanently added the RSA host key for IP address '140.82.113.4' to the list of known hosts.\\n- Since GIT_SSH_COMMAND overrides core.sshCommand, temporarily set the env var when fetching the repo. When creds\\nare persisted, core.sshCommand is leveraged to avoid multiple checkout steps stomping over each other.\\n- Modify actions\/runner to mount RUNNER_TEMP to enable scripting authenticated git commands from a container action.\\n- Refer [here](https:\/\/linux.die.net\/man\/5\/ssh_config) for SSH config details.\\n### Fetch behavior\\nFetch only the SHA being built and set depth=1. This significantly reduces the fetch time for large repos.\\nIf a SHA isn't available (e.g. multi repo), then fetch only the specified ref with depth=1.\\nThe input `fetch-depth` can be used to control the depth.\\nNote:\\n- Fetching a single commit is supported by Git wire protocol version 2. The git client uses protocol version 0 by default. The desired protocol version can be overridden in the git config or on the fetch command line invocation (`-c protocol.version=2`). We will override on the fetch command line, for transparency.\\n- Git client version 2.18+ (released June 2018) is required for wire protocol version 2.\\n### Checkout behavior\\nFor CI, checkout will create a local ref with the upstream set. This allows users to script git as they normally would.\\nFor PR, continue to checkout detached head. The PR branch is special - the branch and merge commit are created by the server. It doesn't match a users' local workflow.\\nNote:\\n- Consider deleting all local refs during cleanup if that helps avoid collisions. More testing required.\\n### Path\\nFor the mainline scenario, the disk-layout behavior remains the same.\\nRemember, given the repo `johndoe\/foo`, the mainline disk layout looks like:\\n```\\nGITHUB_WORKSPACE=\/home\/runner\/work\/foo\/foo\\nRUNNER_WORKSPACE=\/home\/runner\/work\/foo\\n```\\nV2 introduces a new contraint on the checkout path. The location must now be under `github.workspace`. Whereas the checkout@v1 constraint was one level up, under `runner.workspace`.\\nV2 no longer changes `github.workspace` to follow wherever the self repo is checked-out.\\nThese behavioral changes align better with container actions. The [documented filesystem contract](https:\/\/help.github.com\/en\/actions\/automating-your-workflow-with-github-actions\/virtual-environments-for-github-hosted-runners#docker-container-filesystem) is:\\n- `\/github\/home`\\n- `\/github\/workspace` - Note: GitHub Actions must be run by the default Docker user (root). Ensure your Dockerfile does not set the USER instruction, otherwise you will not be able to access `GITHUB_WORKSPACE`.\\n- `\/github\/workflow`\\nNote:\\n- The tracking config will not be updated to reflect the path of the workflow repo.\\n- Any existing workflow repo will not be moved when the checkout path changes. In fact some customers want to checkout the workflow repo twice, side by side against different branches.\\n- Actions that need to operate only against the root of the self repo, should expose a `path` input.\\n#### Default value for `path` input\\nThe `path` input will default to `.\/` which is rooted against `github.workspace`.\\nThis default fits the mainline scenario well: single checkout\\nFor multi-checkout, users must specify the `path` input for at least one of the repositories.\\nNote:\\n- An alternative is for the self repo to default to `.\/` and other repos default to `<REPO_NAME>`. However nested layout is an atypical git layout and therefore is not a good default. Users should supply the path info.\\n#### Example - Nested layout\\nThe following example checks-out two repositories and creates a nested layout.\\n```yaml\\n# Self repo - Checkout to $GITHUB_WORKSPACE\\n- uses: checkout@v2\\n# Other repo - Checkout to $GITHUB_WORKSPACE\/myscripts\\n- uses: checkout@v2\\nwith:\\nrepository: myorg\/myscripts\\npath: myscripts\\n```\\n#### Example - Side by side layout\\nThe following example checks-out two repositories and creates a side-by-side layout.\\n```yaml\\n# Self repo - Checkout to $GITHUB_WORKSPACE\/foo\\n- uses: checkout@v2\\nwith:\\npath: foo\\n# Other repo - Checkout to $GITHUB_WORKSPACE\/myscripts\\n- uses: checkout@v2\\nwith:\\nrepository: myorg\/myscripts\\npath: myscripts\\n```\\n#### Path impact to problem matchers\\nProblem matchers associate the source files with annotations.\\nToday the runner verifies the source file is under the `github.workspace`. Otherwise the source file property is dropped.\\nMulti-checkout complicates the matter. However even today submodules may cause this heuristic to be inaccurate.\\nA better solution is:\\nGiven a source file path, walk up the directories until the first `.git\/config` is found. Check if it matches the self repo (`url = https:\/\/github.com\/OWNER\/REPO`). If not, drop the source file path.\\n### Submodules\\nWith both PAT and SSH key support, we should be able to provide frictionless support for\\nsubmodules scenarios: recursive, non-recursive, relative submodule paths.\\nWhen fetching submodules, follow the `fetch-depth` settings.\\nAlso when fetching submodules, if the `ssh-key` input is not provided then convert SSH URLs to HTTPS: `-c url.\"https:\/\/github.com\/\".insteadOf \"git@github.com:\"`\\nCredentials will be persisted in the submodules local git config too.\\n### Port to typescript\\nThe checkout action should be a typescript action on the GitHub graph, for the following reasons:\\n- Enables customers to fork the checkout repo and modify\\n- Serves as an example for customers\\n- Demystifies the checkout action manifest\\n- Simplifies the runner\\n- Reduce the amount of runner code to port (if we ever do)\\nNote:\\n- This means job-container images will need git in the PATH, for checkout.\\n### Branching strategy and release tags\\n- Create a servicing branch for V1: `releases\/v1`\\n- Merge the changes into the default branch\\n- Release using a new tag `preview`\\n- When stable, release using a new tag `v2`\\n","tokens":63,"id":345,"Predictions":"Decision based on the information in this ADR.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pattern-atlas-ui\/0002-use-a-serverside-compont-to-complete-oauth-flow.md","Context":"## Context and Problem Statement\\nTo add\/modify content on our patternpediacontent repo 'on behalf of the user', we need to use OAuth Authentification in our client-side application.\\nBut requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).\\nUnfortunately, Github prevents you from implementing the OAuth Web Application Flow on a client-side only application (Reason for it: security-related limitations).\\n","Decision":"Chosen option: \"Use our own server-side component\", because\\n* we don't want to provide our their client id and secret to an (possibly untrusted) open reverse proxy (comment from the stackoverflow-article: If the owner of the proxy wants to log credentials from the requests they can)\\n","tokens":107,"id":346,"Predictions":"Decision (A) To implement the OAuth Web Application Flow on a client-side only application.nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).nBy requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)"}
{"File Name":"pattern-atlas-ui\/0001-define-pattern-structure.md","Context":"## Context and Problem Statement\\nWhen creating and updating pattern languages and patterns, the structure should be defined so that users can implement their own renderers for a specific pattern language.\\n## Decision Drivers\\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\n","Decision":"* Pattern languages can be very different, so we should restrict their properties as little as possible.\\nA pattern consists of\\n* name (string), e.g. \"Elastic Infrastructure\"\\n* type (string), e.g. <https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns#CloudComputingPattern>\\n* uri (string, e.g. <https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns\/elasticinfrastructure#ElasticInfrastructure>)\\n* a Map for the section properties of its pattern language (Map<section, string | string[]>),\\ne.g. for the section  https:\/\/purl.org\/patternpedia\/cloudcomputingpatterns#hasLogo we can obtain the corresponding value  \"https:\/\/www.cloudcomputingpatterns.org\/img\/book.png\" or an array of strings\\nTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like \"https:\/\/www.cloudcomputingpatterns.org\/img\/book.png\" as links\/pictures and not only as text.\\n","tokens":60,"id":347,"Predictions":"We should restrict their properties as little as possible.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"twig-infrastructure\/0006-use-ecs-fargate-host-type.md","Context":"## Context\\nAWS Bare Metal rig gives you the choice between EC2 hosting or FARGATE for compute.\\n","Decision":"For the Twig riglet, we will use FARGATE.  Primary driver for this decision is to have a reference the uses FARGATE instead of EC2, and we are in the process of updating the Twig riglet.\\n","tokens":23,"id":348,"Predictions":"Decision (A) The EC2 hosting option is better than the FARGATE option.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"twig-infrastructure\/0002-use-aws-bare-metal-rig-approach.md","Context":"## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n","Decision":"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: CodePipeline, with Jenkins as an eventual target\\n","tokens":24,"id":349,"Predictions":"Decision (A) The riglet should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig.nbr>The twig should be a twig"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"Predictions":"Decision (A) The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.nbr>The riglet flavor should be a newer\/simpler riglet flavor.n"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"Predictions":"Decision (A) The build step should be re-configured to allow for a full rebuild.nbr>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br>br"}
{"File Name":"twig-infrastructure\/0004-use-cloudwatch-logs-for-log-aggregation.md","Context":"## Context\\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\\n","Decision":"We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\\n","tokens":27,"id":352,"Predictions":"We need to make sure the logs are correct.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"twig-infrastructure\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":353,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Marain.Instance\/0002-services-use-arm-and-aad-indirectly-through-common-mechanisms.md","Context":"## Context\\nHistorically, our automated deployment scripts have tended to include a large amount of common boilerplate. This is partly because they are self-contained\u2014they can typically be run on their own to perform a deployment. And it is partly just because they originate from what the Visual Studio tooling creates.\\nThe problem with this is that it makes the project-specific details hard to spot. When looking at a sea of code that's almost identical to every other project, it's hard to see what it's doing that is in any way different from everything else.\\n","Decision":"With `Marain.Instance`, deployment scripts in individual services do not communicate directly with either ARM or Azure AD. (They should not even be aware of what mechanisms are being used to perform this work\u2014they should not need to know whether we are using the PowerShell Az module, the az CLI or even custom library code to talk to Azure, for example.)\\nAnything that needs to be done either in Azure or AAD must be done through operations provided by the shared `Marain.Instance` code. It passes in an object that provides various methods that provide the necessary services.\\n","tokens":112,"id":354,"Predictions":"The problem with this is that it makes the project-specific details hard to spot.nThe answer is No.n[1].nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Marain.Instance\/0003-shared-application-insights.md","Context":"## Context\\nMarain services need to be able to deliver diagnostic information somewhere.\\n","Decision":"`Marain.Instance` creates a single Application Insights instance and makes its key available to all services. All services use it.\\n","tokens":17,"id":355,"Predictions":"Decision Makers should consider the following options: a). The service provider should be able to deliver diagnostic information somewhere. b). The service provider should be able to deliver diagnostic information somewhere. c). The service provider should be able to deliver diagnostic information somewhere. d). The service provider should be able to deliver diagnostic information somewhere. e. The service provider should be able to deliver diagnostic information somewhere. f. The service provider should be able to deliver diagnostic information somewhere. g. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h. The service provider should be able to deliver diagnostic information somewhere. h."}
{"File Name":"Marain.Instance\/0004-powershell-core-6-deployment-scripts.md","Context":"## Context\\nDeployment requirements need to be expressed somehow\u2014either declaratively or in a programming language.\\n","Decision":"Individual Marain services express their deployment requirements in the form of a set of PowerShell scripts. These will be run in PowerShell core v6.\\n","tokens":21,"id":356,"Predictions":"Decision Makers should consider the following: nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"Predictions":"Decision (A) The Marain instance must be a separate instance from the production instance. (B) The Marain instance must be a separate instance from the production instance. (C) The Marain instance must be a separate instance from the production instance. (D) The Marain instance must be a separate instance from the production instance. (A) The Marain instance must be a separate instance from the production instance. (A) The Marain instance must be a separate instance from the production instance. (B) The Marain instance must be a separate instance from the production instance. (C) The Marain instance must be a separate instance from the production instance. (D) The Marain instance must be a separate instance from the production instance. (A) The Marain instance must be a separate instance from the production instance. (A) The Marain instance must be a separate instance from the production instance. (B) The Marain instance must be a separate instance from the production instance. (C) The Marain instance must be a separate instance from the production instance. (D) The Marain instance must be a separate instance from the production instance. (A) The Marain instance must be a separate instance from the production instance. (A) The Marain instance must be a separate instance from the production instance. (B) The Marain instance must be a separate instance from the production instance."}
{"File Name":"Marain.Instance\/0006-process-for-onboarding-new-tenants.md","Context":"## Context\\nWe have defined (in [ADR 0005](0005-multitenancy-approach-for-marain.md)) the way in which we intend to implement Tenancy in Marain instances using the `Marain.Tenancy` service. As noted in that ADR, managing the desired model by hand would be excessively error prone and as such, we need to design tooling that will allow us to create and manage new tenants, and to allow them to use the Marain services they are licenced for.\\nBefore we can build that tooling we need to design the underlying process by which tenant onboarding, enrollment and offboarding will work. This needs to allow new Client Tenants to be onboarded into Marain services without tightly coupling the services so some central thing that knows everything about them.\\n","Decision":"We are envisaging a central control-plane API (referred to for the remainder of this document as the \"Management API\") for Marain which primarily builds on top of the Tenancy service. This will provide the standard operations such as creating new tenants and enrolling them to use Marain services.\\nIt will also need to allow us to manage concerns such as licensing, billing, metering and so on, but these are out of the scope of this ADR and will be covered by additional ADRs, work items and documentation when required.\\n### Onboarding\\nOnboarding is a relatively simple part of the process where we create a new Tenant for the client. We will need to determine how we intend licensing to work and what part, if any, the Management API plays.\\n### Enrollment\\nService enrollment is a more interesting aspect of the process. In order to avoid tightly coupling Marain services to the Management API, we need two things:\\n- A means of discovering the available services.\\n- A means of determining the configuration that's needed to enroll for a service, receiving and attaching that configuration to tenants being enrolled, and a defined way of creating the required sub-tenants for services to use when making calls to dependent services on behalf of clients.\\nAs described in [ADR 0005](0005-multitenancy-approach-for-marain.md), we are envisaging that each service has a Tenant created for it, under a single parent for all Service Tenants. These tenants can then underpin the discovery mechanism that allows the management API to enumerate services that tenants can be enrolled into.\\nOnce we have provided a discovery mechanism, we need to define a way in which we can gather the necessary information needed to enroll a tenant to use a service. We are intending to make this work by defining a common schema through which a service can communicate both the configuration it requires as well as the services upon which it depends. Services can then attach a manifest file containing this information to their Service Tenant via a well known property key, allowing the Management API to obtain the manifest as part of the discovery process.\\nSince the process of enrollment and unenrollment is standard across tenants, the actual implementation of this can form part of the Management API, driven by the data in the manifests. If we ever encounter a situation where services need to perform non-standard actions as part of tenant enrollment, we can extend the process to support a way in which services can be notified of new enrollments - this could be a simple callback URL, or potentially a broadcast-type system using something like Azure Event Grid. Since we don't yet have any services that would need this, we will not attempt to define that mechanism at this time.\\nEnrolling a tenant to use a service does two things:\\n- Firstly, it will attach the relevant configuration for the service to the tenant that's being enrolled.\\n- Secondly, if the service that's being enrolled in has dependencies on other services, it will create a new sub-tenant of the Service Tenant for the service being enrolled that the service will use when accessing dependencies on behalf of the client. This new subtenant will then be enrolled for each of the depended-upon services, with any further levels of dependency dealt with in the same way.\\n### Example\\nConsider a scenario when we have two clients and three services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nThe dependency tree for the services looks like this:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nAs can be seen from this diagram:\\n- Contoso is licenced to use WORKFLOW and FOOBAR\\n- Litware is licenced to use WORKFLOW and OPERATIONS\\n- WORKFLOW has dependencies on OPERATIONS and FOOBAR\\n- OPERATIONS has a dependency on FOOBAR\\nLet's assume that each of the three services require storage configuration, and have a look at what happens when Litware is enrolled to use Workflow.\\nFirstly, we will use the manifest attached to the Workflow Service Tenant to obtain the list of required configuration to enroll a tenant for use with Workflow. The Workflow manifest states that Workflow requires CosmosDB storage configuration, and also that it is dependent on Operations and FooBar. We can then use the manifests on the Operations and FooBar Service Tenants to determine what configuration is required for them. Operations is dependent on FooBar, so the process is repeated there. From this process, we can determine that the list of required configuration for Workflow is the sum of four things: Workflow storage config, Operations storage config, FooBar storage config when invoked from Workflow, and FooBar storage config when invoked from Operations.\\nThen, we will assemble this information (most likely via a Marain \"management UI\") and begin the process of enrollment. First we enroll the Litware tenant to use workflow by attaching the workflow storage configuration to the Litware tenant. Then, because Workflow has dependencies, we create a sub-tenant of the Workflow Service Tenant that will be used to call these dependencies on behalf of Litware and we attach the ID of the new tenant to the Litware tenant using a well known property name specific to Workflow:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nNext, we need to enroll the new WORKFLOW+Litware tenant with the Operations and FooBar services.\\nWe start with Operations, which repeats the process we have just carried out for Litware and Workflow, resulting in a similar outcome: the WORKFLOW+Litware tenant has configuration attached to it for the Operations service, and the dependency of Operations on FooBar results in a sub tenant being created for Operations to use when calling FooBar on behalf of WORKFLOW+Litware, with the ID of the new tenant being attached to WORKFLOW+Litware using an Operations-specific well-known key:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|\\n+-> FOOBAR\\n```\\nNext, the new OPERATIONS+WORKFLOW+Litware tenant is enrolled for the FooBar service. The FooBar service has no dependencies so we do not need to create any further tenants; we simply attach the storage configuration for FooBar to the tenant being enrolled and returns. This also completes the WORKFLOW+Litware tenant's enrollment for Operations.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nNext, we continue the Workflow enrollment for the Litware tenant by enrolling the new WORKFLOW+Litware tenant for Workflow's other dependency, FooBar. As with the Operations service enrolling OPERATIONS+WORKFLOW+Litware with FooBar, this does not result in any further tenants being created, just the FooBar config being attached to WORKFLOW+Litware:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nThis completes Litware's enrollment for the Workflow service. As can be seen, this has resulted in multiple service-specific Litware tenants being created but Litware is never explicitly made aware of the existence of these tenants, nor is it able to use them directly. They are used by their parent services to make calls to their dependencies _on behalf_ of the Litware tenant.\\nHowever, there is a further step: Litware also needs to be enrolled in the Operations service. At present, Workflow is able to use Operations on Litware's behalf using the WORKFLOW+Litware tenant. However, this is an implementation detail of Workflow and something that should be able to change without impacting Litware - as long as it does not result in a change to the public Workflow API. So, in order to allow Litware to use the Operations service directly, the process we went through for Workflow is repeated. The storage configuration for Operations is attached to the Litware tenant, and then a further sub-tenant of Operations will be created for it to use when accessing FooBar on behalf of Litware:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|\\n+-> FOOBAR\\n```\\nThen, the new OPERATIONS+Litware tenant will be enrolled for the FooBar service, which results in FooBar storage configuration being attached to the OPERATIONS+Litware service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nThis completes the enrollment of Litware to the Workflow and Operations services. As can be seen from the above, there are three different paths through which Litware makes indirect use of the FooBar service, and it's possible for the client to use separate storage for each. In fact, this will be the default; even if the client is using Marain storage, the data for their three different usage scenarios for FooBar will be stored in different containers.\\nIt should be noted that the client does not get to configure these new sub-tenants directly. In fact, they will be unaware of them - they are essentially implementation details of our approach to multi-tenancy in Marain. They will not be able to retrieve the sub-tenants from the tenancy service or update them directly. That said, it's likely that the management API will allow the configuration to be changed - but without exposing the fact that these sub-tenants exist.\\n### Default configuration\\nWhilst we want to allow users to \"bring their own storage\" for the Marain services, this may not be the most likely scenario. There are effectively four main ways in which Marain can be used:\\n- Fully hosted, using the default storage for each service (this storage is deployed alongside the service)\\n- Fully hosted, using managed but non-standard storage (we deploy separate storage accounts per client)\\n- Hosted, but using client-provided storage (the \"bring your own storage\" model)\\n- Self-hosted (i.e. deployed into a client's own Azure subscription), in which case we would expect the storage deployed with the service to be used - essentially the same as the fully hosted option.\\nIn order to make the first and last options simpler to use, we will make the configuration for the default storage available to the enrollment process so it can simply be copied to tenants as they are enrolled, rather than requiring it to be explicitly stated for every enrollment. As such, we need the manifest file schema to allow marking configuration as optional, indicating that defaults should be used if that configuration is not provided. The most sensible location to store this default configuration is on the Service Tenant itself.\\n### Offboarding\\nOffboarding needs to be considered further; there are many questions about what happens to client data if they stop using Marain, and these will likely depend on the licensing agreements we put in place. As a result this will be considered at a later date.\\n","tokens":167,"id":358,"Predictions":"Decision (A) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.nB) The Marain.Tenancy service should be designed by a central system.n"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"Predictions":"Decision (A) The Marain system should be multitenanted. (B) The Marain system should be multitenanted. (C) The Marain system should be multitenanted. (D) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (B) The Marain system should be multitenanted. (C) The Marain system should be multitenanted. (D) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (B) The Marain system should be multitenanted. (C) The Marain system should be multitenanted. (D) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (A) The Marain system should be multitenanted. (B) The Marain system should be multitenanted. (C) The Marain system should be multitenanted."}
{"File Name":"fxa\/0024-upgrade-templating-toolset-of-auth-server-emails.md","Context":"## Context and Problem Statement\\n[Handlebars](https:\/\/www.npmjs.com\/package\/mustache), an extension of [Mustache](https:\/\/www.npmjs.com\/package\/mustache) ([see the differences](https:\/\/github.com\/handlebars-lang\/handlebars.js#differences-between-handlebarsjs-and-mustache)) used in `fxa-auth-server` email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system [began here](https:\/\/github.com\/mozilla\/fxa\/issues\/4627). While converting our emails to a more modern templating solution, there is an opportunity to evaluate what stack would be the most ideal for FxA emails beyond a proposed templating solution. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.\\n","Decision":"### Templating and Styling\\n- Option A - Continue to use Mustache and ad-hoc inline styles\\n- Option B - Use React server-side to generate static HTML templates with TailwindCSS\\n- Option C - Use EJS and MJML, using CSS options offered by MJML\\n### Email previewing\\n- Option A - Continue to use the `write-emails` command\\n- Option B - Use Storybook\\n- Option C - Use Mailtrap\\n---\\nFurthermore, there are a few **other decisions** worth noting that won\u2019t necessarily have pros\/cons lists:\\n- How to handle generating plaintext versions\\n- Transitioning to Fluent over GetText for localization\\n- Plans around integration involving feature flagging and the QA process\\n### Templating and Styling\\nChosen option: \"Option C - Use EJS and MJML, using CSS options offered by MJML\", because:\\n- HTML email has a lot of quirks - MJML shifts the burden of maintaining solutions for these off of FxA engineers now and in the future\\n- While we use React and Tailwind in other parts of FxA, React is heavy for an email solution since no state is involved, component reuse across FxA would likely be very minimal, and it involves a more complex build setup than EJS\\n- MJML helps significantly with responsiveness in emails, reducing time spent developing templates and making future email redesigns easier\\n### Email Previewing\\nChosen option: \"Option B - Use Storybook\", because:\\n- It provides us the flexibility to preview all of the different email states together\\n- We have a Storybook deployment already in place so hooking it up for `fxa-auth-server` will ensure consistency\\n- Easy to test and perform QA without needing to touch the codebase\\n- Support for CSS (whether it's plain old CSS, TailwindCSS or something else)\\nNote: Along with using storybook to view the various states of the templates, we are also planning on continued support of [Maildev](https:\/\/www.npmjs.com\/package\/maildev) since it has applications beyond just previewing emails.\\n### Other\\n**Plaintext files**: We'll use [html-to-text](https:\/\/github.com\/html-to-text\/node-html-to-text) to automatically generate plaintext versions of templates rendered to HTML. There may be a scenario in which the automatically-generated plaintext version does not look exactly how we'd like, in which case we can look into exporting both an MJML _and_ plaintext version of the email from the template file.\\n**Localization**: We will upgrade the localization tool from GetText to [Fluent](https:\/\/github.com\/projectfluent\/fluent.js) since it's preferred by the l10n team and other FxA front-ends are using it. With our chosen templating option we can make use of Fluent's [`@fluent\/dom`](https:\/\/github.com\/projectfluent\/fluent.js\/tree\/master\/fluent-dom) package.\\n**Integration & QA**: During development templates will be marked as being part of a release group. This could be in the form of a mapped list of template names or some variable associated with the template file. Each group will have a corresponding environment variable flag. When this release group\u2019s flag is enabled any templates that fall under it will be used when generating an email template in that environment; until a template's corresponding release group is enabled it is not used and the old\/current template will continue to be served. Release groups will be initially enabled in staging until QA has had an opportunity to thoroughly test and evaluate each template in the group, after which they can be incrementally enabled in production. To give us the most flexibiity we can add a new auth-server configuration value that can control which users email are supported and which email templates are supported for mjml. The auth-server can then expose a feature flag method to check this value and then render the correct template.\\n### Positive Consequences\\n- Using MJML abstracts HTML email peculiarities away and handles responsiveness for us\\n- Allows us to move away from inline CSS during development, and improves style reusability and consistency in template files\\n- Using EJS allows us to write templates using JavaScript, removing the complexities of custom syntaxes like JSX and Mustache\\n### Negative Consequences\\n- The Storybook setup will be much more complex than with a React and Tailwind solution\\n- Introduces new dependencies, and MJML introduces a small learning curve\\n- Our email templates have been battle-tested over the years, and this change could introduce potential new bugs across various clients\\n","tokens":196,"id":361,"Predictions":"Decision (A). [Handlebars](https:\/\/www.npmjs.com\/package\/mustache) ([see the differences](https:\/\/github.com\/handlebars-lang\/handlebars.js#differences-between-handlebars-and-mustache)) used in fxa-auth-server email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.n [A] Decision (A). [H] Handlebars [https:\/\/www.npmjs.com\/package\/mustache] ([see the differences](https:\/\/github.com\/handlebars-lang\/handlebars.js#differences-between-handlebars-and-mustache)) used in fxa-auth-server email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.n"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"Predictions":"The db service was moved to be colocated with the auth-server on the frontend servers. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.n## Decision Driversn- Reduce complexityn- Improve performancen- Share coden ## Decision Conclusions The db service was moved to be colocated with the auth-server on the frontend servers. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.n## Decision Driversn- Reduce complexityn- Improve performancen- Share coden ## Decision Finalizationnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"fxa\/0003-event-broker-for-subscription-platform.md","Context":"## Context and Problem Statement\\nExternal OAuth Relying Parties (RPs) that Firefox Account users authenticate\\nto need to be kept up to date about whether the user still has an active\\nsubscription as well as knowing when to delete the user data. For internal\\nMozilla use, Mozilla provides [Firefox Service Notifications][] over SQS\\nqueues. These events are intended for internal trusted use and not suitable\\nfor external RPs.\\nA solution for these concerns is a new system referred to as the FxA Event\\nBroker which will store FxA related events and distribute them via webhooks\\nto relevant RPs that the user has accessed. The initial version described here applies primarily to external RPs which will only receive subscription status\\nchanges and account deletion events. This solution is being built with future\\nexpansion in mind which will require event stream storage per user and\\nnotification of internal Mozilla RPs.\\nDifficulties inherent in this solution lie in where to store the source of\\ntruth for what RPs a user has authenticated to, where the delivery functionality\\nshould reside, where the event streams for a user should reside, what set of\\ndata should be included in [Firefox Service Notifications][], and how this\\ndata should be communicated to the delivery system to avoid additional API\\nquery load on existing FxA services.\\n## Decision Drivers\\n- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\n","Decision":"- Subscription services deadlines\\n- Effort required and experience available for FxA changes\\n- Separation of concerns from existing FxA microservices\\n- Difficulty of schema migrations in existing FxA microservices\\n- Suitability of existing FxA databases for large-scale event storage\\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\\nin-progress)\\nChosen Option: C. Implementing RP notification as a new FxA Event Broker service\\nwith webhook and login activity stored in FxA Event Broker, because\\n- Less subscription platform timeline risk to store new data in new database vs.\\nmodify existing FxA OAuth database.\\n- Storing events at scale has database requirements that don't fit in well with\\nthe limitations with MySQL encountered in FxA Auth\/OAuth.\\n- Having FxA Auth be the only store of which RP to notify would require each\\nnotification to also include what RPs to notify, increasing the load on the\\nFxA Auth database.\\n","tokens":352,"id":363,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"fxa\/0019-use-workspace-dependencies.md","Context":"## Context and Problem Statement\\nSince becoming a monorepo we haven't found a way to share code across packages that works well in local development, CI, and production without fragile, ad-hoc workarounds.\\n## Decision Drivers\\n- Simpler code sharing\\n- Minimal overhead in setup and deployment\\n- Works well for js and ts packages\\n- Low maintenance\\n- Long term viability\\n","Decision":"- Simpler code sharing\\n- Minimal overhead in setup and deployment\\n- Works well for js and ts packages\\n- Low maintenance\\n- Long term viability\\nThe [workspace:\\*](https:\/\/yarnpkg.com\/features\/workspaces) dependency type along with typescript [project references](https:\/\/www.typescriptlang.org\/docs\/handbook\/project-references.html) provide a good internal dependency solution that addresses the shortcomings of our current situation.\\nWorkspaces are not yet supported by npm but are on the [v7 roadmap](https:\/\/blog.npmjs.org\/post\/617484925547986944\/npm-v7-series-introduction) so it's pretty safe to assume that they will be widely supported for the foreseeable future. In order to start using workspaces today we'll need to use yarn. This is a slight risk because although it is well supported it doesn't have the level of backing that npm does, however once workspaces are supported by npm there is nothing locking us in to yarn.\\n### Advantages\\n- No manual linking of dependencies\\n- Simpler builds\\n- Single lock file\\n- Hoisted dependencies\\n### Disadvantages\\n- In the short term this requires us to switch from npm to yarn as our package manager. This will change dev workflows and is another tool to understand.\\n- Some code changes are required to be compatible with yarn, however they're also backward compatible with npm.\\n- Yarn doesn't currently support auditing dependencies like npm does.\\n","tokens":81,"id":364,"Predictions":"The driver is a monorepo. It is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that is a monorepo that"}
{"File Name":"fxa\/0009-testing-stacks.md","Context":"## Context and Problem Statement\\nWe have a variety in tools used for unit & functional tests. This variety means developers need to learn & know many different tools. It would be nice to reduce this mental workload, while still using appropriate tools for testing.\\n## Decision Drivers\\n- Ensuring we're using the right tool for the job.\\n- Reducing developer mental workload for working with tests.\\n","Decision":"- Ensuring we're using the right tool for the job.\\n- Reducing developer mental workload for working with tests.\\nIdentify a consistent stack of testing tools per aspect of the project\\n","tokens":81,"id":365,"Predictions":"We should reduce the developer's workload.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"fxa\/0012-next-two-factor-authentication.md","Context":"## Context and Problem Statement\\nFirefox Account originally implemented multi-factor authentication (MFA) support in Q1 of 2018.\\nThis feature used TOTP based codes and was based on RFC 6238.\\nAdditionally, if users lost their MFA device, they could use one time backup authentication codes to regain access to the account.\\nHaving MFA support has helped secure our users' accounts and given them more security flexibility.\\nHowever, over time it has become more obvious that users that lose their MFA device (usually phone) are at risk of getting locked out of their account because they don't save their backup authentication codes.\\nThere are a non-trivial amount of users that don\u2019t save or download their backup authentication codes, which is currently the only way they can regain access.\\nIn 2019 Q4, FxA started requiring that users confirm backup authentication codes before enabling MFA.\\nWhile this did help reduce lockouts, we still want to reduce it further.\\nWe believe that adding a new MFA method to Firefox Accounts that has the similar security properties as TOTP would allow users to have another method to recover their account.\\n## Decision Drivers\\n- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\n","Decision":"- Improve user's account security\\n- Reduce the risk of account lockout because of lost device\/backup authentication code\\n- Could be completed in roughly a quarter\\nChosen option: Option A, because this is more inline with Mozilla's security and privacy principles than option B.\\nThere is less security risk to users if this feature is added.\\n","tokens":274,"id":366,"Predictions":"Decision Makersn- Improve user's account securityn- Reduce the risk of account lockout because of lost device\/backup authentication coden- Could be completed in roughly a quartern ## Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn"}
{"File Name":"fxa\/0026-expand-subscription-platform-metadata.md","Context":"## Context and Problem Statement\\nThe Subscription Platform uses [Stripe Metadata][stripe-metadata] for numerous\\npurposes, including product checkout content, legal document download URLs,\\nand product support form options. The metadata can be updated by product\\nmanagers through the Stripe dashboard, enabling product managers to manage\\ncertain aspects of their subscription products without involving Subscription\\nPlatform engineers. However, Stripe metadata has a limit on the number of\\nvalues that we will reach in the near future. In order to continue to use\\nmetadata in the same fashion in the Subscription Platform, we need to overcome\\nthe limit placed by Stripe.\\n[stripe-metadata]: https:\/\/stripe.com\/docs\/api\/metadata\\n## Decision Drivers\\n- A higher storage limit than Stripe Metadata\\n- A UI for the managers of products on the Subscription Platform to edit the\\nmetadata\\n- Access management for the UI\\n- Updated data accessible in their respective environments without deployment\\n","Decision":"- A higher storage limit than Stripe Metadata\\n- A UI for the managers of products on the Subscription Platform to edit the\\nmetadata\\n- Access management for the UI\\n- Updated data accessible in their respective environments without deployment\\nChosen option: Google Cloud Firestore, because\\n- Data management UI and user access control provided through the Google Cloud\\nPlatform (GCP) console.\\n- It does not require deployments for data updates.\\n- It is not novel tech; FxA already uses it elsewhere.\\n","tokens":188,"id":367,"Predictions":"Decision Based on the information in this record, we believe that the Subscription Platform should be able to continue to use [Stripe Metadata][stripe-metadata] in the Subscription Platform without involving Subscription Platform engineers. Therefore, we believe that the Subscription Platform should be able to continue to use [Stripe Metadata][stripe-metadata] in the Subscription Platform without involving Subscription Platform engineers. Therefore, we believe that the Subscription Platform should be able to continue to use [Stripe Metadata][stripe-metadata] in the Subscription Platform without involving Subscription Platform engineers. Therefore, we believe that the Subscription Platform should be able to continue to use [Stripe Metadata][stripe-metadata] in the Subscription Platform without involving Subscription Platform engineers. Therefore, we believe that the Subscription Platform should be able to continue to use [Stripe Metadata][stripe-metadata] in the Subscription Platform without involving Subscription Platform engineers. Therefore, we believe that the Subscription Platform should be able to continue to use [Stripe Metadata][stripe-metadata] in the Subscription Platform without involving Subscription Platform engineers. Therefore, we believe that the Subscription Platform should be able to continue to use [Stripe Metadata][stripe-metadata] in the Subscription Platform without involving Subscription Platform engineers."}
{"File Name":"fxa\/0015-use-css-variables-and-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) offers an ideal opportunity to review CSS conventions and architecture for FxA to not only use while building new components for this project, but also for FxA moving forward.\\nThis [part 1](https:\/\/github.com\/mozilla\/fxa\/issues\/4808) of [2](https:\/\/github.com\/mozilla\/fxa\/issues\/5087) ADR concerns the use of CSS variables, how they come into play with SCSS (vs using SCSS variables), CSS-in-JS considerations, and CSS modules.\\nNote that 1) \"CSS variables\" are technically called \"CSS custom properties\" but will be referred to as the common \"variable\" nomenclature throughout this ADR and that 2) SASS is a CSS preprocessor while SCSS is our preferred syntax of SASS (explained [here](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)).\\n## Decision Drivers\\n- Simplicity and clarity\\n- DRY and reusable styles\\n- Easy integration and use\\n- Current engineering experience and preferences\\n","Decision":"- Simplicity and clarity\\n- DRY and reusable styles\\n- Easy integration and use\\n- Current engineering experience and preferences\\nChosen options: \"CSS variables\" and \"SCSS\", because:\\n- While using CSS variables with SASS will prevent using mixins (or SASS functions like `darken`) that rely on variable values, they can be used without impeding core functionality FxA uses SASS for like nested selectors, extends, and functions and mixins that don't rely on variable values like the generation and use of our media queries.\\n- CSS variables are native to the browser and decision to use them now is a prudent one. They have good browser support and can be used without a preprocessor and changed after preprocessor compilation. They are targetable by JavaScript, easily themed, and can be scoped globally or on an element, providing options for usage.\\n- Scoped component solutions (CSS-in-JS, CSS modules) encourage self-contained components rather than building the UI as a whole. While one goal of FxA is to reuse components across the ecosystem where possible (e.g. the `fxa-react` package), FxA will likely reap more benefits from a class-based approach. This allows for globally shared styles and a loosely coupled stylesheet rather than a tightly coupled CSS-in-JS solution with conditional styles based on props and how the component is used. Classes promote a DRYer and more consistent approach.\\n- CSS-in-JS would add one additional layer of tech to learn (like `styled-components` syntax and best practices) while the Settings Redesign project is already introducing other novel tech, and members of the FxA engineering team also as a whole personally prefer not to use CSS-in-JS.\\n- This decision doesn't preclude the option of implementing CSS modules at a later time if it's determined that we would prefer component scoped styles with SCSS, and CSS variables could still be used if we later switch to CSS-in-JS.\\n","tokens":256,"id":368,"Predictions":"Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) offers an ideal opportunity to review CSS conventions and architecture for FxA to not only use while building new components for this project, but also for FxA moving forward.nThis [part 1](https:\/\/github.com\/mozilla\/fxa\/issues\/4808) of [2](https:\/\/github.com\/mozilla\/fxa\/issues\/5087) ADR concerns the use of CSS variables, how they come into play with SCSS (vs using SCSS variables), CSS-in-JS considerations, and CSS modules.nNote that 1) \"CSS variables\" are technically called \"CSS custom properties\" but will be referred to as the common \"variable\" nomenclature throughout this ADR and that 2) SASS is our preferred syntax of SASS (explained [here](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) offers an ideal opportunity to review CSS conventions and architecture for FxA to not only use while building new components for this project, but also for FxA moving forward.nThis [part 1](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/github.com\/mozilla\/fxa\/issues\/5087) ADR concerns the use of CSS variables, how they come into play with SCSS (vs using SCSS variables), CSS-in-JS considerations, and CSS modules.nNote that 1) \"CSS variables\" are technically called \"CSS custom properties\" but will be referred to as the common \"variable\" nomenclature throughout this ADR and that 2) SASS is our preferred syntax of SASS (explained [here](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn: The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/408) of [2](https:\/\/stackoverflow.com\/questions\/5654447\/whats-the-difference-between-scss-and-sass\/5654471#5654471)n# Decision Makersn:"}
{"File Name":"fxa\/0027-document-store-for-payments.md","Context":"## Context and Problem Statement\\nPayment data used in Stripe, PayPal, Apple, and Google IAP are handled as individual documents. We typically need to access different portions of these at different times. In addition, their schema occasionally changes (API updates). Stripe and PayPal have API limits, which means we typically need to duplicate\/cache data we retrieve from them. At the moment we cache this data in Redis. This mostly works, but restricts our ability to run queries across payment data sets without pulling it all into BigQuery, or making substantial amounts of Stripe\/PayPal requests.\\nTo handle IAP, we need to store Apple receipts and Google subscription objects. These should be organized with the FxA user they belong to. They're arbitrary JSON documents that we also need copied to BigQuery for ETL jobs. To future-proof our payment data storage needs we should have the flexibility needed to store Stripe\/Paypal payment data in the same data store with the same ETL capability.\\n## Decision Drivers\\n- Engineering resources\\n- Arbitrary JSON document querying ability\\n- Ease of loading data for ETL jobs\\n- Capable of storing arbitrary JSON data\\n- Integration with Stripe and other payment systems\\n","Decision":"- Engineering resources\\n- Arbitrary JSON document querying ability\\n- Ease of loading data for ETL jobs\\n- Capable of storing arbitrary JSON data\\n- Integration with Stripe and other payment systems\\nGoogle Firestore. We've used Firestore in the event-broker, it has a rich API, and extensions that overlap with our requirements that reduces engineering and operations resources.\\n","tokens":242,"id":369,"Predictions":"Decision Based on the following: The current payment data store is Redis. The current payment data store is Apple. The current payment data store is Google. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Google. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Google. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is PayPal. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Stripe. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple. The current payment data store is Apple"}
{"File Name":"fxa\/0001-isolating-payment-content-with-third-party-widgets-from-general-account-management.md","Context":"## Context and Problem Statement\\nIn the implementation of payment features for subscription services, we've\\ndecided to use third-party JavaScript for payment widgets.\\nBest practices established by our security team indicate that third-party JS\\nshould not be included on the highly-sensitive pages - i.e. such as those used\\nfor general account management on Firefox Accounts.\\nSo, we need a way to isolate the pages responsible for subscription sign-up and\\nmanagement from the rest of Firefox Accounts.\\n## Decision Drivers\\n* Security when dealing with financial transactions.\\n* Security when including with third-party JS code for payment widgets.\\n* Simplicity in user experience flows.\\n* Delivering against the subscription services deadline.\\n","Decision":"* Security when dealing with financial transactions.\\n* Security when including with third-party JS code for payment widgets.\\n* Simplicity in user experience flows.\\n* Delivering against the subscription services deadline.\\nChosen option: \"Option A - Payment pages as separate app supplied with\\npre-generated access token\", because\\n* Further refinements to access token delivery mechanism in Option A do not\\nsignificantly affect the rest of the payments app.\\n* Doesn't preclude an upgrade to Option B in the future - i.e. once [Issue\\n#640](https:\/\/github.com\/mozilla\/fxa\/issues\/640) is resolved.\\n* Doesn't preclude Option C as a future option - e.g. offering embedded\\nsubscription widgets to third-parties.\\n* Fastest practical option given existing record of reviews by security & UX and\\nwork completed so far.\\n* Fresh start with a more modern web stack (i.e. React).\\n","tokens":144,"id":370,"Predictions":"Decision Based on the information in this document, we believe that third-party JavaScript should be included on highly-sensitive pages - i.e. such as those used for general account management on Firefox Accounts.nSo, we need a way to isolate the pages responsible for subscription sign-up andnmanagement from the rest of Firefox Accounts.n## Decision Driversn* Security when dealing with financial transactions.n* Security when including with third-party JS code for payment widgets.n* Simplicity in user experience flows.n* Delivering against the subscription services deadline.n## Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n Decision.n"}
{"File Name":"fxa\/0023-paypal-support.md","Context":"## Context and Problem Statement\\nThe Subscription Platform was implemented with Stripe as the payment processor and driver of subscription logic. Stripe is used as the source of truth for whether an account has subscriptions associated with it. We would like to add PayPal as a payment option for subscriptions, and integrate it appropriately with the existing system in a least-effort manner that is ideally flexible enough to support future expansions of payment processors, such as IAP.\\n## Decision Drivers\\n- Engineering resources\\n- Future effort needed to integrate IAP\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\n","Decision":"- Engineering resources\\n- Future effort needed to integrate IAP\\n- Code complexity as it relates to existing Subscription Platform infrastructure\\nStripe-driven out-of-band invoice processing. This was originally considered to be the slower of the approaches, but using PayPal subscriptions ended up being approximately the same engineering effort. Given additional user experience benefits of the Stripe-driven approach, it is now the recommend implementation path.\\n","tokens":116,"id":371,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"fxa\/0002-use-react-redux-and-typescript-for-subscription-management-pages.md","Context":"## Context and Problem Statement\\nA solution for isolating third-party payment widgets from the rest of account\\nmanagement can include building a separate web app on its own dedicated domain.\\nAs a side effect in building that app, we have the opportunity to choose\\ntechnologies for building it that don't necessarily follow the rest of FxA.\\n## Decision Drivers\\n* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\n","Decision":"* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\nChosen option: \"React, Redux, Typescript\", because\\n* Chance to start with a fresh stack\\n* More vibrant ecosystem\\n* Better tooling & developer ergonomics\\n","tokens":111,"id":372,"Predictions":"Decision Makersn* The solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from the rest of accountnmanagement can include building a separate web app on its own dedicated domain.nA solution for isolating third-party payment widgets from"}
{"File Name":"fxa\/0006-json-schemas-for-messaging.md","Context":"## Context and Problem Statement\\nFirefox Accounts sends and receives messages in JSON utilizing queues. This ADR addresses this\\nuse-case only and does not apply to JSON used in RESTful APIs.\\nTracking and minimizing drift between the accuracy of message format documentation and\\nthe underlying implementation is an error-prone process. In addition, JSON messages are\\nfrequently produced and consumed from multiple systems, and in multiple langauges. Ensuring\\nvalidity of these messages frequently requires implementing validation logic in each\\nlanguage that consumes these JSON messages to ensure correctness for message handling.\\nAdding a field to a message format is non-trivial, as every consumer must also update\\nits own validation logic. Currently there are a variety of places where messages are consumed\\nand produced, most producers do not validate the message they send as the consumer code is not\\nalways in the same service where the validation code typically resides.\\nThere is currently no versioning for our message formats, which also creates difficulties in\\nknowing when validation logic must change and when existing flexibility is sufficient.\\n## Decision Drivers\\n- Accurate documentation via JSON-Schema doc generation\\n- Validation logic generated from JSON-Schema to reduce manually written\/maintained code\\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\\n","Decision":"- Accurate documentation via JSON-Schema doc generation\\n- Validation logic generated from JSON-Schema to reduce manually written\/maintained code\\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\\nChosen option: \"option 3\", because our existing messages can be represented without any\\nmigration needed and external consumers already work with JSON.\\n### [option 1] Keep existing approach\\nCurrently our documentation drifts if a message format is tweaked and the docs are not updated. Ideally\\nour code review process would catch this but sometimes code subleties are not caught that alter the\\nmessage format. Validation changes can also fail to appear in the documentation, even though what is\\nconsidered a valid message key\/value should be documented.\\nWe have to occasionally duplicate `joi` schemas between repos, which should likely be moved to `fxa-shared`\\nto benefit re-use, but this does not help non-Node projects consume our messages.\\n- Pros\\n- No additional work is needed that we don't already do.\\n- Cons\\n- Our documentation frequently drifts.\\n- It's additional work to maintain accurate `joi` schemas spread throughout the code-base _or_\\n- It's additional work to consolidate our `joi` schemas in `fxa-shared`.\\n- `joi` schemas can't be used by consumers not written in JS.\\n### [option 2] Use protobuf\\nUsing protobuf would allow us to validate and document valid message formats that would also consume\\nless space and be more performant to serialize\/deserialize. Validation is built-in as invalid messages\\nwill not deserialize, and documenation can be generated based on the protobuf spec for a message.\\nProtobuf tooling is widely available for most langauges.\\n- Pros\\n- High throughput messaging could benefit from higher efficiency of serializing\/deserializing of messages\\n- Documentation stays in sync with message spec\\n- Validation is built-in to the message serialization\/deserialization process\\n- Cons\\n- Messages are in binary and not easily introspectable\\n- Existing message consumers will need to be updated to handle protobuf\\n- Migration procedure will need to be created\/implemented to shift from existing JSON format\\n### [option 3] Use JSON-Schemas\\nJSON-Schemas can define existing FxA JSON messages used in our queue system. There are libraries in\\nmost languages to consume these schemas for validation purposes, and libraries exist to generate\\ndocumenation from a JSON-Schema.\\n[SchemaVer] can be introduced to existing FxA JSON messages as consumers ignore unknown keys, while\\nconsumers using the JSON-Schema can verify they can handle new message formats or if they're incompatible.\\nSome of this logic is manual, as [SchemaVer] is not an official part of the JSON-Schema specification. There\\nis a recommendation of [SchemaVer] in [Best Web Practices for Data](https:\/\/www.w3.org\/TR\/dwbp\/#dataVersioning).\\n- Pros\\n- No changes needed for existing consumers\\n- Validation can be done based on the JSON-Schema without custom code\\n- Documentation can be generated to stay in sync with message spec\\n- JSON is human readable without further processing.\\n- JSON is widely used on the web and our increasing use of JWTs also result in JSON.\\n- JSON-Schema is widely supported in a variety of langauges.\\n- Cons\\n- [SchemaVer] is a draft that isn't officially used by the JSON-Schema committees, but no other versioning\\nscheme for JSON-Schema appear to exist as of this writing.\\n[schemaver]: https:\/\/snowplowanalytics.com\/blog\/2014\/05\/13\/introducing-schemaver-for-semantic-versioning-of-schemas\/\\n","tokens":269,"id":373,"Predictions":"Decision Makersn: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changesn ## Solution: [Semantic Versioning][schem"}
{"File Name":"fxa\/0007-subscription-claim-jwt-access-token.md","Context":"## Context and Problem Statement\\nA mechanism is needed to inform service providers (SP) that a user has paid for a given subscription. A SP could fetch the user's profile information, however, we prevent some 3rd party SPs from doing so, and instead give them all the information they need in a JWT format access token. Adding subscription information in JWT access tokens will give SPs the information they need to verify users have paid for a subscription.\\n## Decision Drivers\\n- Security - Users should not be able to get access to subscriptions they have not paid for.\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\n","Decision":"- Security - Users should not be able to get access to subscriptions they have not paid for.\\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\\nChosen option: Adding subscription info into its own claim was chosen because FxA's lax scope checking means bad acting users could grant themselves access to subscriptions they have not paid for. See [this bug regarding FxA's lax scope checking][#lax-scope-checking].\\n### Positive Consequences\\n- The JWT access token contains all the information an SP needs to verify the user has paid for a subscription.\\n- Users are unable to grant themselves access to subscriptions they have not paid for.\\n### Negative Consequences\\n- SPs must now check two claims from the JWT to ensure a user is able to access a protected resource.\\n- An additional claim is added that is not defined in [the JWT access token draft spec][#jwt-draft-spec].\\n","tokens":163,"id":374,"Predictions":"Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n## Decision Makers should consider a mechanism to inform service providers that a user has paid for a given subscription.n"}
{"File Name":"fxa\/0010-transition-fxa-from-backbone-to-react.md","Context":"## Context and Problem Statement\\nWhen Firefox Accounts was created some 6-7 years ago, the front-end Javascript framework and HTML templating options were limited significantly to those available at the time. This ADR details the pros and cons of various FE libraries and frameworks, and why it is desirable for FxA to move to a different technology. It does not cover _how_ we will begin the conversion which can be [read about here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md).\\nWhile Backbone and Mustache have served their purposes for FxA thus far, other modern libraries and frameworks not only offer more appealing architecture and developer conveniences, but also offer better data predictability, potential performance boosts, and a strong community of online support.\\n## Decision Drivers\\n- The desire for a more modern FE architecture and a more efficient UI\\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\\n","Decision":"- The desire for a more modern FE architecture and a more efficient UI\\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\\nChosen option: \"Option B - Move to client-side React\", because\\n- The team desires the benefits we will gain from moving to a more modern library (excluding option A)\\n- The team has experience with (and likes) React\\n- We're already using it in other parts of the ecosystem, i.e. the payments server\\n- FxA does not need server-side rendering (option C)\\n- Angular (option D) is heavy, opinionated with a steep learning curve, and offers at least some features we won't need\\n- Web components and libraries around them (option F) are quite new, yet a distant potential architecture shift from React to web components appears to be far less dramatic than from Backbone to React\\nIn the end, it is recognized that Vue _may_ have been a good option for FxA, but React was chosen over Vue due to:\\n- existing FxA engineering team knowledge\\n- the FxA payments server is already using React\\n- React has already been reviewed by our security team and \"passed\" the trial run when it was novel tech in our stack\\n- As one of our desires is to best protect against such a major refactor proposal in the future, we have to allow package age, online support, popularity and backing, and stability to be factors in our decision.\\n","tokens":235,"id":375,"Predictions":"The architecture of Firefox Accounts is a good choice for FxA. It will be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly. It will also be easier to use and more user-friendly."}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"Predictions":"Decision Makersn: The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.nThe existing sessionToken is capable of requesting new OAuth tokens.n Decision Makersn: The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff.n Decision Makersn: The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff.n Decision Makersn: The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff.n Decision Makersn: The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff.n Decision Makersn: The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff.n Decision Makersn: The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not."}
{"File Name":"fxa\/0017-switch-settings-auth-to-sessiontoken.md","Context":"## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\nIn [ADR-0014](0014-auth-for-settings-redesign.md), we'd decided to use OAuth2 as the mechanism. But, upon attempting to implement, we discovered that [context missing from OAuth2 tokens][missing-oauth2-context] rendered the option unusable.\\n[missing-oauth2-context]: https:\/\/github.com\/mozilla\/fxa\/pull\/4931#discussion_r411828476\\n## Decision Drivers\\nBasically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n","Decision":"Basically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\nChosen option: \"Reuse existing sessionToken on content-server with auth-server APIs\", because it's the least novel option requiring fewest changes to auth-server.\\n### Positive Consequences\\n- Easiest path to authenticating on auth-server for API requests, works almost exactly like the settings client does already.\\n### Negative Consequences\\n- We defeat the security promises of Hawk, but we're not really relying on them. We'd also like to move away from Hawk in the future anyway.\\n","tokens":171,"id":378,"Predictions":"Decision Makers: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md):n- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Solution: [ADR-0014](0014-auth-for-settings-redesign.md]"}
{"File Name":"fxa\/0021-use-internjs-testing.md","Context":"## Context and Problem Statement\\nFor over the past 5 years (at least), we have been using [InternJS](https:\/\/github.com\/theintern\/intern) for our functional tests in the content-server. While this has worked well and caught several bugs, we should reevaluate it since we are migrating to react and have an opportunityto use newer\/faster\/better supported testing frameworks.\\n## Decision Drivers\\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\n","Decision":"- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\nUse InternJS for functional testing in the settings v2 app, but also update the tests to support async\/await. The biggest factor was not having to introduce another testing framework for developers to learn.\\n","tokens":104,"id":379,"Predictions":"We should reevaluate the testing frameworks.n## Decision Makersn- Ease of writing testsn- Good community supportn- Learning curve of settings V2 developmentn ## Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn- Decision Makersn"}
{"File Name":"fxa\/0028-retiring-email-service.md","Context":"## Context and Problem Statement\\nThe goal of fxa-email-service was to spin off the email sending responsibilities of FxA into a shared service that multiple Mozilla projects could use. From its readme:\\n> The FxA team had an OKR for Q2 2018 about decoupling the auth server from SES and making it possible to send email via different providers. Subsequently, some other teams expressed an interest in depending on a standalone email service too.\\n> This repo started as our experiment to see what a decoupled email-sending service would look like, written in Rust. It is now handling all FxA email traffic in production, and we are gradually separating it from the FxA stack to run as a standalone service in its own right.\\nHad it achieved the goal of fully decoupling from FxA and being more widely used we'd likely continue using it. However, in the approximately 4 years of its existance it has only really been used as an intermediate step between auth-server and SES. Fortunately in that time it hasn't required much maintenance so its \"weight\", being a fairly large codebase for what it does and our only Rust service, has never been a concern. Recent work to eliminate fxa-auth-db-mysql meant we either needed make changes to it or rethink how FxA sends email. It turned out the work to update email-service was larger than eliminating it and replacing it by sending email directly from auth-server via SES or SMTP.\\n## Decision Drivers\\n- Future maintenance\\n- Email provider flexability\\n","Decision":"- Future maintenance\\n- Email provider flexability\\nWe will eliminate email-service.\\n","tokens":315,"id":380,"Predictions":"fxa-email-service is a decoupled email service that can be used by multiple Mozilla projects.n> The FxA team had an OKR for Q2 2018 about decoupling the auth server from SES and making it possible to send email via different providers. Subsequently, some other teams expressed an interest in depending on a standalone email service too.n> This repo started as our experiment to see what a decoupled email-sending service would look like, written in Rust. It is now handling all FxA email traffic in production, and we are gradually separating it from the FxA stack to run as a standalone service in its own right.nHad it achieved the goal of fully decoupling from FxA and being more widely used we'd likely continue using it. However, in the approximately 4 years of its existance it has only really been used as an intermediate step between auth-server and SES. Fortunately in that time it hasn't required much maintenance so its \"weight\" being a fairly large codebase for what it does and our only Rust service, has never been a concern.nHad it achieved the goal of fully decoupling from FxA and being more widely used we'd likely continue using it. However, in the approximately 4 years of its existance it has only really been used as an intermediate step between auth-server and SES. Fortunately in that time it hasn't required much maintenance so its \"weight\" being a fairly large codebase for what it does and our only Rust service, has never been a concern."}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"Predictions":"**Answer: A. The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem.n** Decision Makersn- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?n## Decision Makersn- **A. The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/5087) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.n** Decision Makersn- **A. The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/5087) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.n** Decision Makersn- **A. The [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/5087](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md)"}
{"File Name":"fxa\/0014-auth-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\n## Decision Drivers\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n","Decision":"- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\nChosen option: \"Authenticate via OAuth2 to use auth-server APIs\", because:\\n- It requires minimal changes to existing auth-server implementation and infrastructure.\\n- It relies on an authentication mechanism with relatively well-known security properties.\\n### Positive Consequences\\n- The OAuth2 access token mechanism is better suited to this purpose than a novel scheme to share session token credentials.\\n- The new Settings Redesign app can use existing auth-server APIs with minimal modifications to accept scoped OAuth2 access tokens. This can constrain most of the novelty in the project to the redesigned & reimplemented settings UX.\\n### Negative Consequences\\n- We will need to modify auth strategies in auth-server APIs, taking care not to affect existing usage in the production settings app.\\n- We don't have an entirely greenfield project, which could be a bit of a drag. But, rediscovering lessons learned in existing code can also be a drag.\\n","tokens":66,"id":382,"Predictions":"The app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.n## Decision Driversn- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Decision Decision Makersn- Smooth UXn- Securityn- Development velocityn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision Decision Makersn- Ease of integrationn ## Decision"}
{"File Name":"fxa\/0013-react-toolchain-for-settings-redesign.md","Context":"## Context and Problem Statement\\nFirefox Accounts is introducing a new visual design for Settings (the interface that exists at the address `https:\/\/accounts.firefox.com\/settings`). The changes involved are significant enough such that it [has been decided](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) that an entirely new React App will be built more or less independent of the existing Settings web app. With this we can beta test and roll out the new design with minimal disruption.\\nAs there are a multitude of ways a React App can be set up and configured, this decision has introduced the question, \"What [toolchain](https:\/\/en.wikipedia.org\/wiki\/Toolchain) should we set up a new React App with?\". This ADR serves to answer that question by going over the various approaches we can take to set up a new React App and how it might integrate with our existing FxA ecosystem.\\nIt's important to note that we are not deciding the languages, libraries, or other tools that we will use within the React development environment, but rather, the system that will be used to develop the React App with.\\nFinally, we are not making decisions about the App's production server here. We'll be using [fxa-content-server](https:\/\/github.com\/mozilla\/fxa\/tree\/main\/packages\/fxa-content-server\/server)'s Express server for the production build.\\n## Decision Drivers\\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\n","Decision":"- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\nChosen option: \"B3 - Create React App and use Rescripts\", because:\\n- Our team has sufficient knowledge in working with CRA over the other options.\\n- We don't need to re-invent the wheel. Our needs for the Settings project are not so great that it would be worth spending time creating a ground-up toolchain. To some extent this option allows us to do what we do best without needing to worry about the specifics of the development workflow.\\n- Rescripts allows us to layer additional configuration overtop of the existing configuration without jeopardizing the integrity or trading away any of the powers of CRA. Specifically, we can use Rescripts to modify CRA to allow external React components to be imported, as demonstrated in [this repo](https:\/\/github.com\/jodyheavener\/rescripts-react-app) (technical notes [here](https:\/\/github.com\/jodyheavener\/rescripts-react-app\/blob\/master\/.rescriptsrc.js)). This capability could be implemented in our other React Apps.\\n- If, down the road, we end up in a place where we absolutely need access to the internals we can always eject.\\n","tokens":437,"id":383,"Predictions":"We will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n** Decide whether we will use the [fxa-content-server] for the production build.n"}
{"File Name":"fxa\/0008-redis-lua-scripts.md","Context":"## Context and Problem Statement\\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n## Decision Drivers\\n- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\n","Decision":"- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\nUse lua scripts. The proof-of-concept [PR](https:\/\/github.com\/mozilla\/fxa\/pull\/3278) showed lua as viable and an overall simpler option.\\n### Positive Consequences\\n- Future use cases should be simpler to implement\\n- Less custom Redis glue code and simpler stack traces\\n- Improved maintainability\\n### Negative Consequences\\n- Additional developer cost of understanding lua\\n- Changes to lua scripts require additional consideration with regard to performance. Performance implications should be a checklist item on any lua script change PR, similar to SQL stored procedures.\\n","tokens":134,"id":384,"Predictions":"Decision (A) The Redis implementation should be a fxa-shared\/redis.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"fxa\/0028-evaluate-playwright.md","Context":"## Context and Problem Statement\\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\\nWe should evaluate other testing stack options to improve reliability.\\n","Decision":"Playwright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some \"happy path\" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\\nP1 tests are the first priority for converting to Playwright.\\n","tokens":160,"id":385,"Predictions":"Decision (A). The functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.nWe should evaluate other testing stack options to improve reliability.n"}
{"File Name":"fxa\/0004-product-capabilities-for-subscription-services.md","Context":"## Context and Problem Statement\\nFor subscription services, we need a way to inform a Relying Party about the\\nrelevant subscription status to enable features for a given user.\\n## Decision Drivers\\n* Engineering simplicity\\n* Security\\n* Business Operations\\n* Marketing\\n","Decision":"* Engineering simplicity\\n* Security\\n* Business Operations\\n* Marketing\\nChosen option A: Single profile assertions consisting of a list of subscription capability\\nstrings based on requesting client ID.\\n* Subscription capabilities are conveyed as a single additional profile assertion.\\n* FxA maintains a mapping of client-to-capabilities, capabilities-to-product\\n* Relying Parties can provide multiple subscription-gated capabilities.\\n* Example:\\n* **RP-A** provides `goldBadge` and `silverBadge`\\n* **RP-B** provides `goldBadge` and `unlimitedStorage`\\n* **RP-C** provides `freePuppies`\\n* FxA can bundle capabilities into cross-RP products as needed for marketing &\\nbusiness purposes.\\n* Example:\\n* **Product A** bundles `goldBadge` & `unlimitedStorage`\\n* RPs are not granted visibility into the user's entire subscription status.\\n* Example:\\n* User subscribes to **Product A**.\\n* **RP-A** will see `goldBadge` but not `unlimitedStorage` in User's `subscriptions` claim in profile.\\n* **RP-B** will see both `goldBadge` and `unlimitedStorage`\\n* **RP-C** sees no capabilities listed\\n","tokens":54,"id":386,"Predictions":"Decision Makersn* Engineering simplicityn* Securityn* Business Operationsn* Marketingn* Marketingnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"Predictions":"Decision based on the information in this ADR.n[1]n[2]n[3]n[4]n[5]n[6]n[7]n[8]n[9]n[10]n[11]n[12]n[13]n[14]n[15]n[16]n[17]n[18]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n[19]n["}
{"File Name":"fxa\/0011-create-new-react-app-for-settings-redesign.md","Context":"## Context and Problem Statement\\n\"Settings\" refers widely to the `accounts.firefox.com\/settings` page where users can manage their Firefox account. The \"Settings Redesign Project\" refers to a 2019\/2020 project centered around giving this page a fresh user interface. The changes are fairly significant and will require a lot of component shuffling and some functionality refactoring. [See the PRD](https:\/\/docs.google.com\/document\/d\/18zu7JCYIsUp8tUMJqb2uErNlzL9f6CQvefLy9HFZ4UY\/edit?pli=1#heading=h.cf57dt1i8634).\\nThe FxA Engineering team desires to migrate from [Backbone and Mustache to React](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0010-transition-fxa-from-backbone-to-react.md). This ADR addresses questions around what level of action we should take regarding this desire and the Settings Redesign Project - should a conversion to React be done at a later time, or are the changes significant enough to justify propelling us into React now?\\n","Decision":"Chosen option: \"Option C - Take the 'ground up' approach and create a new settings React application\", because\\n- The overall goal of the Settings Redesign Project is to offer more flexibility to expand Settings functionality. New features have already been planned. It could be argued that heavy refactoring later (which would be needed for option A or B) would take just as much time as recreating this page with React now.\\n- This approach allows us an opportunity to set up Storybook for the content server and review our tests, CSS, and a11y implementations as we implement the UI changes.\\n- This approach also simplifies our GH workflow and A\/B testing the entire redesign.\\nWe can mitigate risks by avoiding a \"big bang\" surprise replacement by implementing, deploying, and launching smaller pieces of MVP functionality along the way under a new front-end route serving the React application while keeping the Backbone Settings live. While this may be tedious, it can be a safety net against losing integrity and wisdom earned in the original system.\\n","tokens":233,"id":388,"Predictions":"The FxA Engineering team desires to migrate from [Backbone and Mustache](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0010-transition-fxa-from-backbone-to-react.md) to [Backbone and Mustache](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0010-transition-fxa-from-backbone-to-react.md). This ADR addresses questions around what level of action we should take regarding this desire and the Settings Redesign Project - should a conversion to React be done at a later time, or are the changes significant enough to justify propelling us into React now?nbr>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/br>\/"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"Predictions":null}
{"File Name":"asb-client-spa\/0002-use-netlify-for-deployments-and-hosting.md","Context":"## Context\\nGitHub Pages is a perfectly good static site host, however, not having the\\nability for preview builds is problematic when wishing to test changes prior to\\nthem being merged into the main branch. Netlify has the ability to create\\npreview builds for each PR or even each branch along with a number of other\\nbenefits over GitHub Pages as can be seen in this (totally unbiased!)\\n[comparison](https:\/\/www.netlify.com\/github-pages-vs-netlify\/).\\n","Decision":"The decision is to move hosting and deployment onto Netlify\\n","tokens":100,"id":390,"Predictions":"Decision (A). Netlify should be merged into the main branch.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nD. Netlify should be merged into the PR.nA. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.nC. Netlify should be merged into the PR.nB. Netlify should be merged into the PR.n"}
{"File Name":"asb-client-spa\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":391,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"structurizr-python\/0008-package-structure.md","Context":"## Context\\nWe try to structure our package in logical sub-units but we want to maintain a\\nconsistent public interface.\\n","Decision":"We allow for arbitrarily nested sub-packages but export important classes and\\nfunctions to the top level thus exposing a public interface. Our unit tests\\nshould reflect this package structure.\\n","tokens":26,"id":393,"Predictions":"We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.nbr>We should have a public interface.n"}
{"File Name":"structurizr-python\/0007-unit-tests.md","Context":"## Context\\nWe need to make a decision on the testing framework for our project.\\n","Decision":"We will make use of pytest. It is a de facto standard in the Python community\\nand has unrivaled power.\\n","tokens":18,"id":394,"Predictions":"Decision based on the information in the record.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"structurizr-python\/0006-code-testing.md","Context":"## Context\\nSetting up different testing environments and configurations can be a painful\\nand error prone process.\\n","Decision":"We use tox to define, configure, and run different test scenarios.\\n","tokens":21,"id":395,"Predictions":"Decision Makers should be able to identify the best way to set up testing environments and configurations.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"structurizr-python\/0003-python-3-6-only.md","Context":"## Context\\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\\nto natively support f-strings which are sweet.\\n","Decision":"We make an early decision to only support Python 3.6 and above.\\n","tokens":38,"id":396,"Predictions":"Decision (A) The f-strings in Python 3.6 are sweet.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"structurizr-python\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":397,"Predictions":"Decision Makers should consider the following options: a). They should consider the following options: b). They should consider the following options: c). They should consider the following options: d). They should consider the following options: e). They should consider the following options: f). They should consider the following options: g). They should consider the following options: h). They should consider the following options: ii. They should consider the following options: iii. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: iv. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options: i. They should consider the following options."}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"Predictions":"Decision Makers should be able to make the code more stable.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"Predictions":"Decision (A) To de-serialize the remote workspace, the remote workspace should be (de-)serialized from or to JSON.n (B) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n (C) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n (D) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n (A) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n (A) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n (A) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n (A) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n (A) To de-serialize the local workspace, the remote workspace should be (de-)serialized from or to JSON.n"}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"Predictions":"We will standardize on some best-in-class tools.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"structurizr-python\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":401,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"amf-core\/0003-flattened-json-ld-parsing-emission-for-self-encoded-dialects.md","Context":"## Context\\nSelf-encoded dialects define dialect instances which share the same ID between the instance document and the dialect\\ndomain element encoded in such document. This allows the document and the encoded dialect domain element to be treated\\nas the same resource.\\nOn the other hand Flattened JSON-LD emission only renders one node for each ID.\\n","Decision":"We merge both nodes (the document and the encoded domain element) and emit the single merged node with the shared ID.\\nThe merged node contains both the properties from the document and the encoded domain element.\\nWhen parsing the resulting flattened JSON-LD we parse the merged node twice: first as a domain element and then as a\\ndocument. Parsing the properties from the domain element ignores the properties from the document and vice-versa.\\n","tokens":71,"id":402,"Predictions":"Decision (A) The document and the encoded dialect domain element must be treated as the same resource.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"amf-core\/0005-expose-non-scalajs-types-in-amf-client-remote-content.md","Context":"## Context\\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\\nit from export.\\n","Decision":"- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\\n- Add the `toString()` method in `Content` that returns the content in `stream`\\n","tokens":67,"id":403,"Predictions":"We decided to remove the Content.stream field from the scala interface.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"amf-core\/0008-field-deprecation.md","Context":"## Context\\nWe need to start deprecating old fields and have no mechanism to do so.\\n","Decision":"We will start deprecating fields. Getter\/setter methods in Scala and Platform classes will be deprecated also. Deprecated fields will still be set alongside new fields until these are removed in the next major version.\\nHow to deprecate a field:\\n1. Deprecate field definition in model class\\n1. Deprecate field definition with `@deprecated` annotation\\n2. Deprecate field definition with the `deprecated=true` paramter from the `Field` class\\n3. Annotate the `fields` value assignment with `@silent(\"deprecated\")` annotation to avoid compilation errors from deprecated fields\\n2. Deprecate getter\/setter methods in Scala and Platform classes with `@deprecated` annotation\\n3. Update usages of getter\/setter methods to use both legacy and new fields (with the `@silent(\"deprecated\")` annotation)\\n","tokens":21,"id":404,"Predictions":"Decisions are made based on the information in the record.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"Predictions":"The location of the node is not important.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"amf-core\/0004-domainelementmodel-isexternallink-field.md","Context":"## Context\\nLinking nodes between different graphs is a feature not provided by AMF\/AML. This feature is required by some adopters to:\\n* Link nodes from a parsed API specification graph with nodes from a parsed dialect instance graph (RestSDK)\\n* Link nodes between different parsed dialect instances (ANG)\\n","Decision":"Add a boolean field to DomainElementModel called IsExternalLink that marks that a domain element is a reference to a domain element defined in another graph.\\n","tokens":65,"id":406,"Predictions":"Decision (A): The AMF\/AML feature is not required by some adopters to:n* Link nodes from a parsed API specification graph with nodes from a parsed dialect instance graph (RestSDK)n* Link nodes between different parsed dialect instances (ANG)n ## Final answer: No. (A). (b). (c). (d). (e). (f). (g). (h). (ii). (iii). (g). (h). (iv). (iv). (vii). (viii). (v). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii). (viii)."}
{"File Name":"amf-core\/0006-custom-domain-properties-json-ld-rendering.md","Context":"## Context\\nThis ADR serves to document how Custom Domain Properties are rendered in JSON-LD.\\nCustom Domain Properties represent dynamic parts of the model. The original domain is extended with annotations which\\nmay or may not include their own semantic information (when used with semantic extensions).\\nGiven that there is a problem, the AMF Web API model is mostly static. We have static `*Model.scala` objects with each\\nmodel.\\n","Decision":"We will introduce a hack when rendering Custom Domain properties in JSON-LD that will make the graph look like it was\\ngenerated by a dynamic model while still representing the model statically.\\nConcretely, the `DomainElementModel` has a `CustomDomainProperties` field which is an `Array(DomainExtensionModel)`.\\nEach `DomainExtensionModel` has a `DefinedBy: CustomDomainPropertyModel` and `Extension: DataNode` fields that\\nrepresent the definition and value of the domain extension.\\nThis in the \"traditional & static\" parts of the model would be rendered like this (summary):\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myDomainExtension\",\\n\"definedBy\": {\\n\"@id\": \"myCustomDomainProperty\"\\n},\\n\"extension\": {\\n\"@id\": \"myDataNode\"\\n}\\n}\\n]\\n}\\n```\\nInstead a \"dynamic\" model is simulated\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"myCustomDomainProperty\": {\\n\"@id\": \"myDataNode\"\\n},\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myCustomDomainProperty\"\\n}\\n]\\n}\\n```\\nAs you can see a new property was generated using the `@id` of the `CustomDomainProperty` from the original `definedBy`\\nfield which does not exist in the static model. The `customDomainProperties` property now points to an array of custom\\ndomain property nodes rather than an array of domain extension nodes. The new `customDomainProperties` value indicates\\nwhich properties of the serialized node \"are dynamic\".\\n","tokens":89,"id":407,"Predictions":"Decision (A) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (B) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (C) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static. We have static *Model.scala objects with eachnmodel.n  Decision (D) The AMF Web API model is mostly static."}
{"File Name":"amf-core\/0007-no-spec-mediatypes-in-amf-operations.md","Context":"## Context\\nOn AMF 5 Beta's first iteration \"domain+syntax\" mediatypes like \"application\/swagger20+yaml\" were\\nused to decide how the syntax and the domain were to be parsed. This was especially useful to\\nvalidate, transform and render units using compound configurationts (API, WebAPI, RAML, OAS)\\nThis is controversial as:\\n- Resulting mediatypes are strange to the end-user and are not standard. Besides we could\\nonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domain\\ncouldn't be parsed.\\n- Clients that used a specific configuration like RAML10 or OAS30 had to specify the mediatype\\nalthough the configuration they used already specified their intended domain and syntax.\\n","Decision":"Remove those compound mediatypes and instead only keep them for syntax purposes when needed.\\n","tokens":165,"id":408,"Predictions":"Decision nThe mediatypes are strange to the end-user and are not standard. Besides we couldnonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domainncouldn't be parsed.nnnnnnncouldn't be parsed.nnnnnncouldn't be parsed.nnnnncouldn't be parsed.nnnnncouldn't be parsed.nnnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nnnncouldn't be parsed.nn"}
{"File Name":"amf-core\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":409,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"amf-core\/0002-extracttolink-in-json-ld-expanded-emission.md","Context":"## Context\\nExpanded JSON-LD emission embeds node links. This means that whenever a node references another node, instead of using a\\nJSON-LD link we render the referenced node in place. This is because the advantage of the JSON-LD expanded emission is\\nto allow consumer applications (e.g. API Console) to consume JSON-LD as regular JSON; and for that we cannot use JSON-LD\\nlinks.\\n**The problem**: Embedding nodes causes the resulting JSON-LD to be very big (because the same node is rendered many times).\\n_Note: Flattened JSON-LD emission does not suffer this problem because it only renders each node once and then uses\\nJSON-LD links for every node reference._\\n","Decision":"`extractToLink` is a logic we developed for the expanded JSON-LD emission that introduces _AMF links_ (as opposed to\\nJSON-LD links) for some node references. Specifically, it introduces an AMF link and extracts the link target to a\\n_declaration_.\\n_AMF links_ are only available for `Linkable` domain elements, so we use it in only for certain elements. These links\\nare part of the model as opposed to JSON-LD links which are simple graph references.\\n","tokens":153,"id":410,"Predictions":"Decision (A) To render the referenced node in place. (B) To render the referenced node in place. (C) To render the referenced node in place. (D) To render the referenced node in place. (A) To render the referenced node in place. (A) To render the referenced node in place. (A) To render the referenced node in place. (B) To render the referenced node in place. (C) To render the referenced node in place. (D) To render the referenced node in place. (A) To render the referenced node in place. (A) To render the referenced node in place. (B) To render the referenced node in place. (C) To render the referenced node in place. (D) To render the referenced node in place. (A) To render the referenced node in place. (A) To render the referenced node in place. (B) To render the referenced node in place. (C) To render the referenced node in place. (D) To render the referenced node in place. (A) To render the referenced node in place. (A) To render the referenced node in place. (B) To render the referenced node in place. (C) To render the referenced node in place."}
{"File Name":"documentation\/0003-transfere-hash-in-jwt-claim.md","Context":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the json data model.\\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\\nThe validity of the hash must be verified.\\n## Decision Drivers <!-- optional -->\\n* JWT should be used\\n","Decision":"* JWT should be used\\nChosen option: \"Transfer hash in JWT Claim\", because it's the only option when using JWT.\\n### Positive Consequences <!-- optional -->\\n* multiple hashes for different json documents can be added in one JWT\\n","tokens":93,"id":411,"Predictions":"Decision n* JWT should be usednnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"documentation\/0001-use-json-web-tokens.md","Context":"## Context and Problem Statement\\nExternal services must authorize web clients to the WebSocket API.\\nThe WebSocket API is stateless and not maintain a user Session with Cookies.\\nOnly little data should be stored for an open WebSocket connection.\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n","Decision":"* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\nChosen option: \"JWT from pre-shared keys\", because the WebSocket API is loosely coupled and it is well supported on many platforms.\\n### Positive Consequences\\n* Simple to implement\\n* Authorization data can be send in a portable and verifiable way\\n### Negative Consequences\\n* The shared keys must be handled\\n","tokens":72,"id":412,"Predictions":"Decision n1nThe WebSocket API is stateless and not maintain a user Session with Cookies.n2nThe WebSocket API is flexible and well supported on many platformsn2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"documentation\/0002-use-sha256-with-base64url-encoding.md","Context":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the data.\\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\\nThe WebSocket API must able to verify the integrity of the json data.\\n## Decision Drivers <!-- optional -->\\n* Use standard encodings\\n","Decision":"* Use standard encodings\\nChosen option: \"Send SHA256 hash of Base64Url encoded json\", because this method is platform independent and not much session state is required.\\n### Positive Consequences <!-- optional -->\\n* The JWT really function as a verification token for the other requests.\\n* Can be applied to all json data that must be verified.\\n### Negative Consequences <!-- optional -->\\n* The json must be transferred in Base64Url encoding\\n","tokens":77,"id":413,"Predictions":"n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n"}
{"File Name":"documentation\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":414,"Predictions":"Decision Makersn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn* Multi tenant support with own keys for each tenantn ## Decision Makingn"}
{"File Name":"csw-backend\/0004-use-serverless.md","Context":"## Context\\nGiven that we were not going to have the benefit of\\noffloading maintenance responsibility to the PaaS we\\nwanted to keep the cost and the requirement for\\nmaintenance as small as possible.\\n### Architecture patterns considered\\n#### PaaS\\nSee [ADR 3](.\/0003-not-paas.md).\\n#### EC2\\nTraditional load balanced EC2 model running something\\nlike django or rails.\\n#### ECS\\nContainerised version of the above\\n#### Serverless\\nLambda functions using APIGateway and CloudFront\\nThere are many benefits to having the tools of the django\\ninfrastructure.\\nThe downside is that you have a maintenance requirement\\nto maintain the server instances. You also somewhat bind\\nyourself to running the service 24\/7 and paying for it.\\nEven with ECS you are still responsible for maintaining\\nthe container host.\\nWith serverless you only pay when the functions get\\nexecuted and there is a significant free tier.\\nThe downside of serverless is you can't leverage things\\nlike django \/ rails and the communities who contribute\\ncomponents to them.\\nThis limits our cost and our exposure since maintaining\\nthe infrastructure that hosts the lambda functions is\\nAWS' responsibility not ours.\\n","Decision":"We will use AWS Lambda for this service.\\nIn conjunction with an ENI to allow the lambdas access\\nto RDS running inside a VPC if necessary.\\n","tokens":251,"id":415,"Predictions":"We decided to use ECS instead of ECS.n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### EC2nTraditional load balanced EC2 model running somethingnlike django or rails.n### ECSnContainerised version of the aboven#### ServerlessnLambda functions using APIGateway and CloudFrontnThere are many benefits to having the tools of the djangoninfrastructure.nThe downside is that you have a maintenance requirementnto maintain the server instances.nYou also somewhat bindnyourself to running the service 24\/7 and paying for it.nEven with ECS you are still responsible for maintainingnthe container host.nWith serverless you only pay when the functions getnexecuted and there is a significant free tier.nThe downside of serverless is you can't leverage thingsnlike django \/ rails and the communities who contributencomponents to them.nThis limits our cost and our exposure since maintainingnthe infrastructure that hosts the lambda functions isnAWS' responsibility not ours.n### Decision.nn### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md).n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3](.\/0003-not-paas.md].n### Architecture patterns considered PaaSnSee [ADR 3"}
{"File Name":"csw-backend\/0005-serverless-framework.md","Context":"## Context\\nMy first instinct for the tool was to use the\\nNPM [Serverless](https:\/\/serverless.com\/) framework.\\nThe main advantage of serverless is that it is cloud\\nagnostic. However JavaScript is not widely used at GDS.\\nAWS [Chalice](https:\/\/chalice.readthedocs.io) is a similar\\nframework which has a multi-stage deploy process to\\nautomate provisioning Lambda and Api Gateway but written\\nin Python.\\nBoth languages have frameworks for accessing the AWS API:\\n* The [SDK](https:\/\/docs.aws.amazon.com\/sdk-for-javascript)\\nfor JavasSript and\\n* [Boto3](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/index.html)\\nfor Python.\\n","Decision":"Given that we had already made the decision that cloud\\nagnostic was not important for this service and that\\nPython was more in keeping with GDS common practice we\\ndecided to use Chalice.\\n","tokens":158,"id":416,"Predictions":"Decision (A). The tool should be used by GDS.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nD. The tool should be used by Apache.nA. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nD. The tool should be used by Apache.nA. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nC. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.nB. The tool should be used by Apache.n"}
{"File Name":"csw-backend\/0007-implement-a-vpc.md","Context":"## Context\\nIf running serverless we could choose to operate outside\\na VPC. This has advantages in terms of the latency of\\nrequests. We would not need a keep-warm function to\\nensure network interfaces were provisioned.\\nLambdas running outside the VPC could not easily access\\nRDS running inside a VPC.\\nGiven the sensitive nature of the data involved it seems\\nsensible to protect it further by running RDS inside a VPC\\nwhere we have much easier control over ingress and egress.\\n","Decision":"Implement a VPC.\\n","tokens":110,"id":417,"Predictions":"Decision (A). The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture. The serverless architecture is more secure than the serverless architecture."}
{"File Name":"csw-backend\/0006-use-a-relational-database.md","Context":"## Context\\nWe needed some persistent storage of audit results.\\nWe considered:\\n### Schemaless - DynamoDb\\nThis would be the most obvious choice for a lambda based\\nservice.\\nThe dynamo data model is tables of key value pairs.\\nThe main problem with Dynamo is the limit of I think 4K\\nper value.\\nOne of the things we wanted to do was briefly cache API\\nresponses which could easily breach that 4K limit.\\nWith Dynamo the access control is via IAM which would be\\nrelatively easy to manage and encryption at rest can\\nbe easily configured.\\n### Schemaless - MongoDb\\nMongo was a better fit for our service, saving JSON\\nrepresentations of API responses and resources.\\nThe problem with Mongo is it's not AWS native so we'd\\nhave to provision a server, manage the access controls,\\nmaintenance and availability.\\n### Relational - RDS MySQL \/ PostgreSQL\\nRDS gives us the benefit of running a managed service so\\nAWS are responsible for backups and patching minor\\nversion.\\nRelational databases give us the ability to do on-the-fly\\nanalysis more easily.\\nWe can store JSON as blob data although not ideal.\\nIf we want to re-use the database instance as a shared\\nresource across multiple services RDS is more capable.\\nIt's not unlikely that a future development may require\\ndjango.\\nPostgreSQL seems to be the general direction of travel\\nin GDS and is much more capable for a wider range of\\nuse-cases where we don't know what we're building in the\\nfuture.\\n","Decision":"Whilst none of the options were perfect we decided that\\na PostgreSQL RDS was the best option given the\\ninformation available to give us an infrastructure to\\nsupport multiple tools and services.\\n","tokens":328,"id":418,"Predictions":"Decision (A) To choose a service for the audit results. (B) To choose a service for the relational database. (C) To choose a service for the JSON representation. (D) To choose a service for the PostgreSQL database. (E) To choose a service for the Relational database. (F) To choose a service for the Relational database. (G) To choose a service for the Relational database. (H) To choose a service for the Relational database. (G) To choose a service for the PostgreSQL database. (H) To choose a service for the Relational database. (H) To choose a service for the Relational database. (H) To choose a service for the PostgreSQL database. (H) To choose a service for the Relational database. (H) To choose a service for the PostgreSQL database. (H) To choose a service for the Relational database. (H) To choose a service for the PostgreSQL database. (H) To choose a service for the PostgreSQL database. (H) To choose a service for the PostgreSQL database. (H) To choose a service for the PostgreSQL database. (H) To choose a service for the PostgreSQL database. (H) To choose a service for the PostgreSQL database."}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"csw-backend\/0003-not-paas.md","Context":"## Context\\nThe first architecture considered was using the PaaS.\\nThis tool collects data about misconfigurations in a\\ndatabase. Since the data held is quite sensitive we\\nwanted to take all reasonable measures to protect that\\ndata.\\nThere are a lot of advantages to using the PaaS in that\\nit limits the amount of work required for common\\noperational tasks like deployment and monitoring.\\n","Decision":"At the present time we felt the Paas was not a viable\\noption for this tool because of the following:\\n* Shared tenancy RDS\\n* Shared tenancy VPC\\n* No ability to natively control ingress\\n* No ability to control egress\\n__Some of these issues are being addressed by the PaaS so\\nwe may revisit this decision in future.__\\n","tokens":83,"id":420,"Predictions":"Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.nThere are a lot of advantages to using the PaaS in thatnit limits the amount of work required for commonnoperational tasks like deployment and monitoring.nn Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.nn Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.nn Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase. Since the data held is quite sensitive wenwanted to take all reasonable measures to protect thatndata.n Decision (A).nThe first architecture considered was using the PaaS.nThis tool collects data about misconfigurations in andatabase."}
{"File Name":"csw-backend\/0008-interrogation-method-iam-role.md","Context":"## Context\\nWithin GDS product teams operate autonomously and tech-ops\\nis relatively new. There is no central administration of\\nAWS. There is a shared authentication model but it is not\\nused everywhere.\\nTeams are busy with their own mission work and don't have\\ntime allocated to standardisation or security work.\\nTo get things built in a client AWS account we need to:\\n* negotiate time from the product team\\n* provide them with the code to build the resources\\n* code review with them to reassure them that it's OK to\\nrun our code in their environments\\n* repeat for each team we engage\\nWe want to make it easy and low-risk for a team to engage\\nwith us.\\nIf there were centralised management of all accounts, the\\nsensible way to do this would be by configuration the\\nconfig service with a set of config rules administered\\nand maintained centrally by Cyber Security.\\nThese rules would be largely triggered by CloudTrail\\nevents notifying us of compliance as soon as resouces were\\ncreated or changed.\\nHowever this approach would involve quite a large number\\nof resources deployed into each client AWS account which\\nmeans a longer and costlier engagement process.\\nThe simplest way to do this is by provisioning an IAM role\\nand associated policy. That way we can work from the\\nprinciple of least privilege, granting us only the access\\nwe need to run our audit making it low risk.\\nBy providing the role and policy definition in a Terraform\\nmodule we can also ensure that it requires little work by\\nclient account holders.\\nThe advantage of the IAM mechanism is that since we're\\nquerying the configuration of the user account directly\\nvia the API it is very easy to tell whether our service\\nhas been tampered with - the only thing that can change\\nis the definition of our policy which we can check.\\nIn the config service model there are more moving parts\\nwithin the client account which could be switched off,\\naltered or intercepted.\\n","Decision":"Create a terraform module which creates an IAM role and\\nassociated policy granting primarily read-only access\\n(Get, List, Describe API calls) working from the\\nprinciple of least privilege.\\n","tokens":413,"id":421,"Predictions":"Decision (A) The IAM mechanism is a better option than the existing IAM mechanism.nB) The IAM mechanism is more cost-effective than the existing IAM mechanism.nC) The IAM mechanism is more efficient than the existing IAM mechanism.nD) The IAM mechanism is more efficient than the existing IAM mechanism.nAnswer:A.B.C.B.C.B.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C."}
{"File Name":"csw-backend\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":422,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"casa\/0003-multiple-user-tables.md","Context":"## Context\\nThis is planned to be a multi-tenant system. There will be multiple CASA orgs in the system, so every case, case_contact, volunteer, supervisor, casa_admin etc must have a casa_org_id, because no one is allowed to belong to multiple CASAs. Volunteer, supervisor, and casa_admin are all roles for a \"User\" db object. In addition to those existing roles, we want to create a new kind of user: all_casa_admin. We need to handle the case of super users who have access to multiple casa_orgs, so they would be difficult to handle in the existing User table--with null handling around their casa_org_id field. We have used the built-in Devise ability to have multiple user tables, as recommended to us by our Rails expert Betsy. This is to prevent needing null handling around casa_id for User records since all_casa_admin users will not have casa_id populated.\\nAdditionally, all_casa_admin users are currently intended to be allowed to create casa_admin users, but NOT to be able to see or edit any CASA data like volunteer assignments, cases, case_updates etc.\\n","Decision":"We are using two tables for users: \"user\" table for volunteers,supervisors, and casa_admin (all of which must have a casa_id). \"all_casa_admin\" for all_casa_admins, which will have no casa_id.\\n","tokens":236,"id":423,"Predictions":"Decision (A) To create a new user type for all_casa_admin. (B) To create a new user type for all_casa_admin. (C) To create a new user type for all_casa_admin. (D) To create a new user type for all_casa_admin. (A) To create a new user type for all_casa_admin. (A) To create a new user type for all_casa_admin. (A) To create a new user type for all_casa_admin. (B) To create a new user type for all_casa_admin. (C) To create a new user type for all_casa_admin. (D) To create a new user type for all_casa_admin. (A) To create a new user type for all_casa_admin. (A) To create a new user type for all_casa_admin. (A) To create a new user type for all_casa_admin. (B) To create a new user type for all_casa_admin. (C) To create a new user type for all_casa_admin. (D) To create a new user type for all_casa_admin. (A) To create a new user type for all_casa_admin."}
{"File Name":"casa\/0002-disallow-ui-sign-ups.md","Context":"## Context\\nWe want it to be easy for people to join the organization, however we don't want random people signing up and spamming us. We want admin users to have control over who has accounts on the system. We don't have the capacity to handle this properly through the user interface right now.\\n","Decision":"We are going to disable Devise 'registerable' for the user model so that there will no longer be a public sign up option on the site. Creation of new accounts will be done on the backend.\\n","tokens":63,"id":424,"Predictions":"We need to make it easier for people to join the organization.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n","Decision":"Pending\\n","tokens":36,"id":425,"Predictions":"We will use the Bootstrap gem.nbr>Nonnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"casa\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":426,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"GoStudy\/0002-use-c4-for-architecture-diagrams.md","Context":"## Context\\nWe need to document the architecture (precisely, the static model ie. diagrams) of the project.\\n","Decision":"We will use Simon Brown C4 model.\\n","tokens":26,"id":427,"Predictions":"Decision (A) The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.nbr>The architect should be able to provide the information needed to design the project.n"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"Predictions":"Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n To enable different communication ways we need to apply a proper architectural style.n  Decision (B) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (C) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (D) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n  Decision (A) The SDARS application consists of 3 independent components that can be communicated in various ways.n"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"Predictions":"Decision Makers should adopt the architectural style of the application architecture style.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"GoStudy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":430,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"GoStudy\/0003-use-modular-monolith-as-system-architecture-style.md","Context":"## Context\\nWe need to adopt system architecture style adjecent to our architectural drivers\\n","Decision":"We will use modular monolith architecture style\\n","tokens":18,"id":431,"Predictions":"Decision Makers should adopt the architectural style of the system architecture style.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"coders-board-v2\/0001-APP_BACKEND-modules-architecture.md","Context":"## Context\\nWe need to keep extendable architecture in order to prepare for future changes,\\nbecause we work in very flexible manner and requirements changes and evolve a lot.\\n","Decision":"In backend directory:\\n- *libs* - technical parts of software - implementations of database connections etc. In the future will be moved to npm packages,\\nbut we leave it now for easier and faster development to avoid unnecessary complexity.\\nFolders in backend **src** directory are splitted by names of bounded contexts.\\nEach BC, which follow CQRS manner, has following subdirectories:\\n- *write-side* - keeps logical separated parts, which keep ubiquitous language + one domain inside each boundary\\nEach bounded-context will follow directories in manner of Domain-Driven Design Layered Architecture proposed by Eric Evans.\\n- *read-side* - projections from domain events of write-side\\n","tokens":35,"id":432,"Predictions":"Decision Makers should consider the following options: a new building or a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building; a new building;"}
{"File Name":"ADR\/0004-keep-qa-and-devs-html-selectors-separate.md","Context":"## Context\\nCore developers are using JS maps files to keep all the selectors they need to interact with the BO theme. Here is an example of such a file:\\n```\\nexport default {\\nmainDiv: '#order-view-page',\\norderPaymentDetailsBtn: '.js-payment-details-btn',\\norderPaymentFormAmountInput: '#order_payment_amount',\\norderPaymentInvoiceSelect: '#order_payment_id_invoice',\\nviewOrderPaymentsBlock: '#view_order_payments_block',\\nprivateNoteToggleBtn: '.js-private-note-toggle-btn',\\nprivateNoteBlock: '.js-private-note-block'\\n}\\n```\\nQA team is keeping their own set of selectors to interact with during automated tests. Here is an example of such a file:\\n```\\nthis.documentTab = 'a#orderDocumentsTab';\\nthis.documentsTableDiv = '#orderDocumentsTabContent';\\nthis.documentsTableRow = `${this.documentsTableDiv} table tbody tr:nth-child(%ROW)`;\\nthis.documentNumberLink = `${this.documentsTableRow} td:nth-child(3) a`;\\nthis.documentName = `${this.documentsTableRow} td:nth-child(2)`;\\n```\\nSelectors should be mutualized so that when a dev changes a UI component or update a selector, the changes are reflected in the corresponding JS Map file and automated tests using this selector will keep working as intented without human intervention.\\n","Decision":"QA team and Core developers will populate and maintain their own set of selectors. There is no clear advantage to use a fusioned set of selectors.\\nHere are the main arguments against this decision:\\n* Devs use ES6, QA team use ES5. This means using a transpiler and adding libraries (babel).\\n* There is no clear conventions in existing JS Map files (names and content architecture vary).\\n* QA team need some selectors with modifiable input (with strings like %ROW or %COLUMN that must be replaced when used), which mean these types of selectors will be unusable for the Core dev team.\\n* Not the same needs: QA team need navigation selectors, Core dev team need interactive selectors. There is only a few selectors in common.\\n* Low risk: there is no evidence of any selector modification that caused automated tests to fail. When a test break, it's mostly because of a relatively big UI revamp, something that couldn't be fixed with just a selector modification anyway.\\n","tokens":294,"id":433,"Predictions":"nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ADR\/0013-use-typescript-on-the-project.md","Context":"## Context\\nWhile the whole JavaScript community welcomes TypeScript easily, our project's JS is not typed at all. To improve the maintainability of the project and use the latest TC39 features, a good idea would be to use TypeScript.\\nBenefits of using it for the project:\\n- Detect bugs before pushing PRs. TypeScript users say that globally, it allows detecting around 15% of bugs that you would detect by testing.\\n- Use latest features such as Optional Chaining, Tuples, and Records... really early.\\n- Types are increasing the quality of the project because we would be able to detect dangerous changes, related bugs... If we use it on the PHP side, why don't we use types while using JS?\\n- Vue 3 offers a new API: Composition API, this one is pretty easy to use with TypeScript as it's mainly functional programming instead of opinionated APIs of Vue, that would be a good move to preshot the Vue update in the BO.\\n","Decision":"Add the possibility to transpile ts files inside every js folder of the project with webpack.\\n[Here is a POC](https:\/\/github.com\/PrestaShop\/PrestaShop\/pull\/23221) - basically using TypeScript on a small part of the PrestaShop Grid system.\\n","tokens":202,"id":434,"Predictions":"We should use TypeScript for the project.nBy using TypeScript, we can improve the maintainability of the project and use the latest TC39 features.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.nBy using TypeScript, we can detect dangerous changes, related bugs.n"}
{"File Name":"ADR\/0002-mixed-use-of-composer-and-zip-modules.md","Context":"## Context\\nThe normal behavior of a module is to be self-contained, meaning that its code and all its dependencies are stored in the module's directory, including `vendor` directory and autoloader.\\nWhen developing and building PrestaShop, native modules aren't downloaded from marketplace, they are required using composer.\\nIn difference with the normal behavior described above, a module installed using composer will have its autoloader and dependencies merged into the core's and placed in the core's `vendor` directory.\\nThis can prove problematic for native modules:\\n- When the module is uninstalled, both its dependencies and autoloader are left behind in the core.\\n- If the module is upgraded using the marketplace sources, the dependencies are now available twice: once in the core and once in the module.\\nAlternate systems to avoid having modules leak code into the core have been proved unpractical:\\n- Including modules via [Composer script handler](https:\/\/github.com\/PrestaShop\/composer-script-handler) or git clone is too slow.\\n- Using submodules would be a step backwards.\\n- Retrieving modules from the marketplace would be slow as well.\\n","Decision":"1. [The module managment system must be changed to be fully based on composer](https:\/\/github.com\/PrestaShop\/PrestaShop\/issues\/12586). This will require major changes in the marketplace and will have to be analyzed for feasibility.\\n2. In the meantime, we will keep using composer to include native modules.\\n3. Native modules must prepend their autoloader.\\n4. To avoid leaving dependencies in the core, no composer dependencies are to be added to native modules until step 1 has been resolved.\\n- Because of Decision No. 4, we may face technical difficulties in the future.\\n","tokens":239,"id":435,"Predictions":"Decision (A).nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained.nThe module should be self-contained."}
{"File Name":"ADR\/0003-use-of-autowiring.md","Context":"## Context\\nSymfony provides a very useful tool called [Autowiring](https:\/\/symfony.com\/doc\/3.4\/service_container\/autowiring.html). It allows to magically bind classes and their dependencies as long as both are declared as services, and the dependencies are declared using their FQCN as service identifier.\\nAdvantages:\\n- Less boilerplate configuration code for every service as you don't have to manually bind dependencies manually.\\nDisadvantages:\\n- Dependencies must be declared using the FQCN instead of a service identifier like \"prestashop.core.foo.bar\".\\n- Currently existing services would have to be aliased in order to have service names follow the required naming convention for autowiring. This would lead to confusion as to which service name use in code, and in case a module wanted to replace them, they would have to replace both.\\n- Dependencies type-hinted as interfaces can have one and **only one** implementation for autowiring to work.\\n","Decision":"Activiting autoriwing is rejected for the 1.7 version.\\n","tokens":203,"id":436,"Predictions":"Decision (A) The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n- The following are the disadvantages of [Autowiring]:n"}
{"File Name":"ADR\/adr-0001 choosing an RDS hIgh availability solution.md","Context":"## Context\\nOur Production AWS RDS databases are not configured for high availablity which can support blue-green architectures and reduce risk. There exists a few ways to provide HA on RDS. We cover those in this document with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.\\n### Distinguishing between HA and DR:\\nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. Disaster Recovery (DR) provides a recovery solution across a geographically separated distance (multi-region) in the event of a disaster that causes an entire data center to fail.\\nIn this ADR, we select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.\\n","Decision":"We will apply the RDS Multi AZ architecture to add high availability to our RDS production instances as it is the recommended best practice to adding HA to existing RDS instances.\\n#### Consequences\\n- Choosing Aurora DB would be a part of a larger product design decision, outside of this adr scope.\\n- High Availability for failover protection improves the stability of the product framework. We did not address Disaster Recovery, which should also be a part of an overall scope of the product framework.\\n- Multi AZ architecture should be fairly transparent to RDS applications, but the failover conditions and alerting mechanisms should be understood prior to implementation.\\n","tokens":163,"id":437,"Predictions":"We select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. We cover those in this ADR with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.n### Distinguishing between HA and DR:nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails."}
{"File Name":"ADR\/0009-expose-js-components-using-window-variable.md","Context":"## Context\\nIn order for modules to use JavaScript components from the Core, they need to import them using statements like:\\n```\\n\/\/ in order to use translatable type\\nimport TranslatableInput from '..\/..\/..\/..\/..\/admin-dev\/themes\/new-theme\/js\/components\/translatable-input';\\n```\\nThis path is not robust, makes CI\/CD harder, and also is not compatible with some development environments using symlinks or containers.\\n","Decision":"We have decided about a system which resolves in 4 concepts:\\n1. Reusable components in BO will be available globally through `window.prestashop` (name can still be modified in short term).\\nAll PrestaShop components will be bundled together and made available in all pages using this mean. Each controller decides which components it chooses to initialize.\\n2. Reusable components will be available as a namespace `window.prestashop.component`.\\nThe namespace will contain classes like this `prestashop.component.SomeComponent`. If you want to get a new instance of `SomeComponent`, you call `new prestashop.component.SomeComponent(...params)`\\n3. Reusable components will be available as initialized instances through `window.prestashop.instance`. These instances are initialized with default parameters by the `initComponents` function.\\n4. A function `initComponents` available through `prestashop.component` is responsible for building `window.prestashop.instance`.\\n### Why a namespace and a collection of instances\\nSince you have access to both constructors and components, developers are free to choose how to initialize and control their components.\\nIf you don't want to initialize a given component with default parameters, you can always call `new prestashop.component.SomeComponent(...myOwnParameters)`.\\nIf you need to apply some mutation to an already initialized component, you just get the global instance: `prestashop.instance.someComponent.doSomething(...)`.\\n","tokens":88,"id":438,"Predictions":"Decision (A) The path is not robust, makes CI\/CD harder, and also is not compatible with some development environments using symlinks or containers.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ADR\/0014-drop-the-no-use-of-legacy-in-new-code-restriction.md","Context":"## Context\\nOriginally, PrestaShop was made out mainly of static classes, with no dependency injection. To address that problem, it [was decided](https:\/\/build.prestashop.com\/news\/new-architecture-1-6-1-0\/) that non namespaced code would be progressively refactored into a `Core` namespace, which would only contain code using with dependency injection. Furthermore, Core code wouldn't be allowed to depend directly on non namespaced classes, but it could to it indirectly by the means of `Adapter` classes that would act as a bridge between new and old code.\\nThe \"no direct dependency between Core and Legacy\" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication. In some cases, the same service can have a legacy, adapter and core implementations, with subtle differences between each one. Furthermore, the constraints of backward compatibility further increase the difficulties to refactor code into Core, because the surface of the \"public API\" is larger.\\n","Decision":"The following decision applies to both `Core` and `PrestaShopBundle` classes (referred as to \"Core\" for shortness):\\n1. **All new Core classes SHOULD be placed either in the `Core` or the `PrestaShopBundle` namespace**, following on the rules established previously.\\n- New classes MUST NOT be added to the `Adapter` namespace, and SHOULD NOT be added to the legacy (root) namespace.\\n2. **Core classes MAY depend on instances of legacy classes**, provided the following rules are respected:\\n- Legacy classes MAY be used either as injected parameters or constructed within, but caution must be exerted when using legacy classes that produce side effects, have global state or don't guarantee internal consistency. In those cases, these classes SHOULD be accessed through dedicated services which enforce consistency.\\n- Core classes MUST NOT call static methods on other classes, except for factory methods, stateless tool methods, or within services dedicated to encapsulate a static class.\\n- Core classes MAY access to data provided by static classes or methods static classes by relying on dedicated services (Application services, Repositories, Data Providers...).\\n3. **Core classes MUST NOT reimplement code found in legacy classes**, without deprecating the original method\/class (and optionally, making it rely on the new implementation).\\n4. **The Adapter namespace MUST be phased out** eventually:\\n- Classes in the Adapter namespace MUST be copied to the Core namespace.\\n- The original Adapter classes MUST be emptied out, made to extend the Core classes, and deprecated so that they can be fully removed in a following major.\\n- Adapter services MUST be deprecated and copied into the core namespace as well.\\n- Code MUST NOT depend on Adapter classes or services.\\n","tokens":207,"id":439,"Predictions":"Decision (A) The \"no direct dependency between Core and Legacy\" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication.nbr>Answer (A).nbr>Answer (B).nbr>Answer (C).nbr>Answer (D).nbr>Answer (A).nbr>Answer (D).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A).nbr>Answer (A)."}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"up-fiscal-data\/004-receipts.md","Context":"## Context and Problem Statement\\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\\n","Decision":"- Not in the project scope for either of the engagements\\n- Good to have and can be incorporated at a later date\\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\\n","tokens":34,"id":441,"Predictions":"Decision was required on Receipts.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"Predictions":"The final decision is to extract the data from the platform.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"Predictions":"Decision Makersn* Use the data from differnt sections to impute accounting head informationn* Use the data from various sections to create data checksn* Long term project with 3 year pipeline to create a sustainable archirtecturen ## Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn* Decision Makersn"}
{"File Name":"up-fiscal-data\/007-rescoping.md","Context":"## Context and Problem Statement\\nDuplicacy of data scraping from Koshvani platform.\\nFor the following sections, the data has already been extracted from the `DDO-wise expenditure` section.\\n- `Grant-wise (Revenue\/Capital) expenditure`\\n- `Division-wise expenditure`\\n## Decision Drivers\\nBoth these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\\n","Decision":"Both these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\\nOnly the main pages of the aformentioned sections contain new information that will require extraction.\\n","tokens":89,"id":444,"Predictions":" The data has already been extracted from the DDO-wise expenditure section.n- Grant-wise (Revenue\/Capital) expendituren- Division-wise expendituren## Decision DriversnNeither these section repeat the same data granulariy or lesser than the DDO-wise expenditure section.n ## Decision Makersn The data has already been extracted from the DDO-wise expenditure section.n- Grant-wise (Revenue\/Capital) expendituren- Division-wise expendituren## Decision Makersn  The data has already been extracted from the DDO-wise expenditure section.n- Grant-wise (Revenue\/Capital) expendituren- Division-wise expendituren## Decision Makersn  The data has already been extracted from the DDO-wise expenditure section.n- Grant-wise (Revenue\/Capital) expendituren- Division-wise expendituren## Decision Makersn  The data has already been extracted from the DDO-wise expenditure section.n- Grant-wise (Revenue\/Capital) expendituren- Division-wise expendituren## Decision Makersn  The data has already been extracted from the DDO-wise expenditure section.n- Grant-wise (Revenue\/Capital) expendituren- Division-wise expendituren## Decision Makersn  The data has already been extracted from the DDO-wise expenditure section.n- Grant-wise (Revenue\/Capital) expendituren- Division-wise expendituren## Decision Makersn  The data has already been extracted from the DDO-wise expenditure section.n"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"Predictions":"Decision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"Predictions":"Decision based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.n**__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"Predictions":"The Grant-wise expenditure section is now extracted from Koshvani.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"up-fiscal-data\/002-koshvani.md","Context":"## Context and Problem Statement\\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\\n- Expenditure\\n- Receipts\\n","Decision":"Explore both sections to identify sub-sections from which data needs to be extracted and analysed.\\n","tokens":39,"id":448,"Predictions":"Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Expendituren- Receiptsn n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen n Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen Decision nThe Koshvani platfrom to be considered in scope for scoping and analysing the data.n- Budgetn- Revenuen Decision n"}
{"File Name":"up-fiscal-data\/009-dashboard-tooling.md","Context":"## Context and Problem Statement\\nThe GEWST project with HAQ: CRC has a platform development component, which includes creating exploratory dasboard and conducting analysis on an interactive dashboard.\\n## Decision Drivers\\n### Platform Development Requirements\\nAccording to the plan, the data platform comprises of the following components:\\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they\u2019re released as part of the platform.\\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\\n- **Data Visualization Layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\\n### Other Drivers\\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\\n- There is still no clear direction or plan what the tool will look like.\\n- There might be numerous research routes and iterations before finalisation.\\n- Data stories and analysis will play a big part in creating the final platform.\\n","Decision":"### Platform Development Requirements\\nAccording to the plan, the data platform comprises of the following components:\\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they\u2019re released as part of the platform.\\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\\n- **Data Visualization Layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\\n### Other Drivers\\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\\n- There is still no clear direction or plan what the tool will look like.\\n- There might be numerous research routes and iterations before finalisation.\\n- Data stories and analysis will play a big part in creating the final platform.\\nYou can track the decision updates \/ inputs on the following [issue](https:\/\/github.com\/CivicDataLab\/up-fiscal-data-shiny\/issues\/1).\\n","tokens":306,"id":449,"Predictions":"Decision Makersn### Platform Development RequirementsnThe GEWST project with HAQ: CRC has a platform development component, which includes creating exploratory dasboard and conducting analysis on an interactive dashboard.n### Decision Driversn### Platform Development RequirementsnAccording to the plan, the data platform comprises of the following components:n### Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.n### Other Driversn- The bandhus in the Public Finance team are already occupied on OBI and OCP project.n### Other Driversn- There might be numerous research routes and iterations before finalisation.n### Data stories and analysis will play a big part in creating the final platform.n### Decision Makersn### Platform Development RequirementsnAccording to the plan, the data platform comprises of the following components:n### Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.n### Other Driversn- The bandhus in the Public Finance team are already occupied on OBI and OCP project.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Decision Makersn### Platform Development RequirementsnAccording to the plan, the data platform comprises of the following components:n### Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform.n### Other Driversn- The bandhus in the Public Finance team are already occupied on OBI and OCP project.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Decision Makersn### Platform Development RequirementsnAccording to the plan, the data platform comprises of the following components:n### Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform.n### Other Driversn- The bandhus in the Public Finance team are already occupied on OBI and OCP project.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Decision Makersn### Platform Development RequirementsnAccording to the plan, the data platform comprises of the following components:n### Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform.n### Other Driversn- The bandhus in the Public Finance team are already occupied on OBI and OCP project.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.n### Other Driversn- The data visualization layer\/Component**: Develop"}
{"File Name":"nisq-analyzer\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":450,"Predictions":"Decision (A) The components in the PlanQK platform should be splitted into individual repos.n.n2(ii) The components in the PlanQK platform should be splitted into individual repos.n2(iii) The components in the PlanQK platform should be splitted into individual repos.n2(iv) The components in the PlanQK platform should be splitted into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos."}
{"File Name":"nisq-analyzer\/0001-use-URI-for-entities.md","Context":"## Context and Problem Statement\\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\\nWe need references for the links.\\n","Decision":"Chosen option: \"[URIs]\", because UUIDs are generated and thus depend on the underlying database system.\\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\\n### Positive Consequences <!-- optional -->\\n* We follow solid [W3C specification](https:\/\/www.w3.org\/Addressing\/URL\/uri-spec.html)\\n","tokens":39,"id":451,"Predictions":"Decision (A) The QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.nWe need references for the links.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"Predictions":"We will choose a test framework that supports TypeScript and Node.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hmpps-interventions-ui\/0008-use-snyk-to-scan-for-cves.md","Context":"## Context\\nWe want to be aware of CVEs (Common Vulnerabilities and Exposures) before they\\nend up in production, and make sure to block deployments with known high\\nseverity CVEs. Snyk allows us to scan our PRs for CVEs and fail builds if there\\nare any vulnerabilities in the code we've written.\\n","Decision":"We will use Snyk to:\\n- Run scans on PRs provide results and fail builds if any known high severity\\nCVEs are found.\\n- Run scans on main and fail builds on high severity CVEs, posting the results\\nto the Snyk platform for monitoring.\\n- Run nightly scans on the docker image and app dependencies.\\n","tokens":72,"id":453,"Predictions":"Decision (A) The CVEs must be known before they can be used in production. (B) The CVEs must be known before they can be used in production. (C) The CVEs must be known before they can be used in production. (D) The CVEs must be known before they can be used in production. (A) The CVEs must be known before they can be used in production. (A) The CVEs must be known before they can be used in production. (B) The CVEs must be known before they can be used in production. (C) The CVEs must be known before they can be used in production. (D) The CVEs must be known before they can be used in production. (A) The CVEs must be known before they can be used in production. (A) The CVEs must be known before they can be used in production. (B) The CVEs must be known before they can be used in production. (C) The CVEs must be known before they can be used in production. (D) The CVEs must be known before they can be used in production. (A) The CVEs must be known before they can be used in production. (A) The CVEs must be known before they can be used in production. (B) The CVEs must be known before they can be used in production. (C) The CVEs must be known before they can be used in production."}
{"File Name":"hmpps-interventions-ui\/0005-use-prettier-to-format-typescript-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":454,"Predictions":"We should use [Prettier](https:\/\/prettier.io\/) to make the code more familiar across the project.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hmpps-interventions-ui\/0002-use-express.md","Context":"## Context\\nWe want to build a web-based application in Node. Express is a minimal and\\nflexible framework that sets up a lot basic configuration for us like routing\\nand middleware, and makes it easy to incoporate the GOV UK design system UI\\ncomponents.\\n","Decision":"We will use Express as the basis for this project.\\n","tokens":57,"id":455,"Predictions":"Decision (A) The application should be built on Node.nB) The application should be built on Node.nC) The application should be built on Node.nD) The application should be built on Node.nAnswer (A).nnB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nB.nC.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nnB.nnB.nnB.nnB.nnB.nnB.nnB.nnB.nnB.nnB.nnnB.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hmpps-interventions-ui\/0007-use-cypress-for-integration-testing.md","Context":"## Context\\nWe want to be able to automate testing end-to-end user journeys through our\\napplication. Cypress is an alternative to Selenium that runs in the browser to\\ndo this for us, and is used across multiple projects at MOJ.\\n","Decision":"We will use Cypress for integration tests.\\n","tokens":50,"id":456,"Predictions":"Decision (A) The project should be asynchronous.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hmpps-interventions-ui\/0004-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":457,"Predictions":"We will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code.nWe will use [ESLint] to enforce consistency in our code"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"Predictions":"We will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code.nWe will use [Stylelint] to enforce consistency in our code"}
{"File Name":"hmpps-interventions-ui\/0003-use-typescript.md","Context":"## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible. TypeScript is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers\\nwho know JavaScript. It has wide editor support.\\n","Decision":"We will use TypeScript by default.\\n","tokens":63,"id":459,"Predictions":"We should use TypeScript to write the code.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"hmpps-interventions-ui\/0012-use-feature-flags-for-in-progress-features.md","Context":"## Context\\nThere are a few reasons we might not want to put a new feature in front of users as soon as it's been merged into the `main` branch:\\n- The API might not be up-to-date with the latest version of the UI (and vice versa) - because we're building the UI and API side of the service independently, there are times when the two may be out of sync: an endpoint may not yet be providing all the data we need; the backend functionality may not be finished at the time of writing the UI code.\\n- We want to satisfy the Pact contracts between the two sides of the service but not use the new data structure until the UI has been updated.\\n- The new functionality may need to be further tested (either with users by developers) and iterated upon before release.\\n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes - this means we'd want to merge smaller chunks of work at a time, which might not be ready for users.\\n- We want to test interactions between systems (e.g. the Community API) on the Development environment but not release these changes to the public.\\n","Decision":"Any features or behaviour that isn't ready to be interacted with by users will be placed behind a config-based feature flag, configured in `server\/config.ts`, e.g. as below:\\n```\\nfeatures: {\\npreviouslyApprovedActionPlans: get('FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS', 'false') === 'true',\\n}\\n```\\nThis can then be turned on for each environment by adding the environment variable (e.g. `FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS`) to the intended environment.\\nWe'll usually want to enable this for the development environment and test environment, possibly the pre-prod environment but not the production environment.\\nBefore deploying the changes to the production environment, it's a good idea to double check the configuration is as expected e.g. by checking it's hidden in the pre-production environment.\\nOnce the feature is ready to be interacted with by users, we'll remove the feature flag from the UI configuration.\\n","tokens":248,"id":461,"Predictions":"We want to keep the new functionality as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes before release.n"}
{"File Name":"hmpps-interventions-ui\/0009-use-helmet-for-http-security.md","Context":"## Context\\nWe want to make sure we're setting the correct HTTP headers for security e.g.\\nContent Security Policy to protect against XSS attacks.\\n[Helmet](https:\/\/helmetjs.github.io\/) is a package that works well with Express\\nto make it easy to set various HTTP headers for secutiy.\\n","Decision":"We'll use Helmet to set secure HTTP headers.\\n","tokens":67,"id":462,"Predictions":"Decision (A) To set HTTP headers for security. (B) To set HTTP headers for security. (C) To set HTTP headers for security. (D) To set HTTP headers for security. (A) To set HTTP headers for security. (A) To set HTTP headers for security. (B) To set HTTP headers for security. (C) To set HTTP headers for security. (D) To set HTTP headers for security. (A) To set HTTP headers for security. (A) To set HTTP headers for security. (B) To set HTTP headers for security. (C) To set HTTP headers for security. (D) To set HTTP headers for security. (A) To set HTTP headers for security. (A) To set HTTP headers for security. (B) To set HTTP headers for security. (C) To set HTTP headers for security. (D) To set HTTP headers for security. (A) To set HTTP headers for security. (A) To set HTTP headers for security. (B) To set HTTP headers for security. (C) To set HTTP headers for security. (D) To set HTTP headers for security. (A) To set HTTP headers for security. (A) To set HTTP headers for security. (B) To set HTTP headers for security. (C) To set HTTP headers for security. (D) To set HTTP headers for security."}
{"File Name":"hmpps-interventions-ui\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael\\nNygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":463,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"rems\/014-userids.md","Context":"## Problems with the current approach\\n- The `sub` attribute might be opaque and not usable between systems, e.g. when sending entitlements to other systems\\n- Different login methods may have different formats for external user identifiers\\n- Different login methods might use conflicting values for claims\\n- E.g. `foo@csc.fi` via HAKA or CSC SSO (TODO: do we know of cases like this?)\\n- A single user might have multiple external identifiers (TODO: do we know of cases like this?)\\n","Decision":"We'll add internal random user ids to REMS. This internal user id will\\nbe the key that users are referred to within REMS. The user's external\\nid will be stored in the attributes JSON blob.\\nThis will allow us more flexibility in the future when identity\\nrequirements and use cases change, and might also make all sorts of\\nmigrations easier (since internal user ids don't need to be touched).\\n","tokens":110,"id":464,"Predictions":"Decision (A) The current approach is not compatible with the current system.n- The sub attribute might be opaque and not usable between systems.n- Different login methods may have different formats for external user identifiers.n- Different login methods might use conflicting values for claims.n- A single user might have multiple external identifiers.n- Decision (A) The current approach is not compatible with the current system.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The sub attribute might be opaque and not usable between systems.n- The foo@csc.fi via HAKA or CSC SSO (TODO: do we know of cases like this?)n"}
{"File Name":"rems\/017-applicant-ux.md","Context":"# Problems with the current applicant UX\\n","Decision":"Here's a step-by-step plan for fixing these.\\n1. (Problem A) Make save succeed even with validation errors. Show validation errors in yellow (instead of red). [#2766](https:\/\/github.com\/CSCfi\/rems\/issues\/2766)\\n2. (Problem B) Run validations in the frontend after the user has stopped typing. [#2614](https:\/\/github.com\/CSCfi\/rems\/issues\/2614)\\n3. (Problem B) Autosave in the background by replacing\/patching the latest draft-saved event. If the latest event is not a draft-saved then create a new one. [#2767](https:\/\/github.com\/CSCfi\/rems\/issues\/2767)\\n","tokens":8,"id":465,"Predictions":"ii. The current applicant UXn is not a good fit for the project. iii. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project. iv. The current applicant UXn is not a good fit for the project."}
{"File Name":"openlobby-server\/0010-replace-flask-with-django.md","Context":"## Context\\nFlask turned out to be poorly designed piece of software which relays on too\\nmuch magic like manipulations of global objects like `g`.\\nSeems like we will also decide to use relational database.\\n","Decision":"We will switch to Django. It's not only well written server but it has also\\n\"batteries included\" like a good ORM layer. And some other features like\\nmiddlewares will simplify things.\\n","tokens":47,"id":466,"Predictions":"Decision Makers should consider using relational database.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0004-graphql-api.md","Context":"## Context\\nOpen Lobby Server will be written in API First design. Frontend applications\\nwill be based on it's API.\\n","Decision":"We will use GraphQL as API standard. From other options like REST and Graph API\\nit's the most client friendly approach. GraphQL query language is easy to use,\\nvalidated and self documenting.\\nGraphQL allows clients to get everything they need in one request without any\\noverhead of not needed data. That is very important for mobile frontends.\\nGraphiQL tool also provides easy way for developers to inspect and try API. So\\nit's easy to adopt by frontend applications developers or other API users.\\n","tokens":27,"id":467,"Predictions":"Decision (A) The Open Lobby Server should be written in API First design. (B) The Open Lobby Server should be written in API First design. (C) The Open Lobby Server should be written in API First design. (D) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (B) The Open Lobby Server should be written in API First design. (C) The Open Lobby Server should be written in API First design. (D) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (B) The Open Lobby Server should be written in API First design. (C) The Open Lobby Server should be written in API First design. (D) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (A) The Open Lobby Server should be written in API First design. (B) The Open Lobby Server should be written in API First design. (C) The Open Lobby Server should be written in API First design."}
{"File Name":"openlobby-server\/0012-use-postgresql.md","Context":"## Context\\nWe want to add relational database.\\n","Decision":"We will use PostgreSQL. It's a mature database with handy features like JSON\\nand hstore data types. It's fully ACID compliant including schema changes. It\\nhas very good support in Django's ORM.\\nAnother popular option is MySQL\/MariaDB. But because it has a major bug `#28727`\\n(10 years since it has been reported and it's still not fixed) breaking ACID in\\nschema changes it can't be used for any serious project.\\n","tokens":11,"id":468,"Predictions":"We need to add a database.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\\nWe need to choose webserver.\\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"Predictions":"Decision Makers must decide on a webserver.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0007-adopt-graphql-relay-specification.md","Context":"## Context\\nWe need to make API friendly for clients and design pagination.\\n","Decision":"We will adopt GraphQL Relay specification. It solves pagination so we don't\\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\\nIt has a way to define inputs in mutations.\\nGraphene lib has good support for creating API following Relay specifications.\\n","tokens":16,"id":470,"Predictions":"Decision Makers should consider the following: -Answer:A.nnB.nC.nD.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nA.nB.nA.nA.nB.nA.nA.nB.nA.nA.nB.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA."}
{"File Name":"openlobby-server\/0013-black-code-formatter.md","Context":"## Context\\nWe would like to simplify code reviews and unify code style.\\n","Decision":"Use Black code formatter: https:\/\/github.com\/ambv\/black\\n","tokens":16,"id":471,"Predictions":"Decision Makers should consider code style as a priority.nbr>nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\\nWe need to choose main programming language for this project.\\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"Predictions":"Decision Makers must choose a programming language.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0002-use-elasticsearch-for-fulltext-search.md","Context":"## Context\\nWe need a database with fulltext search capable of searching in various\\nlanguages especially in Czech.\\n","Decision":"We will use Elasticsearch. It's well known database with great fulltext search\\ncapabilities based on Apache Lucene. It has also aggregations, highlighting of\\nresults, and many other useful features.\\nWe will use it as database for all data so we have just one database in the\\nsystem.\\n","tokens":23,"id":473,"Predictions":"Decision (A) The database should be a database with fulltext search capable of searching in various languages.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0008-pytest.md","Context":"## Context\\nWe need to choose framework for tests.\\n","Decision":"We will use Pytest. It's much more Pythonic and simplier to use than Unittest\\nfrom standrd library.\\n","tokens":12,"id":474,"Predictions":"Decisions are made based on the information in the record.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"Predictions":"The document types that does not use Elasticsearch's fulltextncapabilities is growing. Recently released Elasticsearch 6 is bringing one typenper index which means management of many indices.n Decision Makers would like to know what Elasticsearch 6 is about.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\\nOpen Lobby must have open stable API.\\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"Predictions":"Decision Makers must consider the following factors: a) Open Lobby must have open stable API; b) Open Lobby must have open stable API; c) Open Lobby must have open stable API; d) Open Lobby must have open stable API; e) Open Lobby must have open stable API; f) Open Lobby must have open stable API; g) Open Lobby must have open stable API; h) Open Lobby must have open stable API; h) Open Lobby must have open stable API; h) Open Lobby must have open stable API; ii) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API; iv) Open Lobby must have open stable API;"}
{"File Name":"openlobby-server\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis article:\\nhttp:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":477,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"openlobby-server\/0009-openid.md","Context":"## Context\\nWe need an authentication mechanism for users. It must be secure and\\nfrontend application independent.\\n","Decision":"We will use OpenID Connect. Open Lobby Server will provide all the hard stuff\\nfor a frontend applications. Ideally over the GraphQL API.\\n","tokens":22,"id":478,"Predictions":"Decision (A) The authentication mechanism must be secure andnfrontend application independent.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nearby-services-api\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":479,"Predictions":"Decision Makers should consider the following options: a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework; a web framework;"}
{"File Name":"nearby-services-api\/0003-use-mongodb-for-data-storage.md","Context":"## Context\\nInformation about pharmacies is required by the application in order to display\\nto users. A Docker image of pharmacies\\n([pharmacy-db](https:\/\/hub.docker.com\/r\/nhsuk\/pharmacy-db\/)) running a MongoDB\\ninstance has been created for use by other applications.\\nNodeJS uses a single threaded event loop architecture and as such works best\\nwhen the work it is doing is non-CPU intensive. Searching through datasets is\\npotentially CPU intensive.\\n","Decision":"We have decided to use the existing Docker image rather than spend effort\\nacquiring the data again.\\n","tokens":98,"id":480,"Predictions":"Decision (A) The application should store the information about pharmacies in a single threaded event loop architecture. (B) The application should store the information about pharmacies in a single threaded event loop architecture. (C) The application should store the information about pharmacies in a single threaded event loop architecture. (D) The application should store the information about pharmacies in a single threaded event loop architecture. (A) The application should store the information about pharmacies in a single threaded event loop architecture. (A) The application should store the information about pharmacies in a single threaded event loop architecture. (B) The application should store the information about pharmacies in a single threaded event loop architecture. (C) The application should store the information about pharmacies in a single threaded event loop architecture. (D) The application should store the information about pharmacies in a single threaded event loop architecture. (A) The application should store the information about pharmacies in a single threaded event loop architecture. (A) The application should store the information about pharmacies in a single threaded event loop architecture. (B) The application should store the information about pharmacies in a single threaded event loop architecture. (C) The application should store the information about pharmacies in a single threaded event loop architecture. (D) The application should store the information about pharmacies in a single threaded event loop architecture."}
{"File Name":"nearby-services-api\/0005-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":481,"Predictions":"Decision (A).nThe application should be able to monitor KPIs of the application.nThe application should be able to respond andnpotentially pro-actively initiate measures as to ensure the application'snhealth if sound. Ultimately providing a better service for our users.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nearby-services-api\/0006-split-nearby-and-open-results-into-separate-endpoints.md","Context":"## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing the\\nease with which it can be used both by the current and future consumers.\\n","Decision":"The decision is to add a new endpoint i.e. `\/open` alongside the current\\n`\/nearby` endpoint. The former endpoint will return only services that are open\\nwhere the latter will be refactored to return only services that are nearby\\nregardless of their opening state.\\n","tokens":100,"id":482,"Predictions":"Decision (A).nThe API should be able to be used both by the current and future consumers.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nearby-services-api\/0004-use-elastic-search.md","Context":"## Context\\nElasticsearch is configured as a cluster for reliability and failover, and\\nprovides a single point for data updates. MongoDB runs as a single instance and\\nis not clustered.\\n","Decision":"nearby-services-api will consume data from Elasticsearch rather than MongoDB.\\n","tokens":41,"id":483,"Predictions":"Is Elasticsearch Clustered or Not?nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nearby-services-api\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":484,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"Predictions":"i.e. if the application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe application loads data from a JSON file into an Elasticsearch instance.nThe..."}
{"File Name":"elasticsearch-updater\/0003-remove-unwanted-indexes.md","Context":"## Context\\nElasticsearch timeouts can cause 'orphaned' indexes from remaining in the cluster after an update.\\nRunning two updaters simultaneously, as happens for pull requests that last several days, can leave\\ntwo indexes with the same alias.\\nElasticsearch watches are run to export data to Prometheus throughout the day.\\nTwo new date stamped 'monitor' and 'watch' indexes are created every day to track the status of the watch job.\\n","Decision":"Orphaned and indexes with duplicate aliases will be removed as part of the update.\\nThe indexes created every day by the watch will also be deleted within the elastic-search updater.\\nThis could be done by an Elasticsearch watch, but a license is required to use this functionality.\\nTo avoid adding more infrastructure components to the system rather than having a standalone service, the cleanup will be\\nperformed in the elastic-search updater. This can be moved into another service in future if required.\\n","tokens":96,"id":486,"Predictions":"Decision (A).nElasticsearch timeouts can cause 'orphaned' indexes from remaining in the cluster after an update.nRunning two updaters simultaneously, as happens for pull requests that last several days, can leaventwo indexes with the same alias.nElasticsearch watches are run to export data to Prometheus throughout the day.nTwo new date stamped'monitor' and 'watch' indexes are created every day to track the status of the watch job.nnIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a bad thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing.nIt is not possible to determine whether the timeouts are a good thing or a bad thing."}
{"File Name":"elasticsearch-updater\/0004-remove-snyk.md","Context":"## Context\\nMany of the vulnerabilities identified by snyk have no fixes, and can only be ignored in the synk config file for a set number of days.\\nAfter the number of days are up, the vulnerabilities will fail the build and prevent merge on new PRs, with the only option to ignore the\\nvulnerabilities again.\\nMany of the snyk failures are for build tools that are not exposed when applications are deployed.\\nThe overhead of ignoring many snyk vulnerabilities to get builds passing outweighs the potential risks.\\n","Decision":"Snyk will be removed and replaced with a combination of Greenkeeper and Github vulnerability subscriptions.\\nGreenkeeper will ensure the dependencies are up to date, and the subscription notifications will provide visibility of serious vulnerabilities.\\nAn automated replacement will be put in place in the near future.\\n","tokens":111,"id":487,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"elasticsearch-updater\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":488,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"the-zoo\/0004-graphql-api.md","Context":"## Context\\nWe need API for the Zoo to expose issues, analytics data, etc.\\n","Decision":"We will create API by GraphQL specification. Pagination will be done according\\nto Relay server specification. We considered to build REST API, but we decided\\nthat for fresh new APIs is the GraphQL right choice.\\n","tokens":19,"id":489,"Predictions":"Decision Makers should use the API to expose issues, analytics data, etc.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"the-zoo\/0002-use-black-code-formatter.md","Context":"## Context\\nWe would like to use code formatter to standardize code look so we stop bothering about it\\nin code reviews and focus on how code actually works.\\n","Decision":"We will use Black: https:\/\/github.com\/ambv\/black\\n","tokens":34,"id":490,"Predictions":"Decision Makers should consider using code formatter to standardize code look.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"the-zoo\/0006-fomantic-ui.md","Context":"## Context\\nWe were using [Semantic UI](https:\/\/semantic-ui.com\/), as it's an open source and robust framework.\\nIt has a wide range of components, it's easily customizable and it uses modern\\napproaches. It's really well documented, although usually documentation is not even\\nneeded as it's really intuitive to work with it.\\nUnfortunately, the project is not as maintained as before, and the development of new components\\nis stale.\\n[Fomantic UI](https:\/\/fomantic-ui.com\/) picked up where Semantic UI left, and offers a drop-in replacement,\\nwith a higher quantity of components, and the development can be considered active.\\n","Decision":"We will use [Fomantic UI](https:\/\/fomantic-ui.com\/), the community fork of `Semantic UI`. The\\nsupport on this project is fast and on point, and the roadmap aligns nicely with our development,\\nas the maintainers are planning to offer first party support for Vue components, and ditch the\\njQuery dependency.\\n","tokens":139,"id":491,"Predictions":"The project is not as maintained as before.n[Fomantic UI](https:\/\/fomantic-ui.com\/) picked up where Semantic UI left, and offers a drop-in replacement,nwith a higher quantity of components, and the development can be considered active.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"the-zoo\/0007-conventional-commits.md","Context":"## Context\\nWe always wanted to have a consistent commit message styling, we used coala's CommitBear to help with\\nthis task but besides just a bit more clarity in the commit history, it doesn't bring that much value.\\nNow that we'll be getting rid of coala in favour of pre-commit hooks, we need an alternative solution,\\nand we can use this change to also step up our game regarding commit messages.\\n","Decision":"We will use the [conventional commits](https:\/\/www.conventionalcommits.org) convention for writing our commit messages\\nfrom now on. Besides forcing us to keep a consistent commit message styling, it will also bring much\\nmore value, like:\\n* Automatically generating CHANGELOGs\\n* Automatically determining a semantic version bump (based on the types of commits landed)\\n* Communicating the nature of changes to teammates, the public, and other stakeholders\\n* Having different CI steps based on the nature of the commits added to the MR\\n* Having insights about how much time\/effort is spent on the different areas of the development\\n","tokens":89,"id":492,"Predictions":"We will consider a new approach to the commit message style.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"the-zoo\/0003-save-adrs-into-adr-dir.md","Context":"## Context\\nWe need dir to save ADRs into.\\n","Decision":"We will save ADRs into top level `adr` dir. Another option was to put them into `docs\/adr`\\nalong with Sphinx docs, but they might be bit hidden there.\\n","tokens":14,"id":493,"Predictions":"Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n Decision Makers must be able to identify the ADRs.n"}
{"File Name":"the-zoo\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":494,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"the-zoo\/0005-semantic-ui.md","Context":"## Context\\nWe were using [Material Design Lite](https:\/\/getmdl.io\/) as our front-end CSS framework.\\nThe selection of offered components is quite limited and most of them just include\\nthe basic logic. We have to dedicate a big amount of time writing front-end code and it's\\nhard to compose the provided components as they are mainly meant to be used in a single\\nway because of the strong Material design guidelines.\\nWe aim to focus on logic, not on developing front-end components. We need a solution\\nthat can offer a high number of already built components. The components should be\\neasy to configure and to compose together as our UI sometimes needs components that\\nare not contemplated on the common guidelines.\\n","Decision":"We will use [Semantic UI](https:\/\/semantic-ui.com\/), as it's an open source and robust framework.\\nIt has a wide range of components, it's easily customizable and it uses modern\\napproaches. It's really well documented, although usually documentation is not even\\nneeded as it's really intuitive to work with it.\\n","tokens":148,"id":495,"Predictions":"We need a solution that can offer a high number of already built components. The components should beneasy to configure and to compose together as our UI sometimes needs components thatnare not contemplated on the common guidelines.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-34.md","Context":"## Context\\nOn PFE, workflows have a complex logic of programmatic workflow selection. [ADR 18](adr-18.md) has documented the original context. This ADR is a partial superseding of the decision on how to select the workflow when navigating to `\/classify`\\n","Decision":"We will continue to use the routing behavior defined by [ADR 18](adr-18.md)\\n### `\/classify` behaviour\\nWe will not programmatically select a workflow except for one case: when there is only one active workflow. For all other cases, we have a UI prompt for the volunteer to manually select which workflow they wish to contribute to.\\n### Error handling\\nCases when the workflow is not available are:\\n- The workflow does not actually exist, so this will 404.\\n- The workflow exists, but is in an inactive state. The activeness state effectively functions as a permissions mechanism since users with the project owner, collaborator, or expert role or Zooniverse admins can still request and load inactive workflows. Users with the correct role should be able to load an inactive workflow with a visual indication in the UI it is inactive. All other users will receive a 404.\\n- The workflow exists and is active, however, the project uses workflow assignment and the workflow has not been assigned to the volunteer yet. The classify page should load, the classifier itself doesn't, and the workflow selection prompt is rendered for the volunteer to choose between the workflows they have been assigned.\\n","tokens":58,"id":496,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-28.md","Context":"## Context\\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\\n","Decision":"### Changes from the TESS LCV\\nThe TESS LCV is built only for their use case and is not configurable. It is hard coded to expect brush stroke annotation for the classifications, zoom only in the x-axis direction, and for only one data series. We will build a generally configurable plots so that other projects can have the flexibility they need. The new plot viewers will be modular so that it can be placed into a composite, complex subject viewer as needed.\\nPreviously, the TESS LCV was built using d3.js, however, mixing d3 and react can be dangerous. The decision at the time was to use d3 because of the custom requirements needed for the TESS LCV and the react + d3 libraries were too opinionated to be used for our needs. The library d3 is also difficult to write tests for because of its chaining API. For this reason, the original TESS LCV is largely untested.\\nSince then, a library called [vx](https:\/\/vx-demo.now.sh\/) containing reusable low-level visualization react component that uses the d3 math utilities, has become more mature to start using. This fits our needs to have the DOM solely rendered by React, but still has the usefulness of a mature library like d3 to do calculations as needed. The new plots will be built using vx.\\nThe long term goal is to swap the TESS LCV over to the new `ScatterPlotViewer`, however, this means adding support for brush annotations which will be investigated at a later time.\\n### ScatterPlotViewer\\nThe scatter plot will be built with support to configure:\\n- Multiple data series\\n- Customizable data series colors to represent information as needed\\n- Pan and zoom in both axes directions or constrainable\\n- Axis inversion\\n- Customizable axis label\\n- Customizable number of axis ticks and direction\\n- Customizable margin and padding for the plot area\\n### BarChartViewer\\nA bar chart plot will be built with support to configure:\\n- Multiple data series\\n- Customizable data series colors to represent information as needed\\n- Labels for axes and for individual bars\\n### VariableStarViewer\\nThe scatter plot and the bar chart together along with the `SingleImageViewer` and a few additional controls will be a complex composite viewer built as the `VariableStarViewer`. The `VariableStarViewer` will have its own control bar that has a toggle for axis inversion, period, data series visibility, and phase focus. Each scatter plot will be individually pan and zoomable.\\n### DataImageViewer\\n_Note: Naming still TBD_\\nThis will be a complex composite consisting of a scatter plot and a single image. We may want to support up to N images, but this is still TBD. The initial build will be just the single scatter plot and single image.\\n### Future plots\\nThere may be requests to build more plot types like a line plot or map plot that renders GeoJSON. We will continue to evaluate our usage of vx at that time and ideally will continue to use it.\\n","tokens":99,"id":497,"Predictions":"Decision (A) The project will be able to represent JSON data as subjects for classification in the classifier.nbr>br>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.nbr>The project will be able to represent JSON data as subjects for classification in the classifier.n"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"Predictions":"Decision based on the information in the text.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-29.md","Context":"## Context\\nFuture projects using the `front-end-monorepo` will require a video player. Some projects may simply need a video to play. Some projects will require users to interact with a video file; such as make annotations on the video to mark locations or item sizes. The video player must be highly customizable, able to be used in a React code base project and support all major browsers.\\n","Decision":"We will implement `react-player` [Github](https:\/\/github.com\/CookPete\/react-player).\\nDemo Page: [Demo](https:\/\/cookpete.com\/react-player\/)\\nThe lengthy list of attributes (props) and Callback Props makes `react-player` a great choice for developers who need customization. Many of these attributes are simply booleans. For example, to play a video, pass in `playing={boolean}`. Full list of [Props](https:\/\/github.com\/CookPete\/react-player#props)\\nCompared to the native HTML video player, `react-player` makes it easy to customize styling so the player looks the same across different browsers.\\nMaking `react-player` responsive is easy by targeting pre-defined classNames.\\n[Responsiveness](https:\/\/github.com\/cookpete\/react-player#responsive-player)\\nOne of the biggest wins of using `react-player` is ease-of-use. This will reduce developer time and reduce the amount of custom methods in our code base.\\n### Media\\n`react-player` is a React component for playing a variety of URLs, including file paths, YouTube, Facebook, Twitch, SoundCloud, Streamable, Vimeo, Wistia, Mixcloud, and DailyMotion. [Supported Media](https:\/\/github.com\/CookPete\/react-player#supported-media)\\nAlthough `react-player` supports a number of externally hosted videos, we will only support video files uploaded to the Zooniverse platform.\\nFile types will be validated to ensure the files are mp4.\\n### Customization\\nA wide range of [Props](https:\/\/github.com\/CookPete\/react-player#props) can be passed in to control playback and react to events.\\nPlease read through the [Github](https:\/\/github.com\/CookPete\/react-player) for a full list of features.\\n","tokens":81,"id":500,"Predictions":"Decision Makers should consider the following options: a video player; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers; a video player that supports all major browsers"}
{"File Name":"front-end-monorepo\/adr-24.md","Context":"## Context\\nPanoptes-Front-End's drawing tools largely developed from previous custom projects and\/or were added one at a time to support a specific project. Because of this, several inconsistencies have been discovered in downsteam analysis and aggregation. To explain the inconsistencies, a few definitions are needed:\\n- RHC: A right handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the +y axis with angle=0 along the +x axis\\n- LHC: A left handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the -y axis with angle=0 along the +x axis\\n- Domain: The range of values a number can take [ or ] is inclusive, ( or ) is exclusive.\\n- Upper origin: The point 0, 0 is in the upper left of the plot\\n- Lower origin: The point 0, 0 is in the lower left of the plot\\nThe inconsistencies comprise of:\\n- The browser SVG coordinate systems use _RHC_ with an _upper origin_ resulting in positive angles rotating clockwise. Most plotting software (R, Python, Matlab) are _RHC_ with a _lower origin_ resulting in positive angles rotating counter-clockwise.\\n- The position of origin has been inconsistent between tools which has an effect on the final annotation too. Most use the center x, y point, but some don't\\n- Some of the drawing tools use _LHC_\\n- Some tools' annotation use `angle` some use `rotation`\\n- It's unclear when the x, y annotation refers to the center point of the shape\\n- It's unclear when the x, y annotation is being used as the point of rotation\\nSome of the mark annotation models have a few other issues as well:\\n- Some shapes have default values which an create bias. For example, the ellipse has a default axis ratio of 0.5 and many volunteers have left the default creating a bias ([comment](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/500#issuecomment-516788821))\\n- The freehand drawing tool has peformance impact on the browser as the drawing is being created and with the job to create classification exports as well. This is because the current annotation consists of every single x, y point created\\n","Decision":"The shape's mark annotation models should change for consistency and improved post-classification analysis in the following ways:\\n- The annotation should use the mathematical standard of _RHC_ with a domain of `[-180, 180]` for consistent angle calculation\\n- The annotation model should use `angle` for naming rotation angles. This replaces usage of `rotation`.\\n- The annotation model should replace `x` and `y` with `x_center` and `y_center` for shapes where that is applicable\\n- The exceptions are non-shapes like point, line, and transcription line tools, and non-symmetric shapes like fan.\\n- Shape clustering in aggregation is always done with the center\\n- All rotations should be defined about `x_center` and `y_center` point. If the rotation cannot be defined around the center point, then the point used should be clearly recorded in the annotation as `x_rotation` and `y_rotation`\\n- Conditional logic in component code can be avoided by using a Mobx-State-Tree's computed view functions to get either `x_center` or `x_rotation` mapped to `mark.x`. Code example:\\n```js\\nconst CircleModel = types\\n.model('CircleModel', {\\nx_center: types.optional(types.number, 0),\\ny_center: types.optional(types.number, 0),\\nradius: types.maybe(types.number),\\nangle: types.maybe(types.number)\\n})\\n.views(self => ({\\nget coords { \/\/ this is naming is reusing what's already been done with Point and Line for consistency.\\nreturn {\\nx: self.x_center,\\ny: self.y_center\\n}\\n}\\n}))\\n```\\n```js\\n\/\/ non-symmetrical shapes like fan use x_rotation, y_rotation\\nconst FanModel = types\\n.model('FanModel', {\\nx_rotation: types.optional(types.number, 0),\\ny_rotation: types.optional(types.number, 0),\\nradius: types.maybe(types.number),\\nangle: types.maybe(types.number),\\nspread: types.maybe(types.number)\\n})\\n.views(self => ({\\nget coords { \/\/ this is naming is reusing what's already been done with Point and Line for consistency.\\nreturn {\\nx: self.x_rotation,\\ny: self.y_rotation\\n}\\n}\\n}))\\n```\\n- Default values should be removed wherever possible. We will replace these with project builder configurable values set in the project builder lab when the tools are setup.\\n- The parameters will _not_ have default values suggested by us. If the parameters are not set in the lab, then when attempting to use that drawing tool in the classifier, the classifier should display an error message that the tool is not fully setup yet. The lab should also prompt for inputing a value.\\n- The lab should include instructions and a warning about the biasing effect.\\n- The tools that have defaults are ellipse, rotate rectangle, fan.\\n- The freehand drawing tools mark annotation will be a string of the SVG's path and it will be the responsibility of post-classification analysis to convert this to usable x,y points. We will include a sample script in the `DataDigging` repo for project owners to reference on how to do this. Aggregation in Caesar will have to be updated to do the conversion first.\\n- `tool` will change to `toolIndex` to clarify it is referring to the index of the input from the task area. The `toolIndex` is useful to distinguish between multiple instances of the same tool tip in a drawing task.\\n- Drawing annotations and drawing tool marks will have a `taskType` and `toolType` attribute added that map to an enumeration of the type of task or tool like `drawing` or `point`, `ellipse`, etc respectively. This enables the aggregation for Caesar code to auto-configure which extractor to use without checking data types ([comment](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/823#issuecomment-493896524)).\\n- Certain annotation models may have internal properties used denoted by a preceding underscore. These properties will be removed with the classification complete action. We will remove them to help prevent confusion by project owners in downstream analysis.\\n","tokens":494,"id":501,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-09.md","Context":"## Context\\nThe big question: which Subject Viewer should we use to view a given Subject?\\nAt the moment, the Classifier tries to \"guess\" which Subject Viewer to use (see\\n`lib-classifier\/src\/store\/Subject.js`, `get viewer()`) by analysing the Subject\\nitself. e.g. if the Subject has multiple images, show a multi-image viewer. If\\nthe Subject has a video, show a video viewer.\\nWhile this method works in a majority of projects, certain projects with\\nextremely specific Subject structures breaks the generalised \"guessing\" logic,\\nor else warps the logic with so many \"what if\" clauses that it collapses into a\\nwrithing mass of nonsense.\\nFor example, a TESS Planet Hunters Subject looks like...\\n```\\nSubject 12345 = {\\n...\\nlocations: [\\n{ \"image\/png\": \"tess-data-12345.png\" },\\n{ \"application\/json\": \"tess-data-12345.json\" }\\n]\\n}\\ntess-data-12345.json = {\\nx: [1,2,3,4,5],\\ny: [6,7,8,9,0]\\n}\\n```\\nIf we were to add to the \"guess the Subject Viewer\" logic by stating that _\"any\\nSubject that has a JSON, and that JSON has x-y coordinates, should use the Light\\nCurve Viewer\",_ then we'd have trouble if, say, a future project needs similar\\nJSONs with x-y coordinates for a Map Viewer or a Line Graph Viewer, etc.\\nSide note:\\n- The Light Curve Viewer can still also serve as a generic Scatterplot Viewer\\ngiven proper tweaks - this should be marked for future dev so JSON data with\\ngeneric x-y data can be \"guessed\".\\n","Decision":"The \"Choose a Subject Viewer\" logic (again, see\\n`lib-classifier\/src\/store\/Subject.js`, `get viewer()`) will _first_ try to check\\nif there's a **Workflow Configuration** stating the preferred Subject Viewer.\\nIf this specific configuration does not exist, the logic will _continue_ to\\n\"guess\" the correct Subject Viewer to use, as per the current system.\\n```\\n\/\/Example of a workflow.configuration for TESS project:\\n{\\nsubject_viewer: \"lightcurveviewer\"\\n}\\n```\\nWe've also decided _not_ to specify the \"Custom\/Specific Subject Viewer\" logic\\nwithin the _Subject_ itself, since it'll make the Subjects more complicated and\\nmaintenance troublesome. (Compare changing the config field of one Workflow vs\\nupdating the metadata or JSON of a million Subjects.)\\ni.e. we should **not** do something like:\\n```\\ntess-data-12345.json = {\\ntype: 'lightcurve',\\nx: [1,2,3,4,5],\\ny: [6,7,8,9,0]\\n}\\n```\\n","tokens":385,"id":502,"Predictions":"Decision (A) To use the Light Curve Viewer to view JSON data. (B) To use the Light Curve Viewer to view JSON data. (C) To use the Light Curve Viewer to view JSON data. (D) To use the Light Curve Viewer to view JSON data. (E) To use the Light Curve Viewer to view JSON data. (F) To use the Light Curve Viewer to view JSON data. (G) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data. (H) To use the Light Curve Viewer to view JSON data."}
{"File Name":"front-end-monorepo\/adr-07.md","Context":"## Context\\nMaking changes to drawing tools in production - specifically, to the structure of the data they output - is an occasionally necessary evil. Unfortunately, there is currently no way to link a marking tool, whether by version, commit or whatever, to its output in a classification.\\nThis means that when changes happen to the tools, we need to be careful to communicate those changes to project managers, since there is every chance that they will break their data pipelines. And in the absence of a schema for the tools' output, these changes have to be handled manually in code by project managers.\\n","Decision":"We include a schema for each marking tool used as part of the classification object submitted to the API.\\nThe schema will be written in JSON, and will live alongside to the marking tool code. It should describe the expected data structure of a marking tool.\\n### Example\\nFor a point tool with the following example output:\\n```json\\n{\\n\"x\": 152.96875,\\n\"y\": 164\\n}\\n```\\nThe schema could look like:\\n```json\\n{\\n\"$schema\": \"http:\/\/json-schema.org\/draft-06\/schema#\",\\n\"$id\": \"https:\/\/zooniverse.org\/schemas\/tools\/point.schema.json\",\\n\"type\": \"object\",\\n\"title\": \"Point\",\\n\"description\": \"A single point on the subject\",\\n\"properties\": {\\n\"x\": {\\n\"description\": \"X value of the point, where `0` is on the left\",\\n\"type\": \"number\"\\n},\\n\"y\": {\\n\"description\": \"Y value of the point, where `0` is at the top\",\\n\"type\": \"number\"\\n}\\n},\\n\"required\": [\\n\"x\",\\n\"y\"\\n]\\n}\\n```\\n### Alternatives\\nWe could version the schema, and simply reference it within the classification by its URL. For example, the point tool, instead of including the entire schema in the classification, has a reference to `https:\/\/zooniverse.org\/schemas\/tools\/point.schema.v1.json`. A change to the point tool output then becomes a new version number.\\nThis would result in smaller classification object sizes, although forgoes the convenience of having a self-contained classification object.\\n","tokens":120,"id":503,"Predictions":"Decision (A) The tool should be changed to a different format. (B) The tool should be changed to a different format. (C) The tool should be changed to a different format. (D) The tool should be changed to a different format. (A) The tool should be changed to a different format. (A) The tool should be changed to a different format. (B) The tool should be changed to a different format. (C) The tool should be changed to a different format. (D) The tool should be changed to a different format. (A) The tool should be changed to a different format. (A) The tool should be changed to a different format. (A) The tool should be changed to a different format. (B) The tool should be changed to a different format. (C) The tool should be changed to a different format. (D) The tool should be changed to a different format. (A) The tool should be changed to a different format. (A) The tool should be changed to a different format. (A) The tool should be changed to a different format. (B) The tool should be changed to a different format. (C) The tool should be changed to a different format. (D) The tool should be changed to a different format. (A) The tool should be changed to a different format. (A) The tool should be changed to a different format."}
{"File Name":"front-end-monorepo\/adr-35.md","Context":"## Context\\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\\n","Decision":"We solved this problem by adding an undo\/redo history stack to the classifier task area. [#2050](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/2050) added a global history stack (`store.annotatedSteps`). [#2520](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/2520) refines this by moving from a global history manager to individual history management for each subject (`subject.stepHistory`.)\\nHistory is managed by the MST middleware [`UndoManager`](https:\/\/github.com\/mobxjs\/mobx-state-tree\/blob\/master\/packages\/mst-middlewares\/README.md#undomanager), which records snapshots of a single subtree in the store. Each history item is a single step key and one or more annotation IDs, representing the current state of the task area. 'Back' loads the previous step key and annotation IDs from history. 'Next' loads the next step key and annotation IDs, or creates a new history item if one doesn't exist already. Going back then changing branches, for a branching workflow, clears any existing redo history and starts a fresh history from the current point.\\n","tokens":76,"id":505,"Predictions":"We wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject)nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-36.md","Context":"## Context\\nWhen we launched the NextJS apps, a single domain could only host one NextJS app. We gave each app its own subdomain: `fe-content-pages.zooniverse.org` and `fe-projects.zooniverse.org` then proxied URLs from `www.zooniverse.org` to those domains. Next data requests, on the `\/_next` URL, are proxied from `www.zooniverse.org\/_next` to `fe-project.zooniverse.org\/_next`. We deliberately broke the content pages app, in favour of supporting projects.\\n[Next 9.5](https:\/\/nextjs.org\/blog\/next-9-5), in July 2020, added support for multiple apps running on the same domain via the [`basePath`](https:\/\/nextjs.org\/blog\/next-9-5#customizable-base-path) config setting.\\n","Decision":"Set base paths of `\/about` and `\/projects` for the content pages app and project app respectively.\\nhttps:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/2519\\n","tokens":180,"id":506,"Predictions":"Decision (A).n[Next 9.5](https:\/\/nextjs.org\/blog\/next-9-5), in July 2020, added support for multiple apps running on the same domain via the [basePath](https:\/\/nextjs.org\/blog\/next-9-5#customizable-base-path) config setting.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}
{"File Name":"front-end-monorepo\/adr-18.md","Context":"## Context\\nOn PFE, there are a couple of different ways workflows are routed:\\n- Navigating to `\/classify`, the user sees either the default workflow for that project, a random active workflow, or the workflow stored in project preferences for logged-in volunteers.\\n- Navigating to `\/classify?workflow=[workflow_id]` shows the workflow determined by the query parameter when the \"Use workflow query\" feature is enabled for the project, or if the user has the correct role (owner, collaborator, tester, or admin).\\nThis is not optimal for several reasons. Users cannot bookmark specific workflows unless they use the query parameter; there can be uncertainty over which actual workflow is being shown (particularly as the workflow name isn't shown in the UI); different workflows can be shown at the same URL at different times; and ultimately, workflows are static resources that should be routed to - that's what URLs are for.\\nAdditionally, [we currently have a requirement for CSSI CitSci to be able to route to specific subjects](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/806#issuecomment-495685027). It's conceivable that a project will have a requirement for routing to a specific subject set linked to a workflow as well.\\n","Decision":"We adopt the following URL structure:\\n```\\n\/projects\/[owner]\/[project]\/classify\/workflow\/:workflow-id\/subject-set\/:subject-set-id\/subject\/:subject-id\\n```\\nThis would be facilitated by the [dynamic routing feature in Next.js 9](https:\/\/github.com\/zeit\/next.js\/#dynamic-routing) (see [#1071](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1071)).\\nWorkflow links from the home page would route directly to their workflow-specific URLs.\\n### `\/classify` behaviour\\nWhen navigating to `\/classify`, the page should redirect the user to the correct workflow URL in the following priority:\\n1. Workflow ID set in user project preferences by the user\\n1. Workflow ID set in user project preferences by the project\\n1. Project default workflow\\n1. Random active selection\\nThe new URL that we redirect the user to should replace the entry for `\/classify` in the browser history.\\n### Error handling\\nWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in `getInitialProps`, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, `404`; incorrect permissions get a `401`\/`403` (depending on login status) etc.\\n","tokens":260,"id":507,"Predictions":"Decision (A) The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.nB. The workflows should be routed to a specific subject set linked to a workflow.n"}
{"File Name":"front-end-monorepo\/adr-26.md","Context":"## Context\\nWe use [Sentry](https:\/\/sentry.io) to provide centralised JavaScript error logging for the project and content pages apps. Sentry supports versioned releases, and GitHub integration, so that fixes can be linked to releases and new issues can be linked to potentially bad commits.\\n","Decision":"The monorepo will be versioned in Sentry, using the git commit SHA to version a release. New releases are deployed to staging on each push to master. A release is finalised and deployed to production when the production-release tag is updated to point to that release. [#1599](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1599) and [#1601](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1601) implement this using GitHub actions.\\n","tokens":60,"id":508,"Predictions":"Decision (A) The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the project.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.nbr>The app should be able to be used in the next version of the app.n"}
{"File Name":"front-end-monorepo\/adr-27.md","Context":"## Context\\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\\n### [multiFrame](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/MultiFrameViewer)\\nA workflow using the multi-frame subject viewer might have a preference regarding:\\n- marks per frame: some might prefer marks filtered per frame, like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page, while other workflows might prefer marks persist between frames like [Space Warps](https:\/\/www.zooniverse.org\/projects\/aprajita\/space-warps-hsc\/classify) or [Power to the People](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people). However, after reviewing projects that enabled marks to persist between frames (`multi_image_clone_markers` [in PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/tasks\/drawing\/markings-renderer.cjsx#L55)) it appears the PFE setting is unclear, as a few of the related workflows do not include a drawing task or the subjects do not have multiple frames.\\n- positioning: some might prefer pan, zoom, and rotation reset per frame, like a transcription workflow where each frame represents a unique page to transcribe, while other workflows might prefer pan, zoom, and rotation maintained between frames, like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/zooniverse\/wildcam-gorongosa\/classify) or [Backyard Worlds](https:\/\/www.zooniverse.org\/projects\/marckuchner\/backyard-worlds-planet-9\/classify) (in flipbook mode, not separate frames)\\n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/SubjectGroupViewer)\\n- A workflow using the subject-group subject viewer might want to define the subject cell width, height, or style, or the subject viewer grid columns or grid rows.\\n","Decision":"Subject viewer configuration is an object stored in `workflow.configuration.subject_viewer_config` and is structured as follows for the subject viewers noted:\\n`multiFrame`:\\n```javascript\\n{\\nfilter_marks_per_frame: <boolean>  \/\/ replaces multi_image_clone_markers in PFE\\npositioning: <enumerable> \/\/ includes pan, zoom, and rotation, enumerable i.e. \"maintain\", \"reset\"\\n}\\n```\\n`subjectGroup`:\\n```javascript\\n{\\ncell_width: <number of pixels>\\ncell_height: <number of pixels>\\ncell_style: { [CSS property]: <CSS property value> }\\ngrid_columns: <number>\\ngrid_rows: <number>\\n}\\n```\\nSubject viewers to define the configuration object in the related subject viewer README, if applicable.\\nThe ScatterPlotViewer and BarChartViewer accept a configuration object directly in the JSON structure of the subject data to support variability in the display settings including the plot labels since it's possible this might vary per subject. The `subject_viewer_config` object should only be used for  configuration options that apply to all subjects linked to the workflow.\\n","tokens":487,"id":509,"Predictions":"Decision based on the information in this ADR.nA workflow determines which subject viewer to use with the workflow.configuration.subject_viewer property.nA workflow using the multi-frame subject viewer might have a preference regarding:n- marks per frame: some might prefer marks filtered per frame, like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page, while other workflows might prefer marks persist between frames like [Space Warps](https:\/\/www.zooniverse.org\/projects\/aprajita\/space-warps-hsc\/classify) or [Power to the People](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people). However, after reviewing projects that enabled marks to persist between frames (multi_image_clone_markers [in PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/tasks\/drawing\/markings-renderer.cjsx#L55)) it appears the PFE setting is unclear, as a few of the related workflows do not include a drawing task or the subjects do not have multiple frames.n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation reset per frame, like a transcription workflow where each frame represents a unique page to transcribe, while other workflows might prefer pan, zoom, and rotation maintained between frames, like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/zooniverse\/wildcam-gorongosa\/classify) or [Backyard Worlds](https:\/\/www.zooniverse.org\/projects\/marckuchner\/backyard-worlds-planet-9\/classify) (in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectViewer\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectViewer\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectViewer\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectViewer\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectViewer\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people).n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/SubjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/subjectGroupViewer)n- positioning: some might prefer pan, zoom, and rotation maintained between frames like [Wildcam Gorongosa\/classify](in flipbook mode, not separate frames)n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/"}
{"File Name":"front-end-monorepo\/adr-13.md","Context":"## Context\\nWe are an open source project and as a result can possibly accept outside contribution to our codebase. To this end, we need to communicate our requirements for the code that is submitted and our process for acceptance of this code. The end goal for the rewrite is to be more modular which we believe will enable both the team and outside contributors flexibility and ease of adding enhanced features.\\n","Decision":"We will add a contributing guide to our documentation files. Once available, Github will show links to this guide automatically to any outside user visiting the front-end-monorepo repository. Any outside contributions must follow this guide to submit code for review.\\n","tokens":79,"id":510,"Predictions":"We will accept outside contributions to our codebase.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-20.md","Context":"## Context\\nIMLS funded us to experiment with transcription tools. The project [Anti-Slavery Manuscripts](https:\/\/www.antislaverymanuscripts.org\/classify) has a workflow where volunteers transcribe lines of text first by marking the line, then adding the transcription in a sub-task. This task is slightly unique in that:\\n- Incomplete classifications can be submitted\\n- Previous transcriptions from caesar are loaded and presented as an option in the sub-task\\n- volunteers can select the previous transcription, edit it, and submit a new transcription\\n- lines with previous transcriptions or retired lines are displayed visually by color\\n","Decision":"We will be porting the ASM functionality to the main classifier as a new transcription task. The task will be composite of:\\n- A drawing line task for transcription that is created by two pointer down and up events to mark the points of the line.\\n- the starting point and ending point will have visual indicators communicating the direction of creation.\\n- the line color will indicate the current status of completion\\n- completed lines cannot be edited\\n- Note that ASM leveraged a hacked polygon tool to do this, but we should have a line tool variant specifically for this instead. Downstream aggregation can leverage polygon type aggregations if it fits.\\n- A sub-task will display once the transcription line mark is made\\n- a text input will display suggestions from previous caesar aggregations.\\n- The suggestions can be selected and inserted as an editable value in the text input\\n- Grommet's [TextInput](https:\/\/storybook.grommet.io\/?path=\/story\/textinput--suggestions) with the suggestions prop will be utilized so that it can be styleable with our theme.\\n- sub-task will not be a modal, but a movable div linked to the currently selected line mark\\n[More detailed user stores are in this google doc](https:\/\/docs.google.com\/document\/d\/16abI-wkRlEXsWgACfFQVqwO76aEopohIjQiRfNQKWiw\/edit)\\n","tokens":129,"id":511,"Predictions":"The project [Anti-Slavery Manuscripts](https:\/\/www.antislaverymanuscripts.org\/classify) has a workflow where volunteers transcribe lines of text first by marking the line, then adding the transcription in a sub-taskn- Previous transcriptions from caesar are loaded and presented as an option in the sub-taskn- volunteers can select the previous transcription, edit it, and submit a new transcriptionn- lines with previous transcriptions or retired lines are displayed visually by colorn.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n."}
{"File Name":"front-end-monorepo\/adr-23.md","Context":"## Context\\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\\n- add new code in three places:\\n- [task views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks).\\n- [task models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/tasks).\\n- [annotation models](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/annotations).\\n- import the new modules by name in several places, and register them:\\n- [registered views](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/components\/Classifier\/components\/TaskArea\/components\/Tasks\/helpers\/getTaskComponent.js).\\n- [import tasks models for workflow steps](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/WorkflowStepStore.js#L5-L18).\\n- [import all annotations to the classification model](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/Classification.js#L3).\\n- [register annotations with the classifications store](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/2e2ac27a442afc8cfaea6f7735b97ebb511367a8\/packages\/lib-classifier\/src\/store\/ClassificationStore.js#L111-L120).\\nIt was easy to forget one of these steps and a lot of this could be automated in code.\\n","Decision":"- Keep all the code together. Store task views and models next to each other in the filesystem. (#1212)\\n- Import named modules to a registry object (or similar) then load them in to other code from that register. (#1212)\\n- Delegate responsibility from the classification to individual tasks. (#1228)\\n### Implementation\\n- Task code was moved to `lib-classifier\/src\/plugins\/tasks`. Each task has its own directory, with these subdirectories:\\n- _components_: React components to render the task.\\n- _models_: MobX State Tree models for the task. One Task model and one Annotation model.\\n- a _taskRegistry_ object was added, which is described in the [tasks README](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/plugins\/tasks\/readme.md).\\n- Responsibility for creating new annotations was removed from the classifications store, removing the need for the classifications store to know about different types of tasks and how to create an annotation for each. New methods were added to the task models to delegate responsibility and make tasks more flexible:\\n- _task.createAnnotation()_ creates a new annotation of the correct type for a specific task.\\n- _task.defaultAnnotation_ (read-only) returns the default annotation for a specific task.\\n","tokens":492,"id":512,"Predictions":"Decision (A) To add new tasks to the classifier. (B) To add new tasks to the classifier. (C) To add new tasks to the classifier. (D) To add new tasks to the classifier. (A) To add new tasks to the classifier. (A) To add new tasks to the classifier. (B) To add new tasks to the classifier. (C) To add new tasks to the classifier. (D) To add new tasks to the classifier. (A) To add new tasks to the classifier. (A) To add new tasks to the classifier. (A) To add new tasks to the classifier. (B) To add new tasks to the classifier. (C) To add new tasks to the classifier. (D) To add new tasks to the classifier. (A) To add new tasks to the classifier. (A) To add new tasks to the classifier. (A) To add new tasks to the classifier. (B) To add new tasks to the classifier. (C) To add new tasks to the classifier. (D) To add new tasks to the classifier. (A) To add new tasks to the classifier. (A) To add new tasks to the classifier. (A) To add new tasks to the classifier. (B) To add new tasks to the classifier. (C) To add new tasks to the classifier."}
{"File Name":"front-end-monorepo\/adr-04.md","Context":"## Context\\nFairly early on in the rebuild of the Classifier, we started using newer technologies such as [CSS Grid](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Grid_Layout), which are not supported in older browsers like Internet Explorer 11.\\nAnd, while Edge is now several major versions in, we still have a percentage of users on IE11. As such, we're now at a point where we have to determine whether to drop support for legacy browsers and risk inconvenience for that segment, or invest significant time in coding and testing fallbacks.\\n","Decision":"We will only officially support the following browsers:\\n### Desktop\\n- Safari\\n- Chrome\\n- Firefox\\n- Edge\\n### Mobile\\n- Safari\\n- Chrome\\n- Opera\\nOf these, we will support the current and last two major versions.\\n","tokens":118,"id":514,"Predictions":"We should drop support for legacy browsers.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-22.md","Context":"## Context\\nWith new drawing tools being developed for the classifier, we need an API that's common to all drawing tools and marks, which can be easily extended by tool developers. This document lays out an overview of the drawing tool model and the public interfaces common to all tools and all marks.\\nTo support drawing, the subject viewer also needs to support:\\n- rendering a static list of marks from previous drawing task annotations.\\n- interacting with pointer events to create, edit and delete new marks for the current drawing task annotation.\\n","Decision":"### The subject viewer\\nThe subject viewer will render two components.\\n- _DrawingToolMarks_ takes an array of marks from drawing task annotations in the classification and renders it as a static, read-only list.\\n- _InteractionLayer_ wraps a _DrawingToolMarks_ component and adds pointer event support, so that the rendered array of marks can be edited and updated. THis component only acts on marks for the active drawing task.\\nMarks created by the _InterctionLayer_ are added to a new drawing task annotation, for the current classification, when we click Next or Done to complete the current task.\\nPointer event support is polyfilled in older browsers with [PEP](https:\/\/github.com\/jquery\/PEP).\\n### The drawing model\\nA drawing task has drawing tools. Each tool creates marks. On task completion, a drawing annotation is created, which is an array of all drawn marks. Each mark has a corresponding React component which renders the SVG for that particular shape.\\n","tokens":108,"id":516,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-02.md","Context":"## Context\\nAt the Zooniverse Team Meeting April 2018, it was decided that Panoptes Front End would be broken up into modular libraries and client-side apps. To aid in this rewrite, it was also decided that we would try out server-side rendering (SSR) since now there are several frameworks for React that make this easy. SSR has several benefits including improved load times and rendered HTML that search providers could crawl to index.\\nIt was initially chosen to use a SSR framework called [Next.js](https:\/\/github.com\/zeit\/next.js\/). Next.js is an opinionated framework that supports out of the box SSR, routing, production build compiling. In particular its API provides a method, `getInitialProps` to easily hydrate client-side app state. However, in the brief experience we have had so far:\\n- It's been difficult to configure. We had to configure it to use [Grommet 2](https:\/\/v2.grommet.io\/) and [styled-components](https:\/\/www.styled-components.com\/) as these are not defaults.\\n- It's been difficult with major version upgrades. Version 6 was released right when we started and the upgrade path took a lot of time.\\n- It's had issues with [Mocha](https:\/\/mochajs.org\/). This is concerning since Mocha is the most popular test framework for javascript, and our current default.\\nThere is an [open issue](https:\/\/github.com\/zeit\/next.js\/issues\/1632) confirming that Next.js will be integrating React Router v4 as its routing solution. React Router v4 is not a router that we wish to use due to its significant API changes.\\nIn addition, Sarah had been recently advised by fellow developers in the Chicago Javascript community that Next.js is difficult to configure which can outweigh any benefits it might provide.\\nRoger and Sarah built several prototypes with other libraries, including [react-server](https:\/\/react-server.io\/), [razzle](https:\/\/github.com\/jaredpalmer\/razzle), and [react-universally](https:\/\/github.com\/ctrlplusb\/react-universally). However, none of these provided the same ease-of-use as Next.js. Additionally, react-server doesn't appear to be actively maintained.\\n","Decision":"For the time being, we will continue to use NextJS. We're currently using v5; v6 is available, and likely to be the last version that is router-agnostic, so we can revisit this decision then.\\n","tokens":471,"id":517,"Predictions":"Decision was made to use Next.js instead of Next.js.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-17.md","Context":"## Context\\nWe have identified a gap in our automated and manual testing process from the deployment of [zooniverse\/front-end-monorepo#1038](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1038). This pull request had been reviewed and tested manually in a development environment on the reviewer's local machine, tests passed, and the app appeared to work as intended with the added bug fix. After automatic deployment to production by merging to master, however, we received a report that Planet Hunters: TESS classification interface was no longer functioning. The classify page was returning a 404.\\nWe had acknowledged previously that we had a need for staging environment deploys for the purposes of design reviews in [zooniverse\/front-end-monorepo#694](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/694). We now have a need to have staging deployments so we can manually check that the pull request functions in a deployed, production-like environment. The next.js builds and creates files specific for the production deployment that running the app locally for development does not replicate, nor is it replicated in automated unit testing.\\nInitially we were considering branch deploys for both of these cases, but in order to do this we would need to use wildcard sub-domains. At this time, [kubernetes ingress does not support wildcards](https:\/\/github.com\/containous\/traefik\/issues\/3884). Therefore, we need to devise a different solution.\\n","Decision":"In practice, we're going to have two kinds of pull request: one that changes a single app (e.g. new widget on project home page), and one that affects multiple apps (e.g. update to the shared component library). For PRs on a single app, we'd like to manually deploy it as a staging branch deployment so it can be tested in isolation. On merging to master, that gets deployed to staging automatically. We'd then do manual integration testing before manually deploying to production. For PRs across multiple apps, we'd test it locally before merging to master using a local Docker image setup to use Panoptes in production. Once it's deployed to staging, we'd do integration testing before manually deploying to production.\\nWe're going to setup a staging deployment that matches production as closely as possible to fill the gap of the need for manual reviews to confirm that the app is functioning. To accomplish this:\\n- Merging to master will be switched to deploy to staging to https:\/\/frontend.preview.zooniverse.org\\n- Production deployment will now be done manually triggered by lita command on slack and using a git tag for production\\n- The Jenkins file will be updated to use the git tags to determine the location of the deployment\\n- Cloudfront will be configured to load the correct microservice app depending on route:\\n- Both the staging (https:\/\/frontend.preview.zooniverse.org) and production domains (www.zooniverse.org) will have cloudfront configurations that will match URL traffic against rules setup in Cloudfront. The staging rules will map to the staging apps and the production rules will map to the production apps.\\n- The cloudfront rules match paths on the incoming URL, i.e. `\/about\/team` maps to a registered service via DNS, e.g.\\n+ When a `GET` request for URL `www.zooniverse.org\/about\/team` hits cloudfront, it maps to the `fe-content-pages.zooniverse.org` service domain.\\n+ Cloudfront then proxies that request via DNS lookup to the Kubneretes (K8) ingress service\\n+ The K8 ingress then looks up the registered service domain and forwards the request, in this case, to a `fe-content-pages` service pod to serve the request and respond to the client.\\n- Generally staging and production would have the same behaviour mappings in Cloudfront and staging will be a place to test these mapping out before setting up in production.\\nA future enhancement will be added for branch deploys for manual reviews. This can possibly be accomplished by:\\n- Lita command on slack\\n- Script is written to deploy to a branch\\n- Helm could be used to tear down the pods after the branch is merged\\n","tokens":305,"id":518,"Predictions":"Decision (A). The next.js app should be deployed locally.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nD. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nD. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nD. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nC. The next.js app should be deployed in a staging environment.nD. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nB. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in a staging environment.nA. The next.js app should be deployed in"}
{"File Name":"front-end-monorepo\/adr-12.md","Context":"## Context\\nAs of writing, Zooniverse apps are being moved from Docker Swarm to Kubernetes, but both require a working Docker image of the app for deployment. However, building separate images from start to finish duplicates a lot of effort in downloading packages, building libraries etc.\\n","Decision":"The [Jenkinsfile](..\/..\/Jenkinsfile) builds Docker images in two stages:\\n1. Build an image containing the entire monorepo at the current commit, then install top- and package-level dependencies, and finally build production versions of the library packages. This image is [zooniverse\/front-end-monorepo](https:\/\/cloud.docker.com\/u\/zooniverse\/repository\/docker\/zooniverse\/front-end-monorepo).\\n1. Loop through all folders in the `packages` folder starting with `app-`, and build a Docker image for each one. These Docker images use the `zooniverse\/front-end-monorepo` image as a base, and then run the production build and start scripts for that app. An example image is [zooniverse\/fe-project](https:\/\/cloud.docker.com\/u\/zooniverse\/repository\/docker\/zooniverse\/fe-project).\\n","tokens":56,"id":519,"Predictions":"The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes. The app is being moved from Docker Swarm to Kubernetes."}
{"File Name":"front-end-monorepo\/adr-37.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. We needed a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.\\nThe subjects table in Panoptes is too large to allow for fast querying, so we also needed to build our own indexing system for indexed sets.\\n","Decision":"- Project owners can flag subject metadata columns as searchable by prefixing the heading with `%` in the manifest eg. these manifest headings `subject_id,image_name_1,%origin,link,%attribution,license,#secret_description` mark `metadata.origin` and `metadata.attribution` as searchable.\\n- Subject sets with indexed subjects have `metadata.indexFields` set to a list of indexed fields eg. `indexFields: 'origin,attribution'`.\\n- Subject metadata for indexed sets is copied to a separate database running on [Datasette](https:\/\/datasette.io). Each set is given its own table, named by subject set ID. Datasette gives us a RESTful API out of the box, allowing us to browse and search subject data as HTML or JSON. See https:\/\/subject-set-search-api.zooniverse.org\/subjects.\\n- From a volunteers point-of-view, the Datasette service is used to find specific subject IDs to work on. Those IDs are then sent to the Panoptes API `\/subjects\/selection` endpoint, which returns those subjects, in order, for classification.\\n","tokens":90,"id":520,"Predictions":"We need to build a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-11.md","Context":"## Context\\nMarkdown is generally safer to use than HTML for user submitted content on the web because it limits what a user can submit to predefined allowable strings that can be easily sanitized and then get converted to HTML. Panoptes-Front-End uses markdown through out the entire application. Currently we support an in-house markdown renderer, [markdownz](https:\/\/github.com\/zooniverse\/markdownz), that uses  [markdown-it](https:\/\/github.com\/markdown-it\/markdown-it). The library markdown-it is mature and has several plug-ins available for it that we've added to markdownz as well as some of our own customizations.\\nMarkdown, however, isn't totally free from being exploitable, nor is React. Markdownz relies on a React method, `dangerouslySetInnerHTML` that potentially open us to vulnerabilities (see this line: https:\/\/github.com\/zooniverse\/markdownz\/blob\/master\/src\/components\/markdown.jsx#L99).\\nNow that we've adopted Grommet as general React component library, Grommet also provides a React [markdown](https:\/\/v2.grommet.io\/markdown) renderer ([code](https:\/\/github.com\/grommet\/grommet\/blob\/master\/src\/js\/components\/Markdown\/Markdown.js)). Grommet's markdown component uses [markdown-to-jsx](https:\/\/github.com\/probablyup\/markdown-to-jsx) which instead converts markdown to React components to use instead of relying on `dangerouslySetInnerHTML`. However, after extensive evaluation, `markdown-to-jsx` does not have the plugin eco-system that we need and so we would need to rewrite a lot of customizations to get to basic parity with what we already support. This defeats the purpose of reducing the maintenance of our own code for common markdown support.\\n","Decision":"We will make a new `Markdownz` React component that will be a part of the Zooniverse React component library. This new component will be built using [`remark`](https:\/\/github.com\/remarkjs\/remark). Remark is a popular markdown rendering library with a good plugin eco-system. It is supported by Zeit, which also supports Next.js, the server-side rendering library we have decided upon.\\nHere is how markdown-it's plugins will map to remark's plugins:\\n|markdown-it plugin\/custom plugin|remark plugin\/custom plugin|notes|\\n|--------------------------------|---------------------------|-----|\\n|markdown-it-emoji|remark-emoji|remark-emoji does not support emoticons like `:-)` but does gemojis like `:smile:`|\\n|markdown-it-sub|remark-sub-super||\\n|markdown-it-sup|remark-sub-super||\\n|markdown-it-footnote|built in|Remark supports this and can be enabled by passing `footnote: true` into its settings object|\\n|markdown-it-imsize|N\/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we have defined a custom function that will set the `width` and `height` props on the Image component if the sizing syntax is defined in the alt tag of the markup. This is in contrast to the sizing syntax originally being defined in the src markup. We do not want to modify the sanitization remark-react does on source urls, so instead we have moved support of syntax to the alt tag area of the markup|\\n|markdown-it-video|deprecating|We are deprecating this because we don't want project owners embedding youtube videos with ads|\\n|markdown-it-table-of-contents|remark-toc|This works instead by looking for a heading that has case insensitive `table of contents`, `toc`, or `table-of-contents`|\\n|markdown-it-anchor|N\/A|Remark has basic anchor support.|\\n|twemoji|N\/A|Do we really need to use images of Twitter's emojis? Unicode support for emojis is fairly ubitiquous now.|\\n|markdown-it-html5-embed|N\/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we define a custom function that returns HTML 5 video instead of an image of the src is a video mime-type or returns HTML 5 audio if the src is an audio mime-type|\\n|replaceSymbols|our own fork of remark-ping|remark-ping supports our needs for doing at-mentions of users, but it is forked to also support talk hashtags and the subject mentions using `^S`|\\n|relNofollow|remark-external-links|This plugin adds nofollow to absolute urls|\\n|markdownNewTab|remark-external-links|remark-external-links plugin adds `target='_blank'` and nofollow, noopener, noreferrer to all absolute urls. `+tab+` in front of the url will no longer work because of the sanitization that remark-react does. It may not be a good idea to modify how the sanitization works to allow this and instead just update our users on how this works instead.|\\n`remark-react` is added to parse the markdown to jsx which is inherantly safer than using `dangerouslySetInnerHTML` and allows customizing which react components get used for html elements like `markdown-to-jsx`.\\n","tokens":364,"id":521,"Predictions":"Decision (A). Markdown is generally safer to use than HTML for user submitted content on the web because it limits what a user can submit to predefined allowable strings that can be easily sanitized and then get converted to HTML. Panoptes-Front-End uses markdown through out the entire application. Currently we support an in-house markdown renderer, [markdownz](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz), that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com\/zooniverse\/markdownz) that uses [markdown-it](https:\/\/github.com"}
{"File Name":"front-end-monorepo\/adr-32.md","Context":"## Context\\nCurrently, the FEM Classifier's default \"Subject Viewer sizing\/fitting\" behaviour is _\"fit to width, no max height\"._\\nThis is analogous to PFE's special \"no-max-height\" behaviour, which had to be _explicitly set._\\nWhile the default \"fit to width, no max height\" behaviour is good in many cases, there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be \"cut off\".\\nWe need to consider how to size\/fit viewers for a variety of Subjects and Subject Viewers.\\n","Decision":"The answer is to use **Layouts**.\\n- The plan for FEM is that every workflow should be able to _set their own Layout_ (e.g. portrait, landscape, fullscreen, etc) each with their own Subject Viewer sizing\/fitting behaviour.\\n- The choice of Layout (presumably per workflow) should be controllable by the project owner.\\n- Whether or not a specific subject viewer size configuration is supported will depend on the Layout.\\nSee also:\\n- The current [Layout code](..\/..\/packages\/lib-classifier\/src\/components\/Classifier\/components\/Layout), as of June 2021, currently only has DefaultLayout.\\n","tokens":149,"id":522,"Predictions":"Decision (A).nThe default \"fit to width, no max height\" behaviour is good in many cases, but there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be \"cut off\".nWe need to consider how to size\/fit viewers for a variety of Subjects and Subject Viewers.nA Decision (A).nThe default \"fit to width, no max height\" behaviour is good in many cases, but there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be \"cut off\".nWe need to consider how to size\/fit viewers for a variety of Subjects and Subject Viewers.nA Decision (A).nThe default \"fit to width, no max height\" behaviour is good in many cases, but there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be \"cut off\".nWe need to consider how to size\/fit viewers for a variety of Subjects and Subject Viewers.nA Decision (A)."}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"Predictions":"We decided that the mappings should be converted to an object.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-08.md","Context":"## Context\\nFor the upcoming the upcoming TESS project (aka Planet Hunters 2019), we need  to create a special Light Curve Viewer component for showing interactive brightness curves.\\nFurther information available on the [TESS Front End documentation](https:\/\/docs.google.com\/document\/d\/1BcX4PyC2khmtC9g035G2e5I1zirZa3z9mWINkWATaPs\/edit?usp=sharing).\\n@rogerhutchings, @shaun.a.noordin and @srallen researched available charting libraries, particularly ones designed for seamless integration with React. Of those, prototypes were built with [Plot.ly](https:\/\/plot.ly\/javascript\/react\/), [Victory](https:\/\/formidable.com\/open-source\/victory\/) and [vanillaD3](https:\/\/d3js.org\/). Sample data was taken from the Planet Hunters project.\\nFrom these, we discovered that:\\n1. Both Plot.ly and Victory suffered from slow performance, especially when using their out-of-the-box features (such as the selection tools).\\n2. Plot.ly and Victory had some API limitations that made fulfilling spec requirements difficult. Plot.ly, for example, had a limited set of events available (`click` events, but not `mousemove`, `drag` etc).\\n3. The out-of-the-box features were often almost perfect, but not quite, and hard to extend. Plot.ly, for example, has a range select tool, but it's designed for selecting data subsets for plotting. To create annotations, we'd have needed to use the range selections and turn them into a variable width bar chart plotted on top of the scatter plot.\\n","Decision":"We decided that trying to use a library to meet our fairly unusual requirements would end up causing more work than would be saved by having the basic features available. As such, the Light Curve Viewer will be built using native D3, implemented into the classifier's React components.\\n","tokens":348,"id":524,"Predictions":"Decision (A). Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project. Plot.ly should be used for the project."}
{"File Name":"front-end-monorepo\/adr-03.md","Context":"## Context\\nAs part of the development of Panoptes.js, we came across the issue of authentication. Currently, the auth library is part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client), and provides support for first-party login and OAuth2 (for custom front ends).\\nHowever, the current client has the following drawbacks:\\n- It only provides the relevant API methods and no front end components, which need to be written by the library consumer.\\n- As a result, the login experience is slightly different across each project.\\n- The OAuth client has been buggy in the past.\\n","Decision":"To write a separate auth library. This will expose:\\n- the __API helpers__ for the first-party auth process, if the consumer wants\/needs to use them.\\n- __+a first-party auth client__ as a standalone React app. This will open an IFrame to the existing Panoptes views for sign in, sign out, registration, and profile management (password reset and account deletion)\\n- __an OAuth client__, also as a standalone React app. This will wrap an existing package to provide OAuth support, such as [js-client-oauth2](https:\/\/github.com\/mulesoft\/js-client-oauth2), and will handle session length, provide session timeout prompts, and provide a session expiry hook or event for a consumer app (e.g. as a prompt to save the current state).\\n","tokens":134,"id":525,"Predictions":"Decision (A). The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.nB. The auth library should be integrated into Panoptes.js.n"}
{"File Name":"front-end-monorepo\/adr-14.md","Context":"## Context\\nThe static content on the Zooniverse (notably, the About, Publications, and Team pages on [PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/)) has traditionally been [baked](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/locales\/en.js) [into](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/lib\/publications.js) the front end code.\\nWhile this works, it creates maintenance issues: to update the list of publications, for example, requires someone to dig through the code to find and modify the correct JSON file, and then create a pull request on GitHub. To do so for multiple publications becomes a long and tedious process.\\nAdditionally, JSON is fragile - a missing (or extra) comma can break the build.\\nAs a result, we're considering using a content management system (CMS) that would allow us to more easily maintain our static content.\\nThese are the options we have:\\n1. Continue to hard code content into the front end code\\n2. Create our own CMS to manage our content\\n3. Install a third party CMS into one of our servers\\n4. Use an online third party CMS\\n","Decision":"We've decided to use [Contentful](https:\/\/www.contentful.com\/), an online third party CMS, to host our content. (Or at least, part of our content, such as the often-changing Publications page.) There will be two users registered: one admin account with full access to the Content Model editor _and_ content, and an editor account for day-to-day use which only has content editing access.\\n**Reasons for choosing Contentful**\\n- [@rogerhutchings](https:\/\/github.com\/rogerhutchings) has used it for projects in the past\\n- It has a free tier\\n- It has a well-documented API\\n- It has an excellent admin interface for editing content and models\\n- It's one of the larger hosted CMS offerings, meaning it's less likely to disappear overnight\\n**Pros**\\n- CMS makes content management easier and updates faster, especially for non-developers.\\n- External platform reduces the need for any sort of code maintenance.\\n- We can run our Zooniverse content on the Free Tier (see [Note of Caution 3: Database Limits](#note-of-caution-3-database-limits))\\n**Cons**\\n- Reliance on external platfrom for storing data introduces the usual third-party risks. (See [Note of Caution 1: Exit Strategy](#note-of-caution-1-exit-strategy)) Possible dangers include:\\n- external CMS has downtime, shutting down our content by extension.\\n- external CMS changes pricing structure, forcing us out of the free tier.\\n- external CMS shuts down, forcing us to find an alternative data store.\\n- We need to maintain an admin account. (Shouldn't be a worry as long as we have Passbolt.)\\n","tokens":266,"id":526,"Predictions":"Decide whether to continue to hard code content into the front end code or use an online third party CMS.n3 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n4 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n5 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n6 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n7 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n8 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n9 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n10 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n11 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n12 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n13 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n14 Decide whether to continue to hard code content into the front end code or use an online third party CMS.n15"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"Predictions":"The auth client is not a source-agnostic solution.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"Predictions":"Decision (A) The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n- The drawing tools should be updated to reflect the new data.n"}
{"File Name":"front-end-monorepo\/adr-33.md","Context":"## Context\\nWe plan to implement simple feedback for the survey task. Simple survey task feedback will compare target to actual choice(s), excluding any comparison of choice question answers, multiple selection of the same choice, or any other aspect of the survey task annotation.\\nCurrently in PFE, the only feedback related to the survey task is for the Gravity Spy project, and is referred to as \"Gravity Spy Gold Standard\". Gravity Spy Gold Standard is a PFE experimental feature (per workflow configuration object) that checks a subject's metadata for `'#Type' === 'Gold'` and provides feedback after classification completion with the [`gs-gold-standard-summary`](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/gs-gold-standard-summary.jsx). Survey task annotation values are compared to subject metadata `'#Label'` values. A message is then shown accordingly (i.e. success, failure, or special message per subject metadata `'#post_classification_feedback'`).\\n","Decision":"To implement a simple survey task feedback strategy within [the existing feedback framework](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/store\/feedback\/strategies). The simple survey task feedback will be titled \"Survey: Simple\". The simple survey task feedback will presume the annotation has one annotation value per survey task choice. The subject metadata will include:\\n- #feedback_N_id (required) - ID of the corresponding workflow task rule.\\n- #feedback_N_choiceIds (required) - comma separated target choice ID(s) (i.e. \"BLIP\" or \"AARDVARK,ELEPHANT,ZEBRA\"). A choice's ID is determined at survey task creation and can be viewed in in the Project Builder from the relevant workflow and survey task editor *Raw task data* section or from the workflow data export. The choice ID is the key for the choice object. For survey tasks created since 2017, the choice ID is the choice name per the provided \"Choices\" CSV with non-word characters removed and all uppercase (`[choice name per CSV].replace(\/\\W\/g, '').toUpperCase()`).\\n- #feedback_N_successMessage (optional) - message to show when the target is correctly annotated. Overrides the default success message set on the workflow task rule.\\n- #feedback_N_failureMessage (optional) - message to show when the target is incorrectly annotated. Overrides the default failure message set on the workflow task rule.\\nThe target choice ID(s) provided must match the choice IDs in the annotation values exactly for the annotation to be considered a success. Any missing target choice or any additional annotation choice will result in failure.\\nFeedback for annotation value (choice specific question) answers and multiple annotation values of the same choice will be addressed with additional survey task feedback strategies.\\n","tokens":203,"id":529,"Predictions":"Decision based on the information provided in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-06.md","Context":"## Context\\nWe need internationalization support for the new app-project, lib-classifier, and any other libraries that will have volunteer facing content. We should decide on an organizational structure that makes it easy for both translation volunteers and Zooniverse developers to maintain. Currently translation locale files exist in a single folder in projects like Panoptes-Front-End that support translations. The development convention we've adopted for the rewrite, however, would have the locale files in the same folder of each component.\\n@eatyourgreens notes:\\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https:\/\/github.com\/zooniverse\/zooniverse-readymade) and [zooniverse](https:\/\/github.com\/zooniverse\/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\\n","Decision":"We have a few proposed options:\\n- A `lib-locales` library that has all of the locale files for the strings. Could be a single file or a directory of folders and files. The library could be imported into the other libs or apps where needed.\\n- Larger locale files quickly get unwieldy to edit, and hard to find errors in. Moving them into one place reduces the portability of components. We could write a couple of scripts to bundle up locale files for our translators, and split them out again afterwards [back into their component folders in each library].\\n","tokens":188,"id":531,"Predictions":"I'd rather avoid that for whoever manages the translations for this repo.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"Predictions":"The classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. The subject queue was originally built to support random or sequential"}
{"File Name":"tamr-client\/0009-separate-types-and-functions.md","Context":"## Context\\nCode must be organized to be compatible with:\\n- Static type-checking via [mypy](https:\/\/github.com\/python\/mypy)\\n- Runtime execution during normal usage and running tests via [pytest](https:\/\/docs.pytest.org\/en\/stable\/)\\n- Static doc generation via [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\nAdditionally:\\n- Functions should be able to refer to any type\\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively \/ cyclically.\\n","Decision":"Put types (`@dataclass(frozen=True)`) into the `_types` module\\nand have all function modules depend on the `_types` module to define their inputs and outputs.\\n","tokens":139,"id":533,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tamr-client\/0010-confirm-performance-issues-before-optimizing.md","Context":"## Context\\nThere are multiple, equally-effective ways to implement many features.  In some cases, the most\\nstraightforward implementation might involve making more API calls than are strictly necessary\\n(e.g. `tc.dataset.create` makes an additional call to retrieve the created dataset from the server\\nto construct the returned `Dataset`).\\n","Decision":"The simplest and most understandably-written implementation of a feature should be prioritized over\\nperformance or reducing the number of API calls.  When real performance issues are identified,\\noptimization should be done on an as-needed basis.\\n","tokens":68,"id":534,"Predictions":"Decision (A) The most efficient way to implement many features is to make more API calls than are strictly necessaryn(e.g. tc.dataset.create makes an additional call to retrieve the created dataset from the servernto construct the returned Dataset).n.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n2.n"}
{"File Name":"tamr-client\/0003-reproducibility.md","Context":"## Context\\nReproducing results from a program is challenging when operating systems, language versions, and dependency versions can vary.\\nFor this codebase, we will focus on consistent Python versions and dependency versions.\\n","Decision":"Manage multiple Python versions via [pyenv](https:\/\/github.com\/pyenv\/pyenv).\\nManage dependencies via [poetry](https:\/\/python-poetry.org\/).\\nDefine tests via [nox](https:\/\/nox.thea.codes\/en\/stable\/).\\nRun tests in automation\/CI via [Github Actions](https:\/\/github.com\/features\/actions).\\n","tokens":43,"id":535,"Predictions":"We will focus on consistent Python versions and dependency versions.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"Predictions":"Decision Makers would like to use static type-checking to verify the type annotations.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tamr-client\/0008-standardized-imports.md","Context":"## Context\\nPython has many ways of importing:\\n```python\\n# option 1: import module\\n# option 1.a\\nimport foo.bar.bazaar as baz\\nbaz.do_the_thing()\\n# option 1.b\\nfrom foo.bar import bazaar as baz\\nbaz.do_the_thing()\\n# option 2: import value\\nfrom foo.bar.bazaar import do_the_thing\\ndo_the_thing()\\n```\\nNot to mention that each of these styles may be done with relative imports (replacing `foo.bar` with `.bar` if the `bar` package is a sibling).\\nConfusingly, Option 1.a and Option 1.b are _conceptually_ the same, but mechanically there are [subtle differences](https:\/\/stackoverflow.com\/questions\/24807434\/imports-in-init-py-and-import-as-statement\/24968941#24968941).\\n","Decision":"Imports within `tamr_client`:\\n- Must import statements for modules, classes, and exceptions\\n- Must `from foo import bar` instead of `import foo.bar as bar`\\n- Must not import functions directly. Instead import the containing module and use `module.function(...)`\\n- Must not use relative imports. Use absolute imports instead.\\n","tokens":189,"id":537,"Predictions":"Option 1a is the best choice because it is easier to import the same style of imports.n Option 2 is the best choice because it is easier to import the same style of imports.n Decision Makers:npython has many ways of importing:npythonn# option 1: import modulen# option 1: import modulen# option 1: import foo.bar.bazaar as baznbaz.do_the_thing()n# option 1: import bazaar as baznbaz.do_the_thing()n# option 1: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# option 2: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# option 3: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# option 4: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# decision makernnpython has many ways of importing:npythonn# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import foo.bar.bazaar as baznbaz.do_the_thing()n# option 2: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# option 3: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# decision makernpython has many ways of importing:npythonn# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import do_the_thingn# option 2: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# decision makernpython has many ways of importing:npythonn# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import do_the_thingn# option 2: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# decision makernpython has many ways of importing:npythonn# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import do_the_thingn# option 1: import valuenfrom foo.bar.bazaar import do_the_thingn# option 3: import valuenfrom foo.bar.bazaar import do_the_thingndo_the_thing()n# decision makernpython has many ways of importing:npythonn# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import modulen# option 1: import do_the_thingn# option 3: import valuenfrom foo.bar.bazaar import do_the_thingn# option 4: import valuenfrom foo.bar.bazaar import do_the_thingn# option 5: import valuenfrom foo.bar.bazaar import do_the_thingn# option 5: import valuenfrom foo.bar.bazaar import do_the_thingn# option 6: import valuenfrom foo.bar.bazaar import do_the_thingn# option 7: import valuenfrom foo.bar.bazaar import do_the_thingn# option 8: import valuenfrom foo.bar.bazaar import do_the_thingn# option 9: import valuenfrom foo.bar.bazaar import do_the_thingn# option 9: import valuenfrom foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuenfrom foo.bar.bazaar import do_the_thingn# option 10: import valuenfrom foo.bar.bazaar import do_the_thingn# option 10: import valuenfrom foo.bar.bazaar import do_the_thingn# option 10: import valuenfrom foo.bar.bazaar import do_the_thingn# option 10: import valuenfrom foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 10: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import do_the_thingn# option 11: import valuen from foo.bar.bazaar import"}
{"File Name":"tamr-client\/0005-composable-functions.md","Context":"## Context\\nWe need a reasonable tradeoff between ease-of-use and maintainability.\\nSpecifically, we need composable, combinable units that can be improved independently.\\n### Approach 1: Classes + Methods\\nOne approach is to embrace Object-Oriented Programming (OOP) with fluent interfaces (i.e. method chaining):\\n```python\\nproject\\n.create(...)\\n.update(...)\\n.delete(...)\\n```\\nCharacteristics:\\n- Ease-of-use is maximized, but this requires each method to `return self`.\\n- Also, this approach implies that if a function can be called with X different object types,\\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\\nHow to enforce these characteristics?\\nAny solution will be a tax on maintainability, as code that adheres to these characteristics will include many non-semantic lines simply going through the motions of `return self` and copying function usage into dedicated methods for each class.\\n### Approach 2: Types + Functions\\nAnother approach is to embrace a functional programming style: simple types and functions (no methods).\\nUsage is not as terse as for OOP:\\n```python\\np = tc.project.create(...)\\nu = tc.project.update(p, ...)\\nd = tc.project.delete(p, ...)\\n```\\nCharacteristics:\\n- Ease-of-use is not optimized, but still reasonable.\\n- With tab-completion, ease-of-use is comparable to OOP.\\n- Each type can be made immutable\\n- Each function can be made pure\\n- Functionality can be shared by calling the same function in user-land, not copying function calls in contributor-land.\\n","Decision":"Use `@dataclass(frozen=True)` to model types and plain Python modules and functions to capture business logic.\\n","tokens":356,"id":538,"Predictions":"Decision 1: We should adopt Object-Oriented Programming (OOP) or Types + Functions (OOP).n### Approach 2: Types + Functions.n### Approach 3: Types + Functions.n### Approach 4: Types + Functions.n### Decision 5: Types + Functions.n### Decision 6: Types + Functions.n### Decision 7: Types + Functions.n### Decision 8: Types + Functions.n### Decision 9: Types + Functions.n### Decision 10: Types + Functions.n### Decision 11: Types + Functions.n### Decision 12: Types + Functions.n### Decision 13: Types + Functions.n### Decision 14: Types + Functions.n### Decision 15: Types + Functions.n### Decision 16: Types + Functions.n### Decision 17: Types + Functions.n### Decision 18: Types + Functions.n### Decision 19: Types + Functions.n### Decision 20: Types + Functions.n### Decision 21: Types + Functions.n### Decision 22: Types + Functions.n### Decision 23: Types + Functions.n### Decision 24: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 24: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions.n### Decision 23: Types + Functions."}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"Predictions":"Decision (A) The existing userbase should be redesigned.nB) The existing userbase should be redesigned.nC) The existing userbase should be redesigned.nD) The existing userbase should be redesigned.nAnswer (A).nB.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.n"}
{"File Name":"tamr-client\/0004-documentation-and-docstrings.md","Context":"## Context\\nDocumentation can take four forms:\\n1. Explanation\\n2. Tutorial\\n3. How-to\\n4. Reference\\nWe need a way to author and host prosey documentation and generate reference docs based on source code.\\n","Decision":"Doc compilation will be done via [sphinx](https:\/\/www.sphinx-doc.org\/en\/master\/).\\nProsey documentation (1-3) via [recommonmark](https:\/\/github.com\/readthedocs\/recommonmark).\\nReference documentation (4) will be generated based on type annotations and docstrings via:\\n- Automatic docs based on docstrings via [sphinx-autodoc](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/autodoc.html), [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\n- Google-style docstrings via [napoleon](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/napoleon.html)\\n- Hosting on [ReadTheDocs](https:\/\/readthedocs.org\/) (RTD)\\n- Build docs in CI and fail on errors or warnings.\\n","tokens":48,"id":540,"Predictions":"Decision (A) To create a way to author and host prosey documentation and generate reference docs based on source code. (B) To create a way to author and host prosey documentation and generate reference docs based on source code. (C) To create a way to author and host prosey documentation and generate reference docs based on source code. (D) To create a way to author and host prosey documentation and generate reference docs based on source code. (A) To create a way to author and host prosey documentation and generate reference docs based on source code. (A) To create a way to author and host prosey documentation and generate reference docs based on source code. (B) To create a way to author and host prosey documentation and generate reference docs based on source code. (C) To create a way to author and host prosey documentation and generate reference docs based on source code. (D) To create a way to author and host prosey documentation and generate reference docs based on source code. (A) To create a way to author and host prosey documentation and generate reference docs based on source code. (A) To create a way to author and host prosey documentation and generate reference docs based on source code. (B) To create a way to author and host prosey documentation and generate reference docs based on source code."}
{"File Name":"tamr-client\/0002-linting-and-formatting.md","Context":"## Context\\nInconsistent code formatting slows down development and the review process.\\nCode should be linted for things like:\\n- unused imports and variables\\n- consistent import order\\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\\n","Decision":"For linting, use [flake8](https:\/\/flake8.pycqa.org\/en\/latest\/) and [flake8-import-order](https:\/\/github.com\/PyCQA\/flake8-import-order).\\nFor formatting, use [black](https:\/\/github.com\/psf\/black).\\n","tokens":55,"id":541,"Predictions":"Decision Makers should consider a code formatting solution.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tamr-client\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":542,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"Predictions":"The data in the Armadillo suite is not being managed.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"molgenis-r-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":546,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"molgenis-r-armadillo\/0003-use-s3-api.md","Context":"## Context\\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\\n* There are existing client libraries for the S3 API.\\n* It is nontrivial to proxy large file uploads through the armadillo server.\\n","Decision":"The Armadillo client will be written as a high-level library on top of an existing S3 API.\\n","tokens":56,"id":547,"Predictions":"Decision (A).n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n*The armadillo server is not a reliable proxy server.n"}
{"File Name":"molgenis-r-armadillo\/0004-build-client-in-R.md","Context":"## Context\\n### Data management\\nData managers (in LifeCycle) use R to upload their data into Opal as this stage. Data managers often create scripts to generate the (harmonised) data they need to expose for a certain collection of variables.\\n### Usage Armadillo\\nWe are required to upload .RData files into the Armadillo service to be used in DataSHIELD. So .RData files need to be created before it can be pushed to the service.\\n### Possible solutions\\nPossible solutions regarding buildframeworks are R, Python or Java. Every framework has pros and cons.\\n**R**\\nTo connect to the current way of working with the data in LifeCycle, R is the most integrated platform to use. You can use the Armadillo client to integrate in the existing scripts of the data managers. You do not need a lot of training to incorparate the Armadillo client in the workflow. RData is the file format Armadillo uses and R is the platform to build .RData in which makes it easier to build the RData file in. When there is a need to do basic checks in the future, you can implement them.\\n**Python**\\nPython can be used together in one script with the molgenis-commander. Which is usefull when you use the MOLGENIS data provider. It allowes you to automate the whole process, from extracting data from MOLGENIS to converting into the right format for the Armadillo service (if Python supports generating .RData). From there you can upload it into the Armadillo. Another advantage is that Python is a language we are more experienced to program in.\\nA disadvantage is that at this point prospect customers are not used to work with python. You will need to teach them how to use Python and then how to use the client.\\n","Decision":"The Armadillo client will be written based on R to integrate easily in the current prospect customers infrastructure. We will look at the possibility of writing other clients in the near future when we need to add other customers as well.\\n","tokens":386,"id":548,"Predictions":"The data management process in LifeCycle is done using R Data management process. The data management process in Opal is done using Python Data management process. The data management process in Armadillo is done using R Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process is done using Python Data management process. The data management process in Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data management process is done using Python Data"}
{"File Name":"gladiator_v2\/12-9-2017_player_interface.md","Context":"## Context:\\nNeed to create a way for the player to interact with their character and potential with their allies such that there is choice involved in combat on the player side instead of randomly decided actions.\\n","Decision":"Create a child class of character to give branching functionality to player-specific characters.\\n","tokens":41,"id":549,"Predictions":"Decision Makers should consider the following options: 1). The player should be able to interact with their character and potential with their allies; 2). The player should be able to interact with their character and potential with their allies; and 3). The player should be able to interact with their character and potential with their allies; and 4). The player should be able to interact with their character and potential with their allies; and 5). The player should be able to interact with their character and potential with their allies; and 6). The player should be able to interact with their character and potential with their allies; and 7). The player should be able to interact with their character and potential with their allies; and 8). The player should be able to interact with their character and potential with their allies; and 9). The player should be able to interact with their character and potential with their allies; and 10). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11). The player should be able to interact with their character and potential with their allies; and 11)."}
{"File Name":"gladiator_v2\/12-12-2017_castable_spells.md","Context":"## Context:\\nWant to give more variation to the combat system beyond physical attacks. This will allow for reasons to create separate builds and more options for player characters and enemies to use during a fight. This will give more of a *game* element to the combat as opposite to a system based on luck alone.\\n","Decision":"Create spells and integrate them into the character and combat systems.\\n","tokens":64,"id":550,"Predictions":"Decision 1). The combat system should be more flexible. 2). The combat system should be more flexible. 3) The combat system should be more flexible. 4) The combat system should be more flexible. 5) The combat system should be more flexible. 6) The combat system should be more flexible. 7) The combat system should be more flexible. 8) The combat system should be more flexible. 9) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible. 10) The combat system should be more flexible."}
{"File Name":"gladiator_v2\/12-6-2017_combat_class.md","Context":"## Context:\\nNeed to create a basic structure for combat involving characters that know how to fight and a middle man referee that ensures that everyone gets a turn and works as a go between to ensure that the characters interact properly. This will create a basic means for the game to function.\\n","Decision":"There should be a combat module specific to the characters and an overarching combat entity that handles the mediation.\\n","tokens":58,"id":551,"Predictions":"Decision 1). The player should be able to interact with the characters. 2). The player should be able to interact with the characters. 3) The player should be able to interact with the characters. 4) The player should be able to interact with the characters. 5) The player should be able to interact with the characters. 6) The player should be able to interact with the characters. 7) The player should be able to interact with the characters. 8) The player should be able to interact with the characters. 9) The player should be able to interact with the characters. 10) The player should be able to interact with the characters. 10) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters. ) The player should be able to interact with the characters."}
{"File Name":"gladiator_v2\/12-12-2017_character_creation.md","Context":"## Context:\\nWe need a way for player characters to be generated intentionally where certain statistics can be decided on by the player instead of having a random outcome. This will give the player some opportunity to have some control over their character's strengths and weaknesses and add to the playability of the game.\\n","Decision":"Create a player module with user interaction to affect the starting statistics of a character.\\n","tokens":61,"id":552,"Predictions":"Decision 1). The player should be able to choose their character's strengths and weaknesses. 2). The player should be able to choose their character's strengths and weaknesses. 3) The player should be able to choose their character's strengths and weaknesses. 4) The player should be able to choose their character's strengths and weaknesses. 5) The player should be able to choose their character's strengths and weaknesses. 6) The player should be able to choose their character's strengths and weaknesses. 7) The player should be able to choose their character's strengths and weaknesses. 8) The player should be able to choose their character's strengths and weaknesses. 9) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses. 10) The player should be able to choose their character's strengths and weaknesses."}
{"File Name":"gladiator_v2\/12-12-2017_useable_items.md","Context":"## Context:\\nWhile we have a class for items already, we need a way for them to be used during combat. This way we can add in more items that will change the flow of battle and eventually a money and shop system to add to combat variation.\\n","Decision":"Create a way to use items and integrate into the combat flow.\\n","tokens":54,"id":553,"Predictions":"We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.nbr>We need to add more items to combat variation.n"}
{"File Name":"gladiator_v2\/12-12-2017_random_character_generation.md","Context":"## Context:\\nIt is desirable that we have a way for characters to be randomly generated with different statistics and abilities in order to keep combat fresh and new with an element of surprise. Creating a way to generate random character systematically will keep us from having to create specific new characters.\\n","Decision":"Create a random character factory that will pump out characters.\\n","tokens":57,"id":554,"Predictions":"Decision (A).nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>We should create random character systematically.nbr>"}
{"File Name":"play-frontend-hmrc\/0008-maintain-parity-with-hmrc-frontend.md","Context":"## Context and Problem Statement\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n","Decision":"* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\nGDS.\\n### Positive Consequences\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\nviewmodel structure from scratch every time\\n* Adding new components can follow a clear and straightforward path\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\n### Negative Consequences\\n* Feedback suggests that some developers do not find the API to be intuitive\\n* Separate decisions need to be made on handling multilingual support\\n* Enrichment of library needs to be done via separate Twirl helpers\\n(see https:\/\/github.com\/hmrc\/play-frontend-hmrc\/blob\/main\/docs\/maintainers\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\n","tokens":156,"id":555,"Predictions":"Decision (A) The frontend components should be easy to use by Scala developers on the platform.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a robust testing strategy for library developers to have faith in when upgradingn"}
{"File Name":"play-frontend-hmrc\/0013-removing-deprecated-helpers.md","Context":"## Context and Problem Statement\\nSince launching `play-frontend-hmrc` we have included deprecated static helpers which were required for Play 2.5 projects.\\n## Decision Drivers\\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n","Decision":"* There is complexity involved in maintaining multiple interfaces for the same behaviour\\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\\n* the platform opinion is to use DI\\n* soft advice from Play has been to use DI since Play 2.6 (https:\/\/www.playframework.com\/documentation\/2.8.x\/ScalaDependencyInjection)\\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\\n* some teams may regard the deprecation notices as small print\\n* we should not re-instate static helpers for Play 2.8\\n* we should remove them in version 1.0.0\\n* we should publish a blog article sooner rather than later that we will be removing them in 1.0.0\\n### Positive Consequences\\n* Our codebase becomes easier to maintain\\n* We have to use DI in our tests which makes our tests more accurate to how these components will be used\\n### Negative Consequences\\n* Anyone who is using the static helpers will have to update - that's why we're including this in version 1.0.0\\n","tokens":156,"id":558,"Predictions":"Decision Makers: The deprecation notices are small print.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n* The deprecation notices are for the deprecation notices.n*"}
{"File Name":"play-frontend-hmrc\/0006-allow-service-name-to-be-specified-using-messages-or-a-parameter.md","Context":"## Context and Problem Statement\\nNearly all services running on MDTP need to include a link to their landing page in the\\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\\nor be able to configure it based on the path. For example, in the case of shared frontend\\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\\nWhen integrating with play-frontend-hmrc we want to\\nminimise boilerplate for service teams while providing flexibility to cope with edge\\ncases.\\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\\nparameter, specify the service name in the\\n`service.name` message in `conf\/messages`, providing any Welsh translation in `conf\/messages.cy`.\\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\\nmechanism is awkward for services not requiring a service name \u2013 they have to override with a blank message \u2013\\nand services needing more than one are unable to use this functionality. This pattern\\nassumes a 1:1 correspondence between frontend microservices and public-facing\\ntax services.\\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\\nservices, insist on explicitly passing the service name or support some combination of the above?\\n## Decision Drivers\\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\n","Decision":"* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\\nbetween frontend microservices and public-facing services.\\n","tokens":390,"id":559,"Predictions":"Decision Makersn* Minimising of boilerplate and lines of code needed in frontend microservicesn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing servicesn* Decision Makersn"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"Predictions":"Decision nThe following are the drivers for the implementation of the solution:nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"play-frontend-hmrc\/0014-keep-current-package-structure-and-naming-conventions.md","Context":"## Context and Problem Statement\\nIn the context of the first major version release of play-frontend-hmrc, are there any fundamental\\nchanges we would like to make to the package structure and naming conventions of the library?\\n## Decision Drivers\\n* Major changes are painful but will only get more painful the longer they are left.\\n* No teams have, to our knowledge, complained or provided any negative feedback around\\nthe naming conventions used in the play-frontend libraries.\\n* The benefits of any name changes need to be weighed against the cost and disruption to\\nservice teams.\\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\\nbreaking change. Deprecation notices left in too long create compilation noise and alert\\nfatigue for teams leading to a risk that they will be ignored.\\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\\non their next library upgrade.\\n* Repetition of suffixes\/prefixes in the same source file makes it more verbose and harder to read.\\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\\nentropy that over time increases the cost of maintenance.\\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\\ntax services.\\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\\nmaking changes available to teams.\\n","Decision":"* Major changes are painful but will only get more painful the longer they are left.\\n* No teams have, to our knowledge, complained or provided any negative feedback around\\nthe naming conventions used in the play-frontend libraries.\\n* The benefits of any name changes need to be weighed against the cost and disruption to\\nservice teams.\\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\\nbreaking change. Deprecation notices left in too long create compilation noise and alert\\nfatigue for teams leading to a risk that they will be ignored.\\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\\non their next library upgrade.\\n* Repetition of suffixes\/prefixes in the same source file makes it more verbose and harder to read.\\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\\nentropy that over time increases the cost of maintenance.\\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\\ntax services.\\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\\nmaking changes available to teams.\\nChosen option: option 1, do nothing, because the cost of change is very high while the\\ntheoretical benefits are extremely difficult to quantify.\\n","tokens":299,"id":561,"Predictions":"Decision Makers: (A). Change the naming conventions of the library. (B). Change the naming conventions of the library. (C). Change the naming conventions of the library. (D). Change the naming conventions of the library. (E). Change the naming conventions of the library. (F). Change the naming conventions of the library. (G). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library. (H). Change the naming conventions of the library."}
{"File Name":"play-frontend-hmrc\/0004-add-contact-and-welsh-information-links-into-footer.md","Context":"## Context and Problem Statement\\nIn the context of classic services' requirement for contact HMRC and Welsh information links in their footer, facing the\\nfact that these links are missing from hmrcStandardFooter, should we add them?\\nThe additional links needed are:\\n* \"Contact\", linking to https:\/\/www.gov.uk\/government\/organisations\/hm-revenue-customs\/contact\\n* \"Rhestr o Wasanaethau Cymraeg\", linking to https:\/\/www.gov.uk\/cymraeg\\n## Decision Drivers\\n* The need for consistency across HMRC services.\\n* Our belief that including them is likely to improve the user experience for tax users.\\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\\n* We have a time sensitive opportunity of acting now while teams are in the process of\\nuplifting their frontend libraries.\\n* The HMRC design community have been consulted on multiple public Slack channels and two\\nsuccessive design system working group meetings, with no objections noted.\\n* Classic services support multiple live services. Not including these links as standard would mean their having to\\nduplicate these links, and associated English and Welsh content, across tens of repositories.\\n","Decision":"* The need for consistency across HMRC services.\\n* Our belief that including them is likely to improve the user experience for tax users.\\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\\n* We have a time sensitive opportunity of acting now while teams are in the process of\\nuplifting their frontend libraries.\\n* The HMRC design community have been consulted on multiple public Slack channels and two\\nsuccessive design system working group meetings, with no objections noted.\\n* Classic services support multiple live services. Not including these links as standard would mean their having to\\nduplicate these links, and associated English and Welsh content, across tens of repositories.\\nChosen option: \"Add the links to hmrcStandardFooter\", because this\\nwill benefit tax users, and we have a unique window of opportunity to act now.\\n### Positive Consequences\\n* Tax users have better information provided to them\\n* Teams do not need to duplicate content and URLs across hundreds of repositories\\n* We can more easily maintain the content and links in a central repository\\n### Negative Consequences\\n* Teams currently using a Welsh link as their language toggle will likely need to switch to using one of the standard components\\nfor language switching e.g. hmrcLanguageSelect.\\n* Teams already including a contact link manually will need to remove it when upgrading.\\n","tokens":254,"id":562,"Predictions":"Decision Taking the following steps: 1. Include the links in the footer of the standard services' footer. 2. Include the links in the footer of the standard services' footer. 3. Include the links in the footer of the standard services' footer. 4. Include the links in the footer of the standard services' footer. 5. Include the links in the footer of the standard services' footer. 6. Include the links in the footer of the standard services' footer. 7. Include the links in the footer of the standard services' footer. 8. Include the links in the footer of the standard services' footer. 9. Include the links in the footer of the standard services' footer. 10. Include the links in the footer of the standard services' footer. 11. Include the links in the footer of the standard services' footer. 12. Include the links in the footer of the standard services' footer. 13. Include the links in the footer of the standard services' footer. 14. Include the links in the footer of the standard services' footer. 15. Include the links in the footer of the standard services' footer. 16. Include the links in the footer of the standard services' footer. 17. Include the links in the footer of the standard services' footer. 18. Include the links in the footer of the standard services' footer. 19. Include the links in the footer of the standard services' footer. 20. Include the links in the footer of the standard services' footer. 20. Include the links in the footer of the standard services' footer. 20. Include the links in the footer of the standard services' footer. 21. Include the links in the footer of the standard services' footer. 22. Include the links in the footer of the standard services' footer. 23. Include the links in the footer of the standard services' footer."}
{"File Name":"play-frontend-hmrc\/0010-resolve-naming-collisions-between-libraries.md","Context":"## Context and Problem Statement\\nGiven a service is using the twirl template imports recommended by the play-frontend libraries. When a developer tries\\nto use an import with a name which is present in both libraries without explicitly importing the one they want. Then\\nthey will get a compilation error:\\n<img alt=\"Example of ambiguous import compilation exception\" src=\"0010-ambiguous-import-exception.png\" width=\"450\">\\nCurrently, we recommend the use of the wildcard twirl imports, but this possible consequence is undocumented behaviour.\\nThe compilation error can be resolved by explicitly importing the class they want.\\n## Decision Drivers\\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\n","Decision":"* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\nWe've decided to immediately implement Option 1.2 and defer further changes to be discussed as part of PLATUI-1294.\\n### Positive Consequences\\n* Less confusing for new developers where imports are coming from when we avoid recommending using TwirlKeys config.\\n* Developers can see as part of our recommended usage instructions the possibility for an ambiguous import exception and\\nwhat they can do to resolve them without.\\n### Negative Consequences\\n* Without an API change the naming of components and view models is not as intuitive as we think it could be which may\\nconfuse new developers.\\n","tokens":158,"id":563,"Predictions":"We recommend the use of the wildcard twirl imports, but this possible consequence is undocumented behaviour.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved by explicitly importing the class they want.nThe compilation error can be resolved"}
{"File Name":"play-frontend-hmrc\/0002-maintain-parity-with-govuk-frontend.md","Context":"## Context and Problem Statement\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n","Decision":"* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\nGDS.\\n### Positive Consequences\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\nviewmodel structure from scratch every time\\n* Adding new components can follow a clear and straightforward path\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\n### Negative Consequences\\n* Feedback suggests that some developers do not find the API to be intuitive\\n* Separate decisions need to be made on handling multilingual support\\n* Enrichment of library needs to be done via separate Twirl helpers,\\nsee [related ADR](..\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\n","tokens":156,"id":564,"Predictions":"Decision (A) The frontend components should be easy to use by Scala developers on the platform.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a clear and quick upgrade path when new versions of govuk-frontend and nhmrc-frontend are released.n* The frontend components should be created a library that has a robust testing strategy for library developers to have faith in when upgradingn"}
{"File Name":"play-frontend-hmrc\/0001-add-implicit-radios-helper.md","Context":"## Context and Problem Statement\\n- `play-fronted-govuk` library is not Play Framework aware\\n- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\\n## Decision Drivers\\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\n","Decision":"- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\nChosen option: Option 5, because it adds useful functionality (class enrichment of `Radios` via implicit\\n`RichRadios`, without adding new Twirl templates to maintain, and without adding new methods directly\\nto `Radios`, which would cause a divergence between `play-frontend-govuk` and `govuk-template` (undesirable).\\n### Positive Consequences\\n* Adds useful optional class enrichment to `Radios`\\n* Reduces code repetition between `Field` and `Radios` (DRY)\\n* Pattern is extensible, i.e. similar implicit helpers can be added to other form inputs\\n### Negative Consequences\\n* Doesn't add as much value as we would like\\n* Need to document carefully what behaviour occurs when information provided via both `Radios` and\\n`Field`, i.e. which takes precedence (currently intended to be `Radios` values take precendence)\\n* Need to document to teams that this helper is available (service developers won't use it if they don't know it's there)\\n","tokens":194,"id":565,"Predictions":"Decision Makersn- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service coden## Decision Driversn- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams migrate off older frontend libraries to play-frontend-govuk and play-frontend-hmrcn- play-frontend-govuk is intended to be a direct port of govuk-frontend, and so helpers should live in annappropriate helper class alongside the view modelsn- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend APIn- We want to avoid replicatign the govuk-frontend parameter lists in the helpers, to keep the overhead of upgrading the library lown## Decision Makersn-  we want to avoid replicatign the govuk-frontend parameter lists in the helpersn- We want to avoid replicatign the govuk-frontend parameter lists in the helpersn- We want to avoid replicatign the govuk-frontend parameter lists in the helpersn- We want to avoid replicatign the govuk-frontend parameter lists in the helpersn"}
{"File Name":"play-frontend-hmrc\/0003-use-the-suffix-helpers-for-helper-components.md","Context":"## Context and Problem Statement\\nFacing the need to create wrappers to make using the Nunjucks components ported from hmrc-frontend more\\nstraightforward and idiomatic on the Scala\/Play MDTP platform, how should we name these components?\\nThe components ported from hmrc-frontend include\\n* implementations of govuk-frontend components that support the Welsh language e.g. hmrcHeader, hmrcFooter\\n* components that meet specific HMRC needs e.g. hmrcBanner, hmrcInternalHeader\\n* components that have not been standardised by GDS but are needed by HMRC e.g. hmrcNotificationBadge, hmrcTimeoutDialog\\nBeing entirely presentational and ported from Nunjucks means they cannot leverage features built into Scala\/Play nor\\nmake use of any MDTP platform knowledge that would simplify their use on MDTP. For example,\\n* they do not and should not know the url structure for common MDTP components e.g. tracking-consent-frontend,\\ncontact-frontend or the accessibility-statement-frontend, services that need to wired in on every public-facing service via\\nstandard headers and footers.\\n* they cannot make use of Play's i18n features and automatic language wiring\\n* they cannot make use of any knowledge encoded in the requests users make e.g. the request URL, referrer URL, cookies\\nor headers.\\nFor the above reasons, we are creating wrappers that implement standardised Play\/platform wiring, to avoid teams having to\\nduplicate this wiring across 100s of repositories. Once implemented we will encourage teams to use these helpers rather than\\nthe underlying Nunjucks ports.\\n## Decision Drivers\\n* The fact that the ideal component names have already been taken by the Nunjucks components.\\n* The preference for not relying on the package name to differentiate the components\\n* The preference for not repeating the word Helper in the package and component name\\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\\ncomponent.\\n* The preference for consistency in naming across the components\\n","Decision":"* The fact that the ideal component names have already been taken by the Nunjucks components.\\n* The preference for not relying on the package name to differentiate the components\\n* The preference for not repeating the word Helper in the package and component name\\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\\ncomponent.\\n* The preference for consistency in naming across the components\\nChosen option: \"Suffix with 'Helper'\", because this is the only option the team are happy with bearing in mind the decision\\ndrivers listed above.\\n### Positive Consequences\\n* We have a consistent naming scheme\\n* Friction around needing to come up with new names for helpers reduced, increasing speed of development.\\n### Negative Consequences\\n* There is a risk that service teams will not know the helpers exist\\n* Repeating the word Helper in the component and having it in the package name is ugly\\n","tokens":434,"id":566,"Predictions":"Decision Makersn* The fact that the ideal component names have already been taken by the Nunjucks components.n* The preference for not repeating the word Helper in the package and component namen* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucksncomponent.n* The preference for consistency in naming across the componentsnn Decision Makersn* The fact that the ideal component names have already been taken by the Nunjucks components.n* The preference for not repeating the word Helper in the package and component namen* The preference for not repeating the word Helper in the package and component namen* The preference for consistency in naming across the componentsnn Decision Makersn* The preference for consistency in naming across the componentsnn Decision Makersn* The preference for consistency in naming across the componentsnn Decision Makersn* The preference for consistency in naming across the componentsnn Decision Makersn* The preference for consistency in naming across the componentsnn Decision Makersn* The preference for consistency in naming across the componentsnn Decision Makersn* The preference for consistency in naming across the componentsnn Decision Makersn* The preference for consistency in naming across the componentsn"}
{"File Name":"play-frontend-hmrc\/0015-introduction-of-implicit-conversions.md","Context":"## Context and Problem Statement\\nFrequently occurring usage patterns of play-frontend-hmrc components require repetitious boilerplate\\ninvolving excessively nested case class instantiations. This has driven many teams to create wrappers to\\nsimplify the construction of components, which are then copied and pasted from service to service.\\nFor example, in order to add a legend to a GovukRadio the following boilerplate is required:\\n```scala\\nSome(Fieldset(\\nlegend = Some(Legend(\\ncontent = Text(\"message.key.for.legend\"),\\nclasses = \"govuk-fieldset__legend--l\",\\nisPageHeading = true\\n))\\n))\\n```\\nIn the above example, the only thing that changes between instantiations is the `message.key.for.legend`, everything\\nelse remains the same.\\nShould we solve this problem through the introduction of implicit conversions that will automatically wrap simple\\nobjects such as Strings with the additional boilerplate necessary for commonly occurring cases?\\n## Decision Drivers\\n* The need for API consistency in play-frontend-hmrc.\\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n* See https:\/\/www.rallyhealth.com\/coding\/implicit-implications-part-2-implicit-conversions,\\nhttps:\/\/contributors.scala-lang.org\/t\/can-we-wean-scala-off-implicit-conversions\/4388\\n* The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\n","Decision":"* The need for API consistency in play-frontend-hmrc.\\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\\nentirely in a future Scala version:\\n* See https:\/\/www.rallyhealth.com\/coding\/implicit-implications-part-2-implicit-conversions,\\nhttps:\/\/contributors.scala-lang.org\/t\/can-we-wean-scala-off-implicit-conversions\/4388\\n* The risk of unintended side effects if these conversions are added unilaterally\\nwithout service developers needing to specifically opt in to them.\\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\\nHint, Key, Label, Legend.\\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\\nshow implicit hints.\\nChosen option: Option 1, because it solves the problem, is consistent with how we have been doing things, maintains\\nthe internal consistency of play-frontend-hmrc and has less risk of breaking services.\\n","tokens":380,"id":567,"Predictions":"Decision Makersn* The need for API consistency in play-frontend-hmrc.n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removednentirely in a future Scala version:n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removednentirely in a future Scala version:n* The risk of unintended side effects if these conversions are added unilaterallynwithout service developers needing to specifically opt in to them.n* Decision Makersn* The need for API consistency in play-frontend-hmrc.n* The risk of unintended side effects if these conversions are added unilaterallynwithout service developers needing to specifically opt in to them.n* Decision Makersn* The need for API consistency in play-frontend-hmrc.n* The risk of unintended side effects if these conversions are added unilaterallynwithout service developers needing to specifically opt in to them.n* Decision Makersn* The need for API consistency in play-frontend-hmrc.n* The risk of unintended side effects if these conversions are added unilaterallynwithout service developers needing to specifically opt in to them.n* Decision Makersn* The need for API consistency in play-frontend-hmrc.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* Decision Makersn* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n* The risk of unintended side effects if these conversions are added unilaterallynwithout turning on advanced IDE features thatnshow implicit hints.n"}
{"File Name":"play-frontend-hmrc\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md","Context":"## Context and Problem Statement\\nThe need to provide an HMRC footer translated into Welsh (PLATUI-752)\\n## Decision Drivers\\n* the lack of support for Welsh in the GDS govukFooter\\ncomponent\\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\\nwith some non-presentational helpers to aid use in Scala\/Play - conceptual integrity\\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\\nmarkup - separation of concerns\\n","Decision":"* the lack of support for Welsh in the GDS govukFooter\\ncomponent\\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\\nwith some non-presentational helpers to aid use in Scala\/Play - conceptual integrity\\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\\nmarkup - separation of concerns\\nChosen option \"maintain parity\" because we want hmrc-frontend to be the source of truth for presentation and\\nmaintain the separation of concerns between hmrc-frontend and\\nplay-frontend-hmrc\\n### Positive Consequences\\n* Maintain separation of concerns between hmrc-frontend and play-frontend-hmrc\\n* Maintain conceptual integrity for play-frontend-hmrc\\n* Be able to test markup using existing test strategy\\n### Negative Consequences\\n* We will need to add and maintain a new hmrcFooter component in\\nhmrc-frontend and play-frontend-hmrc providing a mirror of govukFooter with localised content,\\n* The new hmrcFooter component in hmrc-frontend will not be able to make use of any I18n features,\\n* We will need to create a new `helpers` package within play-frontend-hmrc to clearly demarcate them\\nfrom presentational components,\\n* We will need to redesign the existing hmrcFooter helper to wrap the new hmrcFooter component\\nand move to the helpers package,\\n* The new hmrcFooter component will need to be deprecated when GDS provide a localised version\\nof govukFooter,\\n* We will need to liaise and get approval from the HMRC design system team for adding\\nhmrcFooter to the hmrc\/design-system and hmrc\/hmrc-frontend\\n* We will be adding features that are unlikely to be useful or used by designers because\\nat the prototyping phase content is not stable enough for translation into Welsh\\n","tokens":109,"id":569,"Predictions":"Decision n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n The need to provide an HMRC footer translated into Welsh (PLATUI-752)n"}
{"File Name":"play-frontend-hmrc\/0009-self-publish-webjar.md","Context":"## Context and Problem Statement\\nplay-frontend-hmrc relies on a webjar for [hmrc\/hmrc-frontend](https:\/\/www.github.com\/hmrc\/hmrc-frontend)\\npublished to www.webjars.org. This has a number of drawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the\\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\\nBintray, should we:\\n* automate the creation of the webjars within our own deployment pipelines with no dependency\\non webjars.org\\n* publish the resulting webjars to this repository automatically?\\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\\nwebjar for [alphagov\/govuk-frontend](https:\/\/www.github.com\/alphagov\/govuk-frontend), which is\\ncurrently a dependency for [hmrc\/play-frontend-govuk](https:\/\/www.github.com\/hmrc\/play-frontend-govuk).\\n## Decision Drivers\\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\n","Decision":"* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\\nadditional dependencies.\\n### Existing architecture\\n<img alt=\"Existing architecture\" src=\"0009-webjars-existing.png\" width=\"450\">\\n### To be architecture\\n<img alt=\"To be architecture\" src=\"0009-webjars-tobe.png\" width=\"450\">\\n### Positive Consequences\\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\\nproduction.\\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\\n* Maintaining play-frontend-hmrc is a less frustrating process.\\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\\nproduced indicate webjars.org as the developer in the POM files.\\n* There are fewer external dependencies and moving parts.\\n### Negative Consequences\\n* We have an additional moving part to maintain ourselves.\\n","tokens":423,"id":570,"Predictions":"Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n## Decision Makers: [HMRC] should publish the webjars to the webjars repository automatically.n"}
{"File Name":"play-frontend-hmrc\/0003-patch-welsh-translation-for-skip-to-content-link.md","Context":"## Context and Problem Statement\\nWe previously decided to keep inputs and outputs of play-frontend-govuk components a 1-to-1 match with govuk-frontend\\nnunjucks example implementations.\\nGenerally components from govuk-frontend accept their copy as input, however the skip link in govukTemplate is\\nhard-coded which is causing accessibility failures for services.\\nThis is an accessibility failure because a page translated into welsh may cause assistive technology to mispronounce the\\ncontent of the english skip link because it has no explicit language attribute.\\n## Decision Drivers\\n* Because it's an accessibility failure we want to fix this as a priority quickly\\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\\n* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategy\\n","Decision":"* Because it's an accessibility failure we want to fix this as a priority quickly\\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\\n* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategy\\nWe've chosen option 1 because unlike other components, we don't have any automatically generated fixtures or integration\\ntests for govukTemplate. We maintain test fixtures for the templates manually in the additional fixtures folder. This\\nmakes it possible for us to patch this change into govukTemplate with minimal maintenance cost without reducing our test\\ncoverage \/ diverging far from our current testing strategy.\\n","tokens":179,"id":571,"Predictions":"We will fix the skip link in govukTemplate.n# Decision Driversn* Because it's an accessibility failure we want to fix this as a priority quicklyn* And we would like for this fix to be available as widely as possible without requiring template changes from servicesn* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategyn# Decision Makersn* Because it's an accessibility failure we want to fix this as a priority quicklyn* And we would like for this fix to be available as widely as possible without requiring template changes from servicesn* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategyn# Decision Makersn* Because it's an accessibility failure we want to fix this as a priority quicklyn* And we would like for this fix to be available as widely as possible without requiring template changes from servicesn* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategyn# Decision Makersn* Because it's an accessibility failure we want to fix this as a priority quicklyn* And we would like for this fix to be available as widely as possible without requiring template changes from servicesn* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategyn# Decision Makersn* Because it's an accessibility failure we want to fix this as a priority quicklyn* And we would like for this fix to be available as widely as possible without requiring template changes from servicesn* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategyn# Decision Makersn* Because it's an accessibility failure we want to fix this as a priority quicklyn* And we would like for this fix to be available as widely as possible without requiring template changes from servicesn* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategyn# Decision Makersn* Because it's an accessibility failure we want to fix this as a priority quicklyn* And we would like for this fix to be available as widely as possible without requiring template changes from servicesn* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategyn"}
{"File Name":"kuona\/0002-evaluate-buddy-auth-for-admin-access-controls.md","Context":"## Context\\nKuona instances need to be secured - particularly for deployments that have public access.\\n","Decision":"https:\/\/funcool.github.io\/buddy-auth\/latest\/#example-session Buddy seems to fit the bill and is compatible with Compojure and Ring. Provides a number of options and possible persistance mechanisme.\\n","tokens":21,"id":572,"Predictions":"Decision n Kuona instances need to be secured - particularly for deployments that have public accessnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"verify-onboarding-prototypes\/0003-use-files-to-store-private-keys copy.md","Context":"## Context\\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\\ndecrypt Response Assertions.\\nThey will need to provide these to the verify-service-provider in some, reasonably\\nsecure way. Different users may have different opinions on how best to do this.\\n","Decision":"Initially we'll use files for this.\\nWe chose not to use environment variables because they're visible to other processes.\\nWe chose not to use a more complicated solution because it would be more complicated.\\n","tokens":62,"id":574,"Predictions":"Decision (A) The RPs will need to provide some private keys to sign AuthnRequests andndecrypt Response Assertions.n They will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide some private keys to sign AuthnRequests andndecrypt Response Assertions.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-service-provider in some, reasonablynsecure way.n ## Decision The RPs will need to provide these to the verify-"}
{"File Name":"verify-onboarding-prototypes\/0007-we-will-document-a-strawman-api.md","Context":"## Context\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n","Decision":"We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\\n","tokens":32,"id":575,"Predictions":"Decision (A) The client should communicate using API.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"verify-onboarding-prototypes\/0002-build-a-prototype.md","Context":"## Context\\nWe need to make sure that whatever we build meets the users' needs. To make sure of this we need to\\nconduct some user research, which should involve putting software in front of users and observing them using it.\\n","Decision":"We will build a simple \"prototype\" which we will use to test our assumptions about whether our proposed\\nsolution is the best way of meeting our users needs.\\nThe prototype will be architecturally similar to the product we envisage building, but won't be able to\\ndo the SAML interactions with Verify at this stage.\\nADRs for the first prototype will live in the [prototype-0](prototype-0) directory.\\n","tokens":48,"id":576,"Predictions":"We should conduct user research.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"Predictions":"We want to provide as flexible as possible an integration for them.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"Predictions":"Decision (A) The client code should be a server side code.nB) The server side code should be a client side code.nC) The server side code should be a client side code.nD) The server side code should be a server side code.nAnswer (A).n1212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121"}
{"File Name":"verify-onboarding-prototypes\/0004-users-will-be-able-to-provide-relay-state.md","Context":"## Context\\nIn SAML RPs can provide some extra data along with the request. This is\\ncalled RelayState. Some existing RPs use this, but we're not sure what\\nthey use it for.\\nWe're not aware of any need for the service-provider to use relay state itself.\\n","Decision":"Users will be able to specify whatever relay state they want to and it will be\\nprovided in the response.\\n","tokens":64,"id":579,"Predictions":"We're not sure what relay state is used for.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.nWe're not aware of any need for the service-provider to use relay state itself.n"}
{"File Name":"verify-onboarding-prototypes\/0005-sp-will-generate-request-id.md","Context":"## Context\\nAuthnRequests contain an ID attribute the value of which will be sent back in\\nthe Response as an \"InResponseTo\" attribute.\\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\\n","Decision":"The service provider will generate a random GUID to use as the AuthnRequest ID.\\n","tokens":62,"id":580,"Predictions":"Decision 12nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"Predictions":"We will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.nWe will use the service provider as a test platform.n"}
{"File Name":"verify-onboarding-prototypes\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":582,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"Predictions":"The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to manage and update.n## Decision The solution must be easy to configure."}
{"File Name":"old-web-frontend\/0001-use-react-with-typescript.md","Context":"## Context and Problem Statement\\nWe have to choose the framework and language to build this web application.\\nWhich framework and language should we use?\\n","Decision":"Chosen option: \"React with Typescript\", because\\n* The single developer has experience with React and Ember, but no experience with Vue\\n* This developer favors React over Ember, because he thinks the first contributes to a better separation of concerns\\n* This developer has experiece with TypeScript and JavaScript, but no experience with Elm\\n* This developer favors TypeScript mostly because it's typed.\\n","tokens":30,"id":584,"Predictions":"We should use the framework and language we chose.nWe can't decide on a framework or language.nWe can choose a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a framework or language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a language.nWe can't decide on a framework or language."}
{"File Name":"abracadabra\/0007-usage-of-readthenwrite-vs-write.md","Context":"## Context\\nWe've implemented 2 different methods in the Editor interface:\\n1. `write()`\\n1. `readThenWrite()`\\nMost refactorings use `write()`. They create new code from the AST, then write it in the editor. Still, some refactorings use `readThenWrite()`: they first read some code `string` from the editor, and use it to write new code.\\nThe main problem of `readThenWrite()` is that we end up manipulating strings instead of the AST. This usually means the implementation is more complex. Instead of manipulating the AST, we need to retrieve the correct selection we need to read, then we have to mix this read code with the transformation to produce the final output. Final code is less straightforward and more cases need to be tackled by us, instead of relying on the AST parser to do the job.\\nHowever, there is one key advantage of doing so: it absolutely preserve the code as it was written in the editor. Even though [we use recast to preserve code style as much as possible][recast-usage], it's not perfect. If the refactoring consist in re-using exactly code that was written, `readThenWrite()` is the only way we know to preserve exactly the original style.\\n","Decision":"We'll use `readThenWrite()` if we need to preserve a code exactly as it was written. E.g. if the refactoring consists in moving existing code, without transforming it (Extract Variable, Inline Variable).\\nFor other refactorings, we'll use `write()`. E.g. if the refactoring transforms the code, it's OK to change it while preserving the original style as much as possible with recast.\\n","tokens":263,"id":585,"Predictions":"Decision (A). n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"Predictions":"Decision (A) The extension should be refactored. (B) The extension should be refactored to be a pure component. (C) The extension should be refactored to be a pure component. (D) The extension should be refactored to be a pure component. (A) The extension should be refactored to be a pure component. (A) The extension should be refactored to be a pure component. (A) The extension should be refactored to be a pure component. (B) The extension should be refactored to be a pure component. (C) The extension should be refactored to be a pure component. (D) The extension should be refactored to be a pure component. (A) The extension should be refactored to be a pure component. (A) The extension should be refactored to be a pure component. (A) The extension should be refactored to be a pure component. (B) The extension should be refactored to be a pure component. (C) The extension should be refactored to be a pure component. (D) The extension should be refactored to be a pure component. (A) The extension should be refactored to be a pure component."}
{"File Name":"abracadabra\/0004-use-recast-for-ast-manipulation.md","Context":"## Context\\nWe used Babel to parse code into AST, transform this AST and re-generate code.\\nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!\\nWhat is not great is the code generation part. Babel formats the generated code. That means the code contained inside a transformed node gets reformated. This is not cool.\\n","Decision":"As we want to preserve the original style of the transformed code as much as possible, we went for [Recast][recast].\\nAs the library says:\\n> The magic of Recast is that it reprints only those parts of the syntax tree that you modify.\\nThus, we now use Recast to parse and generate the code. AST transformation is still performed by Babel. Recast uses Babel to parse the code into AST, so we keep Babel benefits such as parsing JSX, TS\u00a0and TSX out of the box.\\n","tokens":104,"id":587,"Predictions":"Decision (A) The code generated by Babel is not good.nBabel is a popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box."}
{"File Name":"abracadabra\/0002-no-integration-test.md","Context":"## Context\\nVS Code has documented [how to test an extension][testing-extension] in integration with VS Code API.\\n> These tests will run inside a special instance of VS Code named the `Extension Development Host`, and have full access to the VS Code API.\\nBut VS Code test runner is adapted to be used with [mocha][mocha] or [Jasmine][jasmine].\\nFor unit tests, we want to use [Jest][jest]. But Jest and mocha have conflicting types, preventing TS to build. A solution would have been to create a custom test runner for VS Code API, that would run integration tests with Jest. But we didn't managed to create one that works.\\nThat's partly because Jest doesn't have an official way to run tests programatically. We weren't able to make VS Code test runner work with Jest `runCLI()` (async) method.\\nAlso, integration tests are not the most important part of the project for the moment.\\n","Decision":"We won't do integration tests. We'll rely on unit tests (e.g. state-based tests, collaboration tests and contract tests).\\n","tokens":205,"id":588,"Predictions":"Decision (A) The integration tests are not the most important part of the project for the moment.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"abracadabra\/0003-use-hosted-image-links-in-readme.md","Context":"## Context\\nRelative paths for images won't work when the README\u00a0is displayed somewhere where the images are not hosted. From experience developing npm libraries, images won't appear on the package manager website.\\nTo solve this, we use absolute paths to hosted images. Since images are hosted in GitHub along the rest of the code, we use these absolute paths.\\nVS Code has a way to deal with that issue. It won't allow you to create a package if README contains relative paths. It will emit this error:\\n> Couldn't detect the repository where this extension is published. The image '.\/docs\/logo\/abracadabra-logo.svg' will be broken in README.md. Please provide the repository URL in package.json or use the --baseContentUrl and --baseImagesUrl options.\\nThus, we could technically use relative paths and provide the correct arguments to make images work on VS Code Marketplace.\\nHowever, using absolute paths is something we're used to, and doesn't come up with a lot of downsides. So we decide to go with absolute paths, until downsides justify otherwise.\\n","Decision":"We use absolute paths to GitHub hosted images in the README (and other documentation files).\\nWe don't use relative paths to images.\\n","tokens":224,"id":589,"Predictions":"We decide to go with absolute paths.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"abracadabra\/0005-use-custom-testeach-instead-of-jest-it-each.md","Context":"## Context\\nJest [`it.each` seemed great][jest-it-each], but it turns out to be limited.\\nSpecifically, it doesn't allow us to have a `.only` on a single test of the list.\\nOf course, this can be done at runtime through Jest runner. But if we use tools like [Wallaby.js][wallaby], we can't do that. We need a way to add things like `.only` on individual tests.\\n","Decision":"We've decided to implement a custom `testEach()` function that will provide a convenient API to run the same test over different data.\\nThis function will provide a way to run individual test of the list with `only: true`.\\n","tokens":98,"id":590,"Predictions":"We need to add things like.only on individual tests.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"abracadabra\/0006-create-generator-to-bootstrap-new-refactorings.md","Context":"## Context\\nCreating a new refactoring kinda always follow the same steps. There is a bunch of boilerplate code that needs to be created.\\nWe could try to refactor this boilerplate into some good abstraction. But we still need to scaffold the same files over and over: the refactoring file, the test file, the command declaration file and eventually the action provider one.\\n","Decision":"To speed up this process, we've decided to use a code generator.\\nWe went for [hygen](https:\/\/www.hygen.io\/) because it's quite simple to use, very fast and flexible.\\nWe're abstracting the usage of hygen behind a npm script alias: `yarn new` will ask you few questions to scaffold a new refactoring.\\n","tokens":77,"id":591,"Predictions":"We need to make sure that the boilerplate code is not a boilerplate.nnWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate.nWe need to make sure that the boilerplate code is not a boilerplate."}
{"File Name":"abracadabra\/0010-integration-tests.md","Context":"## Context\\nWe had to enhance editor capabilities and implement new editor adapters with the latest features. Not having integration tests to cover these changes became more and more risky.\\nAfter giving it another try, we were able to get Jest & Mocha installed together, without compilation errors because of type conflicts. Therefore, it was possible to create integration tests that would have access to VS Code API.\\n","Decision":"We will now cover adapters with integration tests.\\nBecause _integration tests_ has different meanings for different people, we have decided to call them **contract tests** instead. Our intention is to test that all adapters of an interface do follow the same contract.\\n","tokens":79,"id":592,"Predictions":"Decision (A).nThe integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nC. The integration tests would have access to VS Code API.nD. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nA.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.nB. The integration tests would have access to VS Code API.n"}
{"File Name":"abracadabra\/0009-use-custom-github-actions-to-deploy.md","Context":"## Context\\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\\nAlso, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/). You can read [the original issue](https:\/\/github.com\/nicoespeon\/abracadabra\/issues\/163) for more context.\\nSource code is hosted and managed in GitHub, so GitHub Actions make sense.\\nThere is [a custom GitHub Action](https:\/\/github.com\/HaaLeo\/publish-vscode-extension#readme) that would take care of that precise use-case. But what we need to do isn't very complex. Having less intermediate would make it easier to maintain.\\n","Decision":"We've created 2 GitHub Actions:\\n1. One for deploying to the VS Code Marketplace\\n2. One for deploying to the Open VSX Registry\\nThese actions trigger on published releases.\\n","tokens":183,"id":593,"Predictions":"We need to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.n Also, we want to start deploying to [the Open VSX Registry](https:\/\/open-vsx.org\/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https:\/\/vscodium.com\/). You can read [the original issue](https:\/\/github.com\/nicoespeon\/abracadabra\/issues\/163) for more context.nSource code is hosted and managed in GitHub, so GitHub Actions make sense.nThere is [a custom GitHub Action](https:\/\/github.com\/HaaLeo\/publish-vscode-extension#readme) that would take care of that precise use-case.nBut what we need to do isn't very complex. Having less intermediate would make it easier to maintain.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions.nWe decide to use GitHub Actions."}
{"File Name":"abracadabra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":594,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"terraform-provider-harbor\/0001-use-swagger-for-generate-http-client.md","Context":"## Context and Problem Statement\\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\\n","Decision":"Chosen option: \"Swagger Based\", because this solution supports the fastes development Start without writting any boilerplate code.\\n### Positive Consequences\\n* No Painfull HTTP Client Implementation\\n### Negative Consequences\\n* the API Client Implementation dependents to the Swagger Spec Quality...\\n","tokens":31,"id":595,"Predictions":"It is not possible to generate a HarborRestAPI Client.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ehoks-ui\/0003-use-single-callerid-for-both-oppija-and-virkailija.md","Context":"## Context\\nEvery service is required to pass a Caller-Id header with its requests and previously eHOKS frontend had separate\\nids for oppija and virkailija. While the codebase is largely shared between oppija and virkailija the services themselves\\nare separate and hence the separate ids were created. While the headers were simple to add to requests made by\\ncomponents only used by either oppija or virkailija, dynamically figuring out which id should be used in the shared\\ncomponents at any given time proved harder.\\n","Decision":"The separate ids will be replaced by a single frontend Caller-Id. Since all the requests made by both oppija and\\nvirkailija frontends go through the eHOKS backend service and don't call any external services directly this\\nshould be sufficient. The requests from oppija and virkailija can be distinquished from each other via other means,\\neg. they use different backend APIs altogether.\\n","tokens":112,"id":596,"Predictions":"Decision (A).nItem 1: eHOKS frontend had separate ids for oppija and virkailija.nItem 2: oppija and virkailija.nItem 3: oppija and virkailija.nItem 4: oppija and virkailija.nItem 5: oppija and virkailija.nItem 6: oppija and virkailija.nItem 7: oppija and virkailija.nItem 8: oppija and virkailija.nItem 9: oppija and virkailija.nItem 10: oppija and virkailija.nItem 11: oppija and virkailija.nItem 12: oppija and virkailija.nItem 13: oppija and virkailija.nItem 14: oppija and virkailija.nItem 15: oppija and virkailija.nItem 16: oppija and virkailija.nItem 17: oppija and virkailija.nItem 18: oppija and virkailija.nItem 19: oppija and virkailija.nItem 20: oppija and virkailija.nItem 21: oppija and virkailija.nItem 22: oppija and virkailija.nItem 23: oppija and virkailija.nItem 24: oppija and virkailija.nItem 25: oppija and virkailija.nItem 26: oppija and virkailija.nItem 27: oppija and virkailija.nItem 28: oppija and virkailija.nItem 29: oppija and virkailija.nItem 30: oppija and virkailija.nItem 31: oppija and virkailija.nItem 32: oppija and virkailija.nItem 33: oppija and virkailija.nItem 34: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and virkailija.nItem 35: oppija and"}
{"File Name":"ehoks-ui\/0001-record-architecture-and-desing-decisions.md","Context":"## Context\\nWe need to record the architectural and desing decisions made during this project.\\n","Decision":"We will use Architecture Decision Records, as described by\\nMichael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAlso https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\\n","tokens":19,"id":597,"Predictions":"Decision based on the architectural decision record.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"Predictions":"nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"webform\/form-field-variations.md","Context":"## Two Unique Problems to Solve\\n* A radio or checkbox field needs to optionally have an \"Other\" input field which consists of a label and freeform text input. All existing radio\/checkbox functionality must be preserved.\\n* Address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.\\n","Decision":"* Create additional versions for each individual form field and treat them accordingly in the HTML helper. \"version\" will be a new attribute, \"formtype\" and the rest of the JSON will stay the same.\\n```\\nExample of field variation\\n\"data\":\\n[\\n{\\n\/\/ otherless radio, notice no version attribute (default)\\n\"label\":\"Icecream?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream\",\\n\"radios\":\"yes\\nno\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n{\\n\/\/ radio with other\\n\"label\":\"Icecream flavor?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_2\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream_flavor\",\\n\"radios\":\"vanilla\\nchocolate\",\\n\"type\":\"radio\",\\n\"version\": \"other\", \/\/ Variation version\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n]\\n```\\n* Add a new \"groupid\" attribute to the saved JSON form data object.\\n* The new \"groupid\" will act as a dynamic id which will group all fields with the same groupid together.\\n* The \"groupid\" value will be generated once it is dragged\/added to the editing form and be a concatenation of the form group template name (see below) and an incremental number, ie: g_address_streetonly_1\\n* Versions of form groups do not depend on each other.\\n* Added form groups to the form will be non-editable but will otherwise appear as regular fields within the JSON form data object.\\n```\\nExample of form data with a group\\n\"data\":\\n[\\n{\\n\/\/ pizza is not in the group\\n\"label\":\"Pizza\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"pizza_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"pizza\",\\n\"radios\":\"unpopular\\nreally unpopular\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\"\\n},\\n{\\n\"label\":\"Icecream?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_1\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream\",\\n\"radios\":\"yes\\nno\",\\n\"type\":\"radio\",\\n\"required\":\"true\",\\n\"class\":\"custom-class\",\\n\"groupid\":\"g_icecream_all_1\"\\n},\\n{\\n\"label\":\"Icecream flavor?\",\\n\"placeholder\":\"placeholder\",\\n\"help\":\"Supporting help text\",\\n\"id\":\"radio_2\",\\n\"formtype\":\"s08\",\\n\"name\":\"icecream_flavor\",\\n\"radios\":\"vanilla\\nchocolate\",\\n\"type\":\"radio\",\\n\"version\": \"other\", \/\/ Variation version\\n\"required\":\"true\",\\n\"class\":\"custom-class\",\\n\"groupid\":\"g_icecream_all_1\"\\n},\\n]\\n```\\n* HTML Generation: Create a helper listing of form group templates which will describe which form fields belong to which predefined group.\\n* Format would be a JSON with an array of formtypes: each formtype would adopt the same attributes as the existing form field JSON.\\n* Undefined attributes would be filled in by the default value and defined attributes would override the default.\\n* Naming groups will start with g_groupname_templatename\\n```\\nExample of form group templates\\n\"address\" : {\\n\"full\" : [\\n{\\n\"formtype\":\"c08\",\\n\"label\":\"Address 1\",\\n},\\n{\\n\"formtype\":\"c08\",\\n\"label\":\"Address 2\",\\n},\\n{\\n\"formtype\":\"c10\", \/\/city\\n},\\n{\\n\"formtype\":\"s14\", \/\/state\\n},\\n{\\n\"formtype\":\"c14\", \/\/zip\\n}\\n],\\n\"streetzip\" : [\\n{\\n\"formtype\":\"c08\",\\n},\\n{\\n\"formtype\":\"c14\", \/\/zip\\n}\\n]\\n}\\n```\\n","tokens":84,"id":599,"Predictions":"Decision (A) The radio or checkbox field must be preserved.n* The radio or checkbox field must be preserved.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.n* The address can exist as a form group which can contain any combination of street address, city, state, zip, etc."}
{"File Name":"nats-architecture-and-design\/ADR-2.md","Context":"## Context\\nNATS Server has a number of JSON based messages - monitoring, JetStream API and more. These are consumed,\\nand in the case of the API produced, by 3rd party systems in many languages. To assist with standardization\\nof data validation, variable names and more we want to create JSON Schema documents for all our outward facing\\nJSON based communication. Specifically this is not for server to server communication protocols.\\nThis effort is ultimately not for our own use - though libraries like `jsm.go` will use these to do validation\\nof inputs - this is about easing interoperability with other systems and to eventually create a Schema Registry.\\nThere are a number of emerging formats for describing message content:\\n* JSON Schema - transport agnostic way of describing the shape of JSON documents\\n* AsyncAPI - middleware specific API description that uses JSON Schema for payload descriptions\\n* CloudEvents - standard for wrapping system specific events in a generic, routable, package. Supported by all\\nmajor Public Clouds and many event gateways. Can reference JSON Schema.\\n* Swagger \/ OpenAPI - standard for describing web services that uses JSON Schema for payload descriptions\\nIn all of these many of the actual detail like how to label types of event or how to version them are left up\\nto individual projects to solve. This ADR describes how we are approaching this.\\n","Decision":"### Overview\\nWe will start by documenting our data types using JSON Schema Draft 7. AsyncAPI and Swagger can both reference\\nthese documents using remote references so this, as a starting point, gives us most flexibility and interoperability\\nto later create API and Transport specific schemas that reference these.\\nWe define 2 major type of typed message:\\n* `Message` - any message with a compatible `type` hint embedded in it\\n* `Event` - a specialized `message` that has timestamps and event IDs, suitable for transformation to\\nCloud Events. Typically, published unsolicited.\\nToday NATS Server do not support publishing Cloud Events natively however a bridge can be created to publish\\nthose to other cloud systems using the `jsm.go` package that supports converting `events` into Cloud Event format.\\n### Message Types\\nThere is no standard way to indicate the schema of a specific message. We looked at a lot of prior art from CNCF\\nprojects, public clouds and more but found very little commonality. The nearest standard is the Uniform Resource Name\\nwhich still leaves most of the details up to the project and does not conventionally support versioning.\\nWe chose a message type like `io.nats.jetstream.api.v1.consumer_delete_response`, `io.nats.server.advisory.v1.client_connect`\\nor `io.nats.unknown_message`.\\n`io.nats.unknown_message` is a special type returned for anything without valid type hints. In go that implies\\n`map[string]interface{}`.\\nThe structure is as follows: io.nats.`<source>`.`<catagory>`.v`<version>`.`<name>`\\n#### Source\\nThe project is the overall originator of a message and should be short but descriptive, today we have 2 - `server` and `\\njetstream` - as we continue to build systems around Stream Processing and more we'd add more of these types. I anticipate\\nfor example adding a few to Surveyor for publishing significant lifecycle events.\\nGenerated Cloud Events messages has the `source` set to `urn:nats:<source>`.\\n|Project|Description|\\n|-------|-----------|\\n|`server`|The core NATS Server excluding JetStream related messages|\\n|`jetstream`|Any JetStream related message|\\n#### Category\\nThe `category` groups messages by related sub-groups of the `source`, often this also appears in the subjects\\nthese messages get published to.\\nThis is a bit undefined, examples in use now are `api`, `advisory`, `metric`. Where possible try to fit in with\\nexisting chosen ones, if none suits update this table with your choice and try to pick generic category names.\\n|Category|Description|\\n|----|-----------|\\n|`api`|Typically these are `messages` used in synchronous request response APIs|\\n|`advisory`|These are `events` that describe a significant event that happened like a client connecting or disconnecting|\\n|`metric`|These are `events` that relate to monitoring - how long did it take a message to be acknowledged|\\n#### Versioning\\nThe ideal outcome is that we never need to version any message and maintain future compatibility.\\nWe think we can do that with the JetStream API. Monitoring, Observability and black box management is emerging, and we\\nknow less about how that will look in the long run, so we think we will need to version those.\\nThe philosophy has to be that we only add fields and do not significantly change the meaning of existing ones, this\\nmeans the messages stay `v1`, but major changes will require bumps. So all message types includes a single digit version.\\n#### Message Name\\nJust a string identifying what this message is about - `client_connect`, `client_disconnect`, `api_audit` etc.\\n","tokens":281,"id":600,"Predictions":"Decision based on the following: nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nats-architecture-and-design\/ADR-5.md","Context":"## Context\\nThis document describes the _Lame Duck Mode_ server notification. When a server enters lame duck mode, it removes itself from being advertised in the cluster, and slowly starts evicting connected clients as per  [`lame_duck_duration`](https:\/\/docs.nats.io\/nats-server\/configuration#runtime-configuration). This document describes how this information is notified\\nto the client, in order to allow clients to cooperate and initiate an orderly migration to a different server in the cluster.\\n","Decision":"The server notififies that it has entered _lame duck mode_ by sending an [`INFO`](https:\/\/docs.nats.io\/nats-protocol\/nats-protocol#info) update. If the `ldm` property is set to true, the server has entered _lame_duck_mode_ and the client should initiate an orderly self-disconnect or close. Note the `ldm` property is only available on servers that implement the notification feature.\\n","tokens":102,"id":602,"Predictions":"Decision (A) To allow clients to cooperate and initiate an orderly migration to a different server in the cluster.nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>n"}
{"File Name":"nats-architecture-and-design\/ADR-11.md","Context":"## Context and Problem Statement\\nThe client library should take a random IP address when performing a host name resolution prior to creating the TCP connection.\\n","Decision":"This was driven by the fact that the Go client behaves as described above and some users have shown interest in all clients behaving this way.\\nSome users have DNS where the order almost never change, which with client libraries not performing randomization, would cause all clients\\nto connect to the same server.\\n","tokens":29,"id":603,"Predictions":"Decision (A).nThe client library should take a random IP address when performing a host name resolution prior to creating the TCP connection.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ols-client\/0002-api-result-access.md","Context":"## Context\\nWe want to be able to use OLS Api results as objects (Ontology, Term, ...) instead of standard coreapi Document objects.\\nCoreapi API results are Document (a kind of OrderedDict in fact). making access to property annoying for programmer,\\nhaving to know expected keys.\\n","Decision":"To implement.\\n","tokens":62,"id":604,"Predictions":"Decision Makers should consider modifying the coreapi API to make it easier to access property objects.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ols-client\/0003-looping-over-list.md","Context":"## Context\\nWe want to be able to loop simply over Ontologies \/ Terms results, without bothering if a\\nnew call is made to change page.\\nOLS API results are paginated, the page size is a parameter in Query.\\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\\nThe actual calls to API are hidden from final users.\\n```python\\nfrom ebi.ols.api.client import OlsClient\\nclient = OlsClient()\\nontology = client.ontology('fpo')\\nterms = ontology.terms()\\nindividuals = ontology.individuals()\\nproperties = ontology.properties()\\n# work with all 'list' item types\\nfor term in terms:\\n# do whatever\\nprint(term)\\n# Direct List'like access on all list types\\nterm = terms[1254]\\nindividual = individuals[123]\\n# ...\\n```\\n","Decision":"To Implement\\n","tokens":199,"id":605,"Predictions":"nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ols-client\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":606,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"airline-reservation-system\/0002-TCH-RES-use-modular-monolith.md","Context":"## Context and Problem Statement\\n**What type of arichitecture is to be used ?**\\nConsidered options:\\n1. Monolith\\n2. Modular Monolith\\n3. Microservices\\nDrivers:\\n* Project is realized as GreenField\\n* A small development team (~10)\\n* Little experience in implementing distributed software\\n* Load - (50-150 req\/sec) - not so much\\n","Decision":"Option no 2 - Modular Monolith.\\nArchitecture suitable for teams with little experience in implementing distributed systems.\\nIt gives the possibility of gradual migration to distributed. Suitable for  greenfield systems, where high variation in requirements is expected.\\narchitecture.\\n","tokens":85,"id":607,"Predictions":"Decision (A) The project should be a monolith. (B) The project should be a modular monolith. (C) The project should be a microservices. (D) The project should be a distributed software. (A) The project should be a modular monolith. (A) The project should be a distributed software. (B) The project should be a distributed software. (C) The project should be a distributed software. (D) The project should be a distributed software. (A) The project should be a distributed software. (A) The project should be a distributed software. (B) The project should be a distributed software. (C) The project should be a distributed software. (D) The project should be a distributed software. (A) The project should be a distributed software. (A) The project should be a distributed software. (B) The project should be a distributed software. (C) The project should be a distributed software. (D) The project should be a distributed software. (A) The project should be a distributed software. (A) The project should be a distributed software. (B) The project should be a distributed software. (C) The project should be a distributed software. (D) The project should be a distributed software. (A) The project should be a distributed software. (A) The project should be a distributed software."}
{"File Name":"airline-reservation-system\/0004-TCH-RES-use-document-database-as-a-aggregate-data-repository.md","Context":"## Context and Problem Statement\\n**What kind of database should be used to storage aggregates data ?**\\nConsidered options:\\n1. Document database (MongoDB)\\n2. Relational database\\nDrivers:\\n* Simple objects will be saved.\\n* There will be no relationship between objects .\\n* A Read Model combining data from different aggregates is required .\\n","Decision":"Option no 1 - document database - MongoDB.\\n","tokens":78,"id":608,"Predictions":"Decision (A) The document database should be used to store aggregates data.nB. Document database should be used to store simple objects.nC. Relational database should be used to store aggregates data.nD. Relational database should be used to store aggregates data.nAnswer:A.B. Document database should be used to store aggregates data.nB. Relational database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Relational database should be used to store aggregates data.nAnswer:A.B. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects.nC. Document database should be used to store simple objects."}
{"File Name":"airline-reservation-system\/0005-TCH-RES-use-spring-repository.md","Context":"## Context and Problem Statement\\n**What technology is to be used for persistence?**\\nConsidered options:\\n1. Spring Template\\n2. JPA + Spring Repositores\\nDrivers:\\n1. Technology must be simple and well known\\n2. Cannot require a lot of code writing\\n","Decision":"Option no 2 - JPA+Spring Repositories.\\n","tokens":62,"id":609,"Predictions":"Decision (A). The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be used for persistence. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be simple and well known. The technology must be"}
{"File Name":"airline-reservation-system\/0003-TCH-RES-use-ports-and-adapters-architecture.md","Context":"## Context and Problem Statement\\n**What type of application architecture is to be used ?**\\nConsidered options:\\n1. Layered architecture\\n2. Ports and adapters architecture\\nDrivers:\\n* The Reservation module is an example of a deep module.\\n* A large number of business rules.\\n* Some of the rules are quite complicated.\\n* Application should be well - tested .\\n","Decision":"Option no 2 - Ports and Adapters architecture. It is dedicated for modules with high business complexity.\\nIt separates domain logic from application logic. It allows to create independent tests for each type of logic.\\n","tokens":83,"id":610,"Predictions":"Decision (A) The layered architecture should be used.nB) The layered architecture should be used.nC) The layered architecture should be used.nD) The layered architecture should be used.nAnswer:A.B. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nD. The layered architecture should be used.nAnswer:A.B. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nC. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.nB. The layered architecture should be used.n"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"lockfix\/0002-typescript-as-project-language.md","Context":"## Context\\n* This is Open Source app which may be used by big variety of projects\\n* TypeScript is widely known nowadays, so there is high chance to get contribution from other peple if this technology is used\\n* Project's author and main contibutor (@kopach) is fluent in this technology, so no need in spending extra efort in learning something differrent\\n* TypeScript is type safe, so more secure and potentially should prevent from common mistakes\\n* TypeScript integrates well with JavaScript so all libraries from both ecosystems can be used easilly\\n","Decision":"Use TypeScript as main and only programming language in this project\\n","tokens":112,"id":612,"Predictions":"The app should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a small number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a small number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n* TypeScript should be used by a large number of people.n"}
{"File Name":"cloud-platform\/019-Shared-Ingress-Controllers.md","Context":"## Context\\nThe design proposed in [018](018-Dedicated-Ingress-Controllers.md) proved\\nimpossible to implement. Every ingress controller requires an AWS Network Load\\nBalancer (NLB), and AWS have hard limits on the number of NLBs we can create\\nper VPC and availability zone (AZ).\\n","Decision":"We will:\\n* Disable mod-security on the default ingress-controller\\nThis should enable this ingress-controller to comfortably handle thousands of ingresses.\\n* Create and manage a set of ingress controllers with mod-security enabled\\nMost, if not all, production services will want the protection of a web\\napplication firewall, and mod-security is the easiest to enable. We need to\\nensure that each ingress controller only handles as many mod-security-enabled\\ningresses as it can reliably cope with.\\n* Continue to have some dedicated ingress controllers\\nDuring our aborted migration to dedicated ingress controllers for every\\nnamespace, several ingress controllers were created for specific services. We\\nwill leave these in place to avoid additional disruption to these service\\nteams.\\n","tokens":66,"id":613,"Predictions":"Decision (A) The design proposed in [018](018-Dedicated-Ingress-Controllers.md) proved impossible to implement. Every ingress controller requires an AWS Network LoadnBalancer (NLB), and AWS have hard limits on the number of NLBs we can createnper VPC and availability zone (AZ).n ## Final decision.n12n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n2n"}
{"File Name":"cloud-platform\/004-use-kubernetes-for-container-management.md","Context":"## Context\\nMOJ Digital's approach to infrastructure management and ownership has evolved over time, and has led to the following outcomes:\\n- Unclear boundaries on ownership and responsibilities between service teams and the cloud platforms team\\n- Significant variation in deployment, monitoring and lifecycle management across products\\n- Inefficient use of AWS resources due to the use of virtual machine-centric architecture, despite our standardisation on Docker containers\\nThe last few years has seen the advent of several products specifically focused on the problem of running and managing containers in production:\\n- Kubernetes\\n- Mesos \/ Mesosphere \/ DC\/OS\\n- Docker Swarm\\n- AWS ECS\\n- CloudFoundry\\nGiven the technology landscape within MOJ, we require a container management platform that can support a wide range of applications, from \"modern\" cloud-native 12-factor applications through to \"legacy\" stateful monolithic applications, potentially encompassing both Linux- and Windows-based applications; this removes CloudFoundry from consideration, given its focus on modern 12-factor applications and reliance on buildpacks to support particular runtimes.\\nFrom the remaining list of major container platforms, Kubernetes is the clear market leader:\\n- Rapid industry adoption during 2017 establishing it as the emerging defacto industry standard\\n- Managed Kubernetes services from all major cloud vendors\\n- Broad ecosystem of supporting tools and technologies\\n- Increasing support for Kubernetes as a deployment target for commercial and open-source software projects\\nThere is also precedent for Kubernetes use within MOJ, as the Analytical Platform team has been building on top of Kubernetes for around 18 months.\\n","Decision":"Use Kubernetes as the container management component and core technology for our new hosting platform.\\n","tokens":322,"id":614,"Predictions":"Decision (A). The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nD. The Kubernetes platform is a better choice for MOJ Digital.nA. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nD. The Kubernetes platform is a better choice for MOJ Digital.nA. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.nC. The Kubernetes platform is a better choice for MOJ Digital.nB. The Kubernetes platform is a better choice for MOJ Digital.n"}
{"File Name":"cloud-platform\/008-Support-Deployments-from-Third-Party-CI.md","Context":"## Context\\nThe cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able to use it, however, given that:\\n- Switching to Concourse CI would require familiarity with how it works and incur additional overhead\\n- We have not yet developed a streamlined approach to deployments through Concourse CI in order to confidently and properly offer support\\n- Product teams already use third party CI systems\\nWe think it would be good as a starting point to make it easy for teams to deploy directly from the third party CI systems that teams are already using, rather than requiring the deployments to be implemented in Concourse CI.\\n","Decision":"We will support deploying applications to the Cloud Platform from third party CI systems and will offer documentation on how to do so, at least for the most commonly used CI systems.\\n","tokens":140,"id":615,"Predictions":"We believe that the cloud platforms team should be able to use Concourse CI internally.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cloud-platform\/017-Variable-Naming.md","Context":"## Context\\nWe have a lot of scripts, pipeline definitions, terraform files, yaml files and\\ntemplates which need to define and use variables. We want a consistent\\nconvention for naming these so that, as we write code in multiple,\\ninter-dependent repositories, we can be confident that the names we are using\\nare correct.\\n","Decision":"We will always use snake case (e.g. `foo_bar`) for variable names which appear\\nin terraform\/yaml files and templates.\\n","tokens":71,"id":616,"Predictions":"We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n Decision Makers: We need to define and use variables in multiple repositories.n"}
{"File Name":"cloud-platform\/010-live-0-to-live-1-Cluster.md","Context":"## Context\\nMigrating from live-0 to live-1 cluster. The reason behind this is based on the need to move to a dedicated AWS account (moj-cp), which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region as Cloud Platform requirement to host data in the UK, rather than in Europe.\\n","Decision":"After some long consideration of possible options, the decision has been made to migrate from the live-0 cluster to the new live-1 cluster.\\nSince we only want to be running a single cluster, we will need to shut down live-0 as soon as it's no longer needed. Also services migrate from live-0 to live-1 sooner will avoid the complexities of running two parallel clusters.\\n","tokens":83,"id":617,"Predictions":"Decision (A) The move will be a temporary one.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cloud-platform\/009-Naming-convention-for-clusters.md","Context":"## Context\\nAs we are building our new platform on Kubernetes we have already found the need to build quite a few clusters. These have been for a range of purposes including users, testing new ideas (\"sandbox\"), testing new functionality (\"test\"), deploying apps to them (\"non-production\").\\nAs we are still learning we are finding that:\\n1. we need to continue building new clusters for different purposes and\\n2. we often need to test the cluster creation process\\n3. we want to differentiate between clusters that have users on them and those that are for internal testing purposes\\n4. we do not want to differentiate cluster by function (e.g. \"perf-test\", \"sandbox\") or status (\"non-production\").\\nTo make this easier we propose having a naming scheme that makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.\\n","Decision":"We will name all clusters with the following naming scheme:\\n- `live-{n}` for any cluster that have users on them, for instance `live-1`.\\n- `test-{n}` for any cluster that do not have users on them and are used by the cloud platform team only, for instance `test-2`.\\nWe will number the clusters sequentially.\\n","tokens":182,"id":618,"Predictions":"We propose having a naming scheme that makes it easier to understand whether users are on that cluster but makes no other assumptions about what it is used for.n1n2n3n4n5n6n7n8n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cloud-platform\/001-Use-AWS-hosted-elasticsearch.md","Context":"## Context\\nThe cloud platforms team self-host an Elasticsearch cluster with Kibana and Logstash (ELK). This cluster has suffered numerous outages (see [CPT-282](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-282) and [CPT-152](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-152) in Jira) that have been difficult to recover from.\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving ELK\\n* Lengthy downtimes which made data recovery pointless\\n* Self hosted ELK stacks cost was significantly higher than AWS ElasticSearch solution\\n* Not working ELK cluster was also a blocker for product teams as they couldn't see any application logs\\n","Decision":"Replace our self hosted ELK stack with the managed AWS Elasticsearch\\n","tokens":164,"id":619,"Predictions":"Decision (A). The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nD. The ELK cluster should be self-hosted.nA. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nC. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.nB. The ELK cluster should be self-hosted.n"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"Predictions":"Decision (A) The Cloud Platform should be able to support more pods. (B) The Cloud Platform should be able to support more pods. (C) The Cloud Platform should be able to support more pods. (D) The Cloud Platform should be able to support more pods. (A) The Cloud Platform should be able to support more pods. (A) The Cloud Platform should be able to support more pods. (B) The Cloud Platform should be able to support more pods. (C) The Cloud Platform should be able to support more pods. (D) The Cloud Platform should be able to support more pods. (A) The Cloud Platform should be able to support more pods. (A) The Cloud Platform should be able to support more pods. (B) The Cloud Platform should be able to support more pods. (C) The Cloud Platform should be able to support more pods. (D) The Cloud Platform should be able to support more pods. (A) The Cloud Platform should be able to support more pods. (A) The Cloud Platform should be able to support more pods. (B) The Cloud Platform should be able to support more pods. (C) The Cloud Platform should be able to support more pods. (D) The Cloud Platform should be able to support more pods. (A) The Cloud Platform should be able to support more pods."}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"Predictions":"Leaving it until later would have been easier to have upgraded every namespace when the significant change was made to the module.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cloud-platform\/012-One-cluster-for-dev-staging-prod.md","Context":"## Context\\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\\nTo support this, should we have separate clusters for production, development, and staging? Or, should we run a single cluster hosting all these different types of workload?\\n","Decision":"After consideration of the pros and cons of each approach we went with one cluster, using namespaces to partition different workloads.\\nSome important reasons behind this move were:\\n- A single k8s cluster can be made powerful enough to run all of our workloads\\n- Managing a single cluster keeps our operational overhead and costs to a minimum.\\n- Namespaces and RBAC keep different workloads isolated from each other.\\n- It would be very hard to keep multiple clusters (dev\/staging\/prod) from becoming too different to be representative environments\\nTo clarify the last point; to be useful, a development cluster must be as similar as possible to the production cluster. However, given multiple clusters, with different security and other constraints, some 'drift' is inevitable - e.g. the development cluster might be upgraded to a newer kubernetes version before staging and production, or it could have different connectivity into private networks, or different performance constraints from the production cluster.\\nBased on our past experience, these differences tend to increase over time, to the point where the development cluster is too far away from production to be representative. The extra work required to maintain multiple environments becomes wasted effort.\\nIf namespace segregation is sufficient to isolate one production service from another, then it is enough to isolate a team's development environment from a production service.\\nIf namespace segregation is not sufficient for this, then the whole cloud platform idea doesn't work.\\n","tokens":74,"id":622,"Predictions":"Decision (A) The Cloud Platform should have multiple clusters for production, development, and staging. (B) The Cloud Platform should have multiple clusters for development, development, and staging. (C) The Cloud Platform should have multiple clusters for staging. (D) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (B) The Cloud Platform should have multiple clusters for staging. (C) The Cloud Platform should have multiple clusters for staging. (D) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (B) The Cloud Platform should have multiple clusters for staging. (C) The Cloud Platform should have multiple clusters for staging. (D) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (B) The Cloud Platform should have multiple clusters for staging. (C) The Cloud Platform should have multiple clusters for staging. (D) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (A) The Cloud Platform should have multiple clusters for staging. (B) The Cloud Platform should have multiple clusters for staging."}
{"File Name":"cloud-platform\/016-Kibana-is-open-to-all-service-teams.md","Context":"## Context\\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.\\n","Decision":"It has been decided to use a combination of Auth0 and an OIDC proxy app. The application is managed in the [cloud-platform-terraform-monitoring repo][kibana-proxy] and configured ministryofjustice GitHub organization users to access Kibana.\\n","tokens":90,"id":623,"Predictions":"We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>nbr>We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.nbr>"}
{"File Name":"cloud-platform\/011-Introduce-Open-Policy-Agent.md","Context":"## Context\\nOn the Cloud Platform, there is a need to implement various policies to safeguard our tenant applications and to enforce best practices.\\nKubernetes offers various mechanisms that cover some of our needs (eg.: `ResourceQuotas` to prevent resource exhaustion and `PodSecurityPolicies` to enforce non-root containers) but there are other areas for which there is no builtin solution. However, kubernetes implements a Dynamic Admission Control API which introduces [admission webhooks][admission-control]. This API provides an easy way with which to expand on the existing admission controllers (built in the apiserver).\\nOur immediate need was to prevent users from reusing hostnames in `Ingresses`. Although our ingress controller prevents hijacking of hostnames, it does so silently and furthermore, this is not a documented behaviour. Therefore, we decided that the user should not be allowed to reuse hostnames already defined in other `Ingresses` and receive a useful error message if they try to do that.\\n","Decision":"We explored a number of existing solutions in the open source community, as well as the possibility of implementing our own and we also discussed the issue with other organisations that use kubernetes before reaching a conclusion.\\nEventually we decided to introduce the [Open Policy Agent][open-policy-agent]:\\n- It is a generic framework for building and enforcing policies (whereas most other existing implementations were designed around specific problems)\\n- The policies are defined in a declarative, high-level language\\n- It is designed for cloud-native environments\\n- It provides a kubernetes integration\\n- It provides a way by which to unit test the policies\\n- The project is adopted by CNCF\\nAlthough the project is still in alpha and very likely to change in the near future, we decided that it is stable enough for our needs and worth adopting even at these early stages, since the benefits outweigh the cost.\\n","tokens":203,"id":624,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cloud-platform\/003-Use-Concourse-CI.md","Context":"## Context\\nThe cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364)  There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-384).\\nReasons behind this move were:\\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\\n* Unnecessary downtimes\\n* With the move to Kubernetes a cloud native CI\/CD solution is needed, all jobs are written as code\\n* Non working jenkins was a blocker for product teams\\n","Decision":"Replace self hosted Jenkins with self hosted Concourse CI pipeline\\n3. Automate deployment of concourse using Terraform and Helm\\n4. WIP\\n~~Product teams have confirmed their success builds and deploys. zero downtime and minimal maintenance from Cloud Platforms side.~~\\n","tokens":186,"id":625,"Predictions":"Decision (A). The cloud platforms team currently use a self-host Jenkins server for CI\/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover. [CPT-364](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused a number of outages [CPT-384](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-364) There is also a reoccurring issue which has caused "}
{"File Name":"cloud-platform\/002-Use-github-for-architecture-decision-record.md","Context":"## Context\\nThe cloud platforms team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https:\/\/github.com\/ministryofjustice\/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\\nPutting technical documentation in GitHub has additional benefits:\\n* Using pull requests gives us a clear review and approval process\\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\\n* The information can be held in the open and viewed by anyone\\n","Decision":"1. Our architecture decision log for the new cloud platform will be held in GitHub\\n","tokens":167,"id":626,"Predictions":"Decision (A). The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nD. The cloud platforms team will use GitHub for documentation.nA. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.nB. The cloud platforms team will use GitHub for documentation.nC. The cloud platforms team will use GitHub for documentation.n"}
{"File Name":"cloud-platform\/013-Use-RSpec-for-cluster-tests.md","Context":"## Context\\nWe need to be able to test that the production cluster(s) and any test clusters we create behave the way we expect them to.\\nSo, we need automated tests that exercise a cluster and confirm that the effect we get is the one we expected.\\n","Decision":"We chose the ruby testing framework [rspec] for this.\\nAlthough there are some go-based testing frameworks for kubernetes, there are some problems with them, and some benefits to rspec:\\n* The kubernetes testing frameworks seem quite immature, with limited documentation, tooling and other resources such as examples\\n* There is limited go expertise in the team\\n* RSpec is a very mature framework, with a lot of tooling, documentation and support\\n* There is a lot of ruby\/rspec experience in the wider organisation\\n* Ruby is our scripting language of choice, so rspec fits with that\\n","tokens":55,"id":627,"Predictions":"We need to test the effect we get on the cluster.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cloud-platform\/014-Why-we-build-our-own-kubernetes-cluster.md","Context":"## Context\\nMoJ Cloud Platform team has decided to use [kubernetes for container management platform](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/004-use-kubernetes-for-container-management.md) following the outcome of MOJ Digital's approach to infrastructure management. The team needed the below features for the infrastructure management:\\n- An universal authentication mechanism to manage users without depending on the cloud provider\\n- Able to customize the control plane of kubernetes for MOJ requirement on Pod security\\n- Able to integrate external tools easily\\n- Able to manage and configure the control plane for any custom changes\\n","Decision":"There are several leading cloud providers who provide managed production-ready kubernetes cluster:\\n- Amazon Elastic Kubernetes Service (Amazon EKS)\\n- Azure Kubernetes Service (AKS)\\n- Google Kubernetes Engine (GKE)\\nWe decided to host our cluster on AWS because our service team has good development experience working with AWS services. This made it easier for teams to migrate to the kubernetes platform\\nWe decided to manage the kubernetes cluster ourselves rather than using EKS mainly for the below reasons:\\n- When the time MOJ needed to build the kubernetes, Amazon EKS was still in the Alpha stage and was not production ready. Also Amazon EKS require to use IAM for user authentication which will be an overhead for managing users of service teams\\n- Kubernetes(k8s) allows to authenticate using OIDC and therefore it was easy to manage the authentication externally using Auth0\\n","tokens":127,"id":630,"Predictions":"The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the infrastructure management. The team will use [kubernetes for container management platform] to manage the control plane for any custom changes."}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"Predictions":"Decision (A) The Kubernetes interface should be a good interface for service teams.nB) The Kubernetes interface should be a good interface for service teams.nC) The Kubernetes interface should be a good interface for service teams.nD) The Kubernetes interface should be a good interface for service teams.nAnswer:A.nB.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nC.nB.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.n"}
{"File Name":"cloud-platform\/006-Use-github-as-user-directory.md","Context":"## Context\\nAs part of our [planning principles](https:\/\/docs.google.com\/document\/d\/1kHaghp-68ooK-NwxozYkScGZThYJVrdOGWf4_K8Wo6s\/edit) we highlighted \"Building in access control\" as a key principle for planning our building our new cloud platform.\\nMaking this work for the new cloud platform means implementing ways that our users &mdash; mainly developers &mdash; can access the various bits of the new infrastructure. This is likely to include access to Kubernetes (CLI and API), AWS (things like S3, RDS), GitHub, and any tooling we put on top of Kubernetes that users will access as part of running their apps (e.g. ELK, [Prometheus](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/main\/architecture-decision-record\/026-Managed-Prometheus.md#choice-of-prometheus), [Concourse](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/main\/architecture-decision-record\/003-Use-Concourse-CI.md)).\\nAt the current time there is no consistent access policy for tooling. We use a mixture of the Google domain, GitHub and AWS accounts to access and manage the various parts of our infrastructure. This makes it hard for users to make sure that they have the correct permissions to do what they need to do, resulting in lots of requests for permissions. It also makes it harder to manage the user lifecycle (adding, removing, updating user permissions) and to track exactly who has access to what.\\nWe are proposing that we aim for a \"single sign on\" approach where users can use a single logon to access different resources. For this we will need a directory where we can store users and their permissions, including what teams they belong to and what roles they have.\\nThe current most complete source of this information for people who will be the first users of the cloud platform is GitHub. So our proposal is to use GitHub as our initial user directory - authentication for the new services that we are building will be through GitHub.\\n","Decision":"We will use GitHub as the identify provider for the cloud platform.\\nWe will design and build the new cloud platform with the assumption that users will login to all components using a single GitHub id.\\n","tokens":433,"id":632,"Predictions":"a). The new cloud platform will be a \"single sign on\" approach. b). The new cloud platform will be a \"single sign on\" approach. c). The new cloud platform will be a \"single sign on\" approach. d). The new cloud platform will be a \"single sign on\" approach. e. The new cloud platform will be a \"single sign on\" approach. f). The new cloud platform will be a \"single sign on\" approach. g). The new cloud platform will be a \"single sign on\" approach. h). The new cloud platform will be a \"single sign on\" approach. h). The new cloud platform will be a \"single sign on\" approach. ii. The new cloud platform will be a \"single sign on\" approach. iii. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. iv. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach. v. The new cloud platform will be a \"single sign on\" approach."}
{"File Name":"cloud-platform\/007-Use-ECR-As-Container-Registry.md","Context":"## Context\\nThe cloud platforms team currently use Docker Registry for storage of docker images. This solution is self-hosted, needs regular patching, and occasionally has downtime.\\nExample of an issue [CPT-274](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-274).\\nWe want to update the container registry to avoid some of the problems we have been seeing with it. The container registry will also be a key part of our new [Kubernetes based infrastructure](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/004-use-kubernetes-for-container-management.md).\\nThe criteria for selecting a new solution included:\\n* Finding a solution that would work with well GitHub based identity which is one of our [major architectural choices](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/006-Use-github-as-user-directory.md)\\n* Decreasing the amount of day to day ops work, perhaps by using a managed service\\n* Good value for money\\n* Preference for open source tooling\\n* Can or is able to run addons i.e. security scanners\\n* We can make images that we host public by default\\nFrom this process we tentatively chose ECR. Unfortunately none of the SaaS registries in common use support Github authentication, but as ECR uses IAM for authentication, and IAM supports federated identity with Github via Auth0, it will in theory be possible to define ECR access policies that reference Github teams and roles.\\n","Decision":"We will replace our self hosted Docker Registry to managed Elastic Container Registry (ECR).\\n","tokens":312,"id":633,"Predictions":"We chose ECR over Docker Registry.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"io-pagopa-proxy\/0002-use-uuid-v1-for-sequential-requests.md","Context":"## Context\\nWe have to send requests to a server that requires unique Request IDs for each message.\\nSo, we need to generate uuids.\\n","Decision":"We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\\nhttps:\/\/www.npmjs.com\/package\/uuid\\n","tokens":31,"id":634,"Predictions":"We need to generate uuids for each message.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kafka\/0005-require-kafka-v0-11.md","Context":"## Context\\nSupporting multiple Kafka versions is something quite trivial due to the way the\\nprotocol was designed. With the [`ApiVersions`](https:\/\/kafka.apache.org\/protocol#The_Messages_ApiVersions)\\nmessage, clients are able to retrieve the minimum and maximum supported version\\nfor all API messages in a broker.\\nThat is critical part in the connection flow and is suggested to be done right\\nafter the connection has been established, however such message is only\\navailable as of v0.10.\\nIn Kafka v0.11, a new record batch format was introduced. This format offers\\nbetter metadata control, headers, and support for transactions.\\nMore info:\\n- https:\/\/cwiki.apache.org\/confluence\/display\/KAFKA\/KIP-35+-+Retrieving+protocol+version\\n- https:\/\/kafka.apache.org\/documentation\/#messages\\n","Decision":"This library will only be usable and guaranteed to be fully working with Kafka\\nv0.11+, although it's highly recommended always use the latest version\\navailable (v2.2.0 at the moment).\\n","tokens":177,"id":635,"Predictions":"Decision (A). The [ApiVersions](https:\/\/kafka.apache.org\/protocol#The_Messages_ApiVersions)nmessage, clients are able to retrieve the minimum and maximum supported versionnfor all API messages in a broker.nThat is critical part in the connection flow and is suggested to be done rightnafter the connection has been established, however such message is onlynavailable as of v0.10.nIn Kafka v0.11, a new record batch format was introduced. This format offersnbetter metadata control, headers, and support for transactions.nMore info:n- https:\/\/cwiki.apache.org\/confluence\/display\/KAFKA\/KIP-35+-+Retrieving+protocol+versionn- https:\/\/kafka.apache.org\/documentation\/#messagesn ## Decision Makers: [ApiVersions](https:\/\/kafka.apache.org\/protocol#The_Messages_ApiVersions)nmessage, clients are able to retrieve the minimum and maximum supported versionnfor all API messages in a broker.nThat is critical part in the connection flow and is suggested to be done rightnafter the connection has been established, however such message is onlynavailable as of v0.10.nIn Kafka v0.11, a new record batch format was introduced. This format offersnbetter metadata control, headers, and support for transactions.nMore info:n- https:\/\/cwiki.apache.org\/confluence\/display\/KAFKA\/KIP-35+-+Retrieving+protocol+versionn- https:\/\/kafka.apache.org\/documentation\/#messagesn"}
{"File Name":"kafka\/0004-use-arrays-for-mapping-schema.md","Context":"## Context\\nThe Java implementation of the protocol uses `final static` properties to define\\nthe schema for requests and responses. These are defined using objects, making\\nit easy to support multiple versions for each API call.\\nPHP doesn't have this feature so we need decide how to solve this.\\n","Decision":"We're still going to use objects to manipulate data and write\/read content\\nto\/from Kafka, however it would be simpler to use arrays in constants of each\\nrequest\/response class.\\nEach field would be an array item, the key would be the field name and the value\\nwould be the field type (or another array for more complex configuration).\\nThe mapping would like this:\\n```php\\nuse Lcobucci\\Kafka\\Protocol\\Type;\\nfinal class DoSomethingRequest\\n{\\nprivate const SCHEMAS = [\\n[\\n'error_code'   => Type\\Int16::class,\\n'api_versions' => [\\n'type'     => Type\\ArrayOf::class,\\n'nullable' => false, \/\/ optional, default = false\\n'items'    => [ \/\/ just type name if items don't have complex structure\\n'api_key'     => Type\\Int16::class,\\n'min_version' => Type\\Int16::class,\\n'max_version' => Type\\Int16::class,\\n],\\n],\\n],\\n];\\n}\\n```\\n","tokens":61,"id":636,"Predictions":"We need to make the schema for requests and responses in Java.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in Java.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the API call for PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the schema for requests and responses in PHP.nWe need to make the API call for PHP.nWe need to make the schema for requests and responses in PHP."}
{"File Name":"kafka\/0003-port-protocol-s-types-from-java-implementation.md","Context":"## Context\\nKafka's protocol defines some primitive types which are used to send requests\\nand parse responses: https:\/\/kafka.apache.org\/protocol#protocol_types\\nProviding an easy way to define the schema of the requests and responses is\\nquite critical to make this library extensible enough.\\n","Decision":"We've decided to basically port the Java implementation to the PHP world,\\nbecause it was very well written and it simplifies things by a lot.\\nSome minor things obviously have to be adapted and for now we'll leave some\\ntypes to the upcoming releases - just because they aren't need to implement the\\nmessages we're planning to provide at the moment.\\n","tokens":61,"id":637,"Predictions":"Decision (A) The schema of the requests and responses should be simple enough to make this library extensible enough.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kafka\/0007-require-latest-stable-php-version.md","Context":"## Context\\nWe've previously decided to require PHP 7.2, however many months have passed and\\nPHP 7.3 is quite stable nowadays.\\n","Decision":"Bump up requirement to latest stable PHP version (7.3 at the moment).\\n","tokens":33,"id":638,"Predictions":"We will require PHP 7.3 instead of PHP 7.2.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kafka\/0006-require-php-7-3.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":639,"Predictions":"Decision was made to build a new house on the site of the old house.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kafka\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":640,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"Predictions":"Decision Makers should consider the following: a. The library should be able to support many versions of the library. b. The library should be able to support many versions of the library. c. The library should be able to support many versions of the library. d. The library should be able to support many versions of the library. e. The library should be able to support many versions of the library. f. The library should be able to support many versions of the library. g. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h. The library should be able to support many versions of the library. h."}
{"File Name":"cena\/0002-adopt-ddd-approach.md","Context":"## Context\\n`menu-generation` application's scope is not well defined and may cover a lot of concepts around the Menu generation\\nfeatures, such as:\\n- managing a recipes catalog\\n- searching in this catalog for recipes based on multiple criteria\\n- collaborating on this catalog\\n- handling allergies and tastes\\n- generating shopping lists\\nAll above concepts may be complex to model and should not be considered as simple data in a CRUD system. Other new\\nfeatures may emerge in the future. In addition, all those features are related but should not be treated as a whole in\\na big ball of mud architecture.\\n","Decision":"[Domain-driven design](https:\/\/domainlanguage.com\/wp-content\/uploads\/2016\/05\/DDD_Reference_2015-03.pdf) as defined by\\nEric Evans will help modeling the different concepts managed by the `menu-generation` application, keep the ability to\\nadapt the model based on new insights and split the whole application into different bounded contexts to avoid mixing\\nthose concepts in one giant messy code base.\\n","tokens":124,"id":642,"Predictions":"Decision Makers should consider the application's scope as a whole.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cena\/0003-adopt-hexagonal-architecture.md","Context":"## Context\\nAdopting the [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.\\naggregates and services) from the infrastructure (i.e. application clients and persistence).\\n","Decision":"`menu-generation` application will adopt [hexagonal architecture](https:\/\/en.wikipedia.org\/wiki\/Hexagonal_architecture_(software)),\\nas it aims to provide this separation.\\n","tokens":53,"id":643,"Predictions":"Decision (A) The [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.naggregates and services) from the infrastructure (i.e. application clients and persistence).n ## Final answer: [Options:A).n.n.n.n.n.n.n.n.n.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}
{"File Name":"cena\/0006-manage-build-with-gradle.md","Context":"## Context\\n`menu-generation` application will use the Spring framework along with other third party libraries, thus requires a\\ndependency management tool.\\nEffective development lifecycle requires Continuous Integration, thus a build management tool is necessary.\\n[Gradle](https:\/\/gradle.org\/) is one of the two main build management tools for the Java ecosystem with [Maven](http:\/\/maven.apache.org\/).\\nGradle is considered as more extensive and quicker than Maven. It is also well integrated with many tools, including\\nthe Spring framework.\\n","Decision":"Gradle will be used to manage project dependencies and build tasks.\\n","tokens":111,"id":644,"Predictions":"Decision (A) To allow the application to be a \"menu-generation\" application. (B) To allow the application to be a \"menu-generation\" application. (C) To allow the application to be a \"menu-generation\" application. (D) To allow the application to be a \"menu-generation\" application. (A) To allow the application to be a \"menu-generation\" application. (A) To allow the application to be a \"menu-generation\" application. (A) To allow the application to be a \"menu-generation\" application. (B) To allow the application to be a \"menu-generation\" application. (C) To allow the application to be a \"menu-generation\" application. (D) To allow the application to be a \"menu-generation\" application. (A) To allow the application to be a \"menu-generation\" application. (A) To allow the application to be a \"menu-generation\" application. (A) To allow the application to be a \"menu-generation\" application. (B) To allow the application to be a \"menu-generation\" application. (C) To allow the application to be a \"menu-generation\" application. (D) To allow the application to be a \"menu-generation\" application. (A) To allow the application to be a \"menu-generation\" application."}
{"File Name":"cena\/0004-expose-services-through-rest-api.md","Context":"## Context\\n`menu-generation` application needs to expose data and services in order to be used by multiple different applications,\\neither web or mobile front-ends or any other application that will reuse a sub-part of the `menu-generation`\\napplication or enhance the provided services.\\nWe want the application data and services to be easily consumed through well established communication protocols, such\\nas HTTP. It must be intuitive to manipulate the `menu-generation` application as a self-discoverable API, without\\nmanaging cumbersome protocols or data formats.\\n","Decision":"`menu-generation` application data and services will be exposed through a REST API.\\n","tokens":110,"id":645,"Predictions":"Decision (A) The application must be self-discoverable. (B) The application must be self-discoverable. (C) The application must be self-discoverable. (D) The application must be self-discoverable. (A) The application must be self-discoverable. (A) The application must be self-discoverable. (A) The application must be self-discoverable. (B) The application must be self-discoverable. (C) The application must be self-discoverable. (D) The application must be self-discoverable. (A) The application must be self-discoverable. (A) The application must be self-discoverable. (A) The application must be self-discoverable. (B) The application must be self-discoverable. (C) The application must be self-discoverable. (D) The application must be self-discoverable. (A) The application must be self-discoverable. (A) The application must be self-discoverable. (B) The application must be self-discoverable. (C) The application must be self-discoverable. (D) The application must be self-discoverable. (A) The application must be self-discoverable. (A) The application must be self-discoverable. (B) The application must be self-discoverable."}
{"File Name":"cena\/0012-document-apis-with-spring-rest-docs.md","Context":"## Context\\nEven a self-discoverable API needs to be documented to let consumers understand the underlying application features and\\nthe resources attributes significance.\\nA good API documentation will provide requests and responses structure, defining each attribute, giving examples and\\nmixing functional and technical information so that consumers do not need to traverse different documents to get a\\ncomplete overview of the features and technical concerns.\\nThe documentation must also provide links between different parts of the document, so that it is easy to go back and\\nforth between those parts.\\nThe documentation must be kept in sync with the features. Ideally, a documentation is generated from the code.\\n","Decision":"`menu-generation` will generate its API documentation with [Spring Rest-Docs](https:\/\/spring.io\/projects\/spring-restdocs).\\n","tokens":128,"id":646,"Predictions":"Decision (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API. (A) To document the API. (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API. (A) To document the API. (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API. (A) To document the API. (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API. (A) To document the API. (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API. (A) To document the API. (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API. (A) To document the API. (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API. (A) To document the API. (A) To document the API. (B) To document the API. (C) To document the API. (D) To document the API."}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"Predictions":"Decision Makers should use command-query separation to determine the best way to use commands and queries.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cena\/0007-execute-build-tasks-with-make.md","Context":"## Context\\nWe don't want the development tools or Continuous Integration pipeline to be strongly bound to [Gradle](0006-manage-build-with-gradle.md).\\n[Make](https:\/\/linux.die.net\/man\/1\/make) is an utility agnostic of any language or build management tools.\\n","Decision":"Make will be used to execute build tasks, abstracting Gradle and potential other tools used during build execution.\\n","tokens":64,"id":648,"Predictions":"Decision (A).n[Gradle](0006-manage-build-with-gradle.md) is an utility agnostic of any language or build management tools.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"Predictions":"Decision (A) To implement the hexagon architecture in Spring.nB) To implement the hexagon architecture in Spring.nC) To implement the hexagon architecture in Spring.nD) To implement the hexagon architecture in Spring.nAnswer:A.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C.nAnswer:B.nAnswer:C."}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"Predictions":"Decision (A) To define acceptance tests. (B) To define acceptance tests. (C) To define acceptance tests. (D) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (B) To define acceptance tests. (C) To define acceptance tests. (D) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (B) To define acceptance tests. (C) To define acceptance tests. (D) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (B) To define acceptance tests. (C) To define acceptance tests. (D) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (B) To define acceptance tests. (C) To define acceptance tests. (D) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (B) To define acceptance tests. (C) To define acceptance tests. (D) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (A) To define acceptance tests. (B) To define acceptance tests. (C) To define acceptance tests."}
{"File Name":"cena\/0001-use-java-as-language.md","Context":"## Context\\nWe need to choose a programming language to implement the `menu-generation` application.\\n`menu-generation` application will be developed initially during the author's free time, thus this time is limited.\\nDevelopment may involve other developers in the future, and the chosen language should not restrict participation.\\n","Decision":"Java is a broadly used programming language, and the most well mastered one by the author. Thus Java will be the\\nprogramming language used to implement the `menu-generation` application.\\n","tokens":62,"id":651,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cena\/0010-implement-acceptance-tests-with-serenity.md","Context":"## Context\\n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance\\nscenarii. However, to help maintaining an acceptance tests client library, we need to organize this library to be\\nextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.\\nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about\\nwhat wrong happened during API calls.\\n","Decision":"The [Serenity](http:\/\/www.thucydides.info\/#\/) framework will be used to define the acceptance tests library.\\n","tokens":100,"id":652,"Predictions":"Decision (A).n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance tests. However, to help maintaining an acceptance tests client library, we need to organize this library to benextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about what wrong happened during API calls.n Decision (A).n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance tests. However, to help maintaining an acceptance tests client library, we need to organize this library to benextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about what wrong happened during API calls.n Decision (A).n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance tests. However, to help maintaining an acceptance tests client library, we need to organize this library to benextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about what wrong happened during API calls.n"}
{"File Name":"cena\/0008-package-application-in-docker-image.md","Context":"## Context\\n`menu-generation` application is packaged as a [Spring Boot](0005-use-spring-framework.md) executable JAR file.\\nRunning acceptance tests on development machine or during Continuous Integration must be quick, easy and the least\\ndependent of the underlying system.\\n[Docker](https:\/\/www.docker.com\/) is a widespread container based solution that can be used during development lifecycle\\non most operating systems as well as in well established Cloud solutions such as [Kubernetes](https:\/\/kubernetes.io\/).\\n","Decision":"`menu-generation` application will be packaged as a docker image. A `docker-compose` definition will also be provided\\nto help running the application and its dependencies in a consistent and isolated environment.\\n","tokens":107,"id":653,"Predictions":"Decision (A) To develop a [Spring Boot](0005-use-spring-framework.md) executable JAR file. (B) To develop a [Docker](0005-use-spring-framework.md) executable JAR file. (C) To develop a [Kubernetes](0005-use-spring-framework.md) executable JAR file. (D) To develop a [Docker](0005-use-spring-framework.md) executable JAR file. (A) To develop a [Docker](0005-use-spring-framework.md) executable JAR file. (A) To develop a [Docker](0005-use-spring-framework.md) executable JAR file. (A) To develop a [Docker](0005-use-spring-framework.md) executable JAR file. (A) To develop a [Docker](0005-use-spring-framework.md) executable JAR file. (A) To develop a [Docker](0005-use-spring-framework.md) executable JAR file. (A) To develop a [Docker](0005-use-spring-framework.md) executable JAR file."}
{"File Name":"cena\/0011-expose-resources-as-hal.md","Context":"## Context\\n`menu-generation` application will expose its data and services through a [REST API](0004-expose-services-through-rest-api.md).\\nOne of the constraints of the REST architectural style implies exposing **Hyperlinks as the Engine of Application State**,\\nmeaning any resource should provide links to other accessible resources based on its current state in the application,\\nso that clients can discover which actions are available without interpreting the actual resource attributes.\\n[Spring HATEOAS](https:\/\/spring.io\/projects\/spring-hateoas) comes by default with [HAL](http:\/\/stateless.co\/hal_specification.html)\\nmedia type support.\\n","Decision":"`menu-generation` application will expose its resources using the HAL media type.\\n","tokens":134,"id":654,"Predictions":"Decision (A) To expose the data and services through a [REST API](0004-expose-services-through-rest-api.md)nOne of the constraints of the REST architectural style implies exposing **Hyperlinks as the Engine of Application State**,nmeaning any resource should provide links to other accessible resources based on its current state in the application,nso that clients can discover which actions are available without interpreting the actual resource attributes.n[Spring HATEOAS](https:\/\/spring.io\/projects\/spring-hateoas) comes by default with [HAL](http:\/\/stateless.co\/hal_specification.html)nmedia type support.n Decision (A) To expose the data and services through a [REST API](0004-expose-services-through-rest-api.md)none of the constraints of the REST architectural style implies exposing **Hyperlinks as the Engine of Application State**,nmeaning any resource should provide links to other accessible resources based on its current state in the application,nso that clients can discover which actions are available without interpreting the actual resource attributes.n Decision (A) To expose the data and services through a [REST API](0004-expose-services-through-rest-api.md)none of the constraints of the REST architectural style implies exposing **Hyperlinks as the Engine of Application State**,nmeaning any resource should provide links to other accessible resources based on its current state in the application,nso that clients can discover which actions are available without interpreting the actual resource attributes.n"}
{"File Name":"cena\/0013-adopt-contract-first-with-openapi.md","Context":"## Context\\nContract-first approach enforces a definition of the API contract before implementation. This ensures that the API is\\nwell designed for specific use-cases, based on consumers point of view.\\nAdopting a contract-first approach forces to ensure that contract is not broken during implementation and evolution of\\nthe application.\\nWe need to ensure that the contract is well defined through a specification and implementation respects this specification.\\n","Decision":"[OpenAPI Specification](https:\/\/swagger.io\/specification\/) will be used to enforce contract-first approach.\\n","tokens":85,"id":655,"Predictions":"Decision (A).nAdopted a contract-first approach.nB. Adopted a specification-first approach.nC. Adopted a specification-first approach.nD. Adopted a specification-first approach.nAdopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nAdopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification-first approach.nB. Adopted a specification"}
{"File Name":"opensmarthouse-core\/adr0002.md","Context":"## Context\\nMany modern systems require up-front data before a device can be discovered and added to a network. To facilitate this, a Provisioning system is to be added as part of the Discovery services, with the aim to provide the information required for devices to join the network.\\nTwo use cases are currently forseen.\\n1. Users must provide device information prior to discovery. This information is managed by the binding, however a consistent way is needed to provide this data from the user, to the binding. An example of this is systems may require a cryptographic key to be provided before a device is able to be discovered.\\n2. Centrally administered systems may want to provision information about a device from a central repository. Thus a user could be shipped the device, plug it in, and it would automatically be discovered and added with little user interaction.\\nIt is proposed to add a `ProvisioningService` and `ProvisioningRegistry` to act as an interface between the provisioning information (from the user, or other system) and the binding. The binding will be able to query the `ProvisioningRegistry` to retrieve information about any provisioned devices when discovery is enabled, and will be able to update the `ProvisioningRegistry` if a device is successfully discovered. Additionally, the `ProvisioningRegistry` can be queryable to allow the status of provisioned devices to be retrieved.\\nThe provisioning information provided through the `ProvisioningRegistry` is a set of properties that are interpretted by the binding. These are linked by a `ProvisioningUID` which is used to identify the device. This is akin to a `ThingTypeUID`, however used to identify the device before it is discovered.\\n","Decision":"TBD\\n","tokens":354,"id":656,"Predictions":"Decision (A) The provisioning system should be integrated into the Discovery services. (B) The provisioning system should be integrated into the Discovery services. (C) The provisioning system should be integrated into the Discovery services. (D) The provisioning system should be integrated into the Discovery services. (A) The provisioning system should be integrated into the Discovery services. (A) The provisioning system should be integrated into the Discovery services. (B) The provisioning system should be integrated into the Discovery services. (C) The provisioning system should be integrated into the Discovery services. (D) The provisioning system should be integrated into the Discovery services. (A) The provisioning system should be integrated into the Discovery services. (A) The provisioning system should be integrated into the Discovery services. (B) The provisioning system should be integrated into the Discovery services. (A) The provisioning system should be integrated into the Discovery services. (B) The provisioning system should be integrated into the Discovery services. (C) The provisioning system should be integrated into the Discovery services. (D) The provisioning system should be integrated into the Discovery services. (A) The provisioning system should be integrated into the Discovery services. (A) The provisioning system should be integrated into the Discovery services. (B) The provisioning system should be integrated into the Discovery services. (C) The provisioning system should be integrated into the Discovery services. (D) The provisioning system should be integrated into the Discovery services."}
{"File Name":"opensmarthouse-core\/adr0003.md","Context":"## Context\\nMany advanced features require additional information to be added to `Type`s - this could include secondary information as part of a `Command` or `State` update, or security context, `Command` source identifier, priority, etc. The type system structure inheritted from Eclipse SmartHome is difficult to extend due to the class hierarchy. A wrapper could be used around the `Command` or `State`, but this would require considerable rework of core functions.\\nThere is no base class for a `Type` - the ESH structure defines `Type`, `Command` and `State` as interfaces and many concrete `Type`s are derived from an `enum` (eg `OnOffType`, `UpDownType` etc) while others may be derived from Java classes (eg `DecimalType`). The absence of a clear class hierarchy within the framework makes it difficult or impossible for the framework to control and extend the `Type` system - one of the fundamental building blocks of the framework.\\nIt is proposed to rewrite the type system to introduce a clear class hierarchy. `Type`, `Command` and `State` remain as interfaces however an `AbstractBaseType` class is introduced from which all concrete `Type`s are derived. `ComplexType` and `PrimitiveType` are not generally used and are to be removed as they add unnecessary complexity. The type system should be implemented to be compatible at the basic level with the Eclipse SmartHome definitions such that backward compatability with existing bindings is not compromised.\\nThe ESH type system derives `DecimalType` and `QuantityType` from `Number` which makes it impossible to extend the `AbstractBaseType`. To resolve this we provide the core `Number` functionality with a new `NumberType` class. This provides the same interface, but can now extend `AbstractBaseType`. This potentially means that any code within the framework that checks `instanceof Number` will now fail and such checks must instead use `instanceof NumberType`.\\n","Decision":"This change provides a better separation between Java and OpenSmartHouse types, and most importantly allows further enhancements in functionality so is agreed to be implemented.\\n","tokens":405,"id":657,"Predictions":"Decision (A) To implement a new type system. (B) To implement a new type system. (C) To implement a new type system. (D) To implement a new type system. (E) To implement a new type system. (E) To implement a new type system. (A) To implement a new type system. (E) To implement a new type system. (A) To implement a new type system. (B) To implement a new type system. (C) To implement a new type system. (D) To implement a new type system. (A) To implement a new type system. (A) To implement a new type system. (A) To implement a new type system. (B) To implement a new type system. (C) To implement a new type system. (D) To implement a new type system. (A) To implement a new type system. (A) To implement a new type system. (B) To implement a new type system. (A) To implement a new type system. (B) To implement a new type system. (C) To implement a new type system. (D) To implement a new type system. (A) To implement a new type system. (A) To implement a new type system. (B) To implement a new type system. (A) To implement a new type system."}
{"File Name":"opensmarthouse-core\/adr0008.md","Context":"## Context\\nOften a binding will need to provide notifications to users for binding specific administrative purposes. This is not linked to `Item` data, but is designed to provide users feedback on binding specific functions or alerts. Examples of this could be -:\\n* Alerting the user about failed joining, or a trust issue with devices joining\\nIn general, these notifications are designed for display on an administration UI rather than a user UI, but there is of course nothing to prevent any UI from subsribing for such events.\\nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way. This should provide enough information to allow the UI to present the notification in a standard way - showing the severity, and a notification text.\\nThe alerting system may be as simple as real-time notifications direct to the UI. In such case, if the administrative user is not watching the UI, notifications would be lost and not persisted. An enhancement may be to run these through a registry which would allow notifications to be registered and actively dismissed once read by a user with the appropriate access level. At this point, the notification may be removed, or archived to provide an audit trail.\\nFurther extensions to the notification system may be considered to provide images, action buttons etc are considered outside of the scope of this initial proposal.\\n","Decision":"TBD\\n","tokens":283,"id":658,"Predictions":"Decision (A) The concept of a binding notification is not a binding specific administrative purpose.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe alerting system may be as simple as real-time notifications direct to the UI. In such case, if the administrative user is not watching the UI, notifications would be lost and not persisted.nThe notification may be removed, or archived to provide an audit trail.n Decision (A) The concept of a binding notification is not a binding specific administrative purpose.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way - showing the severity, and a notification text.nThe concept is to provide"}
{"File Name":"opensmarthouse-core\/adr0001.md","Context":"## Context\\nEclipse SmartHome defined a static type system. This may not be extended, and uses a static class `TypeParser` to create a `Type`. This limits framework modularity and extensability and is generally against OSGi principals.\\nThe architecture is to be changed to introduce a `TypeProvider` service, and `TypeFactory` interfaces. The `TypeProvider` is implemented to provide a `Type`, or set of `Type`s to the system, and `TypeFactory` is used by users to create `Type`s. Between these is a `TypeRegistry` which consumes the `Type`s from multiple `TypeProvider`s, and supplies the `TypeFactory` interface for system users.\\nThe standard core types are to be provided by a `CoreTypeProvider`, and the current static `TypeParser` is retained for backward compatability with ESH\/OH, but is not to be used by native _OpenSmartHouse_ bundles. Instead, users requiring `TypeParser` functionality should utilise the `TypeRegistry`.\\n","Decision":"The introduction of the `TypeRegistry` and `TypeProvider` services, and refactoring of the current types to be provided by a `CoreTypeProvider` is implemented. Existing users of `TypeParser` within the _OpenSmartHouse_ core bundles is to be refactored to use these services, and new bundles should avoid using the static `TypeParser`.\\n","tokens":214,"id":659,"Predictions":"Decision (A) To introduce a TypeProvider service, and TypeFactory interfaces.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"Predictions":"Decision (A) OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nBy implementing a SPI bundle, the openhab framework would be able to be activated.nC) OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nD) OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nAnswer:A.B. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nC) OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nC) OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nC) OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nB. OpenHAB should be able to build a SPI bundle that is targeted at a specific framework feature.nC"}
{"File Name":"opensmarthouse-core\/adr0007.md","Context":"## Context\\nTo allow the system to know if the ThingType definition has changed, a version identifier should be added to the ThingType. This will allow the system to update the thing based on the new Thing Type definition.\\nThe `version` should be added to the `ThingType`, and provided by thing type providers such as the XML provider. This should be reflected into the instantiated `Thing` and provided in the `ThingDTO` for use in the user interface.\\nThing Type XML definition -:\\n```\\n<thing-type id=\"sample\" version=\"2\">\\n```\\nIt shall then be possible for the user to decide to upgrade to the new definition. This functionality could be provided automatically, however this may result in unexpected changes to a users system as channels could be removed. Such updates are therefore considered to be better if manually initiated by the user.\\nThing REST response (part) -:\\n```\\n{\\nUID: \"test:sample:505f8d7b12\",\\nthingTypeUID: \"test:sample\",\\nthingTypeVersion: 2\\n}\\n```\\nA new REST API is to be added to allow the user to change the thing type via the UI. This uses the `\/thing\/{{thinguid}}\/migrate` call. This migrates the specified thing to the latest version of the ThingType.\\n","Decision":"This has minimal impact on the framework, and allows the user and UI to manage the migration of things without the user having to delete and re-add things, which is quite complex as item links also need to be re-added.  Further enhancements could be added to allow this to (optionally) be undertaken automatically.\\n","tokens":284,"id":661,"Predictions":"Decision based on the information in the text.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"opensmarthouse-core\/adr0004.md","Context":"## Context\\nWith large bindings with many things, a ThingType and ThingHandler may not be implemented for each Thing. Instead the ThingHandler may implement dynamic definition by interrogating the device, or through a static definition via the Thing XML file. This may be implemented as a single ThingHandler for numerous different devices whose functionality is defined or discovered.  In cases where the Thing is statically defined, there will be some discovery information that is retieved from the device and used to link to the ThingType, and it is desirable for the ThingType definition and the discovery information to be defined in a single place.\\nFunctionally, the ThingType definition, and Discovery information should be separate. The proposal here is to introduce a `DiscoveryIndex` service which provides the binding specific thing properties linked to a `ThingThypeUID`. `DiscoveryProperties` are provided by a `DiscoveryIndexProvider` which the binding can query.\\nThe `XmlThingTypeProvider` should be modified to also implement `DiscoveryIndexProvider`, and the ThingType XML definition should be updated to introduce a `discovery-properties` section which can include the discovery information to be passed to the binding. The `discovery-properties` section is similar to the existing Thing `properties` definition, but dedicated to the `DiscoveryIndex` function.\\n","Decision":"TBD\\n","tokens":263,"id":662,"Predictions":"Decision (A) The ThingHandler should implement dynamic definition by interrogating the device, or through a static definition via the Thing XML file. This may be implemented as a single ThingHandler for numerous different devices whose functionality is defined or discovered. In cases where the Thing is statically defined, there will be some discovery information that is retieved from the device and used to link to the ThingType, and it is desirable for the ThingType definition and the discovery information to be defined in a single place.nFunctionally, the ThingType definition, and Discovery information should be separate.nThe proposal here is to introduce a DiscoveryIndex service which provides the binding specific thing properties linked to a ThingThypeUID.nThe ThingType XML definition should be modified to also implement DiscoveryIndexProvider, and the ThingType XML definition should be updated to introduce a discovery-properties section which can include the discovery information to be passed to the binding.n Decision (A) The ThingHandler should implement dynamic definition by interrogating the device, or through a static definition via the Thing XML file. This may be implemented as a single ThingHandler for numerous different devices whose functionality is defined or discovered."}
{"File Name":"uqlibrary-reusable-components\/adr-001.md","Context":"## Context\\nThe UQ purple header is included in primo through include files like `assets.library.uq.edu.au\/primo-sand-box\/reusable-components\/`, via the primo BO.\\nUp until now we have been hard coding the primo-sand-box bit according to which environment we are in, and having to remember to manually change it depending on which environment we were uploading to.\\nManual processes are to be avoided.\\n","Decision":"Generate the branch to be used, by looking at the host name and the vid parameter on the url\\nhttps:\/\/github.com\/uqlibrary\/uqlibrary-reusable-components\/commit\/4f1c182\\n","tokens":87,"id":665,"Predictions":"The primary purpose of the project is to improve the quality of the primary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header.nThe primary purpose of the project is to improve the quality of the secondary header"}
{"File Name":"editions\/01-\ud83d\udeaf-pwa.md","Context":"## Context\\nThis needs to work cross platform.\\nThe UX will be rendered in browser, which may not appear native.\\n","Decision":"To implement Editions as a progressive web app, delivered in a native wrapper. The native wrapper will contain a webview, that renders the PWA.\\nThe wrapper will handle required features that are not supported by PWAs such as background fetch, scheduled downloads and push notification handling.\\n","tokens":25,"id":666,"Predictions":"Decision Makers should consider the following: nThe UX will be rendered in browser, which may not appear nativennnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"editions\/05-\u2705-archiver-s3-event.md","Context":"## Context\\nThe archiver lambda needs to be able to respond to issue published events from tools.\\n","Decision":"For the archiver to react to S3 file created events from the fronts tool.\\n","tokens":21,"id":667,"Predictions":"Decision Makers should consider the following options: a). The archiver should be able to respond to issue published events from tools. b). The archiver should be able to respond to issue published events from tools. c). The archiver should be able to respond to issue published events from tools. d). The archiver should be able to respond to issue published events from tools. e. The archiver should be able to respond to issue published events from tools. f. The archiver should be able to respond to issue published events from tools. g. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. The archiver should be able to respond to issue published events from tools. h. the archiver."}
{"File Name":"editions\/08-\u2705-releases.md","Context":"## Context\\nWe need a consistent and battle tested process for releasing the app on both Android and iOS.\\n","Decision":"### iOS\\n#### Internal Beta\\nOur internal Beta is managed through testflight on the Guardian developer account. The group which this beta is sent to is labelled `GNM`. This includes the team and internal stakeholders within the organisation. We build this automatically through Fastlane and Github Actions once a day. Occasionally we will set off builds to test things on a number of devices.\\nIn github actions we have a [scheduled build](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3Ascheduled-ios-beta) and an [ad-hoc one](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3A%22Upload+ios-beta%22) triggered by a [script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/upload-ios-build.sh)\\nAll builds generate a ['release' in github](https:\/\/github.com\/guardian\/editions\/releases) to help us keep track of build numbers against certain commits. This is handled by the [make-release script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/make-release.js).\\n#### External Beta\\nBefore every release, we aim to do at least one external beta to gather feedback. We have a number groups within testflight that are prefixed with the name `External Testers...`. These different groups represent the different authentication methods we support. When we decide a build is good enough from an internal test, we add the build to the groups.\\n#### Release\\nAfter a successful beta period, we release the same build (identified by its build number) through the app store submission process.\\n#### Post Release\\nWe update the version number in XCode and raise that as a PR. The version number will depend on the goals for the next release. We follow a major and minor number approach with no patch i.e. 5.6\\n### Android\\n#### Internal Beta\\nIn a similar vein as above, the Android internal beta is managed through Google Play. The APK for this is created using Fastlane through TeamCity. The name of this process is `android-beta-deploy`. The list for this is managed within the Google Play console. This process runs once per day. Users will need to update their app through their Google Play store.\\n#### External Beta\\nWe take a slightly different approach to iOS. Due to not being able to determine within the app whether or not the app is in Beta or release, we have a different build configuration for releasing to production - which hides the 'report bug' button.\\nWe build the APK using `android-beta-deploy`. This will release a build to google play to the 'internal beta' group for internal testing by the team. It is then manually promoted within the Google Play console to our external beta testers.\\nAs with iOS, releases to the play store can be tracked in [github releases](https:\/\/github.com\/guardian\/editions\/releases) - each time the teamcity build is run a new github release is created including the play store version code of that release.\\n#### Release\\nAfter a successful external beta test, we **DO NOT** promote the external beta. This is because we have code in the app that attempts to determine whether or not the user is in beta. This does not work on Android as there isn't a distinction.\\nAs a result, we then use the TeamCity process `android-release-deploy` to then build the APK. As you will only want to release a version that has been beta tested, you can use the [releases](https:\/\/github.com\/guardian\/editions\/releases) list to find a release for the build you want to release, and copy the tag for that build. You can then search for this tag in the branch list within teamcity (the `android-release-deploy` config treats github tags as if they were branchs) and run a build on that tag.\\nBe warned, this process will automatically release the new version of the app. You will then need to go into the Google Play console to update the release notes.\\n","tokens":22,"id":668,"Predictions":"Decision Makers should consider releasing the app on both Android and iOS.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"Predictions":"We need to make the data available to Ophan.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"editions\/02-\u2705-react-native.md","Context":"## Context\\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\\n","Decision":"To develop the editions app as a React Native application.\\n","tokens":46,"id":670,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"editions\/04-\u2705-paths.md","Context":"## Context\\nThe editions lambda needs to be able to identify specific versions of an issue.\\n","Decision":"To have two deployments of the backend, one for previewing, and a second for published issues.\\nThe published issues deployment will replace the issueid path parameter with source\/issueid.\\n`source` will identify which file in the published bucket will be retreived to form the issue on.\\n","tokens":19,"id":671,"Predictions":"Decision (A) The editions lambda needs to be able to identify specific versions of an issue.nbr>nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific versions of an issue.nbr>The editions lambda needs to be able to identify specific"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"Predictions":"The archiving process is not time consuming.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Sylius\/2020_03_03_api_translations.md","Context":"### Problem to solve\\nProvide unified way to manage translations for translatable entities in API.\\n### Possible solutions\\nThe translations from collection could be embedded as objects within a `ProductOption`\/`ProductOptionValue` resource\\n**or** provided by [IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)\\n### Decision and reasoning\\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","Decision":"Translations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\\ndo not provide any value alone.\\n","tokens":103,"id":673,"Predictions":"Is this the best solution?n### Solution:nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"Predictions":"Decision Makersn* Solution should not be limited by its convention. We may need to support two different actions of the same name,nbut with different business logic. E.g., one object may have two transitions with the same name but described byntwo different state machines.n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional fields during the execution of requested operations.n* Solution should provide a way of adding additional"}
{"File Name":"Sylius\/2021_02_05_unified_api_prefix.md","Context":"## Context and Problem Statement\\nInitial implementation of Unified API used `\/new-api` prefix, to aggregate all following endpoints. This prefix does not\\nclearly state the version of it and is not future-proof. At some moment of time our \"new api\" can become \"old api\". We should\\nhave clear guidance how to version our APIs.\\n","Decision":"As the underlaying technology, structure and content have changed significantly and taking into account easiness of first solution\\nthe decision is to go with the `\/api\/v2` endpoint path. In the future it does not block us from the usage of the `Accept`\\nheader in addition to this path, however it may be misleading for consumers.\\n#### References:\\n- https:\/\/github.com\/api-platform\/api-platform\/issues\/290\\n- https:\/\/github.com\/api-platform\/core\/issues\/972\\n- https:\/\/stackoverflow.com\/questions\/389169\/best-practices-for-api-versioning\\n- https:\/\/www.troyhunt.com\/your-api-versioning-is-wrong-which-is\/\\n- https:\/\/github.com\/api-platform\/core\/issues\/45\\n- https:\/\/github.com\/api-platform\/docs\/issues\/451\\n- https:\/\/github.com\/api-platform\/docs\/pull\/452\\n- https:\/\/api-platform.com\/docs\/core\/content-negotiation\/#supporting-custom-formats\\n","tokens":72,"id":675,"Predictions":"Decision (A).nIt is not clear how to version our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.nIt is not clear how to update our APIs.n"}
{"File Name":"Sylius\/2021_07_05_api_providing_locales_available_in_active_channel.md","Context":"## Context and Problem Statement\\nCustomer should have access only to locales available in their channel\\n","Decision":"Chosen option: Using Data Provider\\nShops shouldn't have many locales for each channel, so lack of a pagination is smaller problem than creating overcomplicated\\nquery in Doctrine Collection extension\\n","tokens":18,"id":676,"Predictions":"Decision was made that the customer should have access only to locales available in their channel.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Sylius\/2020_11_12_registration_of_handlers_in_message_buses.md","Context":"## Context and Problem Statement\\nWhile defining new handlers, we may define a bus to which particular handler should be assigned to. If not, handler will\\nbe added to all existing buses. Right now, we have only one bus in Sylius, so decision will not change current behaviour.\\nHowever, we may add additional buses in the future and more than one bus is not uncommon thing.\\n## Decision Drivers <!-- optional -->\\n* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\n","Decision":"* Driver 1 - easiness of changing default behaviour and usage of the bus\\n* Driver 2 - backward compatible possibility of changing behaviour\\nChosen option: \"[Explicitly declare, where bus should be registered]\", because this options leaves the most options to end user\\nwithout forcing him to use them. What is more, we may easily adjust this option in the future\\n","tokens":117,"id":677,"Predictions":"Decision n1 nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Sylius\/2020_03_03_feature_coverage_in_api.md","Context":"## Context and Problem Statement\\nWe want our API to cover all the functionalities that are currently implemented in the UI.\\n## Decision Drivers\\n* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\n","Decision":"* All the functionalities implemented for API should be tested\\n* Tracking whether a feature has been covered in the API or not should be easy\\nChosen *Using Behat for the feature coverage*, because it's the only option, that meets all the decision drivers criteria.\\nWe will gradually add `@api` tag to the scenarios currently tagged with `@ui` and then implement the API contexts.\\nAs a consequence, we will have to create a testing tool to use it in Behat contexts.\\n","tokens":56,"id":678,"Predictions":"Decision Makers: (i) The API should be tested. (ii) The API should be easy to use. (iii) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use. (iv) The API should be easy to use."}
{"File Name":"Sylius\/2020_03_03_api_product_option_values.md","Context":"### Problem to solve\\nCover with API all `ProductOption`s related functionality provided by the Admin panel.\\n### Context\\nDuring the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle\\nProduct Option's values collection, to make it efficient and easy to use.\\n### Possible solutions\\nValues from collection could be embedded as objects within a `ProductOption` resource **or** provided by\\n[IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)\\n### Decision and reasoning\\nProduct option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\\n(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) `ProductVariant`).\\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\\nresult in a massive and unmanageable response.\\n","Decision":"Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\\n(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) `ProductVariant`).\\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\\nresult in a massive and unmanageable response.\\n","tokens":215,"id":679,"Predictions":"Decision nValues from collection could be embedded as objects within a ProductOption resource ** or** provided byn[IRIs](https:\/\/en.wikipedia.org\/wiki\/Internationalized_Resource_Identifier)n### Decision and reasoningnProduct option values should always be provided with their IRIs. They can exist outside of the ProductOption resourcen(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) ProductVariant).nMoreover, values embedded together with their translations in the ProductOption that already has its translations wouldnresult in a massive and unmanageable response.n### Solution to this problem is to provide values embedded together with their translations in the ProductOption that already has its translations.n### Decision nThe IRIs should always be provided with the Product Option's values.n### Decision and reasoningnProduct option values should always be provided with their IRIs. They can exist outside of the ProductOption resourcen(e.g. be [related with](https:\/\/github.com\/Sylius\/Sylius\/blob\/master\/src\/Sylius\/Component\/Product\/Model\/ProductVariant.php#L44) ProductVariant).n"}
{"File Name":"Sylius\/2021_06_15_api_platform_config_customization.md","Context":"## Context and Problem Statement\\nSylius is by design application that will be adjusted to customer needs.\\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\\n","Decision":"Chosen option: \"Config merging\", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\\n","tokens":98,"id":680,"Predictions":"Decision (A) The current state of API Platform integration requires the replacement of the whole config of the whole resource.nBy design application that will be adjusted to customer needs.nTherefore each part of it has to be crafted with replaceability or customizability at its core.nNevertheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.nBy design application that will be adjusted to customer needs.nTherefore each part of it has to be crafted with replaceability or customizability at its core.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration requires the replacement of the whole config of the whole resource.nHowever, the current state of API Platform integration"}
{"File Name":"Sylius\/2020_11_18_sending_emails_via_api.md","Context":"## Context and Problem Statement\\nTo achieve 100% API coverage, we need to handle emails by API.\\n","Decision":"Chosen option: \"Using events\", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport.\\n","tokens":23,"id":681,"Predictions":"Decision (A).nTo achieve 100% API coverage, we need to handle emails by API.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Sylius\/2020_09_01_admin_and_shop_split.md","Context":"## Context and Problem Statement\\nWhile developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach\\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\\nwe have merged a unification of our endpoints in https:\/\/github.com\/Sylius\/Sylius\/pull\/11397. This move was dictated by\\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\\ndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\\nand simplify the relations between resources. However, this change rose a lot of new issues to solve:\\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\\nadmin can see many more links than the regular visitor or shop user.\\n* Endpoints like \"Product Show\" are resolved differently depending on the context as well. The shop products are determined\\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\\nseparation blurs the product identifier, and it is not clear which route should be considered as IRI.\\n* the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the\\ndecision that we want to be explicit, which user is trying to log in.\\nUsing one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able\\nto expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin\\nresources are channel and locale agnostic, and we should use `codes` instead.\\n## Decision Drivers\\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\n","Decision":"* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\nChosen option: \"Moving back to prefixed paths for back-office and shop\". This option was selected because it allows us to\\neasily leverage the Symfony Security component's benefits like firewalls and ACL. The only second and third option provides\\npredictable behavior and clear separation of concerns for admin and shop. Still, suffixes on most of the routes seem like\\na worse solution compared to the common prefix. The common prefix will indicate that these endpoints are grouped in the\\nsame context.\\n","tokens":476,"id":682,"Predictions":"Driver 3: A clear separation of available resources and paths for visitor and admin may be expectednnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Sylius\/2021_04_15_using_iri_as_api_resource_identifier_in_request_instead_of_code_id.md","Context":"## Context and Problem Statement\\nAPI Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.\\nWhile we were designing Sylius new API, we have decided to use commands in many endpoints.\\nIn some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`\/`id`.\\nIn the past, we have been using `code`\/`id` instead of IRI, then we have been using both approaches.\\nNow we are trying to unify the new API and replace codes and ids with IRI everywhere.\\nThe main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`\/`code`in its commands and command handlers.\\n","Decision":"Chosen option: \"Handling and transforming IRI to `id`\/`code`\". Request that is based on command and needed information like `code`\/`id` should get it as IRI\\n","tokens":208,"id":683,"Predictions":"Decision (A).nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A."}
{"File Name":"va.gov-team\/0006-scale-vagov-content-gql-not-cms-export.md","Context":"## Context\\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\\n","Decision":"Halt implementation on CMS Export and focus on scaling GraphQL.\\n","tokens":69,"id":684,"Predictions":"Decision (A) The CMS Export initiative was not expected to be completed in time for planned product (VAMC) rollout.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"va.gov-team\/0002-deploy-pact-on-aws.md","Context":"## Context\\nThe Pact Broker has been on Heroku during development, but its database is running out of rows for the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\\n","Decision":"We will move the Pact Broker onto our AWS infrastructure.\\n","tokens":51,"id":687,"Predictions":"The Pact Broker will be moved onto AWS infrastructure.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"va.gov-team\/0007-remove-user-model-va-profile.md","Context":"## Context\\nAs part of the larger effort to [refactor the User & Identity on vets-api](https:\/\/app.zenhub.com\/workspaces\/vsp-identity-5f5bab705a94c9001ba33734\/issues\/department-of-veterans-affairs\/va.gov-team\/21012), we have decided to more tightly control access to user attributes through specific & improved getter methods. The `va_profile` methods on the User class revealed too much information about a user's MPI profile and has been rendered uneccessary by the updated getter methods.\\n","Decision":"The decision made is to transfer the User's `va_profile_status` and `va_profile_error` methods to `mpi_status` and `mpi_error` since that is what they are abstractions of, and to remove the `va_profile` method entirely in favor of relying on getter methods for specific attributes.\\n","tokens":117,"id":688,"Predictions":"Decision (A). The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be refactored.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.nbr>The user class should be re-engineered.n"}
{"File Name":"va.gov-team\/0004-return-uninflected-va-keys.md","Context":"## Context\\nUsing the Rails 6 zeitwerk file loader requires strict naming conventions of files. So without a Rails inflection, `va_something` class files are expected to have the class name of `VaSomething`. With an inflection, that class name would be `VASomething`. `vets-api` uses a mix of `VA` and `Va` in class names. To make things consistent, as well as support instances of `humanize` outputing `VA` in caps as an acronym, the inflection will be introduced.\\nHowever, using this inflection has a downstream affect on json responses transformed into camelCase by the [olive_branch](https:\/\/github.com\/vigetlabs\/olive_branch) gem. Previously keys structured like `the_va_something` were transformed into `theVaSomething`, but with inflection they will now be `theVASomething`.\\n","Decision":"Create a patch implemented for the olive_branch middleware that returns keys containing `VA` as if \"VA\" was not an acronym.\\nIn the future vets-api should introduce a version that doesn't support olive_branch.\\n","tokens":185,"id":689,"Predictions":"Decision (A) To make things consistent, as well as support instances of humanize outputing VA in caps as an acronym, the inflection will be introduced.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"va.gov-team\/0001-record-architecture-decisions.md","Context":"## Context\\nWe want to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described in this Confluence document](https:\/\/vfs.atlassian.net\/l\/c\/58vSL6ZP).\\n","tokens":16,"id":690,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"qc-atlas\/0009-joined-table-for-knowledge-artifact.md","Context":"## Context and Problem Statement\\nThe class `KnowledgeArtifact` is a helper base class that was not intended to have its own database table (see Issue [#182](https:\/\/github.com\/UST-QuAntiL\/qc-atlas\/issues\/182)).\\nThe current implementation as a joined table generates a database table.\\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\\n## Decision Drivers <!-- optional -->\\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\n","Decision":"* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\\nChosen option: \"[Joined table]\", because it is already implemented, allows for references in both ways and has no significant downside.\\n### Positive Consequences <!-- optional -->\\n* The current implementation can stay\\n","tokens":107,"id":691,"Predictions":"Decision Makers!-- optional -->n* The KnowledgeArtifact table is referenced by a foreign key from DiscussionTopicn ## Decision Making Processnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"qc-atlas\/0007-junit-for-testing.md","Context":"## Context and Problem Statement\\nSince the project started out with both JUnit4 and JUnit5, we only want one unit-testing framework in order to make testing writing consistent.\\n","Decision":"Chosen option: \"[JUnit5]\", because it is the newer version and therefore has a higher maturity and a wider feature-set.\\n### Positive Consequences <!-- optional -->\\n* Uniform tests\\n* More and newer features\\n","tokens":37,"id":692,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"Predictions":"Decision (A) The API must be documented in human and machine readable form.n* The API must be documented in machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in human and machine readable form.n* The API must be documented in machine readable form.n* The API must be documented in human and machine readable form.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable.n* The driver must be readable."}
{"File Name":"qc-atlas\/0003-model-mapper.md","Context":"## Context and Problem Statement\\nDue to the use of two different object types for transfering and storing data, the conversion between the types becomes a necessity. Therefore we need means to perform the respective conversion.\\n","Decision":"Chosen option: \"[Model Mapper](http:\/\/www.modelmapper.org)\", because it fulfils our requirements, is highly configurable and less error prone then manually written converters.\\n### Positive Consequences <!-- optional -->\\n* Less boilerplate code\\n","tokens":42,"id":694,"Predictions":"Decision (A) The two types of data are transferred to different locations.nB) The two types of data are transferred to different locations.nC) The two types of data are transferred to different locations.nD) The two types of data are transferred to different locations.nAnswer:A.nB.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.n"}
{"File Name":"qc-atlas\/0004-postgres-for-service-tests.md","Context":"## Context and Problem Statement\\nTo ensure high test coverage within the project, functionalities that interact with the database must be tested as well.\\nThere are different approaches to testing these functionalities that require a database running.\\n","Decision":"Chosen option: \"Use Production-like [Postgres](http:\/\/www.postgresql.org) database\", because it ensures a production-like behavior.\\n### Positive Consequences <!-- optional -->\\n* Almost same configuration for test and runtime.\\n* Less database technologies used.\\n* Guaranteed that production and tests behave the same.\\n","tokens":43,"id":695,"Predictions":"Decision (A).nThe database must be running.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"qc-atlas\/0006-model-assemblers.md","Context":"## Context and Problem Statement\\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\\nConstructing such objects, as well as adding the desired links to them is a common operation that\\nrequires entity-specific boilerplate code.\\nHow can duplicate code in nearly all controller methods be avoided?\\n## Decision Drivers <!-- optional -->\\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\n","Decision":"* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\\nand our DTO classes.\\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\\n","tokens":97,"id":696,"Predictions":"Decision Makers!-- optional -->n* Avoid duplicate code to create HATEOAS modelsn* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Avoid duplicate code to create HATEOAS modelsn* Decision Makers!-- optional -->n* Avoid duplicate code to create HATEOAS modelsn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn* Decision Makers!-- optional -->n* Decouple link creation from normal entity logicn"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"Predictions":"Decision (A) The components in the PlanQK platform should be splitted into individual repos.n.n2(ii) The components in the PlanQK platform should be splitted into individual repos.n2(iii) The components in the PlanQK platform should be splitted into individual repos.n2(iv) The components in the PlanQK platform should be splitted into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos.n2(iv) The components in the PlanQK platform should be split into individual repos."}
{"File Name":"qc-atlas\/0009-remove-hal-links.md","Context":"## Context and Problem Statement\\nSince we use the OpenAPI for frontend client code generations, the HATEOAS links are no longer needed or used.\\n## Decision Drivers\\n* readable API documentation\\n* development effort to test\/gather the HATEOAS links\\n","Decision":"* readable API documentation\\n* development effort to test\/gather the HATEOAS links\\nChosen option: remove the HATEOAS links from all entities, because this further simplifies the DTO entities\\n","tokens":55,"id":698,"Predictions":"Decision (A) The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer needed or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used.n## Decision The HATEOAS links are no longer used or used."}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"Predictions":"Decision (A) To allow the container to be manually launched or started automatically. (B) To allow the container to be manually launched or started automatically. (C) To allow the container to be manually started. (D) To allow the container to be manually launched or started. (E) To allow the container to be manually started. (F) To allow the container to be manually launched. (A) To allow the container to be manually started. (A) To allow the container to be manually launched. (B) To allow the container to be manually started. (C) To allow the container to be manually launched. (D) To allow the container to be manually started. (A) To allow the container to be manually launched. (A) To allow the container to be manually started. (B) To allow the container to be manually launched. (A) To allow the container to be manually launched. (B) To allow the container to be manually launched. (C) To allow the container to be manually launched. (D) To allow the container to be manually launched. (A) To allow the container to be manually launched. (A) To allow the container to be manually launched. (B) To allow the container to be manually launched. (A) To allow the container to be manually launched. (B) To allow the container to be manually launched. (C) To allow the container to be manually launched. (D) To allow the container to be manually launched. (A) To allow the container to be manually launched."}
{"File Name":"qc-atlas\/0001-use-URI-for-entities.md","Context":"## Context and Problem Statement\\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\\nWe need references for the links.\\n","Decision":"Chosen option: \"[URIs]\", because UUIDs are generated and thus depend on the underlying database system.\\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\\n### Positive Consequences <!-- optional -->\\n* We follow solid [W3C specification](https:\/\/www.w3.org\/Addressing\/URL\/uri-spec.html)\\n","tokens":39,"id":700,"Predictions":"Decision (A) The QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.nWe need references for the links.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"Predictions":"Decision based upon the following: a. The project should be a public-private partnership. b. The project should be a public-private partnership. c. The project should be a public-private partnership. d. The project should be a public-private partnership. e. The project should be a public-private partnership. f. The project should be a public-private partnership. g. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. e. The project should be a public-private partnership. f. The project should be a public-private partnership. h. The project should be a public-private partnership. g. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. e. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. e. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. e. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. e. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. h. The project should be a public-private partnership. i. The project should be a public-private partnership. ii. The project should be a public-private partnership. iii. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. v. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. v. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. v. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. v. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. h. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. iv. The project should be a public-private partnership. v. The project should be a public-private partnership. iv. The project should be a public-private partnership. v. The project should be a public-private partnership. iv. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership. v. The project should be a public-private partnership"}
{"File Name":"radiant-mlhub\/0002-resolving-api-keys.md","Context":"## Context\\nWe need a convenient system for managing API keys used by the Python client. This system should give the user multiple options for\\nproviding an API key to be used when making a request to the API. These options should include:\\n* Storing API keys on the users system\\n* Reading an API key from the environment\\n* Passing an API key directly to the API request methods\\nUsers may have multiple valid API keys associated with their account at any given time. The system for storing API keys on the user's\\nsystem must accommodate this and provide a clear, deterministic way of resolving an API key for a given project.\\nWe anticipate the need to store other data related to Radiant MLHub for uses unrelated to authentication. For instance, we may have a need to\\ntrack the progress of downloads so that they can be resumed if interrupted, or we may want to specify a base URL in a config file so that\\ndevelopers can test against the staging environment. The method that we choose for storing API keys on the user's system must not preclude\\nus from storing this additional information.\\n","Decision":"The Python client will resolve the API key to be used in a request in the following order:\\n1) Passing an `api_key` argument directly to the method\\n2) Setting an `MLHUB_API_KEY` environment variable\\n3) Passing a `profile` argument directly to the method. This will read the API key from the given profile (see below for details)\\n4) Setting an `MLHUB_PROFILE` environment variable. This will read the API key from the given profile (see below for details)\\n5) Using the API from the `default` profile\\nProfiles will be stored in a `.mlhub\/profiles` file in the user's home directory. This file will be an INI file containing at least a\\n`[default]` section with an `api_key` value. The file may contain other sections corresponding to named profiles. Any `profile` argument\\npassed to a method must correspond to one of these section names, or it will raise an exception.\\n","tokens":228,"id":702,"Predictions":"Decision (A) The user should be able to store API keys on the user's system. (B) The user should be able to store other data related to Radiant MLHub. (C) The user should be able to store other data related to Radiant MLHub. (D) The user should be able to store other data related to Radiant MLHub. (A) The user should be able to store other data related to Radiant MLHub. (A) The user should be able to store other data related to Radiant MLHub. (A) The user should be able to store other data related to Radiant MLHub. (B) The user should be able to store other data related to Radiant MLHub. (C) The user should be able to store other data related to Radiant MLHub. (D) The user should be able to store other data related to Radiant MLHub. (A) The user should be able to store other data related to Radiant MLHub. (A) The user should be able to store other data related to Radiant MLHub. (A) The user should be able to store other data related to Radiant MLHub. (B) The user should be able to store other data related to Radiant MLHub. (C) The user should be able to store other data related to Radiant MLHub."}
{"File Name":"radiant-mlhub\/0003-will-not-implement-get-items.md","Context":"## Context\\nThe Radiant MLHub API implements the `\/items` endpoint as described in the [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)\\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\\n","Decision":"To avoid a confusing user experience when working with Items, and to avoid inadvertently swamping the API with requests, we will not provide\\na method in either the low-level client or on the `Collection` classes to loop over the items in a collection. Preliminary work had adapted\\nthe [`Collection.get_items`](https:\/\/pystac.readthedocs.io\/en\/latest\/api.html#pystac.Catalog.get_items) method to make paginated requests to\\nthe `\/items` endpoint. Instead, this method will raise a `NotImplementedError` to indicate that this feature is not available.\\n*Work is planned to add an endpoint to the Radiant MLHub API to enable downloading a single archive containing all items associated with a\\nCollection. Support for this endpoint in the Python client may be the subject of a separate ADR.*\\n","tokens":153,"id":703,"Predictions":"Decision (A) The Radiant MLHub API implements the \/items endpoint as described in the [STAC API - Features](https:\/\/github.com\/radiantearth\/stac-api-spec\/tree\/master\/ogcapi-features)ndocumentation for retrieving the STAC Items associated with a given Collection.n Since this is a paginated endpoint with an opaque next token,npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collectionnmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining thentotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.n ## Final answer: No.n[1].n[2].n[3].n[4].n[5].n[6].n[7].n[8].n[9].n[10].n[11].n[12].n[13].n[14].n[15].n[16].n[17].n[18].n[19].n[20].n[21].n[22].n[20].n[22].n[20].n[22].n[20].n[22].n[20].n[22].n[20].n[22].n[20].n[22].n[20].n[22].n[20].n[22].n[20].n[22]."}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"auth-account-koa\/0002_use_lodash.md","Context":"## Context\\nThere is the problem where to store different utilities and write every time the same code on all projects.\\n","Decision":"Use [lodash](https:\/\/lodash.com\/docs\/) possibilities as main package for utilities on the JS microservices, in order to spend less time and write less code. And use utils directory as a wrapper for it in order to be able to replace it any time.\\n","tokens":24,"id":705,"Predictions":"Decision Makers should store the utilities in a single location.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"auth-account-koa\/0001_use_adr.md","Context":"## Context\\nA way to document and track my design choices is needed, for myself in order to see how project architecture is changing.\\n","Decision":"I will use ADR to document any important architectural decisions I make.\\nADRs will be checked into the repository as numbered md files in the folder docs\/architecture\/adr\\nI will follow the template described [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\nAll Records will contain, Title, Context, Decision, Status and Consequences\\nIf a decision is changed or overruled we don't delete the record, but change the status accordingly (to superseded or deprecated).\\nIf a decision is superseded or deprecated we should add a link to the new decision. In the format Superseded by [link]\\n","tokens":28,"id":706,"Predictions":"Decision Makers should be able to access the record.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pfb-network-connectivity\/adr-0004-asynchronous-task-queue-2.md","Context":"## Context\\nIn ADR0003, we described the 'Bicycle Network Analysis' task to be run via an asynchronous task queue. Since then, Amazon Web Services (AWS) released a new service simply named 'Batch'. This service provides a managed task queue, with Boto and HTTP API interfaces for creating queues and jobs, and triggering new jobs. Each job in AWS Batch is configured to run a Docker container provided to the job configuration. AWS Batch manages ordering and execution of tasks in the queue. In almost every way, AWS Batch is a superior choice to the strategy outlined in ADR 0003, for a few key reasons:\\n- AWS Batch manages the queue and task autoscaling without any management from the parent application. The service can be trivially configured to scale up or down on a few different resource considerations. If there are no jobs in the queue, the pool of workers will automatically scale to zero, saving on hosting costs.\\n- AWS Batch, in comparison with a manually managed stack of celery workers + broker + result backend, is easy to configure, as it only requires defining a \"worker\" stack via a JSON cofiguration.\\n- Switching from a Celery and ECS task based solution will be easy, as AWS Batch workers are configured with Docker containers in the same way as ECS tasks would be\\n- It will be easier to trigger jobs from Django using AWS Batch, since direct calls can be made via Boto, rather than having to write some management layer to trigger ECS tasks or work with the Celery API.\\n","Decision":"The team will build the Bicycle Network Analysis task queue on AWS Batch. The reduction in manual task queue management and ease of configuration should vastly outweigh having to learn how to develop applications using an unfamiliar service. While relatively new, AWS Batch has support in both Boto and via HTTP API and manual setup of a Batch stack was relatively straightforward.\\n","tokens":316,"id":707,"Predictions":"Decision (A) The AWS Batch service should be used to run the Bicycle Network Analysis task. (B) The AWS Batch service should be used to run the Django task. (C) The AWS Batch service should be used to run the Celery task. (D) The AWS Batch service should be used to run the Django task. (A) The AWS Batch service should be used to run the Django task. (A) The AWS Batch service should be used to run the Django task. (B) The AWS Batch service should be used to run the Django task. (C) The AWS Batch service should be used to run the Django task. (D) The AWS Batch service should be used to run the Django task. (A) The AWS Batch service should be used to run the Django task. (A) The AWS Batch service should be used to run the Django task. (B) The AWS Batch service should be used to run the Django task. (C) The AWS Batch service should be used to run the Django task. (D) The AWS Batch service should be used to run the Django task. (A) The AWS Batch service should be used to run the Django task. (A) The AWS Batch service should be used to run the Django task. (B) The AWS Batch service should be used to run the Django task."}
{"File Name":"pfb-network-connectivity\/adr-0000-architecture-documentation.md","Context":"## Context\\nWe need a way to document major architecture decisions; in the past we have used the [Architecture Decision Record (ADR) format](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). On past projects, we have found the ADR format to be a useful way to write and manage architecture decisions.\\nWe have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST \/ Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\\n","Decision":"We have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST \/ Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\\nWe will continue to use the ADR format for writing architecture decisions for this project. We will use Markdown for formatting ADR documents.\\n","tokens":141,"id":708,"Predictions":"We will use Markdown for the ADRs.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pfb-network-connectivity\/adr-0003-asynchronous-task-queue.md","Context":"## Context\\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into a PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\\n- Download related census block and job data for the boundary and import to PostgreSQL\\n- Generate a network graph from the imported data\\n- Run a series of client-provided analyses on the graph and imported data\\n- Export relevant data to an external file store for archival\\n- Generate a tile set of the network graph for display on a web map\\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\\nCelery has multiple options for brokers:\\n| Broker | Advantages | Disadvantages |\\n| ------ | ---------- | ------------- |\\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http:\/\/docs.celeryproject.org\/en\/latest\/getting-started\/brokers\/sqs.html#caveats) |\\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\\n","Decision":"The team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\\n","tokens":415,"id":709,"Predictions":"Decision based on the following: The project dependencies and resources required are not conducive to that environment.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pfb-network-connectivity\/adr-0002-backend-language-framework.md","Context":"## Context\\nA backend web framework significantly eases the amount of boilerplate necessary for an application to handle web requests. Additionally, it provides scalable, out of the box, well-tested solutions for common features such as user management, authentication, database interaction, and a public API.\\nThis project will mostly require only the common components described above, so a backend web framework that we are familiar with will speed development for the project. The only unknown is the integration of the asynchronous 'Bicycle Network Analysis' task mentioned in [ADR 0001](adr-0001-development-environment.md)\\nThe team is most familiar with Python, Django and the Django Rest Framework. Due to project constraints and the desired functionality, no other backend frameworks were considered for this project.\\n","Decision":"The team will use Django with the Django Rest Framework plugin, written in Python. The team's familiarity with this stack is too much of a positive to pass up. In addition, Django provides many third-party solutions for integrating the asynchronous 'Bicycle Network Analysis' task. This allows the team to be flexible when choosing a solution, without sacrificing development efficiency.\\n","tokens":154,"id":710,"Predictions":"Decision (A).nA backend web framework is needed for this project.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.nB. The team is familiar with Python, Django and the Django Rest Framework.n"}
{"File Name":"pfb-network-connectivity\/adr-0001-development-environment.md","Context":"## Context\\nThis application will require a fairly standard application stack - web server, database store and an asynchronous task queue. In the past, the team has used either Vagrant + Ansible or Docker + Docker Compose to build these application stacks.\\nIn general, Docker containers improve the CI build and deployment workflows, at the expense of a slightly more complicated development environment, especially for users not using Linux. In the past, the team has wrapped the Docker environment within a barebones Ubuntu VM using Vagrant to streamline the development workflow across different OSes. Recently however, Docker released a new tool, Docker for Mac, which attempts to streamline native use of containers on macOS. This tool may eliminate the need for the wrapper VM, but has some potential pitfalls.\\nPrevious projects defaulted to an Ubuntu VM, with the containers using the simplest debian-based OS. Azavea maintains a series of Docker containers that provide the building blocks for the application stack we will be building.\\nThis project contains a 'Bicycle Network Analysis' task which is runs on a self-contained PostgreSQL instance. Running this via Vagrant+Ansible or a Docker container should be relatively straightforward either way, since the task has known software dependencies and does not have any external dependencies. When this project begain, this task was configured via Vagrant+Ansible. Some additional work would be necessary to convert this task to a Docker container.\\n","Decision":"In order to take advantage of the better deployment and provisioning workflows provided by AWS ECS when using containers, we decided to construct the development environment using a Docker Compose environment wrapped within an Ubuntu VM. While Docker for Mac looks compelling, it has a few downsides:\\n- We cannot control the version of Docker installed, which could be problematic as the project ages\\n- There are potential incompatibilies for users with the older Docker Toolbox installed\\n- It may be difficult to cull outdated container images across projects\\n- It may be difficult to isolate various project instances and their dependencies\\nUsing the wrapper VM avoids these issues and provides us with a relatively 'known good' experience for a project with somewhat limited budget constraints.\\n","tokens":284,"id":711,"Predictions":"The team will use Docker containers to build the application stack.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Corvus.Tenancy\/0001-tenants-will-not-inherit-their-parents-properties.md","Context":"## Context\\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\\nFirstly, we can control on a per-tenant basis where the child tenant data for that tenant are stored. For example, with two sibling tenants A and B (i.e. tenants that are children of the same parent tenant), the data for the child tenants of A can be stored in a completely different location to that of B. By default, this will be a separate container in the same storage account, but it could be a completely separate storage account.\\nSecondly, it can be used to enable better organisation of tenants by using parent tenants to group related tenants together.\\nOne of of the functions of tenants is to hold client-specific configuration for the applications that a client is using. An example would be for a client using the Workflow service. Their tenant will contain two pieces of storage information, one for Workflows and one for Workflow instances. This configuration is stored in a collection of key-value pairs attached to the tenant.\\nIt is possible for tenants to have child tenants in the hierarchy. If a tenant that uses the Workflow service has children, they may also need to use the Workflow service. In this case we have a choice: we can decide that we will allow the workflow storage configuration from a tenant to be inherited by its children, or we can require each tenant to contain all of its own configuration.\\n","Decision":"We have determined that we will not make properties of tenants available to their children by default. Applications which consume this library can implement that functionality for themselves if required - for example, by manually copying properties from parent to children when new tenants are created.\\nWhilst property inheritance seems desirable from a development perspective - for example, creating temporary tenants for testing purposes, or setting up tenants for developers - it is likely to be less useful in envisaged production scenarios.\\nIn the case when hierarchy is used for organisational purposes, inheritance is not relevant; parent tenants are there solely to group their children and configuration for the parent tenant is irrelevant, as it does not exist to be used as a tenant in its own right.\\nIn the case where hierarchy represents a genuine parent-child relationship there are many potential reasons for this, and the goal of the project is not to dictate specific use cases. However, in making the decision not to implement property inheritance it is only necessary to find a use case where it is not desirable.\\nOur use case here is a PaaS product providing multiple services - endjin's Marain platform. This platform contains several base services - Tenancy, Workflow, Operations and Claims - which can be licensed by clients.\\nA client may choose to use these services to build their own platform, and use Marain's tenancy service to provide their own platform services to their own customers. In this case, the client's customers will be represented by child tenants of its own tenant.\\nIn this situation there are two negative outcomes from allowing configuration to inherit from parent to child tenants.\\n1. The client may make use of Marain services (e.g. Workflow) to provide services to its customers. Configuration for these services is stored as configuration on the client tenant. Automatic property inheritance would mean that by default, child tenants of the client would also have the ability to access these services, which should not be the case.\\n1. The configuration attached to a client's tenant contains various pieces of sensitive information. For example, it may contain storage account details for storage that is not directly owned by the client. For this reason, Marain does not allow clients to view their own configuration data, or that of their parents. However, clients do need to be able to view and modify the configuration of child tenants. If we automatically allowed properties to be inherited by child tenants, it would be possible for a client to create a child tenant and examine those inherited properties to access what is effectively the client's own configuration data.\\n","tokens":294,"id":712,"Predictions":"We can allow the workflow storage configuration from a tenant to be inherited by its children, or we can require each tenant to contain all of its own configuration.n12n2n3n4n5n6n7n8n9n9n9n9n9n8n9n9n9n9n8n9n9n9n8n9n9n9n8n9n9n8n9n8n9n8n9n8n9n8n9n8n9n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Corvus.Tenancy\/0002-stable-tenant-ids.md","Context":"## Context\\nUp until now, tenant Ids have always been automatically generated. As we have used Corvus.Tenancy as part of Marain, we have come across scenarios where it is useful to control what a tenant's Id will be. This primarily applies to the tenants we use as top-level containers for Client and Service tenants, as well as the service tenants themselves.\\nIn the absence of the ability to know tenant Ids in advance we have fallen back on well known names for these tenants, effectively introducing an alternative well-known Id for them. As well as introducing a potential point of failure due to the need to keep these names unique, it is far less efficient to locate a tenant by name than by Id.\\n","Decision":"To avoid the need to effectively introduce an alternative way of identifying tenants, we will allow the Id of tenants to be controlled when they are created.\\nThis will be done by adding a new method to the `ITenantProvider` interface: `CreateWellKnownChildTenantAsync`. This allows callers to specify a GUID which is used to generate the new tenant's ID, rather than generating a random GUID internally (which is what the existing `CreateChildTenantAsync` method does).\\nThe resulting Id of the new tenant will still be generated by concatenating the parent tenant's Id with the hash of the provided GUID. This means that in order for a tenant to have a well known Id, all of its ancestors must also have well-known Ids.\\nIn order to prevent two tenants being created with the same Id, it is necessary to check that the Id is not already in use prior to creating the new tenant.\\n","tokens":148,"id":713,"Predictions":"Decision (A) To introduce a new tenant Id for the Marain building. (B) To introduce a new tenant Id for the Marain building. (C) To introduce a new tenant Id for the Marain building. (D) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (B) To introduce a new tenant Id for the Marain building. (C) To introduce a new tenant Id for the Marain building. (D) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (A) To introduce a new tenant Id for the Marain building. (B) To introduce a new tenant Id for the Marain building. (C) To introduce a new tenant Id for the Marain building. (D) To introduce a new tenant Id for the Marain building."}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"Predictions":"Decision (A) The v3 architecture must be a clear path of how a system will be upgraded.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.nbr>Answer: A.n"}
{"File Name":"dropapp\/adr_auth0.md","Context":"## Context or Problem Statement\\nOur handling of user data is far from bullet-proof. We have stumbled upon cases where SQL injection is possible and the current password encryption is only md5. Additionally, we are building a new mobile app in React and Flask and need to implement a way to handle authentication there.\\n## Decision Drivers\\n1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\n","Decision":"1. Security \/ Risk for us\\n2. Ease of Use\\n3. Cost\\nWe are going for Auth0 since\\n- We have prior experience with Auth0 in the team\\n- a first test in an afternoon coding session were satisfying\\n- Auth0 offers a free plan for Open-source projects\\n- Auth0 is one of the product leaders in CIAM (Visit Gartner for detail at [https:\/\/www.g2.com\/categories\/customer-identity-and-access-management-ciam#grid] - Read recent Leadership Compass Identity API Platforms By KuppingerCole [https:\/\/static.carahsoft.com\/concrete\/files\/6315\/6840\/5267\/kuppingercole-leadership-compass-identity-api-platforms.pdf])\\n- Using Auth0 management API and automatic migration and bulk import it can be integrated to PHP app\\n- Auth0 offers full featured authentication and authorization solution and it's capable of customized rules and flows.\\n- We are not building our own authentication solution to reduce the security risks coming with handling of passwords.\\n","tokens":84,"id":716,"Predictions":"The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in React and Flask. The solution is to implement a way to handle authentication in Flask."}
{"File Name":"read-more-api\/0005-use-docker.md","Context":"## Context\\nVisual Studio 2017 added support for packaging applications using Docker and running them using Docker Compose.\\n","Decision":"We will use Docker for packaging and running the application in a Linux container.\\n","tokens":24,"id":717,"Predictions":"Decision Makers should consider the following: - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system. - The package management system should be able to handle the package management system."}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"Predictions":"The application will be deployed on a cluster of machines running Docker containers. The application needs to be deployed somewhere.nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container_registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.nHeroku provides a [Container Registry](https:\/\/devcenter.her"}
{"File Name":"read-more-api\/0011-backlog-email-address.md","Context":"## Context\\nOne of the sources for links that are eventually saved to Pocket are email newsletters. However, a user often does not want to put all\\narticles from a newsletter in to Pocket. So, newsletters waiting to be reviewed can clutter an email inbox.\\nA user could be provided with an email address that they could send emails to. Links would then be extracted from a received email\\nand be added to a backlog. A webpage would need to be provided for users to view this backlog, which could be part of the existing\\nchrome extension.\\nHandling inbound email would require the use of a transactional email service, such as MailGun, that can POST to an API endpoint\\nwhen an email is received.\\nThe recipient email address, containing a unique identifier associated with a particular user, would be used to determine what list to add\\narticles to.\\nA protected account ID in the email address is likely to be too long for users to want to use.\\nUsing an unprotected account ID in the email address for a user would expose the plaintext and cipher, which could make it easier for an\\nattacker to determine the private key material.\\nTo prevent both of the above issues, a new UUID would need to be generated for use in the email address for a user.\\nEmails would consist of a variety of formats, which may cause issues in identifying links.\\nSome links would not be relevant and would need to be removed (i.e. unsubscribe links).\\nSome emails contain a link to a webpage containing the same content, in case the user's email client is not able to correctly render the\\nemail. This link could be added to Pocket, rather than the links within the email. However, there is not a common format to identifying\\nthis email address.\\nOther services, such as If This Then That, allow triggers to be setup where links can be added to Pocket when an email is received.\\nThe suggested backlog approach may be viewed as unnecessary by users. It may be easier for them to add to Pocket and filter while\\nbrowsing through their list of articles. Linked with the difficulties in parsing emails, this feature could be difficult to implement\\nand be of little actual use.\\n","Decision":"We will not provide users with an email address they can send emails containing articles to.\\n","tokens":447,"id":719,"Predictions":"Decision (A) The proposed backlog approach may be viewed as unnecessary by users. It may be easier for them to add to Pocket and filter while browsing through their list of articles.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"read-more-api\/0009-service-layer.md","Context":"## Context\\nA Controller is responsible for receiving a request, executing it and returning an appropriate response.\\nA service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.\\n","Decision":"We will use a service layer to ensure that Controllers do not contain business logic.\\n","tokens":51,"id":720,"Predictions":"Decision based on the information in the record.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate response.nA Controller is responsible for receiving a request, executing it and returning an appropriate"}
{"File Name":"read-more-api\/0008-protect-exposed-ids.md","Context":"## Context\\nTo access and modify a user's articles stored in Pocket, they have to give us permission. Once permission has been granted, we have to be careful not to leak this privillaged access to malicious parties.\\nASP.NET Core has [Data Protection APIs](https:\/\/docs.microsoft.com\/en-us\/aspnet\/core\/security\/data-protection\/introduction) for managing the encryption and decryption of data. These APIs requires the private key data to be persisted in a location where it will not be destroyed when the application is restarted.\\n","Decision":"We will use the encrypted ID of a PocketAccount entity as an access token, representing the right to access a particular user's articles.\\nWe will encrypt the IDs of Pocket articles returned in API responses, using the ASP.NET Core Data Protection APIs.\\nWe will store the private key material in the database.\\n","tokens":108,"id":721,"Predictions":"Decision (A) To allow the user to access the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nC) To allow the user to access the articles stored in Pocket.nD) To allow the user to modify the articles stored in Pocket.nAnswer (A) None of the above choices.nB) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nC) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nC) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nC) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nC) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nC) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.nB) To allow the user to modify the articles stored in Pocket.n"}
{"File Name":"read-more-api\/0002-use-asp-net-core.md","Context":"## Context\\nWith the introduction of .NET Core, we need to decide whether to use ASP.NET with .NET v4.x or ASP.NET Core.\\n","Decision":"We will use ASP.NET Core.\\n","tokens":32,"id":722,"Predictions":"Decide whether to use ASP.NET with.NET v4.x or ASP.NET Core.nbr>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>No Answer>n\/br>"}
{"File Name":"read-more-api\/0003-use-postgresql.md","Context":"## Context\\nA particular database technology needs to be chosen for the storage of data.\\nHistorically Microsoft's SQL Server has been the default choice for ASP.NET applications. SQL Server could only be run on a machine running Windows until the release of SQL Server 2017.\\nPostgreSQL is a popular choice for use with other web frameworks (i.e. Rails) and is widely used on a range of platforms, including Linux, MacOS and Windows.\\nPostgreSQL is open source and free to use for commercial use. SQL Server has a free version for development purposes but require the purchase of a license for commercial use.\\n","Decision":"We will use PostgreSQL for the storage of data.\\n","tokens":126,"id":723,"Predictions":"Decision (A) The database technology should be chosen for the storage of data.nB) The database technology should be chosen for PostgreSQL.nC) The database technology should be chosen for PostgreSQL.nD) The database technology should be chosen for PostgreSQL.nAnswer (A).nB.nB.nC.nD.nAnswer (B).nAnswer (C).nAnswer (D).nAnswer (A).nAnswer (B).nAnswer (C).nAnswer (D).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A).nAnswer (A)."}
{"File Name":"read-more-api\/0010-feature-toggles.md","Context":"## Context\\nWhen releasing some features, we might want to only make them available to a sub set of users initially, to gain feedback and reduce the potential impact of bugs.\\nWe also want to be able to continue development of a feature in master over a longer period of time, without it being available in an unfinished state.\\n","Decision":"We will associate a set of feature toggles with a Pocket Account.\\n","tokens":67,"id":724,"Predictions":"We want to make the feature available to a sub set of users initially.nWe want to be able to continue development of a feature in master over a longer period of time, without it being available in an unfinished state.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially.nWe decide to make the feature available to a sub set of users initially."}
{"File Name":"read-more-api\/0006-use-circleci.md","Context":"## Context\\nWe want a mechanism that allows for the tests to be run whenever changes are made to the project.\\nWe also want the project to be deployed whenever these tests pass.\\nCircleCI allows for builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests.\\nCircleCI provides open source projects with four free linux containers for running builds.\\nAll of the configuration for a project built with CircleCI is stored in the project, except for environment variables.\\nMicrosoft provides the \"aspnetcore-build\" Docker image, which contains all the dependencies required to build an ASP.NET Core application.\\n","Decision":"We will use CircleCI to handle continuous integration builds and deployments.\\n","tokens":145,"id":725,"Predictions":"Decision (A) To allow for the tests to be run whenever changes are made to the project. (B) To allow for the project to be deployed whenever these tests pass. (C) To allow for the tests to be run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests. (D) To allow for the builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests. (A) To allow for the builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests. (A) To allow for the builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests. (A) To allow for the builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests. (A) To allow for the builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests."}
{"File Name":"read-more-api\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":726,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"read-more-api\/0004-use-dapper-for-data-access.md","Context":"## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https:\/\/docs.microsoft.com\/en-us\/ef\/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https:\/\/samsaffron.com\/archive\/2011\/03\/30\/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n","Decision":"We will use Dapper with the [repository pattern](http:\/\/blog.mantziaris.eu\/blog\/2016\/10\/24\/the-repository-and-unit-of-work-pattern\/) to access data stored in the database.\\n","tokens":416,"id":727,"Predictions":"ASP.NET applications use [Entity Framework (EF)](https:\/\/docs.microsoft.com\/en-us\/ef\/), an Object Relational Mapper (ORM) that helps access data stored in database.nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet.nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned.nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https:\/\/samsaffron.com\/archive\/2011\/03\/30\/How+I+learned+to+stop-worrying+and+write+my+own+ORM])."}
{"File Name":"datalab\/0019-react-js-for-front-end-ui.md","Context":"## Context\\nWe need to decide which front-end web framework to use for the Datalabs application.\\nThe choices we evaluated were [React.js](https:\/\/reactjs.org\/) and\\n[Angular](https:\/\/angular.io\/).\\n","Decision":"We have decided to use the React framework for the following reasons:\\n* Preferred the \"batteries not included\" approach of React vs the \"batteries\\nincluded\" approach of Angular.\\n* Better development tooling available for React.\\n* Larger community for React.\\n* The team had previous experience with React.\\n","tokens":48,"id":728,"Predictions":"Decide which front-end web framework to use for the Datalabs application.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"Predictions":"Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (B) The externally accessible services will be via a Bastion server.nbr> Decision (C) The externally accessible services will be via a Bastion server.nbr> Decision (D) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr> Decision (B) The externally accessible services will be via a Bastion server.nbr> Decision (B) The externally accessible services will be via a Bastion server.nbr> Decision (B) The externally accessible services will be via a Bastion server.nbr> Decision (A) The externally accessible services will be via a Bastion server.nbr>"}
{"File Name":"datalab\/0025-mongodb-for-data-store.md","Context":"## Context\\nWe need to store persistent data in a form that is easy to query and need to select the\\nappropriate data store. We consider the choice to be between a relational database or a\\ndocument database.\\n","Decision":"We have decided to use [MongoDB](https:\/\/www.mongodb.com\/) as our database for datalabs\\ndata. We felt that the data model for the parts of the system known at this time, a\\ndocument structure provided more flexibility and easier integration with our Node.js\\napplication.\\nMongoDB also provides the ability to run in cluster providing the option for greater\\nresilience. For the time being we are opting to run a single node storing data to\\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\\ndoes not give service resilience or time based backups.\\n","tokens":45,"id":730,"Predictions":"We will choose a relational database over an document database.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"Predictions":"Decide whether to use the SSH key pair authentication or not.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0041-user-driven-dask-and-spark.md","Context":"## Context\\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\\nfrom notebook environments. However since this decision a number of other options\\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\\nschedulers.\\nWe are now moving to a pattern of users being able to spin up their own clusters\\nwhen required.\\n","Decision":"We have decided to collapse the centralised Dask & Spark clusters in favour of writing\\ndocumentation\/working with users to provision their own clusters using projects such\\nas [Dask Labextension](https:\/\/github.com\/dask\/dask-labextension), which is now supported\\nwithin the Labs environment.\\n","tokens":78,"id":732,"Predictions":"We are moving to a pattern of users being able to spin up their own clusters.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"Predictions":"We will consider the Nginx Ingress controller as a Kubernetes native route.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0032-jest-for-node-js-testing.md","Context":"## Context\\nWe need to select a testing framework to use for testing node.js applications. There are\\nmany choices in this space including [Jasmine](https:\/\/jasmine.github.io\/) and\\n[Jest](https:\/\/facebook.github.io\/jest\/).\\n","Decision":"We have decided to use Jest as our testing framework as it provides mocking and\\nexpectation functions which would have to be provided separately to Jasmine. Additionally\\nthe snapshot testing mechanism can be used to simplify certain types of testing and is\\nparticularly beneficial to front end unit testing.\\n","tokens":53,"id":734,"Predictions":"Decide which testing framework to use.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0034-custom-k8s-deployment-tool.md","Context":"## Context\\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\\nenvironments as currently we would have to manually update them in order to apply them to\\ndifferent environments. The options available are to either use\\n[Helm](https:\/\/github.com\/kubernetes\/helm) or to build a custom tool.\\n","Decision":"We have decided to build a custom tool called Bara to deploy our templates. This will use\\nthe simple mustache rendering engine to allow template YAML files to be rendered and then\\napplied using the command line `kubectl` tool.\\nThis approach seemed easier than learning and deploying Helm and building Helm charts for\\neach independent component given our current requirements are very simple and the tool\\nwill only take a few hours to write.\\n","tokens":66,"id":735,"Predictions":"Decision (A) To allow Kubernetes manifest files to be applied to different environments. (B) To build a custom tool. (C) To allow Kubernetes manifest files to be applied to different environments. (D) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (B) To allow Kubernetes manifest files to be applied to different environments. (C) To allow Kubernetes manifest files to be applied to different environments. (D) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (A) To allow Kubernetes manifest files to be applied to different environments. (B) To allow Kubernetes manifest files to be applied to different environments. (C) To allow Kubernetes manifest files to be applied to different environments."}
{"File Name":"datalab\/0028-spark-for-distributed-compute.md","Context":"## Context\\nOne of the aims of the Datalabs project is to provide the climate science community easy\\naccess to the compute capabilities of JASMIN. In order to do this we need to find a way\\nto distribute computationally expensive calculations across a cluster and need to explore\\nthe available options for doing this.\\n","Decision":"We have decided to use [Apache Spark](https:\/\/spark.apache.org\/) as our first option for\\ndistributed compute. It has established itself as the successor to Hadoop and provides\\nstrong integration with interactive notebook technologies such as Jupyter and Zeppelin.\\nSpark also provides support for multiple languages including Scala, Java, Python and R.\\nThis makes is a flexible platform that should appeal to many users.\\nAdditionally, Spark provides Streaming and Machine Learning capabilities which may be of\\ninterest later in the project.\\n","tokens":67,"id":736,"Predictions":"Decision (A) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.nB) The data will be distributed across a cluster.n"}
{"File Name":"datalab\/0042-remove-discourse.md","Context":"## Context\\nWe originally provisioned a discourse instance alongside DataLab as a user forum, however\\nin practise we have found that it is not used as discussion takes place either in\\nperson or on Slack, and we can use the documentation page where required.\\n","Decision":"Discourse will be removed from the stack.\\n","tokens":53,"id":737,"Predictions":"We will not use the documentation page.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0018-node-js-for-services.md","Context":"## Context\\nWe need to select a language for our web services. We restricted our options to Java,\\nNode.js and Go.\\n","Decision":"We have decided to use [Node.js](https:\/\/nodejs.org\/en\/) for our web services. We felt that the existing team\\nexperience, combined with the flexibility provided by a dynamic language made this the\\nright choice.\\nJava would have provided static typing and object orientation but we opted against this\\nas it felt a little heavy weight.\\nGo would have provided a lighter weight modern statically typed option but given neither\\nthe team nor Tessella had existing experience we viewed it as too great a risk for this\\nproject.\\n","tokens":28,"id":738,"Predictions":"Decided to use Java over Node.js.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0047-remove-need-for-wildcard-certificates.md","Context":"## Context\\nDataLabs makes extensive use of reverse proxying, to give users access to resources (such as Minio or JupyterLabs).  These resources need individually from an external URL to an internal service URL.  There are four design options for reverse proxying (<http:\/\/sawers.com\/blog\/reverse-proxying-with-nginx\/>):\\n1. Subdomain - this allows the external path and internal path to be the same, probably with a default root base path (\/).  Different services are identified by the external URL's hostname.  This has some disadvantages - multiple hostnames require a wildcard certificate, or multiple certificates if a wildcard certificate can not be acquired; and it makes the development environment more difficult, because you can not just use localhost.\\n2. Port - this also allows the external path and internal path to be the same, probably with a default root base path (\/).  Different services are identified by the external URL's port.  This has the disadvantage that some organisational firewalls restrict http traffic to unusual hosts.\\n3. Symmetric Path - this allows the external path and internal path to be the same, but with that path configured.  Different services are identified by the path.  This is the best option, but the internal service must allow the path to be configurable.\\n4. Asymmetric Path - here the external and internal paths are different.  Different services are identifiable by the external path.  This requires a search-and-replace of the path on the rendered HTML and JavaScript, so unless these are simple, then this is too fragile.\\nHistorically DataLabs has used Subdomain proxying.\\n","Decision":"Where possible, Symmetric Path or Asymmetric Path proxying should be used.  If this is not possible, a ConfigMap option should determine whether the remaining proxying strategy should be Subdomain or Port proxying.\\n","tokens":338,"id":739,"Predictions":"Decision (A) To allow users to access resources from an external URL to an internal service URL.n2 To allow users to access resources from an external URL to an internal service URL.n3 To allow users to access resources from an external URL to an internal service URL.n4 To allow users to access resources from an external URL to an internal service URL.n5 To allow users to access resources from an external URL to an internal service URL.n6 To allow users to access resources from an external URL to an internal service URL.n7 To allow users to access resources from an external URL to an internal service URL.n8 To allow users to access resources from an external URL to an internal service URL.n9 To allow users to access resources from an external URL to an internal service URL.n10 To allow users to access resources from an external URL to an internal service URL.n11 To allow users to access resources from an external URL to an internal service URL.n12 To allow users to access resources from an external URL to an internal service URL.n13 To allow users to access resources from an external URL to an internal service URL.n14 To allow users to access resources from an external URL to an internal service URL.n15 To allow users to access resources from an external URL to an internal service URL.n16 To allow users to access resources from an external URL to an internal service URL.n17 To allow users to access resources from an external URL to an internal service URL.n18 To allow users to access resources from an external URL to an internal service URL.n20 To allow users to access resources from an external URL to an internal service URL.n20 To allow users to access resources from an external URL to an internal service URL.n21 To allow users to access resources from an external URL to an internal service URL.n22 To allow users to access resources from an external URL to an internal service URL.n23 To allow users to access resources from an external URL to an internal service URL.n24 To allow users to access resources from an external URL to an internal service URL.n25 To allow users to access resources from an external URL to an internal service URL.n26 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n26 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n26 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access resources from an external URL to an internal service URL.n27 To allow users to access services from an external URL to an internal service URL.n27 To allow users to access services from an external URL to an internal service URL.n27 To allow users to access services from an external URL to an internal service URL.n27 To allow users to access services from an external URL to an internal service URL.n27 To allow users to access services from an external URL to an internal service URL.n27 To allow users to access services from an external URL to an internal service URL.n"}
{"File Name":"datalab\/0016-glusterfs-for-storage.md","Context":"## Context\\nContainers running on Kubernetes only provide ephemeral storage. We need to provide\\npersistent storage that allows volumes to be mounted into multiple containers. This\\nrestricts us to [Kubernetes Volume Types](https:\/\/kubernetes.io\/docs\/concepts\/storage\/persistent-volumes\/) that support \"Read Write Many\". Specifically, we are selecting\\nbetween [NFS](https:\/\/help.ubuntu.com\/lts\/serverguide\/network-file-system.html),\\n[GlusterFS](https:\/\/www.gluster.org\/) and [Rook](https:\/\/rook.io\/).\\n","Decision":"We have decided to use GlusterFS to provide distributed persistent storage.\\nWe have opted not to use Rook as it feels that it isn't yet ready for production usage. Also, while it would be easy to deploy hyper-converged we would need a second Kubernetes\\ncluster to run isolated storage as we require.\\nWe feel that simple NFS storage isn't sufficient as it won't give any data resilience.\\nGiven we have no backups, the data replication will give us limited disaster recover\\ncapability.\\n","tokens":116,"id":740,"Predictions":"We need to select [GlusterFS] or [Rook] for persistent storage.n Decide if we should use [NFS] or [GlusterFS] for persistent storage.n Decide if we should use [Rook] or [NFS] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n Decide if we should use [Rook] or [Rook] for persistent storage.n"}
{"File Name":"datalab\/0035-use-of-microbadger-for-docker-containers.md","Context":"## Context\\nWe have a growing number of Docker containers and it is useful to have at a glance\\ninformation available about them. [MicroBadger](https:\/\/microbadger.com\/) provides\\na way to inspect and visualise Docker containers.\\n","Decision":"We have decided to use MicroBadger for new containers and will update existing containers\\nas we make updates to them.\\n","tokens":51,"id":741,"Predictions":"Decision Makers should consider the following options: [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers; [MicroBadger](https:\/\/microbadger.com\/) provides information about Docker containers;"}
{"File Name":"datalab\/0004-ansible-vs-terraform-for-vm-provisioning.md","Context":"## Context\\nThe JASMIN cloud portal and vCloud director portals give manual options for provisioning\\nservers into the JASMIN tennancy. This brings significant effort to rebuild a cluster\\nas all servers would need to be manually deleted and recreated. The new OpenStack\\ninfrastructure is supposed to be available in a matter of months and we need to decide\\nwhether we want to invest effort in automating server creation.\\nIf we do decide to automate we need to decide which technology to use between Ansible\\nand Terraform. Terraform is known to work with vCloud director but it would be preferable\\nto use the same tool for server creation as software provisioning.\\n","Decision":"We have decided to use Ansible as our server provisioning tool as there are significant\\nbenefits to keeping a single tool for all provisioning steps.\\n","tokens":139,"id":742,"Predictions":"We need to decide whether to automate server creation.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0031-kubernetes-namespace-for-environment-isolation.md","Context":"## Context\\nWe need to run multiple instances of the Datalabs system to allow us to continue to\\ndevelop while giving early adopters access to the system. We intend to run both a test\\nand production environment and need to decide whether to do this as a completely separate\\nKubernetes cluster or to isolate the environments using Kubernetes namespaces.\\n","Decision":"We have decided to run both environments on the same Kubernetes cluster but with a\\nseparate reverse proxy to allow testing of the proxy configuration. This decision was\\ntaken to avoid the maintenance overhead of having two clusters.\\n","tokens":71,"id":743,"Predictions":"We need to decide whether to isolate the environments using Kubernetes namespaces.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0043-user-material-ui-lab-library.md","Context":"## Context\\nThe core [Material UI React component library](https:\/\/material-ui.com\/) is used for much of\\nthe DataLabs UI. Before new components make it into the core library, they pass through\\n[\"The Lab\"](https:\/\/material-ui.com\/components\/about-the-lab\/). This provides a platform\\nfor the new components to be tested in real world applications.\\nAt the time of writing, the [Autocomplete](https:\/\/material-ui.com\/components\/autocomplete\/)\\ncomponent would be beneficial to use but it is currently in the lab. Therefore, a decision\\nneeds to be made about whether we are happy to use components from the lab. The\\nkey considerations are as follows.\\n### Pros\\n* Gives us access to components that will reduce development effort required to\\nimplement functionality.\\n* Gives components that have a simpler interface to the components they would be replacing\\n(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift) in the case of Autocomplete)\\nmaking them easier to maintain and adapt.\\n### Cons\\n* Components in the lab are free to have breaking changes on a more regular basis\\ncompared to the ones in the core library.\\n* The components being used might not migrate from the lab into the core library.\\n","Decision":"We will use the Material UI Lab component library. The ease to develop and maintain the\\nuser interface using the components in the lab outweighs the downsides. We don't have to\\nupdate the lab package if a new version removes a component that is currently in use\\nor introduces breaking changes into the component.\\n","tokens":263,"id":744,"Predictions":"We are happy to use components from the lab.n### Prosn* Gives us access to components that will reduce development effort required tonimplement functionality.n### Consn* Gives components that have a simpler interface to the components they would be replacingn(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift) in the case of Autocomplete)nmaking them easier to maintain and adapt.n### Consn* Gives components that have a simpler interface to the components they would be replacingn(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift) in the case of Autocomplete)nmaking them easier to maintain and adapt.n### Consn* Gives components that have a simpler interface to the components they would be replacingn(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift) in the case of Autocomplete)nmaking them easier to maintain and adapt.n### Consn* Gives components that have a simpler interface to the components they would be replacingn(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift) in the case of Autocomplete)nmaking them easier to maintain and adapt.n### Consn* Gives components that have a simpler interface to the components they would be replacingn(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift)nmaking them easier to maintain and adapt.n### Consn* Gives components that have a simpler interface to the components they would be replacingn(e.g. [Downshift](https:\/\/github.com\/downshift-js\/downshift)nmaking them easier to maintain and adapt.n### Decision"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"Predictions":"We will not be able to access the JASMIN tenancy.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0027-thredds-to-serve-netcdf-data.md","Context":"## Context\\nThe NOC use case requires access to a large (~1TB) NetCDF dataset currently stored on the\\nARCHER system. The current usage requires data to be extracted using shell scripts and\\nthis process takes a long time. We need to identify a better way to access this\\ndataset to allow the Datalabs environment to make best use of it.\\n","Decision":"We have decided to use a [Thredds](http:\/\/www.unidata.ucar.edu\/software\/thredds\/current\/tds\/)\\nserver to present a unified view of the dataset as it should provide significant\\nperformance improvements over manual scripting.\\nIn order to achieve this, we need the data to be moved to a JASMIN Group Workspace (GWS)\\nto allow us to provision a Thredds server in the JASMIN managed cloud.\\n","tokens":77,"id":746,"Predictions":"We need to identify a better way to access thisndataset to allow the Datalabs environment to make best use of it.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0046-create-dask-clusters-within-datalabs.md","Context":"## Context\\nCurrently, creating Dask clusters is not ideal:\\n* The service has create-pod permission which is unwise\\n* Different projects can currently use each other's clusters, which is poor resource management and could allow algorithm inspection from the Desk scheduler plan\\n* There is sometimes a requirement to give Dask clusters storage access, to share data or Conda environments.  This is not self-service, and potentially exposes project data since the Dask clusters are not secured by project.\\n* Dask Gateway was investigated, but does not fit in our authentication model (since it does not use OIDC), and it still provides no straight-forward route to storage access.\\n","Decision":"After prototyping a working solution, we have decided to handle Dask cluster creation within the infrastructure-api service.\\nThis will allow us to securely mount volumes and utilise user-defined Conda environments.\\n","tokens":137,"id":747,"Predictions":"Decision (A).n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A.n*Answer:A."}
{"File Name":"datalab\/0007-kubernetes-for-container-orchestration.md","Context":"## Context\\nWe are expecting to have to run a large number of containers across several servers\\nand in different environments. Given this, we think that we need a Container\\nOrchestration tool and are selecting between Kubernetes, Docker Swarm and Mesos.\\n","Decision":"We have selected to use Kubernetes as our container orchestration platform. This is due\\nto it being the choice that the JASMIN team have made and also that it has established\\nitself as a clear industry favourite.\\n","tokens":52,"id":748,"Predictions":"Decide which tool to choose.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0023-use-of-apparmor-and-bane.md","Context":"## Context\\nWhile Docker containers now provide good root isolation from the host compared to earlier\\nversions of Docker there are still security risks. We intend to provide sudo access\\nfor users of the Notebook containers and this significantly magnifies the risks.\\n","Decision":"We have decided to use [AppArmor](https:\/\/wiki.ubuntu.com\/AppArmor) to improve our\\ncontainer security with the intention to make all policies as restrictive as possible.\\nTo make it easier to build AppArmor profiles we have also decided to use\\n[Bane](https:\/\/github.com\/jessfraz\/bane).\\nTo further secure containers we have also decided to run all of our custom build\\ncontainers as a non root user.\\n","tokens":49,"id":749,"Predictions":"We will not provide sudo access to the notebook containers.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0030-kubernetes-direct-api-access.md","Context":"## Context\\nIn order to dynamically orchestrate the containers running in the Datalab environment we\\nneed to interact with the Kubernetes API. There are several choices for this:\\n* Use one of the officially [supported clients](https:\/\/kubernetes.io\/docs\/reference\/client-libraries\/#officially-supported-kubernetes-client-libraries).\\n* Use one of the Node.js community clients\\n* Directly interact with the Kubernetes REST API.\\n","Decision":"We have decided to directly interact with the Kubernetes REST API as this presented the\\neasiest option for development.\\nWe ruled out using a supported client as we didn't want to have to write a service in a\\nlanguage we were not familiar with.\\nWe trialled all of the Node.js community clients but didn't feel that they were complete\\nenough to meet our needs and were poorly documented.\\n","tokens":88,"id":750,"Predictions":"Decide which of the following is the best way to interact with the Kubernetes API:n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n* Directly interact with the Kubernetes REST API.n"}
{"File Name":"datalab\/0038-yarn-workspaces-for-monorepo.md","Context":"## Context\\nWe are using a mono-repository structure for the Datalabs project, this permits\\nsharing of components across independent services. In NodeJS sharing of code\\nwithout duplication requires linking of modules to the dependant service.\\nThere are a few libraries to manages code-sharing:\\n* NPM Link\\n* Yarn Workspaces\\n* Learna\\n","Decision":"We have decided to use Yarn Workspaces as is very lightweight and offers\\nmanagement of the links required for module share, including auto discovery of\\nother services.\\nWe have ruled out using straight NPM Links as they are difficult to set-up and\\nshare between development team. We have used Learna on other project and found\\nit to be very heavyweight and requires that is adds git commits to releasing new\\nversions.\\n","tokens":74,"id":751,"Predictions":"Decision (A) The data is shared across independent services. (B) The data is shared across independent services. (C) The data is shared across independent services. (D) The data is shared across independent services. (A) The data is shared across independent services. (A) The data is shared across independent services. (B) The data is shared across independent services. (C) The data is shared across independent services. (D) The data is shared across independent services. (A) The data is shared across independent services. (A) The data is shared across independent services. (B) The data is shared across independent services. (C) The data is shared across independent services. (D) The data is shared across independent services. (A) The data is shared across independent services. (A) The data is shared across independent services. (B) The data is shared across independent services. (C) The data is shared across independent services. (D) The data is shared across independent services. (A) The data is shared across independent services. (A) The data is shared across independent services. (B) The data is shared across independent services. (C) The data is shared across independent services. (D) The data is shared across independent services. (A) The data is shared across independent services. (A) The data is shared across independent services. (B) The data is shared across independent services. (C) The data is shared across independent services. (D) The data is shared across independent services."}
{"File Name":"datalab\/0026-auth0-for-authentication.md","Context":"## Context\\nUser Authentication is a complex problem, can be time consuming to implement and errors\\nin implementation can lead to security vulnerabilities. We feel that authentication,\\nwhile critical, is not a differentiating factor and want to offload the work to a\\nmanaged service.\\n","Decision":"We have opted to use [Auth0](https:\/\/auth0.com\/) as our Identify provider. This gives us\\na quick way to integrate authentication into our application with minimal effort and as\\nan open source project we are able to use the service free of charge.\\n","tokens":56,"id":752,"Predictions":"We will implement user authentication on a service that is managed.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0044-use-helm-as-k8s-deployment-tool.md","Context":"## Context\\nIn [ADR 34](0034-custom-k8s-deployment-tool.md), it was decided that a custom tool would be created instead of using [Helm](https:\/\/helm.sh\/).\\nSince this decision was made, Helm has matured and established itself as _the_ way to share and install applications that run in Kubernetes.\\nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.\\nWith this in mind, the decision to write and use the custom tool, Bara, needs to be reconsidered.\\n","Decision":"We have decided that we should use Helm rather than continuing to use Bara.\\nThis will aid with the portability of the system as Helm is the established way of installing applications into Kubernetes meaning it should be familiar to others installing DataLabs.\\nAs Helm is well established, cloud providers tend to have support for installing via Helm, simplifying potential deployments to the cloud.\\nHelm also provides mechanisms that allow for the installation of complicated applications into Kubernetes, therefore should  provide DataLabs with plenty of room to grow.\\n","tokens":123,"id":753,"Predictions":"Decision Makers should consider the following: [ADR 34](0034-custom-k8s-deployment-tool.md), it was decided that a custom tool would be created instead of using [Helm](https:\/\/helm.sh\/).nSince this decision was made, Helm has matured and established itself as _the_ way to share and install applications that run in Kubernetes.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in"}
{"File Name":"datalab\/0024-vault-for-secret-store.md","Context":"## Context\\nIn order to dynamically provide secure Notebook containers it will be necessary to\\ndynamically generate and securely store secrets. We want to isolate this from the\\ndatabase and use a dedicated solution for this problem.\\n","Decision":"We have decided to use [Hashicorp Vault](https:\/\/www.vaultproject.io\/) to store secrets.\\nIt provides a dedicated system to securely store and manage access to secrets.\\n","tokens":46,"id":754,"Predictions":"We will isolate this from the database.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0037-single-ingress-controller-per-cluster.md","Context":"## Context\\nCurrently we have an Nginx Ingress controller deployed in the same namespace\\nas the Datalabs application. This means that when multiple instances of\\nDatalabs are deployed to a single cluster (such as prod and test), ingress\\ncontrollers are deployed to each of these namespaces.\\nAs we are in the process of implementing multiple project functionality within\\nDatlabs, there is now a need for an ingress controller than can fulfill ingress\\nrules across all namespaces. This also will move control of ingress to be a\\nplatform service and not part of the deployment of the application itself,\\nwhich will aid to decouple Datalabs into being more of a standalone\\napplication.\\n","Decision":"We have decided to deploy a single Nginx Ingress Controller into the\\nkube-system namespace that will handle the ingress rules for the entire\\ncluster.\\n","tokens":141,"id":755,"Predictions":"Decision i.e., the ingress controller should be deployed in the same namespace as the Datalabs application.nBy implementing multiple project functionality within the Datalabs application, there is now a need for an ingress controller that can fulfill ingressnrules across all namespaces.nBy implementing multiple project functionality within the Datalabs application, there is now a need for an ingress controller that can fulfill ingressnrules across all namespaces.nBy implementing multiple project functionality within the Datalabs application, there is now a need for an ingress controller that can fulfill ingressnrules across all namespaces.nBy implementing multiple project functionality within the Datalabs application, there is now a need for an ingress controller that can fulfill ingressnrules across all namespaces.nBy implementing multiple project functionality within the Datalabs application, there is now a need for an ingress controller that can fulfill ingressnrules across all namespaces.nBy implementing multiple project functionality within the Datalabs application, there is now a need for an ingress controller that can fulfill ingressnrules across all namespaces.nBy implementing multiple project functionality within the Datalabs application, there is now a need for an ingress controller that can fulfill ingressnrules across all namespaces.n"}
{"File Name":"datalab\/0022-material-ui-for-component-framework.md","Context":"## Context\\nWe need to select a front end component framework to style the web application. The\\nchoices available are [MaterialUI](https:\/\/material-ui-next.com\/),\\n[SemanticUI](https:\/\/react.semantic-ui.com\/introduction) and\\n[ReactBootstrap](https:\/\/react-bootstrap.github.io\/).\\n","Decision":"We have decided to use MaterialUI for the front end component library. Additionally, we\\nhave chosen to use the vNext version that is still in alpha as it provides a\\nsignificantly different API and we expect it to be released prior to the end of the\\nproject.\\n","tokens":65,"id":756,"Predictions":"Decide which is the best front end component framework.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0010-new-default-deploy-user-on-servers.md","Context":"## Context\\nIn order to use Ansible to provision the servers there needs to be a user with sudo\\naccess. The default administrator user configured onto the base VM is not configured\\ncorrectly and we are unable to use SSH keypairs with this user. After investigation\\nwe are not clear what the issue is with the administrator user and need alternative\\noption.\\n","Decision":"We have decided to remove the password authentication from the administrator user having\\nfirst provisioned a new deploy user with ssh keys for the team in the authorized keys.\\nThis user will require a password for sudo which will be stored in the ansible vault to\\nallow automated provisioning.\\n","tokens":75,"id":757,"Predictions":"Decision (A) The administrator user should be able to use the SSH keypairs with this user.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0017-separate-storage-cluster-rather-than-hyper-converged.md","Context":"## Context\\nWe need to decide whether to run our storage cluster as a standalone cluster or\\nhyper-converged by running pods on the Kubernetes cluster.\\n","Decision":"We have decided to run a standalone storage cluster. The reason for using a separate\\ncluster is that by keeping the persistent data separate we keep flexibility over the\\nKubernetes cluster and can drop and recreate it without having to worry about the data.\\n","tokens":32,"id":758,"Predictions":"We need to decide whether to run our storage cluster as a standalone cluster or hyper-converged by running pods on the Kubernetes cluster.n Decisions:nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0003-ubuntu-16-04-as-server-os.md","Context":"## Context\\nWe need to select a base operating system to install on all virtual machines that form\\nthe Datalabs environment. There are three choices available through the JASIMN portal\\nUbuntu 14.04, Ubuntu 16.04 and CentOS 6.9.\\n","Decision":"We have selected Ubuntu 16.04 as the base operating system for our servers for several\\nreasons:\\n* The team are more familiar with Ubuntu over CentOS.\\n* Packages are likely to be more easily available on Ubuntu.\\n* CentOS 6.9 is no longer being updated (last update 10\/5\/2017).\\n* Ubuntu 16.04 will be supported for far longer. 14.04 end of life is early 2019.\\n","tokens":58,"id":759,"Predictions":"Decide which operating system to choosenthe Datalabs environment.nThe Ubuntu 14.04 option is the best choicenthe Ubuntu 16.04 option is the CentOS 6.9 optionnthe Datalabs environment.nThe Ubuntu 16.04 option is the best choicenthe Datalabs environment.nThe Ubuntu 16.04 option is the CentOS 6.9 optionnthe Datalabs environment.nThe Ubuntu 16.04 option is the CentOS 6.9 optionnthe Datalabs environment.nThe Ubuntu 16.04 option is the CentOS 6.9 optionnthe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs environment.nThe Datalabs"}
{"File Name":"datalab\/0021-use-create-react-app.md","Context":"## Context\\nWe need to decide whether to configure our web application code base ourselves or make\\nuse of the [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app)\\nto provide project configuration.\\n","Decision":"We have decided to use the Create React App project to provide the base for our React\\nproject as this brings best practice configuration and build and is actively being\\ndeveloped.\\n","tokens":46,"id":760,"Predictions":"Decision (A) The [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app) will be used to configure our web application code base.nbr> Decision (B) The [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app) will be used to configure our web application code base.nbr> Decision (C) The [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app) will be used to configure our web application code base.nbr> Decision (A) The [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app) will be used to configure our web application code base.nbr> Decision (A) The [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app) will be used to configure our web application code base.nbr> Decision (A) The [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app) will be used to configure our web application code base.nbr> Decision (A) The [Create React App](https:\/\/github.com\/facebookincubator\/create-react-app) will be used to configure our web application code base.nbr> Decision ()"}
{"File Name":"datalab\/0040-stack-sharing-and-visiblity.md","Context":"## Context\\nThere is demand from the users to be able to \"share\" their Notebooks such that other users\\nare be able to open the Notebook, change some values and then re-run cells within the\\nNotebook.\\nThere is also the requirement to be able to \"share\" the results of a notebook in a way\\nthat involves less interaction from the viewer. This requirement is fulfilled by Sites,\\nbut there needs to be a way to configure who is able to see the Site.\\nThere is the need to define whether these are two truly separate concepts and how\\n\"sharing\" should be implemented in Datalabs.\\n","Decision":"We have decided to split these into two different concepts: `sharing` and `visibility`.\\n`Sharing` is the means through which a user will allow others access to make changes to\\nthe item that is being shared.\\n`Visibility` is the means through which a user will allow others to view the output of\\ntheir work.\\nIt is possible for an item to have both a `sharing` and `visibility` status. For example,\\na Site might have a `visibility` status that means anyone is allowed to look at the\\ncontent and it might also have a `sharing` status that anyone in the Project the Site\\nbelongs to can edit the configuration of the Site.\\n","tokens":130,"id":761,"Predictions":"Decision (A) The notebooks should be able to \"share\" with other users.nB) The notebooks should be able to \"share\" with other users.nC) The notebooks should be able to \"share\" with other users.nD) The notebooks should be able to \"share\" with other users.nAnswer:A.nB.nB.nC.nD.nA.nB.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nC.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.nB.n"}
{"File Name":"datalab\/0020-redux-for-state-store.md","Context":"## Context\\nBuilding a complex web application brings challenges around how to manage state. The\\nRedux website provides an excellent [motivation page](http:\/\/redux.js.org\/docs\/introduction\/Motivation.html) that discusses the challenges in detail.\\n","Decision":"We have decided to adopt the [Redux](http:\/\/redux.js.org\/) architecture to provide a clean separation between\\nour views, actions and state store.\\n","tokens":49,"id":762,"Predictions":"Decision Makers should consider the following options: a). The state of the application; b). The state of the application; c). The state of the application; d). The state of the application; e). The state of the application; f). The state of the application; g). The state of the application; h). The state of the application; h). The state of the application; ii. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; j. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv. The state of the application; iv."}
{"File Name":"datalab\/0005-docker-containers-for-everything.md","Context":"## Context\\nThe Datalabs project has challenging vision for dynamic creation of scientific analysis\\nenvironments. We need to decide what technology will enable us to meet this vision.\\n","Decision":"We do not think that regular VM orchestration will meet the vision and have instead\\nopted to deploy all services (where possible) as Docker Containers. Alternative container\\ntechnologies were not evaluated as the team has previous Docker experience and it is the\\nclear leader in this space.\\n","tokens":35,"id":763,"Predictions":"Decision Makers should consider the following technologies: (i) The Datalabs project has a high degree of complexity and complexity; (ii) The Datalabs project has a high degree of complexity and complexity; (iii) The Datalabs project has a high degree of complexity and complexity; (iv) The Datalabs project has a high degree of complexity and complexity; (vi) The Datalabs project has a high degree of complexity and complexity; (vii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity; (viii) The Datalabs project has a high degree of complexity and complexity;"}
{"File Name":"datalab\/0039-prometheus-operator-helm-chart.md","Context":"## Context\\nWe need to decide between [Kube Prometheus](https:\/\/github.com\/coreos\/kube-prometheus)\\nand [Prometheus Operator](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/prometheus-operator)\\nfor use in deploying the prometheus monitoring solution for DataLabs.\\n","Decision":"We have decided to use the [Prometheus Operator Helm Chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/prometheus-operator)\\nas it gives us the option to use an Helm chart over kubectl used in the kube-prometheus\\noption. The Prometheus Operator Helm chart provides a similar feature set to the\\nkube-prometheus option.\\n","tokens":63,"id":764,"Predictions":"Decision (A) The Prometheus solution should be deployed on the [Kube Prometheus] platform.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0013-custom-redbird-proxy.md","Context":"## Context\\nWe need to select a proxy server to proxy requests to services running in the private\\nKubernetes cluster.\\n","Decision":"We have decided to use [RedBird](https:\/\/github.com\/OptimalBits\/redbird) as a reverse\\nproxy. This was selected as it is written in Node.js which we expect our applications\\nto be written in and will allow us to extend to support authentication, logging and other\\nedge concerns.\\n","tokens":25,"id":765,"Predictions":"Decision (A) The proxy server must be a private server.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0033-enzyme-for-react-testing.md","Context":"## Context\\nWe need to choose a rendering library to allow testing of React components.\\n","Decision":"We have chosen to use [Enzyme](https:\/\/github.com\/airbnb\/enzyme) to provide a library\\nfor shallow rendering.\\n","tokens":18,"id":766,"Predictions":"Decision Makers should choose a rendering library.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0006-dockerhub-for-container-registry.md","Context":"## Context\\nHaving selected to run all services and applications in Docker containers we need a\\nregistry to store them in.\\n","Decision":"We have decided to store the Docker containers in [DockerHub](https:\/\/hub.docker.com\/u\/nerc\/dashboard\/). Given the project is Open Source this seemed to be the easiest option\\nas most tools default to this registry.\\n","tokens":25,"id":767,"Predictions":"ii.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0008-weavenet-for-overlay-network.md","Context":"## Context\\nKubernetes does not provide an overlay network out of the box and we need to choose\\nwhich one to use from [here](https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/networking\/).\\n","Decision":"We have decided to use the [WeaveNet](https:\/\/www.weave.works\/oss\/net\/) network as this\\nhas already been used by the JASMIN team. It also appears easy to use and there is good\\ndocumentation.\\n","tokens":46,"id":768,"Predictions":"We need to choose an overlay network.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0015-use-kong-for-reverse-proxy.md","Context":"## Context\\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\\nDashboard. We have tested using an Nginx proxy and have found that this provides the\\nsupport required but does not provide an API for configuration.\\n","Decision":"We have decided to use [Kong](https:\/\/getkong.org\/) for our reverse proxy as it is a\\ncustom build of Nginx that provides an API. We have tested with all of our services and\\nit appears to meet our needs.\\n","tokens":53,"id":769,"Predictions":"We will not support the Traefik proxy.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0014-use-traefik-for-reverse-proxy.md","Context":"## Context\\nThe Redbird proxy does not support WebSockets which are required to support the\\ninteractive notebooks. We have also had problems with the reliability\\nof the proxy and have found it difficult to configure.\\n","Decision":"We have decided to replace the custom Redbird proxy with a [Traefik](https:\/\/traefik.io\/)\\nproxy as this looks easier to configure and claims Web Socket support.\\n","tokens":44,"id":770,"Predictions":"Decision (A) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.nB) The proxy should be configured to support the interactive notebooks.n"}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"Predictions":"Decision i.e., we need to provide servers and software for the datalabs project.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0045-kubernetes-secrets-as-secret-store.md","Context":"## Context\\nCurrently, secrets that are dynamically created for notebooks etc. are stored in Hashicorp Vault as decided in [24. Vault for Secret Store](0024-vault-for-secret-store.md).\\nVault has been the source of operational challenges such as sealing when it is rescheduled.\\nThis has added extra complexity such as requiring a cronjob to periodically check that Vault has not been sealed.\\nVault is also non-trivial to initially configure for use; a concern when aiming for portability across different services, especially those that would be self-serve e.g. JASMIN's Cluster as a Service (CaaS).\\n","Decision":"It has been decided to move to using [Kubernetes' native secret solution](https:\/\/kubernetes.io\/docs\/concepts\/configuration\/secret) to make DataLabs simpler to both deploy and maintain.\\n","tokens":129,"id":772,"Predictions":"Decision (A) To store secrets in Hashicorp Vault.nB) To store secrets in Hashicorp Vault.nC) To store secrets in Hashicorp Vault.nD) To store secrets in Hashicorp Vault.nAnswer:A.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nC. To store secrets in Hashicorp Vault.nD. To store secrets in Hashicorp Vault.nAnswer:A.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nAnswer:A.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nAnswer:A.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nAnswer:A.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nB. To store secrets in Hashicorp Vault.nAnswer:A."}
{"File Name":"datalab\/0029-dask-for-python-distributed-compute.md","Context":"## Context\\nFollowing a meeting with the Met Office it is clear that their Python users were seeing\\ngreat success using [Dask](https:\/\/dask.pydata.org\/en\/latest\/) as their distributed\\ncompute environment. Dask appears that it could be easier to use than Spark for users\\nwho already know Python and NumPy.\\n","Decision":"We have decided to offer Dask in addition to Spark within the Datalabs platform. This\\nenables us to appeal to more users at limited cost.\\n","tokens":69,"id":773,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"Predictions":"Decision based on the following options: a) a remoting mechanism for sessions\/connections which selected extraFields for each incoming event; b) a cache based enriching of the data for sessions\/connections which selected extraFields for each incoming event; c) a policy enforcement mechanism for each single request; d) a remoting mechanism for sessions\/connections which selected extraFields for each incoming event; e) a policy enforcement mechanism for each single request; f) a policy enforcement mechanism for each single request; g) a policy enforcement mechanism for each single request; h) a policy enforcement mechanism for each single request; h) a policy enforcement mechanism for each single request; ii) a policy enforcement mechanism for each single request; iii) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; iv) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy enforcement mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each single request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each request; v) a policy filtering mechanism for each"}
{"File Name":"ditto\/DADR-0005-semantic-versioning.md","Context":"## Context\\nEclipse Ditto project exited the incubation phase with release 1.0.0.\\nHenceforth, any change to the Ditto API enters a Ditto release according to [semantic versioning](https:\/\/semver.org):\\n- Incompatible API changes increment major version number (e.g., 1.7.5 -> 2.0.0);\\n- Compatible API changes increment minor version number (e.g., 1.2.3 -> 1.3.0);\\n- Changes in the implementation without any API change increment patch version number (e.g., 1.0.0 -> 1.0.1).\\nThis document defines what _API compatibility_ means,\\nthe modules which are considered API and for which semantic versioning holds,\\nand the enforcement of semantic versioning.\\n","Decision":"### API compatibility\\nFor Eclipse Ditto, API compatibility means _binary compatibility_ defined by\\nthe [Java Language Specification, Java SE 8 Edition, chapter 13](https:\/\/docs.oracle.com\/javase\/specs\/jls\/se8\/html\/jls-13.html).\\nExamples of binary-compatible changes:\\n- Adding a top-level interface or class.\\n- Making a non-public interface or class public.\\n- Adding classes to a class's set of superclasses without introducing circular inheritance.\\n- Adding type parameters without changing the signature of existing methods.\\n- Renaming type parameters.\\n- Deleting private members.\\n- Adding enums.\\n- Adding abstract methods to interfaces.\\n- Adding members to a class that do not collide with any other member in all its subclasses in Ditto.\\n- Adding default methods to an interface that do not collide with any other method in all subclasses of the interface\\nin Ditto.\\nBinary compatibility guarantees that any user code of Ditto does not break on minor version upgrades, provided that\\n- the user code does not implement Ditto interfaces, and\\n- the user code does not extend Ditto classes.\\nInheritance from Ditto classes and interfaces is excluded from API compatibility because Ditto interfaces are often\\ndefined to hide implementation details from user code. Compatibility for user-defined subclasses, or source\\ncompatibility, is not a part of Ditto's semantic versioning. Inheriting user classes may break after a minor Ditto\\nversion upgrade.\\n### Modules considered API\\nPublic classes, interfaces and their public members of the following modules, and their submodules are considered\\nDitto API. Changes to them must enter Ditto release in accord with semantic versioning.\\nModules not on this list are not considered API; they may contain incompatible changes for any Ditto version change.\\n```\\nditto-json\\nditto-base-model\\nditto-messages-model\\nditto-jwt-model\\nditto-rql-model\\nditto-rql-query\\nditto-rql-parser\\nditto-rql-search-option-parser\\nditto-policies-model\\nditto-things-model\\nditto-thingsearch-model\\nditto-connectivity-model\\nditto-placeholders\\nditto-protocol\\nditto-utils-jsr305\\n```\\n### Javadoc documentation using `@since`\\nWhen adding new public visible API (e.g. new interfaces, classes or methods in existing code) in the defined API modules,\\na `@since <version>` javadoc annotation shall be added.\\nExample:\\n```java\\n\/**\\n* Returns the extra information which enriches the actual value of this change.\\n*\\n* @return the extra data or an empty Optional.\\n* @since 1.1.0\\n*\/\\nOptional<JsonObject> getExtra();\\n```\\nExisting public API without `@since` can be interpreted as `@since 1.0.0` and can be added when adjusting a class.\\n### Enforcement of semantic versioning\\nSemantic versioning is enforced through binary compatibility check by `japicmp-maven-plugin`.\\n```xml\\n<plugin>\\n<groupId>com.github.siom79.japicmp<\/groupId>\\n<artifactId>japicmp-maven-plugin<\/artifactId>\\n<\/plugin>\\n```\\nDeviations of the behavior of `japicmp-maven-plugin` from binary compatibility defined by the Java language\\nspecification are to be corrected through overrides. If `japicmp-maven-plugin` breaks the build for a branch,\\nthen a major version increment for the next release is required to merge the branch into Ditto master.\\nCheck with the whole Ditto team before adding anything to the exclusion list of `japicmp-maven-plugin`.\\n","tokens":175,"id":776,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"Predictions":"Decision (A) To silence SonarQube, we should call Thread.currentThread().interrupt() in the catch block.nB) To silence SonarQube, we should call Thread.interrupt() in the catch block.nC) To silence SonarQube, we should call Thread.interrupt() in the catch block.nB) To silence SonarQube, we should call Thread.interrupt() in the catch block.nC) To silence SonarQube, we should call Thread.interrupt() in the catch block.nB) To silence SonarQube, we should call Thread.interrupt() in the catch block.nC) To silence SonarQube, we should call Thread.interrupt() in the catch block.nB) To silence SonarQube, we should call Thread.interrupt() in the catch block.nC) To silence SonarQube, we should call Thread.interrupt() in the catch block.nB) To silence SonarQube, we should call Thread.interrupt() in the catch block.nC) To silence SonarQube, we should call Thread.interrupt() in the catch block.nB) To silence SonarQube, we should call Thread.interrupt() in the catch block.n"}
{"File Name":"ditto\/DADR-0001-record-architecture-decisions.md","Context":"## Context\\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\n[article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) and also use the proposed\\n[template](https:\/\/github.com\/joelparkerhenderson\/architecture_decision_record\/blob\/master\/adr_template_by_michael_nygard.md).\\n","tokens":26,"id":778,"Predictions":"We want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"Predictions":"Decision Makers should consider the following: -Answer:A.nB.nC.nD.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nC.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nB.nA.nA.nB.nA.nA.nB.nA.nA.nB.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.nA.n"}
{"File Name":"ditto\/DADR-0002-replace-akka-pubsub-for-event-publishing.md","Context":"## Context\\nUp to now Ditto used Pekko's distributed publish\/subscribe in order to emit e.g. `ThingEvent`s to interested other services:\\n* gateway\\n* websocket\/SSE sessions publishing events\\n* connectivity\\n* AMQP 1.0 \/ AMQP 0.9.1 \/ MQTT \/ Kafka sessions publishing events\\n* things-search\\n* ThingUpdater updating the search index\\nThat naive approach works, but does not provide *horizontal scalability*:\\n* each single service instance generally interested in `ThingEvent`s gets all of them, regardless of whether someone is actually interested in them\\n* as a result a lot of avoidable JSON deserialization is done\\n* when Ditto needs to scale the event publishing horizontally, adding new gateway or connectivity instances will not help scaling the event publishing\\n* still all instances will have to process each `ThingEvent` and discard if not relevant\\n","Decision":"We will implement a custom Ditto pub\/sub which\\n* uses \"authorization subjects\" as topics when subscribing\\n* uses \"read subjects\" as topics when publishing\\n* manages and distributes the active subscriptions via Pekko Distributed Data (ddata)\\n* emits `ThingEvent`s only to service instances where at least one consumer consumes the event\\n","tokens":186,"id":780,"Predictions":"Decision (A) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (B) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (C) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (D) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (A) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (A) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (B) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (C) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (D) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (A) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (A) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services. (B) Ditto should not use Pekko's distributed publish\/subscribe to publish events to other services."}
{"File Name":"openmrs-module-gpconnect\/0001-add-additional-data-to-resource.md","Context":"## Context\\nDeprecated design:\\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\\nThe problem:\\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\\n- However, this would have made the Practitioner resource more complex.\\*\\n\\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\\n","Decision":"- We decided to use the existing OpenMRS model for creating attributes for each resource.\\n- There is a one-to-many relationship between a resource and its attributes.\\n- Each attribute has an attribute type, which is stored in the attribute_type table.\\n- For each new piece of data to be added to a resource:\\n- Create a new attribute_type that describes the data type - this should be included in the seed data.\\n- Create a new attribute with the attribute_type that you just created and link this new attribute to your resource.\\n- Update the translator for the resource (from FHIR to OpenMRS and vice versa) to ensure that the new piece of data is populated.\\n- Please see [the addition of the SDS Role Profile Id field to the Practitioner resource](https:\/\/github.com\/Open-GP\/openmrs-module-gpconnect\/commit\/f0ff30c18f0d386e338d87f22a847b0dc3dff5ff) to see an implementation of the following pattern:\\n![Provider Attribute Design](..\/images\/ProviderAttributeDesign2.png)\\n","tokens":159,"id":781,"Predictions":"Decision (A) The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be expanded to accommodate the new resource.n- The existing resource should be"}
{"File Name":"frontend-library\/0002-use-yarn-rather-than-npm.md","Context":"## Context\\nPackage versions specified in `package.json` are often never any more precise than the major version.\\nBy default, [Yarn](https:\/\/yarnpkg.com\/lang\/en\/) records the exact version of the packages. In contrast, [npm](https:\/\/www.npmjs.com\/) version 4 uses an opt-in feature called [npm-shrinkwrap.json](https:\/\/docs.npmjs.com\/files\/shrinkwrap.json) and in version [5](http:\/\/blog.npmjs.org\/post\/161081169345\/v500) this has been superseded by a new file - [package-lock.json](https:\/\/docs.npmjs.com\/files\/package-lock.json). The feature no longer opt-in for version 5 of `npm`.\\n","Decision":"We will use `Yarn` as we have more experience of this than version 5 of `npm` and `Yarn` has been proven to work for our use cases.\\n","tokens":155,"id":782,"Predictions":"Decision (A) The [npm](https:\/\/www.npmjs.com\/lang\/en\/) version 4 uses an opt-in feature called [npm-shrinkwrap.json](https:\/\/docs.npmjs.com\/files\/shrinkwrap.json) and in version [5](http:\/\/blog.npmjs.org\/post\/161081169345\/v500) this has been superseded by a new file - [package-lock.json](https:\/\/docs.npmjs.com\/files\/package-lock.json). The feature no longer opt-in for version 5 of npm.n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  "}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"Predictions":"Decision (A). The style of NHSUK sites should be consistent.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe style of NHSUK sites should be as consistent as possible.nThe ability to review the styles in a development environment before they arenmade available to a wider audience is important.nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to review the styles in a development environment before they arenmade available to a wider audience is important.nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to review the styles in a development environment before they arenmade available to a wider audience is important.nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to review the styles in a development environment before they arenmade available to a wider audience is important.nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe ability to use wellnknown technologies such as [SASS](http:\/\/sass-lang.com\/).nThe"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"infrastructure-adrs\/0003-extract-technical-metadata-on-per-file-basis.md","Context":"## Context and Problem Statement <!-- required -->\\nCurrently, we extract technical metadata per-object and run one extraction job serially per-file. This takes a problematically long time for objects with many files; blocks other objects from accessioning; and complicates restarts which must begin again and process the entire object.\\nNOTE: Needs discussion: Fedora 3 does not support concurrent writes on the same datastream so we can either split out filesets as a first-class objects in the F3 data model or use temporary caching to generate a consolidated techMD datastream.\\n## Decision Drivers <!-- optional -->\\n* Blocker for Google Books project\\n* Slows down accessioning process\\n","Decision":"* Blocker for Google Books project\\n* Slows down accessioning process\\nTBD!\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":139,"id":785,"Predictions":"We need to consider other ways to reduce the time it takes to access the datastream.n#1: Using temporary caching to generate a consolidated techMD datastream.n#2: Using temporary caching to generate a consolidated techMD datastream.n#3: Using temporary caching to generate a consolidated techMD datastream.n#4: Using temporary caching to generate a consolidated techMD datastream.n#5: Using temporary caching to generate a consolidated techMD datastream.n#6: Using temporary caching to generate a consolidated techMD datastream.n#7: Using temporary caching to generate a consolidated techMD datastream.n#8: Using temporary caching to generate a consolidated techMD datastream.n#9: Using temporary caching to generate a consolidated techMD datastream.n#10: Using temporary caching to generate a consolidated techMD datastream.n#11: Using temporary caching to generate a consolidated techMD datastream.n#12: Using temporary caching to generate a consolidated techMD datastream.n#13: Using temporary caching to generate a consolidated techMD datastream.n#14: Using temporary caching to generate a consolidated techMD datastream.n#15: Using temporary caching to generate a consolidated techMD datastream.n#16: Using temporary caching to generate a consolidated techMD datastream.n#17: Using temporary caching to generate a consolidated techMD datastream.n#18: Using temporary caching to generate a consolidated techMD datastream.n#19: Using temporary caching to generate a consolidated techMD datastream.n#20: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#23: Using temporary caching to generate a consolidated techMD datastream.n#20: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#22: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#21: Using temporary caching to generate a consolidated techMD datastream.n#"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"Predictions":"Decision (A) To decouple services.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"infrastructure-adrs\/0008-separate-dor-index-from-argo-index.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":787,"Predictions":"Decision!-- [driver 1]n* [driver 2]n* [driver 3]n* [driver 4]n* [driver 5]n* [driver 6]n* [driver 7]n* [driver 8]n* [driver 9]n* [driver 10]n* [driver 11]n* [driver 12]n* [driver 13]n* [driver 14]n* [driver 15]n* [driver 16]n* [driver 17]n* [driver 18]n* [driver 19]n* [driver 20]n* [driver 21]n* [driver 22]n* [driver 23]n* [driver 24]n* [driver 25]n* [driver 26]n* [driver 27]n* [driver 28]n* [driver 29]n* [driver 30]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n"}
{"File Name":"infrastructure-adrs\/0011-use-api-tokens.md","Context":"## Context and Problem Statement <!-- required -->\\nSDR is implemented by way of many different discrete services, some of which make network requests of each other to do their work.  For example, at present:\\n* Argo calls Preservation Catalog to retrieve computed checksums for file content, and to retrieve actual preserved file content.\\n* Many different services use dor-services-app for read and write operations on our digital repository objects.\\nOf course, we want to make sure that not just any client on the network can use these services, since access to them should be limited to authorized callers.\\nIn the past, this was accomplished primarily through network access restrictions (e.g., firewall whitelisting IPs of services that should have access, limiting access to clients inside the VPN, etc).  However, sole use of this approach has been deprecated by the industry at large and by Stanford UIT in particular (though it is still an important component of security).\\n## Decision Drivers <!-- optional -->\\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\n","Decision":"* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\nThe infrastructure team came to consensus in a weekly planning meeting that, going forward, we should gate access to API endpoints using JWTs (minted by the service, provided with requests by the client).  This ADR is meant to capture and flesh out that decision.\\n### Positive Consequences <!-- optional -->\\n* More robust and less circumventable than restricting access solely by way of network and firewall configuration.\\n* Does not preclude keeping appropriate firewall restrictions, which should remain part of our security practice.\\n* Should a network re-configuration result in accidental loosening of firewall restrictions, token based authentication provides a robust additional line of access control.\\n* For the ways we've built our applications, token based authentication is at least as easy to implement and maintain as client certificate based authenication, and should provide similarly adequate security.\\n* Minting tokens has become a common approach in industry for authenticating client access to APIs, so there is a wealth of current information and advice available online for this practice.\\n### Negative Consequences <!-- optional -->\\n* More work for developers than solely relying on firewall rules and network configuration (tasks which typically fall to operations and which should happen anyway).\\n","tokens":242,"id":788,"Predictions":"We want to make sure that not just any client on the network can use these services, since access to them should be limited to authorized callers.nIn the past, this was accomplished primarily through network access restrictions (e.g., firewall whitelisting IPs of services that should have access, limiting access to clients inside the VPN, etc). However, sole use of this approach has been deprecated by the industry at large and by Stanford UIT in particular (though it is still an important component of security).n## Decision Drivers!-- optional -->n* We want to secure access to our API endpoints.n* UIT wants us to secure access to our API endpoints.n* We would like an approach that's relatively easy to understand and maintain.n## Decision Makers!-- optional -->n* We want to secure access to our API endpoints.n* We would like an approach that's relatively easy to understand and maintain.n## Decision Makers!-- optional -->n* We want to secure access to our API endpoints.n* UIT wants us to secure access to our API endpoints.n## Decision Makers!-- optional -->n* We want to secure access to our API endpoints.n## Decision Makers!-- optional -->n* We want to secure access to our API endpoints.n"}
{"File Name":"infrastructure-adrs\/0012-dor-datastore.md","Context":"## Context and Problem Statement <!-- required -->\\nAs described in [ADR-0001](0001-migrate-off-fedora-3.md)], it is necessary to migrate from Fedora 3 as a repository for DOR metadata. Since the drafting of that original ADR significant progress has been made in isolating Fedora 3 from other SDR systems. In addition, COCINA has been adopted as a data model for DOR digital objects and the Fedora data model has been mapped to the COCINA data model.\\nThe purpose of this ADR is to propose an alternative datastore.\\n## Decision Drivers\\n* Support CRUD and querying\/indexing of JSON data.\\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n* Support transactions.\\n* Support for large number of records.\\n* Currently 2 million digital objects.\\n* Expect significant growth in number of digital objects.\\n* Depending on implementation, may possibly have multiple records per digital object.\\n* Ability to migrate JSON model changes.\\n* Broad community support for datastore and expected long term viability.\\n* High availability deployment configurations.\\n* Local Docker deployment configurations.\\n* Can be supported by Operations team.\\n* Robust export support (e.g., if needed to exit the technology)\\n","Decision":"* Support CRUD and querying\/indexing of JSON data.\\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\\n* Support transactions.\\n* Support for large number of records.\\n* Currently 2 million digital objects.\\n* Expect significant growth in number of digital objects.\\n* Depending on implementation, may possibly have multiple records per digital object.\\n* Ability to migrate JSON model changes.\\n* Broad community support for datastore and expected long term viability.\\n* High availability deployment configurations.\\n* Local Docker deployment configurations.\\n* Can be supported by Operations team.\\n* Robust export support (e.g., if needed to exit the technology)\\nNOT YET.\\n","tokens":280,"id":789,"Predictions":"Decision Makers: The proposed datastore should be able to support CRUD and querying\/indexing of JSON data.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The proposed datastore should support a large number of records per digital object.n#Conclusion: The"}
{"File Name":"infrastructure-adrs\/0002-extract-only-useful-technical-metadata.md","Context":"## Context and Problem Statement <!-- required -->\\nCurrently we are using JHOVE 1.x to generate voluminous technical metadata for every file of every object accessioned in SDR, and we do not use most of this metadata. This is problematic especially for large & many files: we cannot currently accessioning books with many pages because the technical metadata robot consumes all system memory which causes the virtual machine to kill the JHOVE process. We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.  Note: SMPL content ships with its own metadata typically from MediaInfo rather than JHOVE.\\n## Decision Drivers <!-- optional -->\\n* Cannot accession large files (objects > 1GB or so)\\n* Cannot accession objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n","Decision":"* Cannot accession large files (objects > 1GB or so)\\n* Cannot accession objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n**Preferred** (by Infrastructure Team) option: option 2, because:\\n* Option 1 is preventing us from accessioning books and other large objects, which is unacceptable to SDR customers\\n* Option 3 is an unsound preservation strategy and does not meet SDR user needs\\n* Option 4 has already been pursued a number of times already, and there's only so much we can toss at the worker machines\\n* Option 5 has been rejected as a general deployment strategy for now\\nThus, option 2 is the only option that currently meets the department's and its customers' needs.\\nAs part of this work, we will move forward with a two-prong strategy in order to resolve the tension between the need to come up with a sound, community-oriented preservation practice and the need to accession large-scale content now.\\nIn the short-term, we will come up with a short list of technical metadata attributes that will be extracted from all files and from all files of certain types. We will convene a \u201ctechnical metadata strike team\u201d in short order that will review attributes being used in Samvera and make recommendations based thereupon. The aim is for this group to finalize their recommendations in advance of the January 2020 Google Books work cycle.\\nIn parallel, we will pursue a longer-term effort for determining what an ideal, community-oriented strategy is for doing this work building on best practices (which are currently murky\/non-emergent). Along with this longer-term work, we will look into how to support on-demand regeneration of technical metadata so that we can iterate on the short-term work in the prior bullet.\\n","tokens":185,"id":790,"Predictions":"We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.n## Decision Makers!-- optional -->n* Cannot accession large files (objects > 1GB or so)n* Cannot accession objects with many pages, such as booksn* Blocker for Google Books projectn* Causes extreme delays accessioning other contentn## Decision Makers!-- optional -->n* Cannot accession large files (objects > 1GB or so)n* Cannot accession objects with many pages, such as booksn* Blocker for Google Books projectn* Causes extreme delays accessioning other contentn## Decision Makers!-- optional -->n* Cannot accesssion large files (objects > 1GB or so)n* Cannot accession objects with many pages, such as booksn* Blocker for Google Books projectn* Causes extreme delays accessioning other contentn## Decision Makers!-- optional -->n* Cannot accesssion large files (objects > 1GB or so)n* Cannot accession objects with many pages, such as booksn* Blocker for Google Books projectn* Causes extreme delays accessioning other contentn## Decision Makers!-- optional -->n*"}
{"File Name":"infrastructure-adrs\/0005-use-deposit-api-in-etd-application.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":791,"Predictions":"Decision!-- [driver 1]n* [driver 2]n* [driver 3]n* [driver 4]n* [driver 5]n* [driver 6]n* [driver 7]n* [driver 8]n* [driver 9]n* [driver 10]n* [driver 11]n* [driver 12]n* [driver 13]n* [driver 14]n* [driver 15]n* [driver 16]n* [driver 17]n* [driver 18]n* [driver 19]n* [driver 20]n* [driver 21]n* [driver 22]n* [driver 23]n* [driver 24]n* [driver 25]n* [driver 26]n* [driver 27]n* [driver 28]n* [driver 29]n* [driver 30]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n"}
{"File Name":"infrastructure-adrs\/0004-remove-workflow-datastream.md","Context":"## Context and Problem Statement <!-- required -->\\nTo advance SDR evolution towards decoupling from Fedora, we should store workflow state outside of Fedora (in the workflow service's database).\\n","Decision":"Remove the datastream.\\nThis was done in dor-services v9.0.0 ([commit](https:\/\/github.com\/sul-dlss\/dor-services\/commit\/8745e7c2e86edbbaa7577af85779c4ea06258dd3)).\\n","tokens":39,"id":792,"Predictions":"Decision (A) The SDR evolution towards decoupling from Fedora should be accelerated by decoupling workflow state outside of Fedora.n2(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)n3(2)"}
{"File Name":"infrastructure-adrs\/0007-change-dor-from-registry-to-repository.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":793,"Predictions":"Decision!-- [driver 1]n* [driver 2]n* [driver 3]n* [driver 4]n* [driver 5]n* [driver 6]n* [driver 7]n* [driver 8]n* [driver 9]n* [driver 10]n* [driver 11]n* [driver 12]n* [driver 13]n* [driver 14]n* [driver 15]n* [driver 16]n* [driver 17]n* [driver 18]n* [driver 19]n* [driver 20]n* [driver 21]n* [driver 22]n* [driver 23]n* [driver 24]n* [driver 25]n* [driver 26]n* [driver 27]n* [driver 28]n* [driver 29]n* [driver 30]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"Predictions":"Decision!-- [driver 1]n* [driver 2]n* [driver 3]n* [driver 4]n* [driver 5]n* [driver 6]n* [driver 7]n* [driver 8]n* [driver 9]n* [driver 10]n* [driver 11]n* [driver 12]n* [driver 13]n* [driver 14]n* [driver 15]n* [driver 16]n* [driver 17]n* [driver 18]n* [driver 19]n* [driver 20]n* [driver 21]n* [driver 22]n* [driver 23]n* [driver 24]n* [driver 25]n* [driver 26]n* [driver 27]n* [driver 28]n* [driver 29]n* [driver 30]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n* [driver 31]n"}
{"File Name":"infrastructure-adrs\/0006-replace-assembly-wf-with-deposit-api.md","Context":"## Context and Problem Statement <!-- required -->\\nCurrently the way to create new objects in the SDR is to:\\n1. Register an object (typically using Argo)\\n1. Move files into the `\/dor` mount\\n1. Run the assembly workflow (pre-assembly?)\\nThe Google Books project will grow SDR by millions of new objects. This growth is an opportunity to inject an API into the SDR architecture that serves as the \"face\" of SDR. Creating an external-facing API allows SDR to accept deposits from on-campus stakeholders such as Big Local News and Open Neuro. Fronting SDR with a new API affords us a mechanism to validate the completeness and correctness of deposited data very early in the accessioning process. Furthermore, we might also use such an API to refactor internal projects (such as ETDs) so these projects no longer require intimate knowledge of the internals of SDR, which reduces coupling.\\n## Decision Drivers <!-- optional -->\\n* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\\n","Decision":"* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\\nOption 2: we will create a deposit API as part of work on the 2020 Google Books work cycle.\\n","tokens":289,"id":795,"Predictions":"We would like to sever this coupling.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.n## Solution: Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier."}
{"File Name":"infrastructure-adrs\/0001-migrate-off-fedora-3.md","Context":"## Context and Problem Statement <!-- required -->\\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management \"repository,\" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\\n## Decision Drivers <!-- optional -->\\n* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\n","Decision":"* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\nNo decision made yet. See status field above.\\n","tokens":241,"id":796,"Predictions":"Decision Makers!-- optional -->nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management \"repository,\" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.n## Decision Drivers!-- optional -->n* Fedora 3 is unsupported and unlikely to be supportedn* Fedora 3 will be harder to install on newer operating systemsn* The Fedora 3 data model is not inherently validatablen* The Fedora 3 community is disappearing, so we are increasingly going it alonen* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous riskn* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own effortsn* We have (unverified) concerns about the scalability of Fedora 3n## Decision Makers!-- optional -->n* Fedora 3 is unsupported and unlikely to be supportedn* Fedora 3 will be harder to install on newer operating systemsn* The Fedora 3 data model is not inherently validatablen* The Fedora 3 community is disappearing, so we are increasingly going it alonen* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous riskn* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own effortsn* We have (unverified) concerns about the scalability of Fedora 3n"}
{"File Name":"form-design-system\/file_organization.md","Context":"## Context and Problem Statement\\nWe would like to generally flatten the directory structure in FDS as well as improving the\\nimport statements for client applications. This will be a breaking change for a major\\nversion, as clients will need to update imports once we make the change.\\n## Decision Drivers\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n","Decision":"- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n### Import statements\\nBecause individual imports for each component has a negative performance impact, we decided\\nto use destructured imports and rely on a tree shaking strategy:\\n```diff\\n-import Flex from '@cbinsights\/fds\/lib\/components\/layout\/Flex';\\n-import FlexItem from '@cbinsights\/fds\/lib\/components\/layout\/FlexItem';\\n-import TextInput from '@cbinsights\/fds\/lib\/components\/forms\/TextInput';\\n+import { Flex, FlexItem, TextInput } from '@cbinsights\/fds\/lib\/components';\\n```\\nThe \"root\" file for each import corresponds to an FDS code module.\\n| code module   | Import example                                                                        |\\n| ------------- | ------------------------------------------------------------------------------------- |\\n| `dictionary`  | `import { COLOR_TIMBERWOLF, COLOR_PURPLE } from '@cbinsights\/fds\/lib\/dictionary';`    |\\n| `components`  | `import { Avatar, TextInput } from '@cbinsights\/fds\/lib\/components\/';`                |\\n| `icons`       | `import { CloudIcon, BellIcon } from '@cbinsights\/fds\/lib\/icons';`                    |\\n| `assets`      | Use path `@cbinsights\/fds\/lib\/assets\/<file>` where applicable                         |\\n| `base-styles` | not applicable; we only publish this CSS as part of the main styles file in `assets\/` |\\n### `components` file structure\\nCategorization directories will be removed.\\nEach component will have its own directory under `components\/`.\\nTests, stories, private components, and helpers will be colocated in component\\ndirectories.\\n```\\ncomponents\/\\n\u251c\u2500\u2500 Foo\/\\n\u2502   \u251c\u2500\u2500 FooHelper.jsx\\n\u2502   \u251c\u2500\u2500 foo.test.jsx\\n\u2502   \u251c\u2500\u2500 foo.stories.jsx\\n\u2502   \u251c\u2500\u2500 utils.js\\n\u2502   \u2514\u2500\u2500 index.js  \/\/ contains Foo component code\\n...\\n```\\n### Documentation\\nThe \"Components\" section in Storybook will no longer have categorization. All components\\nwill be documented flat under the section heading.\\n","tokens":109,"id":797,"Predictions":"We will make the directory structure in FDS flatten.n## Decision Driversn- Remove unnecessary hierarchy in components dirn- Simplify documentationn- Adopt the unofficial CBI standard of organizing by component directories (each componentnhas its own directory).n- Improve ergonomics and performance of import statements for client applicationsn## Decision Makersn- Remove unnecessary hierarchy in components dirn- Simplify documentationn- Adopt the unofficial CBI standard of organizing by component directories (each componentnhas its own directory).n- Improve ergonomics and performance of import statements for client applicationsn## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n## Decision Makersn- Make the directory structure in FDS flatten.n"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"form-design-system\/remove_mui_theme.md","Context":"## Context and Problem Statement\\nThe [material-ui](https:\/\/material-ui.com) library is considered \"legacy\". In FDS, we\\naim to provide component coverage so that in the future, consumers will no longer need to\\nrely on material-ui.\\nThe FDS node module however, still provides a `mui-theme` theming object for the\\nmaterial-ui library.\\n","Decision":"Removed `mui-theme` from FDS.\\n### Negative Consequences <!-- optional -->\\n- Consumers will need to handle the breaking change. Consumers are now responsible for\\ndefining their own material-ui base theme.\\n","tokens":78,"id":799,"Predictions":"Decision (A). The [material-ui](https:\/\/material-ui.com) library is considered \"legacy\". In FDS, wenaim to provide component coverage so that in the future, consumers will no longer need tonrely on material-ui.nThe FDS node module however, still provides a mui-theme the theming object for thenmaterial-ui library.nn Decision (A). The [material-ui](https:\/\/material-ui.com) library is considered \"legacy\". In FDS, wenaim to provide component coverage so that in the future, consumers will no longer need tonrely on material-ui.nThe FDS node module however, still provides a mui-theme the theming object for thenmaterial-ui library.nn Decision (A). The [material-ui](https:\/\/material-ui.com) library is considered \"legacy\". In FDS, wenaim to provide component coverage so that in the future, consumers will no longer need tonrely on material-ui.nThe FDS node module however, still provides a mui-theme the theming object for thenmaterial-ui library.nn"}
{"File Name":"form-design-system\/reasonable_utility_classes.md","Context":"## Context and Problem Statement\\nWe take a utility class based approach (aka \"functional CSS\" aka \"expressive CSS\") in FDS.\\nWe have a lot of utility classes that do only one thing (e.g. `display--none`).\\nAt one point, we discussed adding a `position--relative` class, which led us to a key question\\nwe needed to clarify:\\n> When does it make sense to add a utility class?\\n","Decision":"Chosen option: \"Only add utility classes that are useful on their own\"\\n### Example of a utility class that is not \"useful on its own\"\\n```html\\n<div class=\"position--relative\">\\n<div>\\nIf this element is absolutely positioned, we need additional ad-hoc CSS rules beyond\\nwhat the utility class above provides. `position--relative` is not helpful on its own.\\n<\/div>\\n<\/div>\\n```\\n### Example of a utility class that is \"useful on its own\"\\n```html\\n<div class=\"color--red\">\\n<div>\\nThis text will be red. No additional CSS rules are needed for the above utility class\\nto be useful.\\n<\/div>\\n<\/div>\\n```\\n","tokens":95,"id":800,"Predictions":"Decision (A).nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.nWe need to clarify the question.n"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"Predictions":"Decision Makers!-- optional -->nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.n ## Decision Makers!-- optional -->nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.n"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"Predictions":"Decision (A) The component should accept a classNames prop. (B) The component should accept a classNames prop. (C) The component should accept a classNames prop. (D) The component should accept a classNames prop. (E) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop. (F) The component should accept a classNames prop."}
{"File Name":"form-design-system\/avoid_spread_props.md","Context":"## Context and Problem Statement\\nWhen should we spread props in components?\\n**Example**\\n```\\nconst Component = ({ foo, bar, ...restProps}) => {\\n<element foo={foo} bar={bar} {...restProps} \/>\\n}\\n```\\nIf we spread rest props on every component, we create an opportunity for developers to\\ncustomize components via code instead of explicitly defined props.\\nFor components that render attribute-heavy DOM elements (like `button`), it may make\\nsense to spread props. For more complex components, spreading props could lead to bugs,\\nside effects, and unnecessary customization.\\n","Decision":"Chosen option: \"Only components that render attribute-heavy DOM elements spread rest props\", because...\\n- Compromise that allows us to create the right prop interface for the job of each component\\n- Components that conceptually map to attribute-heavy DOM elements (i.e. html form elements)\\nwill allow spread props to keep the explicitly defined props easier to reason about.\\n- Components that **do not** conceptually map to attribute-heavy DOM elements (e.g. `Toast`)\\nwill be able to continue to define an explicit prop interface instead of spreading props.\\n### Negative Consequences <!-- optional -->\\n- Not all components will behave the same way.\\n- **We must document when a component supports spread props**.\\n","tokens":133,"id":803,"Predictions":"Decision (A) To spread props in components. (B) To spread props in components. (C) To spread props in components. (D) To spread props in components. (A) To spread props in components. (A) To spread props in components. (B) To spread props in components. (C) To spread props in components. (D) To spread props in components. (A) To spread props in components. (A) To spread props in components. (A) To spread props in components. (B) To spread props in components. (C) To spread props in components. (D) To spread props in components. (A) To spread props in components. (A) To spread props in components. (B) To spread props in components. (C) To spread props in components. (D) To spread props in components. (A) To spread props in components. (A) To spread props in components. (B) To spread props in components. (A) To spread props in components. (B) To spread props in components. (C) To spread props in components. (D) To spread props in components. (A) To spread props in components. (A) To spread props in components. (B) To spread props in components. (C) To spread props in components."}
{"File Name":"cosmos-sdk\/adr-024-coin-metadata.md","Context":"## Context\\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\\nAll account types have a set of balances that are composed of `Coins`. The `x\/bank` module keeps\\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\\napplication.\\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\\noperations in the Cosmos SDK work off of units of 10^6.\\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\\nas a result the total amount of asset types increases. We propose to have `x\/bank` additionally keep\\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\\nUX and remove the requirement for making any assumptions on the unit of denomination.\\n","Decision":"The `x\/bank` module will be updated to store and index metadata by `denom`, specifically the \"base\" or\\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\\nMetadata may also include a non-zero length list of denominations. Each entry contains the name of\\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\\nand on exchanges. The values in those fields link to an entry in the list of denominations.\\nThe list in `denom_units` and the `display` entry may be changed via governance.\\nAs a result, we can define the type as follows:\\n```protobuf\\nmessage DenomUnit {\\nstring denom    = 1;\\nuint32 exponent = 2;\\nrepeated string aliases = 3;\\n}\\nmessage Metadata {\\nstring description = 1;\\nrepeated DenomUnit denom_units = 2;\\nstring base = 3;\\nstring display = 4;\\n}\\n```\\nAs an example, the ATOM's metadata can be defined as follows:\\n```json\\n{\\n\"name\": \"atom\",\\n\"description\": \"The native staking token of the Cosmos Hub.\",\\n\"denom_units\": [\\n{\\n\"denom\": \"uatom\",\\n\"exponent\": 0,\\n\"aliases\": [\\n\"microatom\"\\n],\\n},\\n{\\n\"denom\": \"matom\",\\n\"exponent\": 3,\\n\"aliases\": [\\n\"milliatom\"\\n]\\n},\\n{\\n\"denom\": \"atom\",\\n\"exponent\": 6,\\n}\\n],\\n\"base\": \"uatom\",\\n\"display\": \"atom\",\\n}\\n```\\nGiven the above metadata, a client may infer the following things:\\n* 4.3atom = 4.3 * (10^6) = 4,300,000uatom\\n* The string \"atom\" can be used as a display name in a list of tokens.\\n* The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\\nan explicit decision to choose a different representation.\\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\\nas the base framework for this already exists in the Cosmos SDK.\\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x\/bank` module which is also\\nindexed by the base `denom`.\\n```go\\ntype GenesisState struct {\\nSendEnabled   bool        `json:\"send_enabled\" yaml:\"send_enabled\"`\\nBalances      []Balance   `json:\"balances\" yaml:\"balances\"`\\nSupply        sdk.Coins   `json:\"supply\" yaml:\"supply\"`\\nDenomMetadata []Metadata  `json:\"denom_metadata\" yaml:\"denom_metadata\"`\\n}\\n```\\n","tokens":336,"id":804,"Predictions":"We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution: We propose to have x\/bank additionally keepntrack of metadata per denom in order to help clients, wallet providers, and explorers improve theirnUX and remove the requirement for making any assumptions on the unit of denomination.n ## Solution:"}
{"File Name":"cosmos-sdk\/adr-007-specialization-groups.md","Context":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the concept of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","Decision":"A specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n* Membership Admittance\\n* Membership Acceptance\\n* Membership Revocation\\n* (probably) Without Penalty\\n* member steps down (self-Revocation)\\n* replaced by new member from governance\\n* (probably) With Penalty\\n* due to breach of soft-agreement (determined through governance)\\n* due to breach of hard-agreement (determined by code)\\n* Execution of Duties\\n* Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n* Compensation\\n* Group compensation (further distribution decided by the specialization group)\\n* Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admission to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possibilities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n","tokens":173,"id":805,"Predictions":"Decision (A).nThe idea of a decentralized Computer Emergency Response Team (dCERT) was first conceived of in order to fulfill the use case of thencreation of a decentralized Computer Emergency Response Team (dCERT), whosenmembers would be elected by a governing community and would fulfill the role ofncoordinating the community under emergency situations.nThis idea was first conceived of in order to fulfill the use case of thencreation of a decentralized Computer Emergency Response Team (dCERT), whosenmembers would be elected by a governing community and would fulfill the role ofncoordinating the community under emergency situations.nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.nnThe creation of these groups are the beginning of specialization capabilitiesnwithin a wider blockchain community which could be used to enable a certainnlevel of delegated responsibilities.n"}
{"File Name":"cosmos-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rogue software on a user\u2019s machine.\\n","tokens":215,"id":806,"Predictions":"We are seeking a solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-042-group-module.md","Context":"## Context\\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations:\\n* Key rotation is not possible, although this can be solved with [account rekeying](adr-034-account-rekeying.md).\\n* Thresholds can't be changed.\\n* UX is cumbersome for non-technical users ([#5661](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5661)).\\n* It requires `legacy_amino` sign mode ([#8141](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8141)).\\nWhile the group module is not meant to be a total replacement for the current multi-signature accounts, it provides a solution to the limitations described above, with a more flexible key management system where keys can be added, updated or removed, as well as configurable thresholds.\\nIt's meant to be used with other access control modules such as [`x\/feegrant`](.\/adr-029-fee-grant-module.md) and [`x\/authz`](adr-030-authz-module.md) to simplify key management for individuals and organizations.\\n","Decision":"We propose merging the `x\/group` module with its supporting ORM\/Table Store package ([#7098](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7098)) into the Cosmos SDK and continuing development here. There will be a dedicated ADR for the ORM package.\\n### Group\\nA group is a composition of accounts with associated weights. It is not\\nan account and doesn't have a balance. It doesn't in and of itself have any\\nsort of voting or decision weight.\\nGroup members can create proposals and vote on them through group accounts using different decision policies.\\nIt has an `admin` account which can manage members in the group, update the group\\nmetadata and set a new admin.\\n```protobuf\\nmessage GroupInfo {\\n\/\/ group_id is the unique ID of this group.\\nuint64 group_id = 1;\\n\/\/ admin is the account address of the group's admin.\\nstring admin = 2;\\n\/\/ metadata is any arbitrary metadata to attached to the group.\\nbytes metadata = 3;\\n\/\/ version is used to track changes to a group's membership structure that\\n\/\/ would break existing proposals. Whenever a member weight has changed,\\n\/\/ or any member is added or removed, the version is incremented and will\\n\/\/ invalidate all proposals from older versions.\\nuint64 version = 4;\\n\/\/ total_weight is the sum of the group members' weights.\\nstring total_weight = 5;\\n}\\n```\\n```protobuf\\nmessage GroupMember {\\n\/\/ group_id is the unique ID of the group.\\nuint64 group_id = 1;\\n\/\/ member is the member data.\\nMember member = 2;\\n}\\n\/\/ Member represents a group member with an account address,\\n\/\/ non-zero weight and metadata.\\nmessage Member {\\n\/\/ address is the member's account address.\\nstring address = 1;\\n\/\/ weight is the member's voting weight that should be greater than 0.\\nstring weight = 2;\\n\/\/ metadata is any arbitrary metadata to attached to the member.\\nbytes metadata = 3;\\n}\\n```\\n### Group Account\\nA group account is an account associated with a group and a decision policy.\\nA group account does have a balance.\\nGroup accounts are abstracted from groups because a single group may have\\nmultiple decision policies for different types of actions. Managing group\\nmembership separately from decision policies results in the least overhead\\nand keeps membership consistent across different policies. The pattern that\\nis recommended is to have a single master group account for a given group,\\nand then to create separate group accounts with different decision policies\\nand delegate the desired permissions from the master account to\\nthose \"sub-accounts\" using the [`x\/authz` module](adr-030-authz-module.md).\\n```protobuf\\nmessage GroupAccountInfo {\\n\/\/ address is the group account address.\\nstring address = 1;\\n\/\/ group_id is the ID of the Group the GroupAccount belongs to.\\nuint64 group_id = 2;\\n\/\/ admin is the account address of the group admin.\\nstring admin = 3;\\n\/\/ metadata is any arbitrary metadata of this group account.\\nbytes metadata = 4;\\n\/\/ version is used to track changes to a group's GroupAccountInfo structure that\\n\/\/ invalidates active proposal from old versions.\\nuint64 version = 5;\\n\/\/ decision_policy specifies the group account's decision policy.\\ngoogle.protobuf.Any decision_policy = 6 [(cosmos_proto.accepts_interface) = \"cosmos.group.v1.DecisionPolicy\"];\\n}\\n```\\nSimilarly to a group admin, a group account admin can update its metadata, decision policy or set a new group account admin.\\nA group account can also be an admin or a member of a group.\\nFor instance, a group admin could be another group account which could \"elects\" the members or it could be the same group that elects itself.\\n### Decision Policy\\nA decision policy is the mechanism by which members of a group can vote on\\nproposals.\\nAll decision policies should have a minimum and maximum voting window.\\nThe minimum voting window is the minimum duration that must pass in order\\nfor a proposal to potentially pass, and it may be set to 0. The maximum voting\\nwindow is the maximum time that a proposal may be voted on and executed if\\nit reached enough support before it is closed.\\nBoth of these values must be less than a chain-wide max voting window parameter.\\nWe define the `DecisionPolicy` interface that all decision policies must implement:\\n```go\\ntype DecisionPolicy interface {\\ncodec.ProtoMarshaler\\nValidateBasic() error\\nGetTimeout() types.Duration\\nAllow(tally Tally, totalPower string, votingDuration time.Duration) (DecisionPolicyResult, error)\\nValidate(g GroupInfo) error\\n}\\ntype DecisionPolicyResult struct {\\nAllow bool\\nFinal bool\\n}\\n```\\n#### Threshold decision policy\\nA threshold decision policy defines a minimum support votes (_yes_), based on a tally\\nof voter weights, for a proposal to pass. For\\nthis decision policy, abstain and veto are treated as no support (_no_).\\n```protobuf\\nmessage ThresholdDecisionPolicy {\\n\/\/ threshold is the minimum weighted sum of support votes for a proposal to succeed.\\nstring threshold = 1;\\n\/\/ voting_period is the duration from submission of a proposal to the end of voting period\\n\/\/ Within this period, votes and exec messages can be submitted.\\ngoogle.protobuf.Duration voting_period = 2 [(gogoproto.nullable) = false];\\n}\\n```\\n### Proposal\\nAny member of a group can submit a proposal for a group account to decide upon.\\nA proposal consists of a set of `sdk.Msg`s that will be executed if the proposal\\npasses as well as any metadata associated with the proposal. These `sdk.Msg`s get validated as part of the `Msg\/CreateProposal` request validation. They should also have their signer set as the group account.\\nInternally, a proposal also tracks:\\n* its current `Status`: submitted, closed or aborted\\n* its `Result`: unfinalized, accepted or rejected\\n* its `VoteState` in the form of a `Tally`, which is calculated on new votes and when executing the proposal.\\n```protobuf\\n\/\/ Tally represents the sum of weighted votes.\\nmessage Tally {\\noption (gogoproto.goproto_getters) = false;\\n\/\/ yes_count is the weighted sum of yes votes.\\nstring yes_count = 1;\\n\/\/ no_count is the weighted sum of no votes.\\nstring no_count = 2;\\n\/\/ abstain_count is the weighted sum of abstainers.\\nstring abstain_count = 3;\\n\/\/ veto_count is the weighted sum of vetoes.\\nstring veto_count = 4;\\n}\\n```\\n### Voting\\nMembers of a group can vote on proposals. There are four choices to choose while voting - yes, no, abstain and veto. Not\\nall decision policies will support them. Votes can contain some optional metadata.\\nIn the current implementation, the voting window begins as soon as a proposal\\nis submitted.\\nVoting internally updates the proposal `VoteState` as well as `Status` and `Result` if needed.\\n### Executing Proposals\\nProposals will not be automatically executed by the chain in this current design,\\nbut rather a user must submit a `Msg\/Exec` transaction to attempt to execute the\\nproposal based on the current votes and decision policy. A future upgrade could\\nautomate this and have the group account (or a fee granter) pay.\\n#### Changing Group Membership\\nIn the current implementation, updating a group or a group account after submitting a proposal will make it invalid. It will simply fail if someone calls `Msg\/Exec` and will eventually be garbage collected.\\n### Notes on current implementation\\nThis section outlines the current implementation used in the proof of concept of the group module but this could be subject to changes and iterated on.\\n#### ORM\\nThe [ORM package](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9156) defines tables, sequences and secondary indexes which are used in the group module.\\nGroups are stored in state as part of a `groupTable`, the `group_id` being an auto-increment integer. Group members are stored in a `groupMemberTable`.\\nGroup accounts are stored in a `groupAccountTable`. The group account address is generated based on an auto-increment integer which is used to derive the group module `RootModuleKey` into a `DerivedModuleKey`, as stated in [ADR-033](adr-033-protobuf-inter-module-comm.md#modulekeys-and-moduleids). The group account is added as a new `ModuleAccount` through `x\/auth`.\\nProposals are stored as part of the `proposalTable` using the `Proposal` type. The `proposal_id` is an auto-increment integer.\\nVotes are stored in the `voteTable`. The primary key is based on the vote's `proposal_id` and `voter` account address.\\n#### ADR-033 to route proposal messages\\nInter-module communication introduced by [ADR-033](adr-033-protobuf-inter-module-comm.md) can be used to route a proposal's messages using the `DerivedModuleKey` corresponding to the proposal's group account.\\n","tokens":231,"id":807,"Predictions":"Decision (A) To allow the group module to be used with other access control modules. (B) To allow the group module to be used with other access control modules. (C) To allow the group module to be used with other access control modules. (D) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (B) To allow the group module to be used with other access control modules. (C) To allow the group module to be used with other access control modules. (D) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (B) To allow the group module to be used with other access control modules. (C) To allow the group module to be used with other access control modules. (D) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (A) To allow the group module to be used with other access control modules. (B) To allow the group module to be used with other access control modules."}
{"File Name":"cosmos-sdk\/adr-045-check-delivertx-middlewares.md","Context":"## Context\\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9406) and [refunding unused gas](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\\nAn naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9182)), which includes making BaseApp more lightweight and modular.\\n","Decision":"We decide to transform Baseapp's implementation of ABCI `{Check,Deliver}Tx` and its own `Simulate` methods to use a middleware-based design.\\nThe two following interfaces are the base of the middleware design, and are defined in `types\/tx`:\\n```go\\ntype Handler interface {\\nCheckTx(ctx context.Context, req Request, checkReq RequestCheckTx) (Response, ResponseCheckTx, error)\\nDeliverTx(ctx context.Context, req Request) (Response, error)\\nSimulateTx(ctx context.Context, req Request (Response, error)\\n}\\ntype Middleware func(Handler) Handler\\n```\\nwhere we define the following arguments and return types:\\n```go\\ntype Request struct {\\nTx      sdk.Tx\\nTxBytes []byte\\n}\\ntype Response struct {\\nGasWanted uint64\\nGasUsed   uint64\\n\/\/ MsgResponses is an array containing each Msg service handler's response\\n\/\/ type, packed in an Any. This will get proto-serialized into the `Data` field\\n\/\/ in the ABCI Check\/DeliverTx responses.\\nMsgResponses []*codectypes.Any\\nLog          string\\nEvents       []abci.Event\\n}\\ntype RequestCheckTx struct {\\nType abci.CheckTxType\\n}\\ntype ResponseCheckTx struct {\\nPriority int64\\n}\\n```\\nPlease note that because CheckTx handles separate logic related to mempool priotization, its signature is different than DeliverTx and SimulateTx.\\nBaseApp holds a reference to a `tx.Handler`:\\n```go\\ntype BaseApp  struct {\\n\/\/ other fields\\ntxHandler tx.Handler\\n}\\n```\\nBaseapp's ABCI `{Check,Deliver}Tx()` and `Simulate()` methods simply call `app.txHandler.{Check,Deliver,Simulate}Tx()` with the relevant arguments. For example, for `DeliverTx`:\\n```go\\nfunc (app *BaseApp) DeliverTx(req abci.RequestDeliverTx) abci.ResponseDeliverTx {\\nvar abciRes abci.ResponseDeliverTx\\nctx := app.getContextForTx(runTxModeDeliver, req.Tx)\\nres, err := app.txHandler.DeliverTx(ctx, tx.Request{TxBytes: req.Tx})\\nif err != nil {\\nabciRes = sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\\nreturn abciRes\\n}\\nabciRes, err = convertTxResponseToDeliverTx(res)\\nif err != nil {\\nreturn sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\\n}\\nreturn abciRes\\n}\\n\/\/ convertTxResponseToDeliverTx converts a tx.Response into a abci.ResponseDeliverTx.\\nfunc convertTxResponseToDeliverTx(txRes tx.Response) (abci.ResponseDeliverTx, error) {\\ndata, err := makeABCIData(txRes)\\nif err != nil {\\nreturn abci.ResponseDeliverTx{}, nil\\n}\\nreturn abci.ResponseDeliverTx{\\nData:   data,\\nLog:    txRes.Log,\\nEvents: txRes.Events,\\n}, nil\\n}\\n\/\/ makeABCIData generates the Data field to be sent to ABCI Check\/DeliverTx.\\nfunc makeABCIData(txRes tx.Response) ([]byte, error) {\\nreturn proto.Marshal(&sdk.TxMsgData{MsgResponses: txRes.MsgResponses})\\n}\\n```\\nThe implementations are similar for `BaseApp.CheckTx` and `BaseApp.Simulate`.\\n`baseapp.txHandler`'s three methods' implementations can obviously be monolithic functions, but for modularity we propose a middleware composition design, where a middleware is simply a function that takes a `tx.Handler`, and returns another `tx.Handler` wrapped around the previous one.\\n### Implementing a Middleware\\nIn practice, middlewares are created by Go function that takes as arguments some parameters needed for the middleware, and returns a `tx.Middleware`.\\nFor example, for creating an arbitrary `MyMiddleware`, we can implement:\\n```go\\n\/\/ myTxHandler is the tx.Handler of this middleware. Note that it holds a\\n\/\/ reference to the next tx.Handler in the stack.\\ntype myTxHandler struct {\\n\/\/ next is the next tx.Handler in the middleware stack.\\nnext tx.Handler\\n\/\/ some other fields that are relevant to the middleware can be added here\\n}\\n\/\/ NewMyMiddleware returns a middleware that does this and that.\\nfunc NewMyMiddleware(arg1, arg2) tx.Middleware {\\nreturn func (txh tx.Handler) tx.Handler {\\nreturn myTxHandler{\\nnext: txh,\\n\/\/ optionally, set arg1, arg2... if they are needed in the middleware\\n}\\n}\\n}\\n\/\/ Assert myTxHandler is a tx.Handler.\\nvar _ tx.Handler = myTxHandler{}\\nfunc (h myTxHandler) CheckTx(ctx context.Context, req Request, checkReq RequestcheckTx) (Response, ResponseCheckTx, error) {\\n\/\/ CheckTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, checkRes, err := txh.next.CheckTx(ctx, req, checkReq)\\n\/\/ CheckTx specific post-processing logic\\nreturn res, checkRes, err\\n}\\nfunc (h myTxHandler) DeliverTx(ctx context.Context, req Request) (Response, error) {\\n\/\/ DeliverTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, err := txh.next.DeliverTx(ctx, tx, req)\\n\/\/ DeliverTx specific post-processing logic\\nreturn res, err\\n}\\nfunc (h myTxHandler) SimulateTx(ctx context.Context, req Request) (Response, error) {\\n\/\/ SimulateTx specific pre-processing logic\\n\/\/ run the next middleware\\nres, err := txh.next.SimulateTx(ctx, tx, req)\\n\/\/ SimulateTx specific post-processing logic\\nreturn res, err\\n}\\n```\\n### Composing Middlewares\\nWhile BaseApp simply holds a reference to a `tx.Handler`, this `tx.Handler` itself is defined using a middleware stack. The Cosmos SDK exposes a base (i.e. innermost) `tx.Handler` called `RunMsgsTxHandler`, which executes messages.\\nThen, the app developer can compose multiple middlewares on top on the base `tx.Handler`. Each middleware can run pre-and-post-processing logic around its next middleware, as described in the section above. Conceptually, as an example, given the middlewares `A`, `B`, and `C` and the base `tx.Handler` `H` the stack looks like:\\n```text\\nA.pre\\nB.pre\\nC.pre\\nH # The base tx.handler, for example `RunMsgsTxHandler`\\nC.post\\nB.post\\nA.post\\n```\\nWe define a `ComposeMiddlewares` function for composing middlewares. It takes the base handler as first argument, and middlewares in the \"outer to inner\" order. For the above stack, the final `tx.Handler` is:\\n```go\\ntxHandler := middleware.ComposeMiddlewares(H, A, B, C)\\n```\\nThe middleware is set in BaseApp via its `SetTxHandler` setter:\\n```go\\n\/\/ simapp\/app.go\\ntxHandler := middleware.ComposeMiddlewares(...)\\napp.SetTxHandler(txHandler)\\n```\\nThe app developer can define their own middlewares, or use the Cosmos SDK's pre-defined middlewares from `middleware.NewDefaultTxHandler()`.\\n### Middlewares Maintained by the Cosmos SDK\\nWhile the app developer can define and compose the middlewares of their choice, the Cosmos SDK provides a set of middlewares that caters for the ecosystem's most common use cases. These middlewares are:\\n| Middleware              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| RunMsgsTxHandler        | This is the base `tx.Handler`. It replaces the old baseapp's `runMsgs`, and executes a transaction's `Msg`s.                                                                                                                                                                                                                                                                                                                                                                             |\\n| TxDecoderMiddleware     | This middleware takes in transaction raw bytes, and decodes them into a `sdk.Tx`. It replaces the `baseapp.txDecoder` field, so that BaseApp stays as thin as possible. Since most middlewares read the contents of the `sdk.Tx`, the TxDecoderMiddleware should be run first in the middleware stack.                                                                                                                                                                                   |\\n| {Antehandlers}          | Each antehandler is converted to its own middleware. These middlewares perform signature verification, fee deductions and other validations on the incoming transaction.                                                                                                                                                                                                                                                                                                                 |\\n| IndexEventsTxMiddleware | This is a simple middleware that chooses which events to index in Tendermint. Replaces `baseapp.indexEvents` (which unfortunately still exists in baseapp too, because it's used to index Begin\/EndBlock events)                                                                                                                                                                                                                                                                         |\\n| RecoveryTxMiddleware    | This index recovers from panics. It replaces baseapp.runTx's panic recovery described in [ADR-022](.\/adr-022-custom-panic-handling.md).                                                                                                                                                                                                                                                                                                                                                  |\\n| GasTxMiddleware         | This replaces the [`Setup`](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/v0.43.0\/x\/auth\/ante\/setup.go) Antehandler. It sets a GasMeter on sdk.Context. Note that before, GasMeter was set on sdk.Context inside the antehandlers, and there was some mess around the fact that antehandlers had their own panic recovery system so that the GasMeter could be read by baseapp's recovery system. Now, this mess is all removed: one middleware sets GasMeter, another one handles recovery. |\\n### Similarities and Differences between Antehandlers and Middlewares\\nThe middleware-based design builds upon the existing antehandlers design described in [ADR-010](.\/adr-010-modular-antehandler.md). Even though the final decision of ADR-010 was to go with the \"Simple Decorators\" approach, the middleware design is actually very similar to the other [Decorator Pattern](.\/adr-010-modular-antehandler.md#decorator-pattern) proposal, also used in [weave](https:\/\/github.com\/iov-one\/weave).\\n#### Similarities with Antehandlers\\n* Designed as chaining\/composing small modular pieces.\\n* Allow code reuse for `{Check,Deliver}Tx` and for `Simulate`.\\n* Set up in `app.go`, and easily customizable by app developers.\\n* Order is important.\\n#### Differences with Antehandlers\\n* The Antehandlers are run before `Msg` execution, whereas middlewares can run before and after.\\n* The middleware approach uses separate methods for `{Check,Deliver,Simulate}Tx`, whereas the antehandlers pass a `simulate bool` flag and uses the `sdkCtx.Is{Check,Recheck}Tx()` flags to determine in which transaction mode we are.\\n* The middleware design lets each middleware hold a reference to the next middleware, whereas the antehandlers pass a `next` argument in the `AnteHandle` method.\\n* The middleware design use Go's standard `context.Context`, whereas the antehandlers use `sdk.Context`.\\n","tokens":190,"id":808,"Predictions":"Decision (A) To make BaseApp more lightweight and modular. (B) To make BaseApp more modular. (C) To make BaseApp more modular. (D) To make BaseApp more modular. (A) To make BaseApp more lightweight and modular. (A) To make BaseApp more modular. (A) To make BaseApp more modular. (B) To make BaseApp more lightweight and modular. (C) To make BaseApp more modular. (D) To make BaseApp more modular. (A) To make BaseApp more lightweight and modular. (A) To make BaseApp more modular. (A) To make BaseApp more lightweight and modular. (A) To make BaseApp more modular. (B) To make BaseApp more lightweight and modular. (C) To make BaseApp more modular. (D) To make BaseApp more lightweight and modular. (A) To make BaseApp more modular. (A) To make BaseApp more lightweight and modular. (A) To make BaseApp more modular. (B) To make BaseApp more lightweight and modular. (C) To make BaseApp more modular. (D) To make BaseApp more lightweight and modular. (A) To make BaseApp more modular. (A) To make BaseApp more lightweight and modular. (A) To make BaseApp more modular. (B) To make BaseApp more lightweight and modular. (C) To make BaseApp more modular. (D) To make BaseApp more lightweight and modular."}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"Predictions":"Decision (A).n[ADR] is a continuation of the motivation, design, and context established in [ADR 019](.\/adr-019-protobuf-state-encoding.md) and [ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)nto specify the encoding of queries.nAnswer:A.nThe ADR is a continuation of the motivation, design, and context established in [ADR 019](.\/adr-019-protobuf-state-encoding.md) and [ADR 020](.\/adr-020-protobuf-transaction-encoding.md)nto specify the encoding of queries.nAnswer:A.nThe ADR is a continuation of the motivation, design, and context established in [ADR 019](.\/adr-019-protobuf-state-encoding.md) and [ADR 020](.\/adr-020-protobuf-transaction-encoding.md)nto specify the encoding of queries.nAnswer:A."}
{"File Name":"cosmos-sdk\/adr-046-module-params.md","Context":"## Context\\nCurrently, in the Cosmos SDK, modules that require the use of parameters use the\\n`x\/params` module. The `x\/params` works by having modules define parameters,\\ntypically via a simple `Params` structure, and registering that structure in\\nthe `x\/params` module via a unique `Subspace` that belongs to the respective\\nregistering module. The registering module then has unique access to its respective\\n`Subspace`. Through this `Subspace`, the module can get and set its `Params`\\nstructure.\\nIn addition, the Cosmos SDK's `x\/gov` module has direct support for changing\\nparameters on-chain via a `ParamChangeProposal` governance proposal type, where\\nstakeholders can vote on suggested parameter changes.\\nThere are various tradeoffs to using the `x\/params` module to manage individual\\nmodule parameters. Namely, managing parameters essentially comes for \"free\" in\\nthat developers only need to define the `Params` struct, the `Subspace`, and the\\nvarious auxiliary functions, e.g. `ParamSetPairs`, on the `Params` type. However,\\nthere are some notable drawbacks. These drawbacks include the fact that parameters\\nare serialized in state via JSON which is extremely slow. In addition, parameter\\nchanges via `ParamChangeProposal` governance proposals have no way of reading from\\nor writing to state. In other words, it is currently not possible to have any\\nstate transitions in the application during an attempt to change param(s).\\n","Decision":"We will build off of the alignment of `x\/gov` and `x\/authz` work per\\n[#9810](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/9810). Namely, module developers\\nwill create one or more unique parameter data structures that must be serialized\\nto state. The Param data structures must implement `sdk.Msg` interface with respective\\nProtobuf Msg service method which will validate and update the parameters with all\\nnecessary changes. The `x\/gov` module via the work done in\\n[#9810](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/9810), will dispatch Param\\nmessages, which will be handled by Protobuf Msg services.\\nNote, it is up to developers to decide how to structure their parameters and\\nthe respective `sdk.Msg` messages. Consider the parameters currently defined in\\n`x\/auth` using the `x\/params` module for parameter management:\\n```protobuf\\nmessage Params {\\nuint64 max_memo_characters       = 1;\\nuint64 tx_sig_limit              = 2;\\nuint64 tx_size_cost_per_byte     = 3;\\nuint64 sig_verify_cost_ed25519   = 4;\\nuint64 sig_verify_cost_secp256k1 = 5;\\n}\\n```\\nDevelopers can choose to either create a unique data structure for every field in\\n`Params` or they can create a single `Params` structure as outlined above in the\\ncase of `x\/auth`.\\nIn the former, `x\/params`, approach, a `sdk.Msg` would need to be created for every single\\nfield along with a handler. This can become burdensome if there are a lot of\\nparameter fields. In the latter case, there is only a single data structure and\\nthus only a single message handler, however, the message handler might have to be\\nmore sophisticated in that it might need to understand what parameters are being\\nchanged vs what parameters are untouched.\\nParams change proposals are made using the `x\/gov` module. Execution is done through\\n`x\/authz` authorization to the root `x\/gov` module's account.\\nContinuing to use `x\/auth`, we demonstrate a more complete example:\\n```go\\ntype Params struct {\\nMaxMemoCharacters      uint64\\nTxSigLimit             uint64\\nTxSizeCostPerByte      uint64\\nSigVerifyCostED25519   uint64\\nSigVerifyCostSecp256k1 uint64\\n}\\ntype MsgUpdateParams struct {\\nMaxMemoCharacters      uint64\\nTxSigLimit             uint64\\nTxSizeCostPerByte      uint64\\nSigVerifyCostED25519   uint64\\nSigVerifyCostSecp256k1 uint64\\n}\\ntype MsgUpdateParamsResponse struct {}\\nfunc (ms msgServer) UpdateParams(goCtx context.Context, msg *types.MsgUpdateParams) (*types.MsgUpdateParamsResponse, error) {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n\/\/ verification logic...\\n\/\/ persist params\\nparams := ParamsFromMsg(msg)\\nms.SaveParams(ctx, params)\\nreturn &types.MsgUpdateParamsResponse{}, nil\\n}\\nfunc ParamsFromMsg(msg *types.MsgUpdateParams) Params {\\n\/\/ ...\\n}\\n```\\nA gRPC `Service` query should also be provided, for example:\\n```protobuf\\nservice Query {\\n\/\/ ...\\nrpc Params(QueryParamsRequest) returns (QueryParamsResponse) {\\noption (google.api.http).get = \"\/cosmos\/<module>\/v1beta1\/params\";\\n}\\n}\\nmessage QueryParamsResponse {\\nParams params = 1 [(gogoproto.nullable) = false];\\n}\\n```\\n","tokens":317,"id":811,"Predictions":"Decision based on the information in this passage.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"Predictions":"Decision (A). The x\/bank keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nB. The x\/upgrade keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nC. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nD. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nA. The x\/upgrade keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nB. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nC. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nB. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nC. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nB. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nC. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nB. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nC. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nB. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nC. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nB. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.nC. The x\/staking keeper is a good choice for a thriving ecosystem of Cosmos SDK modules.n"}
{"File Name":"cosmos-sdk\/adr-009-evidence-module.md","Context":"## Context\\nIn order to support building highly secure, robust and interoperable blockchain\\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\\nevidence can be submitted, evaluated and verified resulting in some agreed upon\\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any\\n[IBC](https:\/\/github.com\/cosmos\/ibc) or\\ncross-chain validation protocol implementation in order to support the ability\\nfor any misbehavior to be relayed back from a collateralized chain to a primary\\nchain so that the equivocating validator(s) can be slashed.\\n","Decision":"We will implement an evidence module in the Cosmos SDK supporting the following\\nfunctionality:\\n* Provide developers with the abstractions and interfaces necessary to define\\ncustom evidence messages, message handlers, and methods to slash and penalize\\naccordingly for misbehavior.\\n* Support the ability to route evidence messages to handlers in any module to\\ndetermine the validity of submitted misbehavior.\\n* Support the ability, through governance, to modify slashing penalties of any\\nevidence type.\\n* Querier implementation to support querying params, evidence types, params, and\\nall submitted valid misbehavior.\\n### Types\\nFirst, we define the `Evidence` interface type. The `x\/evidence` module may implement\\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\\nIn addition, other modules may implement their own `Evidence` types in a similar\\nmanner in which governance is extensible. It is important to note any concrete\\ntype implementing the `Evidence` interface may include arbitrary fields such as\\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\\nWhen submitting evidence to the `x\/evidence` module, the concrete type must provide\\nthe validator's consensus address, which should be known by the `x\/slashing`\\nmodule (assuming the infraction is valid), the height at which the infraction\\noccurred and the validator's power at same height in which the infraction occurred.\\n```go\\ntype Evidence interface {\\nRoute() string\\nType() string\\nString() string\\nHash() HexBytes\\nValidateBasic() error\\n\/\/ The consensus address of the malicious validator at time of infraction\\nGetConsensusAddress() ConsAddress\\n\/\/ Height at which the infraction occurred\\nGetHeight() int64\\n\/\/ The total power of the malicious validator at time of infraction\\nGetValidatorPower() int64\\n\/\/ The total validator set power at time of infraction\\nGetTotalPower() int64\\n}\\n```\\n### Routing & Handling\\nEach `Evidence` type must map to a specific unique route and be registered with\\nthe `x\/evidence` module. It accomplishes this through the `Router` implementation.\\n```go\\ntype Router interface {\\nAddRoute(r string, h Handler) Router\\nHasRoute(r string) bool\\nGetRoute(path string) Handler\\nSeal()\\n}\\n```\\nUpon successful routing through the `x\/evidence` module, the `Evidence` type\\nis passed through a `Handler`. This `Handler` is responsible for executing all\\ncorresponding business logic necessary for verifying the evidence as valid. In\\naddition, the `Handler` may execute any necessary slashing and potential jailing.\\nSince slashing fractions will typically result from some form of static functions,\\nallow the `Handler` to do this provides the greatest flexibility. An example could\\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\\nby governance. The `Evidence` type should provide all the external information\\nnecessary in order for the `Handler` to make the necessary state transitions.\\nIf no error is returned, the `Evidence` is considered valid.\\n```go\\ntype Handler func(Context, Evidence) error\\n```\\n### Submission\\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\\nhandled by the `x\/evidence` module's `SubmitEvidence`.\\n```go\\ntype MsgSubmitEvidence struct {\\nEvidence\\n}\\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\\nreturn err.Result()\\n}\\n\/\/ emit events...\\nreturn Result{\\n\/\/ ...\\n}\\n}\\n```\\nThe `x\/evidence` module's keeper is responsible for matching the `Evidence` against\\nthe module's router and invoking the corresponding `Handler` which may include\\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\\n```go\\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\\nhandler := keeper.router.GetRoute(evidence.Route())\\nif err := handler(ctx, evidence); err != nil {\\nreturn ErrInvalidEvidence(keeper.codespace, err)\\n}\\nkeeper.setEvidence(ctx, evidence)\\nreturn nil\\n}\\n```\\n### Genesis\\nFinally, we need to represent the genesis state of the `x\/evidence` module. The\\nmodule only needs a list of all submitted valid infractions and any necessary params\\nfor which the module needs in order to handle submitted evidence. The `x\/evidence`\\nmodule will naturally define and route native evidence types for which it'll most\\nlikely need slashing penalty constants for.\\n```go\\ntype GenesisState struct {\\nParams       Params\\nInfractions  []Evidence\\n}\\n```\\n","tokens":163,"id":813,"Predictions":"Decision based on the following: a mechanism in which arbitrarynevidence can be submitted, evaluated and verified resulting in some agreed uponnpenalty for any misbehavior committed by a validator, such as equivocation (double-voting),nsigning when unbonded, signing an incorrect state transition (in the future), etc.n Furthermore, such a mechanism is paramount for anyn[IBC](https:\/\/github.com\/cosmos\/ibc) orncross-chain validation protocol implementation in order to support the abilitynfor any misbehavior to be relayed back from a collateralized chain to a primarynchain so that the equivocating validator(s) can be slashed.n ## Final decision.n1n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n2n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1n1"}
{"File Name":"cosmos-sdk\/adr-036-arbitrary-signature.md","Context":"## Context\\nHaving the ability to sign messages off-chain has proven to be a fundamental aspect of nearly any blockchain. The notion of signing messages off-chain has many added benefits such as saving on computational costs and reducing transaction throughput and overhead. Within the context of the Cosmos, some of the major applications of signing such data includes, but is not limited to, providing a cryptographic secure and verifiable means of proving validator identity and possibly associating it with some other framework or organization. In addition, having the ability to sign Cosmos messages with a Ledger or similar HSM device.\\nFurther context and use cases can be found in the references links.\\n","Decision":"The aim is being able to sign arbitrary messages, even using Ledger or similar HSM devices.\\nAs a result signed messages should look roughly like Cosmos SDK messages but **must not** be a valid on-chain transaction. `chain-id`, `account_number` and `sequence` can all be assigned invalid values.\\nCosmos SDK 0.40 also introduces a concept of \u201cauth_info\u201d this can specify SIGN_MODES.\\nA spec should include an `auth_info` that supports SIGN_MODE_DIRECT and SIGN_MODE_LEGACY_AMINO.\\nCreate the `offchain` proto definitions, we extend the auth module with `offchain` package to offer functionalities to verify and sign offline messages.\\nAn offchain transaction follows these rules:\\n* the memo must be empty\\n* nonce, sequence number must be equal to 0\\n* chain-id must be equal to \u201c\u201d\\n* fee gas must be equal to 0\\n* fee amount must be an empty array\\nVerification of an offchain transaction follows the same rules as an onchain one, except for the spec differences highlighted above.\\nThe first message added to the `offchain` package is `MsgSignData`.\\n`MsgSignData` allows developers to sign arbitrary bytes valid offchain only. Where `Signer` is the account address of the signer. `Data` is arbitrary bytes which can represent `text`, `files`, `object`s. It's applications developers decision how `Data` should be deserialized, serialized and the object it can represent in their context.\\nIt's applications developers decision how `Data` should be treated, by treated we mean the serialization and deserialization process and the Object `Data` should represent.\\nProto definition:\\n```protobuf\\n\/\/ MsgSignData defines an arbitrary, general-purpose, off-chain message\\nmessage MsgSignData {\\n\/\/ Signer is the sdk.AccAddress of the message signer\\nbytes Signer = 1 [(gogoproto.jsontag) = \"signer\", (gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"];\\n\/\/ Data represents the raw bytes of the content that is signed (text, json, etc)\\nbytes Data = 2 [(gogoproto.jsontag) = \"data\"];\\n}\\n```\\nSigned MsgSignData json example:\\n```json\\n{\\n\"type\": \"cosmos-sdk\/StdTx\",\\n\"value\": {\\n\"msg\": [\\n{\\n\"type\": \"sign\/MsgSignData\",\\n\"value\": {\\n\"signer\": \"cosmos1hftz5ugqmpg9243xeegsqqav62f8hnywsjr4xr\",\\n\"data\": \"cmFuZG9t\"\\n}\\n}\\n],\\n\"fee\": {\\n\"amount\": [],\\n\"gas\": \"0\"\\n},\\n\"signatures\": [\\n{\\n\"pub_key\": {\\n\"type\": \"tendermint\/PubKeySecp256k1\",\\n\"value\": \"AqnDSiRoFmTPfq97xxEb2VkQ\/Hm28cPsqsZm9jEVsYK9\"\\n},\\n\"signature\": \"8y8i34qJakkjse9pOD2De+dnlc4KvFgh0wQpes4eydN66D9kv7cmCEouRrkka9tlW9cAkIL52ErB+6ye7X5aEg==\"\\n}\\n],\\n\"memo\": \"\"\\n}\\n}\\n```\\n","tokens":129,"id":814,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-013-metrics.md","Context":"## Context\\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\\nBy default, when telemetry is enabled, the application should track and expose metrics that are\\nstored in-memory. The operator may choose to enable additional sinks, where we support only\\n[Prometheus](https:\/\/prometheus.io\/) for now, as it's battle-tested, simple to setup, open source,\\nand is rich with ecosystem tooling.\\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\\nmetrics may be added or removed at will and without much friction. To do this, we will use the\\n[go-metrics](https:\/\/github.com\/hashicorp\/go-metrics) library.\\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\\nwill be exposed via `\/metrics?format={text|prometheus}` via the API server.\\n","Decision":"We will add an additional configuration block to `app.toml` that defines telemetry settings:\\n```toml\\n###############################################################################\\n###                         Telemetry Configuration                         ###\\n###############################################################################\\n[telemetry]\\n# Prefixed with keys to separate services\\nservice-name = {{ .Telemetry.ServiceName }}\\n# Enabled enables the application telemetry functionality. When enabled,\\n# an in-memory sink is also enabled by default. Operators may also enabled\\n# other sinks such as Prometheus.\\nenabled = {{ .Telemetry.Enabled }}\\n# Enable prefixing gauge values with hostname\\nenable-hostname = {{ .Telemetry.EnableHostname }}\\n# Enable adding hostname to labels\\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\\n# Enable adding service to labels\\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\\n```\\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\\n```go\\n\/\/ Metrics defines a wrapper around application telemetry functionality. It allows\\n\/\/ metrics to be gathered at any point in time. When creating a Metrics object,\\n\/\/ internally, a global metrics is registered with a set of sinks as configured\\n\/\/ by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\\n\/\/ dump of formatted recent metrics will be sent to STDERR.\\ntype Metrics struct {\\nmemSink           *metrics.InmemSink\\nprometheusEnabled bool\\n}\\n\/\/ Gather collects all registered metrics and returns a GatherResponse where the\\n\/\/ metrics are encoded depending on the type. Metrics are either encoded via\\n\/\/ Prometheus or JSON if in-memory.\\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\\nswitch format {\\ncase FormatPrometheus:\\nreturn m.gatherPrometheus()\\ncase FormatText:\\nreturn m.gatherGeneric()\\ncase FormatDefault:\\nreturn m.gatherGeneric()\\ndefault:\\nreturn GatherResponse{}, fmt.Errorf(\"unsupported metrics format: %s\", format)\\n}\\n}\\n```\\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\\nhandler accordingly.\\n```go\\nfunc (s *Server) Start(cfg config.Config) error {\\n\/\/ ...\\nif cfg.Telemetry.Enabled {\\nm, err := telemetry.New(cfg.Telemetry)\\nif err != nil {\\nreturn err\\n}\\ns.metrics = m\\ns.registerMetrics()\\n}\\n\/\/ ...\\n}\\nfunc (s *Server) registerMetrics() {\\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\\nformat := strings.TrimSpace(r.FormValue(\"format\"))\\ngr, err := s.metrics.Gather(format)\\nif err != nil {\\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(\"failed to gather metrics: %s\", err))\\nreturn\\n}\\nw.Header().Set(\"Content-Type\", gr.ContentType)\\n_, _ = w.Write(gr.Metrics)\\n}\\ns.Router.HandleFunc(\"\/metrics\", metricsHandler).Methods(\"GET\")\\n}\\n```\\nApplication developers may track counters, gauges, summaries, and key\/value metrics. There is no\\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\\n```go\\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\\ndefer metrics.MeasureSince(time.Now(), \"MintCoins\")\\n\/\/ ...\\n}\\n```\\n","tokens":230,"id":815,"Predictions":"Decision (A) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.nB) To enable telemetry in the Cosmos SDK.n"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"Predictions":"Decision (A).nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.nThe current documentation contains a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user"}
{"File Name":"cosmos-sdk\/adr-003-dynamic-capability-store.md","Context":"## Context\\nFull implementation of the [IBC specification](https:\/\/github.com\/cosmos\/ibc) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\\nas described in [ICS 5](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\\nobject-capability keys at this time.\\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L132))\\nand passed to Keepers as fixed arguments ([example](https:\/\/github.com\/cosmos\/gaia\/blob\/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4\/app\/app.go#L160)). Keepers cannot create or store capability keys during transaction execution \u2014 although they could call `NewKVStoreKey` and take the memory address\\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional \u2014 were this not the case, the keys would be predictable and couldn't serve as object capabilities).\\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\\nThis ADR proposes such an interface & mechanism.\\n","Decision":"The Cosmos SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\\ninvoked.\\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\\nchain is running.\\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\\nThe persistent `KVStore` tracks which capability is owned by which modules.\\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\\na reverse mapping that map from module name, capability name to the capability index.\\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\\ngo-map to retrieve the capability at the original memory location.\\nThe `CapabilityKeeper` will define the following types & functions:\\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\\na name. A `String()` method is provided for debugging.\\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\\n```go\\ntype Capability struct {\\nindex uint64\\n}\\n```\\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\\n```go\\ntype CapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleNames   map[string]interface{}\\nsealed        bool\\n}\\n```\\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\\npassed by other modules.\\n```go\\ntype ScopedCapabilityKeeper struct {\\npersistentKey StoreKey\\nmemKey        StoreKey\\ncapMap        map[uint64]*Capability\\nmoduleName    string\\n}\\n```\\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\\nIt MUST be called before `InitialiseAndSeal`.\\n```go\\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\\nif ck.sealed {\\npanic(\"cannot scope to module via a sealed capability keeper\")\\n}\\nif _, ok := ck.scopedModules[moduleName]; ok {\\npanic(fmt.Sprintf(\"cannot create multiple scoped keepers for the same module name: %s\", moduleName))\\n}\\nck.scopedModules[moduleName] = struct{}{}\\nreturn ScopedKeeper{\\ncdc:      ck.cdc,\\nstoreKey: ck.storeKey,\\nmemKey:   ck.memKey,\\ncapMap:   ck.capMap,\\nmodule:   moduleName,\\n}\\n}\\n```\\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\\ncreation of any new `ScopedCapabilityKeeper`s.\\n```go\\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\\nif ck.sealed {\\npanic(\"capability keeper is sealed\")\\n}\\npersistentStore := ctx.KVStore(ck.persistentKey)\\nmap := ctx.KVStore(ck.memKey)\\n\/\/ initialise memory store for all names in persistent store\\nfor index, value := range persistentStore.Iter() {\\ncapability = &CapabilityKey{index: index}\\nfor moduleAndCapability := range value {\\nmoduleName, capabilityName := moduleAndCapability.Split(\"\/\")\\nmemStore.Set(moduleName + \"\/fwd\/\" + capability, capabilityName)\\nmemStore.Set(moduleName + \"\/rev\/\" + capabilityName, index)\\nck.capMap[index] = capability\\n}\\n}\\nck.sealed = true\\n}\\n```\\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\\nreference. The newly created capability is automatically persisted; the calling module need not\\ncall `ClaimCapability`.\\n```go\\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\\n\/\/ check name not taken in memory store\\nif capStore.Get(\"rev\/\" + name) != nil {\\nreturn nil, errors.New(\"name already taken\")\\n}\\n\/\/ fetch the current index\\nindex := persistentStore.Get(\"index\")\\n\/\/ create a new capability\\ncapability := &CapabilityKey{index: index}\\n\/\/ set persistent store\\npersistentStore.Set(index, Set.singleton(sck.moduleName + \"\/\" + name))\\n\/\/ update the index\\nindex++\\npersistentStore.Set(\"index\", index)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to index\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, index)\\n\/\/ set the in-memory mapping from index to capability pointer\\ncapMap[index] = capability\\n\/\/ return the newly created capability\\nreturn capability\\n}\\n```\\n`AuthenticateCapability` can be called by any module to check that a capability\\ndoes in fact correspond to a particular name (the name can be untrusted user input)\\nwith which the calling module previously associated it.\\n```go\\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\\n\/\/ return whether forward mapping in memory store matches name\\nreturn memStore.Get(sck.moduleName + \"\/fwd\/\" + capability) === name\\n}\\n```\\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\\nso that future `GetCapability` calls will succeed.\\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\\nthey will all own it.\\n```go\\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\n\/\/ set forward mapping in memory store from capability to name\\nmemStore.Set(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ set reverse mapping in memory store from name to capability\\nmemStore.Set(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.add(sck.moduleName + \"\/\" + name)\\npersistentStore.Set(capability.Index(), owners)\\n}\\n```\\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\\nThe module is not allowed to retrieve capabilities which it does not own.\\n```go\\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\\n\/\/ fetch the index of capability using reverse mapping in memstore\\nindex := memStore.Get(sck.moduleName + \"\/rev\/\" + name)\\n\/\/ fetch capability from go-map using index\\ncapability := capMap[index]\\n\/\/ return the capability\\nreturn capability\\n}\\n```\\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\\nmore owners exist, the capability will be deleted globally.\\n```go\\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\\npersistentStore := ctx.KVStore(sck.persistentKey)\\nname := capStore.Get(sck.moduleName + \"\/fwd\/\" + capability)\\nif name == nil {\\nreturn error(\"capability not owned by module\")\\n}\\n\/\/ delete forward mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/fwd\/\" + capability, name)\\n\/\/ delete reverse mapping in memory store\\nmemoryStore.Delete(sck.moduleName + \"\/rev\/\" + name, capability)\\n\/\/ update owner set in persistent store\\nowners := persistentStore.Get(capability.Index())\\nowners.remove(sck.moduleName + \"\/\" + name)\\nif owners.size() > 0 {\\n\/\/ there are still other owners, keep the capability around\\npersistentStore.Set(capability.Index(), owners)\\n} else {\\n\/\/ no more owners, delete the capability\\npersistentStore.Delete(capability.Index())\\ndelete(capMap[capability.Index()])\\n}\\n}\\n```\\n### Usage patterns\\n#### Initialisation\\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\\n```go\\nck := NewCapabilityKeeper(persistentKey, memoryKey)\\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(\"mod1\"), ....)\\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(\"mod2\"), ....)\\n\/\/ other initialisation logic ...\\n\/\/ load initial state...\\nck.InitialiseAndSeal(initialContext)\\n```\\n#### Creating, passing, claiming and using capabilities\\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\\nModule 1 would have the following code:\\n```go\\ncapability := scopedCapabilityKeeper.NewCapability(ctx, \"resourceABC\")\\nmod2Keeper.SomeFunction(ctx, capability, args...)\\n```\\n`SomeFunction`, running in module 2, could then claim the capability:\\n```go\\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\\nk.sck.ClaimCapability(ctx, capability, \"resourceABC\")\\n\/\/ other logic...\\n}\\n```\\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\\n```go\\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\\ncapability := k.sck.GetCapability(ctx, name)\\nmod1.UseResource(ctx, capability, \"resourceABC\")\\n}\\n```\\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\\n```go\\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\\nif !k.sck.AuthenticateCapability(name, capability) {\\nreturn errors.New(\"unauthenticated\")\\n}\\n\/\/ do something with the resource\\n}\\n```\\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\\n","tokens":437,"id":817,"Predictions":"Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nAt present, the Cosmos SDK does not have the ability to do this.nKeepers cannot create or store capability keys during transaction execution \u2014 although they could call NewKVStoreKey and take the memory addressnof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional \u2014 were this not the case, the keys would be predictable and couldn't serve as object capabilities).nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A). The state machine must be able to create and authenticate object-capability keys at runtime (i.e., during transaction execution).nA Decision (A)."}
{"File Name":"cosmos-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```protobuf\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary representation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\ngoogle.protobuf.Any public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n* **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n* **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n* requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n* leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```protobuf\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n* Pull account number and sequence from the state.\\n* Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n* Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n* Verify the signature at the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n* important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n* they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need an unknown field filter that:\\n* always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n* rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement the `cryptotypes.PubKey` interface.\\nWe propose to use `Any` for protobuf encoding as we are doing with other interfaces (for example, in `BaseAccount.PubKey` and `SignerInfo.PublicKey`).\\nThe following public keys are implemented: secp256k1, secp256r1, ed25519 and legacy-multisignature.\\nEx:\\n```protobuf\\nmessage PubKey {\\nbytes key = 1;\\n}\\n```\\n`multisig.LegacyAminoPubKey` has an array of `Any`'s member to support any\\nprotobuf public key type.\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\nIf the account is sending its first transaction, the account number must be set to 0. This is due to the account not being created yet.\\n```go\\ntype AccountRetriever interface {\\nGetAccount(clientCtx Context, addr sdk.AccAddress) (client.Account, error)\\nGetAccountWithHeight(clientCtx Context, addr sdk.AccAddress) (client.Account, int64, error)\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `Codec`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModule.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":818,"Predictions":"Decision (A).nnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nSpecifically, the client-side migration path primarily includes tx generation andnsigning, message construction and routing, in addition to CLI & REST handlers andnbusiness logic (i.e. queriers).nWith this in mind, we will tackle the migration path via two main areas, txs andnquerying, however, this ADR solely focuses on transactions.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnnThis ADR is a continuation of the motivation, design, and context established inn[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design thenProtocol Buffer migration path for the client-side of the Cosmos SDK.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-034-account-rekeying.md","Context":"## Context\\nCurrently, in the Cosmos SDK, the address of an auth `BaseAccount` is based on the hash of the public key.  Once an account is created, the public key for the account is set in stone, and cannot be changed.  This can be a problem for users, as key rotation is a useful security practice, but is not possible currently.  Furthermore, as multisigs are a type of pubkey, once a multisig for an account is set, it can not be updated.  This is problematic, as multisigs are often used by organizations or companies, who may need to change their set of multisig signers for internal reasons.\\nTransferring all the assets of an account to a new account with the updated pubkey is not sufficient, because some \"engagements\" of an account are not easily transferable.  For example, in staking, to transfer bonded Atoms, an account would have to unbond all delegations and wait the three week unbonding period.  Even more significantly, for validator operators, ownership over a validator is not transferrable at all, meaning that the operator key for a validator can never be updated, leading to poor operational security for validators.\\n","Decision":"We propose the addition of a new feature to `x\/auth` that allows accounts to update the public key associated with their account, while keeping the address the same.\\nThis is possible because the Cosmos SDK `BaseAccount` stores the public key for an account in state, instead of making the assumption that the public key is included in the transaction (whether explicitly or implicitly through the signature) as in other blockchains such as Bitcoin and Ethereum.  Because the public key is stored on chain, it is okay for the public key to not hash to the address of an account, as the address is not pertinent to the signature checking process.\\nTo build this system, we design a new Msg type as follows:\\n```protobuf\\nservice Msg {\\nrpc ChangePubKey(MsgChangePubKey) returns (MsgChangePubKeyResponse);\\n}\\nmessage MsgChangePubKey {\\nstring address = 1;\\ngoogle.protobuf.Any pub_key = 2;\\n}\\nmessage MsgChangePubKeyResponse {}\\n```\\nThe MsgChangePubKey transaction needs to be signed by the existing pubkey in state.\\nOnce, approved, the handler for this message type, which takes in the AccountKeeper, will update the in-state pubkey for the account and replace it with the pubkey from the Msg.\\nAn account that has had its pubkey changed cannot be automatically pruned from state.  This is because if pruned, the original pubkey of the account would be needed to recreate the same address, but the owner of the address may not have the original pubkey anymore.  Currently, we do not automatically prune any accounts anyways, but we would like to keep this option open the road (this is the purpose of account numbers).  To resolve this, we charge an additional gas fee for this operation to compensate for this this externality (this bound gas amount is configured as parameter `PubKeyChangeCost`). The bonus gas is charged inside the handler, using the `ConsumeGas` function.  Furthermore, in the future, we can allow accounts that have rekeyed manually prune themselves using a new Msg type such as `MsgDeleteAccount`.  Manually pruning accounts can give a gas refund as an incentive for performing the action.\\n```go\\namount := ak.GetParams(ctx).PubKeyChangeCost\\nctx.GasMeter().ConsumeGas(amount, \"pubkey change fee\")\\n```\\nEvery time a key for an address is changed, we will store a log of this change in the state of the chain, thus creating a stack of all previous keys for an address and the time intervals for which they were active.  This allows dapps and clients to easily query past keys for an account which may be useful for features such as verifying timestamped off-chain signed messages.\\n","tokens":250,"id":819,"Predictions":"Decision (A) To update the hash of the public key for an account. (B) To update the hash of the public key for an account. (C) To update the hash of the public key for an account. (D) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (B) To update the hash of the public key for an account. (C) To update the hash of the public key for an account. (D) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (B) To update the hash of the public key for an account. (C) To update the hash of the public key for an account. (D) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (A) To update the hash of the public key for an account. (B) To update the hash of the public key for an account. (C) To update the hash of the public key for an account."}
{"File Name":"cosmos-sdk\/adr-018-extendable-voting-period.md","Context":"## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficiently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n","Decision":"We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params\\n* The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the default voting period that all governance proposal voting periods start with.\\n* There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the gauge the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n","tokens":74,"id":820,"Predictions":"Decision (A) The time period for the voting of governance proposals is suboptimal.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-017-historical-header-module.md","Context":"## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the Cosmos SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n","Decision":"The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\\n```go\\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper) error {\\ninfo := HistoricalInfo{\\napphash: ctx.HeaderInfo().AppHash,\\nTime: ctx.HeaderInfo().Time,\\nNextValidatorsHash: ctx.CometInfo().NextValidatorsHash,\\n}\\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\\nn := keeper.GetParamRecentHeadersToStore()\\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\\n\/\/ continue handling request\\n}\\n```\\nAlternatively, the application MAY store only the hash of the validator set.\\nThe application MUST make these past `n` committed headers available for querying by Cosmos SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x\/staking` or `x\/ibc`).\\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\\n","tokens":66,"id":821,"Predictions":"Decision 1). The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 2). The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 3) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 4) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 5) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 6) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 7) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 8) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 9) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics).n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specification](https:\/\/github.com\/cosmos\/ics].n 10) The Cosmos SDK must implement the [IBC specificationn"}
{"File Name":"cosmos-sdk\/adr-019-protobuf-state-encoding.md","Context":"## Context\\nCurrently, the Cosmos SDK utilizes [go-amino](https:\/\/github.com\/tendermint\/go-amino\/) for binary\\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\\nFrom the Amino docs:\\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\\n> support. See the [Proto3 spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3) for more\\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\\n>\\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\\nAmino also aims to have the following goals (not a complete list):\\n* Binary bytes must be decode-able with a schema.\\n* Schema must be upgradeable.\\n* The encoder and decoder logic must be reasonably simple.\\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\\nclients written in various languages while providing virtually little in the way of true backwards\\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1<\/sup>. This is\\nlargely reflected in the performance of simulations and application transaction throughput.\\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\\n* Language agnostic\\n* Platform agnostic\\n* Rich client support and thriving ecosystem\\n* High performance\\n* Minimal encoded message size\\n* Codegen-based over reflection-based\\n* Supports backward and forward compatibility\\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\\nmade to address client-side encoding.\\n","Decision":"We will adopt [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) for serializing\\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\\nwill provide two concrete implementations of the `Marshaler` interface: `AminoCodec` and `ProtoCodec`.\\n* `AminoCodec`: Uses Amino for both binary and JSON encoding.\\n* `ProtoCodec`: Uses Protobuf for both binary and JSON encoding.\\nModules will use whichever codec that is instantiated in the app. By default, the Cosmos SDK's `simapp`\\ninstantiates a `ProtoCodec` as the concrete implementation of `Marshaler`, inside the `MakeTestEncodingConfig`\\nfunction. This can be easily overwritten by app developers if they so desire.\\nThe ultimate goal will be to replace Amino JSON encoding with Protobuf encoding and thus have\\nmodules accept and\/or extend `ProtoCodec`. Until then, Amino JSON is still provided for legacy use-cases.\\nA handful of places in the Cosmos SDK still have Amino JSON hardcoded, such as the Legacy API REST endpoints\\nand the `x\/params` store. They are planned to be converted to Protobuf in a gradual manner.\\n### Module Codecs\\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\\n`Marshaler` that will be a `ProtoCodec`. This migration is simple as things will just work as-is.\\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\\n[gogoprotobuf](https:\/\/github.com\/cosmos\/gogoproto) Value types.\\nExample:\\n```go\\nts, err := gogotypes.TimestampProto(completionTime)\\nif err != nil {\\n\/\/ ...\\n}\\nbz := cdc.MustMarshal(ts)\\n```\\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\\nto the module and will contain method contracts that know how to serialize the needed interfaces.\\nExample:\\n```go\\n\/\/ x\/auth\/types\/codec.go\\ntype Codec interface {\\ncodec.Codec\\nMarshalAccount(acc exported.Account) ([]byte, error)\\nUnmarshalAccount(bz []byte) (exported.Account, error)\\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\\n}\\n```\\n### Usage of `Any` to encode interfaces\\nIn general, module-level .proto files should define messages which encode interfaces\\nusing [`google.protobuf.Any`](https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/master\/src\/google\/protobuf\/any.proto).\\nAfter [extension discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030),\\nthis was chosen as the preferred alternative to application-level `oneof`s\\nas in our original protobuf design. The arguments in favor of `Any` can be\\nsummarized as follows:\\n* `Any` provides a simpler, more consistent client UX for dealing with\\ninterfaces than app-level `oneof`s that will need to be coordinated more\\ncarefully across applications. Creating a generic transaction\\nsigning library using `oneof`s may be cumbersome and critical logic may need\\nto be reimplemented for each chain\\n* `Any` provides more resistance against human error than `oneof`\\n* `Any` is generally simpler to implement for both modules and apps\\nThe main counter-argument to using `Any` centers around its additional space\\nand possibly performance overhead. The space overhead could be dealt with using\\ncompression at the persistence layer in the future and the performance impact\\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\\nwith user experience as the higher order concern.\\nNote, that given the Cosmos SDK's decision to adopt the `Codec` interfaces described\\nabove, apps can still choose to use `oneof` to encode state and transactions\\nbut it is not the recommended approach. If apps do choose to use `oneof`s\\ninstead of `Any` they will likely lose compatibility with client apps that\\nsupport multiple chains. Thus developers should think carefully about whether\\nthey care more about what is possibly a pre-mature optimization or end-user\\nand client developer UX.\\n### Safe usage of `Any`\\nBy default, the [gogo protobuf implementation of `Any`](https:\/\/pkg.go.dev\/github.com\/cosmos\/gogoproto\/types)\\nuses [global type registration]( https:\/\/github.com\/cosmos\/gogoproto\/blob\/master\/proto\/properties.go#L540)\\nto decode values packed in `Any` into concrete\\ngo types. This introduces a vulnerability where any malicious module\\nin the dependency tree could register a type with the global protobuf registry\\nand cause it to be loaded and unmarshaled by a transaction that referenced\\nit in the `type_url` field.\\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\\nvalues into concrete types through the `InterfaceRegistry` interface which\\nbears some similarity to type registration with Amino:\\n```go\\ntype InterfaceRegistry interface {\\n\/\/ RegisterInterface associates protoName as the public name for the\\n\/\/ interface passed in as iface\\n\/\/ Ex:\\n\/\/   registry.RegisterInterface(\"cosmos_sdk.Msg\", (*sdk.Msg)(nil))\\nRegisterInterface(protoName string, iface interface{})\\n\/\/ RegisterImplementations registers impls as a concrete implementations of\\n\/\/ the interface iface\\n\/\/ Ex:\\n\/\/  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\\nRegisterImplementations(iface interface{}, impls ...proto.Message)\\n}\\n```\\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\\nto communicate the list of concrete types that satisfy an interface to clients.\\nIn .proto files:\\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\\nmay be used via code generation, reflection &\/or static linting.\\nThe same struct that implements `InterfaceRegistry` will also implement an\\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\\n```go\\ntype InterfaceUnpacker interface {\\n\/\/ UnpackAny unpacks the value in any to the interface pointer passed in as\\n\/\/ iface. Note that the type in any must have been registered with\\n\/\/ RegisterImplementations as a concrete type for that interface\\n\/\/ Ex:\\n\/\/    var msg sdk.Msg\\n\/\/    err := ctx.UnpackAny(any, &msg)\\n\/\/    ...\\nUnpackAny(any *Any, iface interface{}) error\\n}\\n```\\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\\nusage of `Any`, it just introduces a security and introspection layer for\\ngolang usage.\\n`InterfaceRegistry` will be a member of `ProtoCodec`\\ndescribed above. In order for modules to register interface types, app modules\\ncan optionally implement the following interface:\\n```go\\ntype InterfaceModule interface {\\nRegisterInterfaceTypes(InterfaceRegistry)\\n}\\n```\\nThe module manager will include a method to call `RegisterInterfaceTypes` on\\nevery module that implements it in order to populate the `InterfaceRegistry`.\\n### Using `Any` to encode state\\nThe Cosmos SDK will provide support methods `MarshalInterface` and `UnmarshalInterface` to hide a complexity of wrapping interface types into `Any` and allow easy serialization.\\n```go\\nimport \"github.com\/cosmos\/cosmos-sdk\/codec\"\\n\/\/ note: eviexported.Evidence is an interface type\\nfunc MarshalEvidence(cdc codec.BinaryCodec, e eviexported.Evidence) ([]byte, error) {\\nreturn cdc.MarshalInterface(e)\\n}\\nfunc UnmarshalEvidence(cdc codec.BinaryCodec, bz []byte) (eviexported.Evidence, error) {\\nvar evi eviexported.Evidence\\nerr := cdc.UnmarshalInterface(&evi, bz)\\nreturn err, nil\\n}\\n```\\n### Using `Any` in `sdk.Msg`s\\nA similar concept is to be applied for messages that contain interfaces fields.\\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\\nan interface:\\n```protobuf\\n\/\/ x\/evidence\/types\/types.proto\\nmessage MsgSubmitEvidence {\\nbytes submitter = 1\\n[\\n(gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"\\n];\\ngoogle.protobuf.Any evidence = 2;\\n}\\n```\\nNote that in order to unpack the evidence from `Any` we do need a reference to\\n`InterfaceRegistry`. In order to reference evidence in methods like\\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\\ninterfaces before they're needed.\\n### Unpacking Interfaces\\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\\ninterfaces wrapped in `Any` before they're needed, we create an interface\\nthat `sdk.Msg`s and other types can implement:\\n```go\\ntype UnpackInterfacesMessage interface {\\nUnpackInterfaces(InterfaceUnpacker) error\\n}\\n```\\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\\nstruct itself with a public getter `GetCachedValue() interface{}`.\\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\\nand stored in `cachedValue` for reference later.\\nThen unpacked interface values can safely be used in any code afterwards\\nwithout knowledge of the `InterfaceRegistry`\\nand messages can introduce a simple getter to cast the cached value to the\\ncorrect interface type.\\nThis has the added benefit that unmarshaling of `Any` values only happens once\\nduring initial deserialization rather than every time the value is read. Also,\\nwhen `Any` values are first packed (for instance in a call to\\n`NewMsgSubmitEvidence`), the original interface value is cached so that\\nunmarshaling isn't needed to read it again.\\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\\n`GetEvidence` as follows:\\n```go\\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\\nvar evi eviexported.Evidence\\nreturn ctx.UnpackAny(msg.Evidence, *evi)\\n}\\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\\n}\\n```\\n### Amino Compatibility\\nOur custom implementation of `Any` can be used transparently with Amino if used\\nwith the proper codec instance. What this means is that interfaces packed within\\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\\nhave been registered properly with Amino).\\nIn order for this functionality to work:\\n* **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\\nnow a wrapper which properly handles `Any`**\\n* **all new code should use `Marshaler` which is compatible with both amino and\\nprotobuf**\\n* Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\\n### Why Wasn't X Chosen Instead\\nFor a more complete comparison to alternative protocols, see [here](https:\/\/codeburst.io\/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\\n### Cap'n Proto\\nWhile [Cap\u2019n Proto](https:\/\/capnproto.org\/) does seem like an advantageous alternative to Protobuf\\ndue to it's native support for interfaces\/generics and built in canonicalization, it does lack the\\nrich client ecosystem compared to Protobuf and is a bit less mature.\\n### FlatBuffers\\n[FlatBuffers](https:\/\/google.github.io\/flatbuffers\/) is also a potentially viable alternative, with the\\nprimary difference being that FlatBuffers does not need a parsing\/unpacking step to a secondary\\nrepresentation before you can access data, often coupled with per-object memory allocation.\\nHowever, it would require great efforts into research and full understanding the scope of the migration\\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\\nuntrusted inputs.\\n","tokens":439,"id":822,"Predictions":"Decision based on the following: We believe that Amino does not fully meet the needs of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK sup>1\/sup>. We believe that Amino has proven to be a big pain-point in regards to supporting object serialization acrossnclients written in various languages while providing virtually little in the way of true backwardsncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has been shown to be an extremely large performance bottleneck in the Cosmos SDK sup>1\/sup>. This is largely reflected in the performance of simulations and application transaction throughput. We need to adopt an encoding protocol that meets the following criteria for state serialization: Language agnosticn* Platform agnosticn* Rich client support and thriving ecosystemn* High performancen* Minimal encoded message sizen* Codegen-based over reflection-basedn* Supports backwardsncompatibility and upgradeability.n Therefore, we need to adopt an encoding protocol that meets the following criteria for state serialization: Language agnosticn* Platform agnosticn* Rich client support and thriving ecosystemn* High performancen* Minimal encoded message sizen* Codegen-based over reflection-basedn* Supports backwardsncompatibility and upgradeability.n"}
{"File Name":"cosmos-sdk\/adr-040-storage-and-smt-state-commitments.md","Context":"## Context\\nCurrently, Cosmos SDK uses IAVL for both state [commitments](https:\/\/cryptography.fandom.com\/wiki\/Commitment_scheme) and data storage.\\nIAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkelized key\/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems:\\n* Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level.\\n* Each edge traversal requires a DB query.\\n* Creating snapshots is [expensive](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7215#issuecomment-684804950). It takes about 30 seconds to export less than 100 MB of state (as of March 2020).\\n* Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck.\\n* The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, [ref](https:\/\/github.com\/cosmos\/iavl\/blob\/master\/docs\/node\/node.md\\n).\\nMoreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\\n","Decision":"We propose to separate the concerns of state commitment (**SC**), needed for consensus, and state storage (**SS**), needed for state machine. Finally we replace IAVL with [Celestia's SMT](https:\/\/github.com\/lazyledger\/smt). Celestia SMT is based on Diem (called jellyfish) design [*] - it uses a compute-optimised SMT by replacing subtrees with only default values with a single node (same approach is used by Ethereum2) and implements compact proofs.\\nThe storage model presented here doesn't deal with data structure nor serialization. It's a Key-Value database, where both key and value are binaries. The storage user is responsible for data serialization.\\n### Decouple state commitment from storage\\nSeparation of storage and commitment (by the SMT) will allow the optimization of different components according to their usage and access patterns.\\n`SC` (SMT) is used to commit to a data and compute Merkle proofs. `SS` is used to directly access data. To avoid collisions, both `SS` and `SC` will use a separate storage namespace (they could use the same database underneath). `SS` will store each record directly (mapping `(key, value)` as `key \u2192 value`).\\nSMT is a merkle tree structure: we don't store keys directly. For every `(key, value)` pair, `hash(key)` is used as leaf path (we hash a key to uniformly distribute leaves in the tree) and `hash(value)` as the leaf contents. The tree structure is specified in more depth [below](#smt-for-state-commitment).\\nFor data access we propose 2 additional KV buckets (implemented as namespaces for the key-value pairs, sometimes called [column family](https:\/\/github.com\/facebook\/rocksdb\/wiki\/Terminology)):\\n1. B1: `key \u2192 value`: the principal object storage, used by a state machine, behind the Cosmos SDK `KVStore` interface: provides direct access by key and allows prefix iteration (KV DB backend must support it).\\n2. B2: `hash(key) \u2192 key`: a reverse index to get a key from an SMT path. Internally the SMT will store `(key, value)` as `prefix || hash(key) || hash(value)`. So, we can get an object value by composing `hash(key) \u2192 B2 \u2192 B1`.\\n3. We could use more buckets to optimize the app usage if needed.\\nWe propose to use a KV database for both `SS` and `SC`. The store interface will allow to use the same physical DB backend for both `SS` and `SC` as well two separate DBs. The latter option allows for the separation of `SS` and `SC` into different hardware units, providing support for more complex setup scenarios and improving overall performance: one can use different backends (eg RocksDB and Badger) as well as independently tuning the underlying DB configuration.\\n### Requirements\\nState Storage requirements:\\n* range queries\\n* quick (key, value) access\\n* creating a snapshot\\n* historical versioning\\n* pruning (garbage collection)\\nState Commitment requirements:\\n* fast updates\\n* tree path should be short\\n* query historical commitment proofs using ICS-23 standard\\n* pruning (garbage collection)\\n### SMT for State Commitment\\nA Sparse Merkle tree is based on the idea of a complete Merkle tree of an intractable size. The assumption here is that as the size of the tree is intractable, there would only be a few leaf nodes with valid data blocks relative to the tree size, rendering a sparse tree.\\nThe full specification can be found at [Celestia](https:\/\/github.com\/celestiaorg\/celestia-specs\/blob\/ec98170398dfc6394423ee79b00b71038879e211\/src\/specs\/data_structures.md#sparse-merkle-tree). In summary:\\n* The SMT consists of a binary Merkle tree, constructed in the same fashion as described in [Certificate Transparency (RFC-6962)](https:\/\/tools.ietf.org\/html\/rfc6962), but using as the hashing function SHA-2-256 as defined in [FIPS 180-4](https:\/\/doi.org\/10.6028\/NIST.FIPS.180-4).\\n* Leaves and internal nodes are hashed differently: the one-byte `0x00` is prepended for leaf nodes while `0x01` is prepended for internal nodes.\\n* Default values are given to leaf nodes with empty leaves.\\n* While the above rule is sufficient to pre-compute the values of intermediate nodes that are roots of empty subtrees, a further simplification is to extend this default value to all nodes that are roots of empty subtrees. The 32-byte zero is used as the default value. This rule takes precedence over the above one.\\n* An internal node that is the root of a subtree that contains exactly one non-empty leaf is replaced by that leaf's leaf node.\\n### Snapshots for storage sync and state versioning\\nBelow, with simple _snapshot_ we refer to a database snapshot mechanism, not to a _ABCI snapshot sync_. The latter will be referred as _snapshot sync_ (which will directly use DB snapshot as described below).\\nDatabase snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a _copy on write_ and it allows DB state to be efficiently delivered at a certain stage.\\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs.\\nOne of the Stargate core features is a _snapshot sync_ delivered in the `\/snapshot` package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a `SS` at a certain version together with a header chain.\\nA new database snapshot will be created in every `EndBlocker` and identified by a block height. The `root` store keeps track of the available snapshots to offer `SS` at a certain version. The `root` store implements the `RootStore` interface described below. In essence, `RootStore` encapsulates a `Committer` interface. `Committer` has a `Commit`, `SetPruning`, `GetPruning` functions which will be used for creating and removing snapshots. The `rootStore.Commit` function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the `Committer` interface.\\nNOTE: `Commit` must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into `Committer` and `PruningCommitter` - only the multiroot should implement `PruningCommitter` (cache and prefix store don't need pruning).\\nNumber of historical versions for `abci.RequestQuery` and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions.\\nPruning old snapshots is effectively done by a database. Whenever we update a record in `SC`, SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions.\\nTo manage the active snapshots we will either use a DB _max number of snapshots_ option (if available), or we will remove DB snapshots in the `EndBlocker`. The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\\n#### Accessing old state versions\\nOne of the functional requirements is to access old state. This is done through `abci.RequestQuery` structure.  The version is specified by a block height (so we query for an object by a key `K` at block height `H`). The number of old versions supported for `abci.RequestQuery` is configurable. Accessing an old state is done by using available snapshots.\\n`abci.RequestQuery` doesn't need old state of `SC` unless the `prove=true` parameter is set. The SMT merkle proof must be included in the `abci.ResponseQuery` only if both `SC` and `SS` have a snapshot for requested version.\\nMoreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution.\\nWe positively [validated](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/8297) a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\\n### State Proofs\\nFor any object stored in State Store (SS), we have corresponding object in `SC`. A proof for object `V` identified by a key `K` is a branch of `SC`, where the path corresponds to the key `hash(K)`, and the leaf is `hash(K, V)`.\\n### Rollbacks\\nWe need to be able to process transactions and roll-back state updates if a transaction fails. This can be done in the following way: during transaction processing, we keep all state change requests (writes) in a `CacheWrapper` abstraction (as it's done today). Once we finish the block processing, in the `Endblocker`,  we commit a root store - at that time, all changes are written to the SMT and to the `SS` and a snapshot is created.\\n### Committing to an object without saving it\\nWe identified use-cases, where modules will need to save an object commitment without storing an object itself. Sometimes clients are receiving complex objects, and they have no way to prove a correctness of that object without knowing the storage layout. For those use cases it would be easier to commit to the object without storing it directly.\\n### Refactor MultiStore\\nThe Stargate `\/store` implementation (store\/v1) adds an additional layer in the SDK store construction - the `MultiStore` structure. The multistore exists to support the modularity of the Cosmos SDK - each module is using its own instance of IAVL, but in the current implementation, all instances share the same database. The latter indicates, however, that the implementation doesn't provide true modularity. Instead it causes problems related to race condition and atomic DB commits (see: [\\#6370](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6370) and [discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/8297#discussioncomment-757043)).\\nWe propose to reduce the multistore concept from the SDK, and to use a single instance of `SC` and `SS` in a `RootStore` object. To avoid confusion, we should rename the `MultiStore` interface to `RootStore`. The `RootStore` will have the following interface; the methods for configuring tracing and listeners are omitted for brevity.\\n```go\\n\/\/ Used where read-only access to versions is needed.\\ntype BasicRootStore interface {\\nStore\\nGetKVStore(StoreKey) KVStore\\nCacheRootStore() CacheRootStore\\n}\\n\/\/ Used as the main app state, replacing CommitMultiStore.\\ntype CommitRootStore interface {\\nBasicRootStore\\nCommitter\\nSnapshotter\\nGetVersion(uint64) (BasicRootStore, error)\\nSetInitialVersion(uint64) error\\n... \/\/ Trace and Listen methods\\n}\\n\/\/ Replaces CacheMultiStore for branched state.\\ntype CacheRootStore interface {\\nBasicRootStore\\nWrite()\\n... \/\/ Trace and Listen methods\\n}\\n\/\/ Example of constructor parameters for the concrete type.\\ntype RootStoreConfig struct {\\nUpgrades        *StoreUpgrades\\nInitialVersion  uint64\\nReservePrefix(StoreKey, StoreType)\\n}\\n```\\n<!-- TODO: Review whether these types can be further reduced or simplified -->\\n<!-- TODO: RootStorePersistentCache type -->\\nIn contrast to `MultiStore`, `RootStore` doesn't allow to dynamically mount sub-stores or provide an arbitrary backing DB for individual sub-stores.\\nNOTE: modules will be able to use a special commitment and their own DBs. For example: a module which will use ZK proofs for state can store and commit this proof in the `RootStore` (usually as a single record) and manage the specialized store privately or using the `SC` low level interface.\\n#### Compatibility support\\nTo ease the transition to this new interface for users, we can create a shim which wraps a `CommitMultiStore` but provides a `CommitRootStore` interface, and expose functions to safely create and access the underlying `CommitMultiStore`.\\nThe new `RootStore` and supporting types can be implemented in a `store\/v2alpha1` package to avoid breaking existing code.\\n#### Merkle Proofs and IBC\\nCurrently, an IBC (v1.0) Merkle proof path consists of two elements (`[\"<store-key>\", \"<record-key>\"]`), with each key corresponding to a separate proof. These are each verified according to individual [ICS-23 specs](https:\/\/github.com\/cosmos\/ibc-go\/blob\/f7051429e1cf833a6f65d51e6c3df1609290a549\/modules\/core\/23-commitment\/types\/merkle.go#L17), and the result hash of each step is used as the committed value of the next step, until a root commitment hash is obtained.\\nThe root hash of the proof for `\"<record-key>\"` is hashed with the `\"<store-key>\"` to validate against the App Hash.\\nThis is not compatible with the `RootStore`, which stores all records in a single Merkle tree structure, and won't produce separate proofs for the store- and record-key. Ideally, the store-key component of the proof could just be omitted, and updated to use a \"no-op\" spec, so only the record-key is used. However, because the IBC verification code hardcodes the `\"ibc\"` prefix and applies it to the SDK proof as a separate element of the proof path, this isn't possible without a breaking change. Breaking this behavior would severely impact the Cosmos ecosystem which already widely adopts the IBC module. Requesting an update of the IBC module across the chains is a time consuming effort and not easily feasible.\\nAs a workaround, the `RootStore` will have to use two separate SMTs (they could use the same underlying DB): one for IBC state and one for everything else. A simple Merkle map that reference these SMTs will act as a Merkle Tree to create a final App hash. The Merkle map is not stored in a DBs - it's constructed in the runtime. The IBC substore key must be `\"ibc\"`.\\nThe workaround can still guarantee atomic syncs: the [proposed DB backends](#evaluated-kv-databases) support atomic transactions and efficient rollbacks, which will be used in the commit phase.\\nThe presented workaround can be used until the IBC module is fully upgraded to supports single-element commitment proofs.\\n### Optimization: compress module key prefixes\\nWe consider a compression of prefix keys by creating a mapping from module key to an integer, and serializing the integer using varint coding. Varint coding assures that different values don't have common byte prefix. For Merkle Proofs we can't use prefix compression - so it should only apply for the `SS` keys. Moreover, the prefix compression should be only applied for the module namespace. More precisely:\\n* each module has it's own namespace;\\n* when accessing a module namespace we create a KVStore with embedded prefix;\\n* that prefix will be compressed only when accessing and managing `SS`.\\nWe need to assure that the codes won't change. We can fix the mapping in a static variable (provided by an app) or SS state under a special key.\\nTODO: need to make decision about the key compression.\\n","tokens":391,"id":823,"Predictions":"Is the IAVL project inefficient?nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-011-generalize-genesis-accounts.md","Context":"## Context\\nCurrently, the Cosmos SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","Decision":"In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"cosmossdk.io\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n* Type registration on `auth`'s codec (shown above)\\n* A `Validate` method for each `Account` concrete type\\n","tokens":112,"id":824,"Predictions":"Decision (A) The Cosmos SDK allows for custom account types; the auth keeper stores any type fulfilling its Account interface. However auth does not handle exporting or loading accounts to\/from a genesis file, this is done by genaccounts, which only handles one of 4 concrete account types (BaseAccount, ContinuousVestingAccount, DelayedVestingAccount and ModuleAccount).nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify genaccounts.n1212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212"}
{"File Name":"cosmos-sdk\/adr-044-protobuf-updates-guidelines.md","Context":"## Context\\nThe Cosmos SDK maintains a set of [Protobuf definitions](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/proto\/cosmos). It is important to correctly design Protobuf definitions to avoid any breaking changes within the same version. The reasons are to not break tooling (including indexers and explorers), wallets and other third-party integrations.\\nWhen making changes to these Protobuf definitions, the Cosmos SDK currently only follows [Buf's](https:\/\/docs.buf.build\/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. For example:\\n* Adding fields to `Msg`s. Adding fields is a not a Protobuf spec-breaking operation. However, when adding new fields to `Msg`s, the unknown field rejection will throw an error when sending the new `Msg` to an older node.\\n* Marking fields as `reserved`. Protobuf proposes the `reserved` keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn't generate anything for `reserved` fields. See [#9446](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9446) for more details on this issue.\\nMoreover, module developers often face other questions around Protobuf definitions such as \"Can I rename a field?\" or \"Can I deprecate a field?\" This ADR aims to answer all these questions by providing clear guidelines about allowed updates for Protobuf definitions.\\n","Decision":"We decide to keep [Buf's](https:\/\/docs.buf.build\/) recommendations with the following exceptions:\\n* `UNARY_RPC`: the Cosmos SDK currently does not support streaming RPCs.\\n* `COMMENT_FIELD`: the Cosmos SDK allows fields with no comments.\\n* `SERVICE_SUFFIX`: we use the `Query` and `Msg` service naming convention, which doesn't use the `-Service` suffix.\\n* `PACKAGE_VERSION_SUFFIX`: some packages, such as `cosmos.crypto.ed25519`, don't use a version suffix.\\n* `RPC_REQUEST_STANDARD_NAME`: Requests for the `Msg` service don't have the `-Request` suffix to keep backwards compatibility.\\nOn top of Buf's recommendations we add the following guidelines that are specific to the Cosmos SDK.\\n### Updating Protobuf Definition Without Bumping Version\\n#### 1. Module developers MAY add new Protobuf definitions\\nModule developers MAY add new `message`s, new `Service`s, new `rpc` endpoints, and new fields to existing messages. This recommendation follows the Protobuf specification, but is added in this document for clarity, as the SDK requires one additional change.\\nThe SDK requires the Protobuf comment of the new addition to contain one line with the following format:\\n```protobuf\\n\/\/ Since: cosmos-sdk <version>{, <version>...}\\n```\\nWhere each `version` denotes a minor (\"0.45\") or patch (\"0.44.5\") version from which the field is available. This will greatly help client libraries, who can optionally use reflection or custom code generation to show\/hide these fields depending on the targeted node version.\\nAs examples, the following comments are valid:\\n```protobuf\\n\/\/ Since: cosmos-sdk 0.44\\n\/\/ Since: cosmos-sdk 0.42.11, 0.44.5\\n```\\nand the following ones are NOT valid:\\n```protobuf\\n\/\/ Since cosmos-sdk v0.44\\n\/\/ since: cosmos-sdk 0.44\\n\/\/ Since: cosmos-sdk 0.42.11 0.44.5\\n\/\/ Since: Cosmos SDK 0.42.11, 0.44.5\\n```\\n#### 2. Fields MAY be marked as `deprecated`, and nodes MAY implement a protocol-breaking change for handling these fields\\nProtobuf supports the [`deprecated` field option](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto#options), and this option MAY be used on any field, including `Msg` fields. If a node handles a Protobuf message with a non-empty deprecated field, the node MAY change its behavior upon processing it, even in a protocol-breaking way. When possible, the node MUST handle backwards compatibility without breaking the consensus (unless we increment the proto version).\\nAs an example, the Cosmos SDK v0.42 to v0.43 update contained two Protobuf-breaking changes, listed below. Instead of bumping the package versions from `v1beta1` to `v1`, the SDK team decided to follow this guideline, by reverting the breaking changes, marking those changes as deprecated, and modifying the node implementation when processing messages with deprecated fields. More specifically:\\n* The Cosmos SDK recently removed support for [time-based software upgrades](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/8849). As such, the `time` field has been marked as deprecated in `cosmos.upgrade.v1beta1.Plan`. Moreover, the node will reject any proposal containing an upgrade Plan whose `time` field is non-empty.\\n* The Cosmos SDK now supports [governance split votes](.\/adr-037-gov-split-vote.md). When querying for votes, the returned `cosmos.gov.v1beta1.Vote` message has its `option` field (used for 1 vote option) deprecated in favor of its `options` field (allowing multiple vote options). Whenever possible, the SDK still populates the deprecated `option` field, that is, if and only if the `len(options) == 1` and `options[0].Weight == 1.0`.\\n#### 3. Fields MUST NOT be renamed\\nWhereas the official Protobuf recommendations do not prohibit renaming fields, as it does not break the Protobuf binary representation, the SDK explicitly forbids renaming fields in Protobuf structs. The main reason for this choice is to avoid introducing breaking changes for clients, which often rely on hard-coded fields from generated types. Moreover, renaming fields will lead to client-breaking JSON representations of Protobuf definitions, used in REST endpoints and in the CLI.\\n### Incrementing Protobuf Package Version\\nTODO, needs architecture review. Some topics:\\n* Bumping versions frequency\\n* When bumping versions, should the Cosmos SDK support both versions?\\n* i.e. v1beta1 -> v1, should we have two folders in the Cosmos SDK, and handlers for both versions?\\n* mention ADR-023 Protobuf naming\\n","tokens":314,"id":825,"Predictions":"Decision based on the following: n* The Cosmos SDK maintains a set of [Protobuf definitions](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/proto\/cosmos). n* The Cosmos SDK currently only follows [Buf's](https:\/\/docs.buf.build\/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. n* Adding fields to Msgs. Adding fields is a not a Protobuf spec-breaking operation. However, when adding new fields to Msgs, the unknown field rejection will throw an error when sending the new Msg to an older node. n* Marking fields as reserved. Protobuf proposes the reserved keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn't generate anything for reserved fields. n* Decision based on the following: n* The Cosmos SDK maintains a set of [Protobuf definitions](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/proto\/cosmos). n* The Cosmos SDK maintains a set of [Protobuf definitions](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/proto\/cosmos). n* The Cosmos SDK currently only follows [Buf's](https:\/\/docs.buf.build\/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. n*"}
{"File Name":"cosmos-sdk\/adr-035-rosetta-api-support.md","Context":"## Context\\n[Rosetta API](https:\/\/www.rosetta-api.org\/) is an open-source specification and set of tools developed by Coinbase to\\nstandardise blockchain interactions.\\nThrough the use of a standard API for integrating blockchain applications it will\\n* Be easier for a user to interact with a given blockchain\\n* Allow exchanges to integrate new blockchains quickly and easily\\n* Enable application developers to build cross-blockchain applications such as block explorers, wallets and dApps at\\nconsiderably lower cost and effort.\\n","Decision":"It is clear that adding Rosetta API support to the Cosmos SDK will bring value to all the developers and\\nCosmos SDK based chains in the ecosystem. How it is implemented is key.\\nThe driving principles of the proposed design are:\\n1. **Extensibility:** it must be as riskless and painless as possible for application developers to set-up network\\nconfigurations to expose Rosetta API-compliant services.\\n2. **Long term support:** This proposal aims to provide support for all the supported Cosmos SDK release series.\\n3. **Cost-efficiency:** Backporting changes to Rosetta API specifications from `master` to the various stable\\nbranches of Cosmos SDK is a cost that needs to be reduced.\\nWe will achieve these delivering on these principles by the following:\\n1. There will be a package `rosetta\/lib`\\nfor the implementation of the core Rosetta API features, particularly:\\na. The types and interfaces (`Client`, `OfflineClient`...), this separates design from implementation detail.\\nb. The `Server` functionality as this is independent of the Cosmos SDK version.\\nc. The `Online\/OfflineNetwork`, which is not exported, and implements the rosetta API using the `Client` interface to query the node, build tx and so on.\\nd. The `errors` package to extend rosetta errors.\\n2. Due to differences between the Cosmos release series, each series will have its own specific implementation of `Client` interface.\\n3. There will be two options for starting an API service in applications:\\na. API shares the application process\\nb. API-specific process.\\n","tokens":106,"id":826,"Predictions":"Decision Makers should consider the following options: [i] They should consider the following alternatives: [ii] They should consider the following alternatives: [iii] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [iv] They should consider the following alternatives: [v] They should consider the following alternatives: [iv] They should consider the following alternatives: [v] They should consider the following alternatives: [v] They should consider the following alternatives: [v] They should consider the following alternatives: [v] They should consider the following alternatives:"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"Predictions":"Decision (A) To implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065)nAnswer:An* irismod\/nft and modules\/incubator\/nftn* CW721n* DID NFTsn* interNFTnAnswer:An* irismod\/nft and modules\/incubator\/nftn* CW721n* DID NFTsn* interNFTnAnswer:An* irismod\/nft and modules\/incubator\/nftn* CW721n* DID NFTsn* interNFTnAnswer:An* irismod\/nft and modules\/incubator\/nftn* CW721n* DID NFTsn* interNFTnAnswer:An* irismod\/nft and modules\/incubator\/nftn* CW721n* DID NFTsn* interNFTnAnswer:A"}
{"File Name":"cosmos-sdk\/adr-008-dCERT-group.md","Context":"## Context\\nIn order to reduce the number of parties involved with handling sensitive\\ninformation in an emergency scenario, we propose the creation of a\\nspecialization group named The Decentralized Computer Emergency Response Team\\n(dCERT).  Initially this group's role is intended to serve as coordinators\\nbetween various actors within a blockchain community such as validators,\\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\\naggregate and relay input from a variety of stakeholders to the developers who\\nare actively devising a patch to the software, this way sensitive information\\ndoes not need to be publicly disclosed while some input from the community can\\nstill be gained.\\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\\nthat this privilege should be enabled\/disabled globally with a governance\\nparameter such that this privilege could start disabled and later be enabled\\nthrough a parameter change proposal, once a dCERT group has been established.\\nIn the future it is foreseeable that the community may wish to expand the roles\\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\\nsecurity update on behalf of the community prior to a full community\\nwide vote whereby the sensitive information would be revealed prior to a\\nvulnerability being patched on the live network.\\n","Decision":"The dCERT group is proposed to include an implementation of a `SpecializationGroup`\\nas defined in [ADR 007](.\/adr-007-specialization-groups.md). This will include the\\nimplementation of:\\n* continuous voting\\n* slashing due to breach of soft contract\\n* revoking a member due to breach of soft contract\\n* emergency disband of the entire dCERT group (ex. for colluding maliciously)\\n* compensation stipend from the community pool or other means decided by\\ngovernance\\nThis system necessitates the following new parameters:\\n* blockly stipend allowance per dCERT member\\n* maximum number of dCERT members\\n* required staked slashable tokens for each dCERT member\\n* quorum for suspending a particular member\\n* proposal wager for disbanding the dCERT group\\n* stabilization period for dCERT member transition\\n* circuit break dCERT privileges enabled\\nThese parameters are expected to be implemented through the param keeper such\\nthat governance may change them at any given point.\\n### Continuous Voting Electionator\\nAn `Electionator` object is to be implemented as continuous voting and with the\\nfollowing specifications:\\n* All delegation addresses may submit votes at any point which updates their\\npreferred representation on the dCERT group.\\n* Preferred representation may be arbitrarily split between addresses (ex. 50%\\nto John, 25% to Sally, 25% to Carol)\\n* In order for a new member to be added to the dCERT group they must\\nsend a transaction accepting their admission at which point the validity of\\ntheir admission is to be confirmed.\\n* A sequence number is assigned when a member is added to dCERT group.\\nIf a member leaves the dCERT group and then enters back, a new sequence number\\nis assigned.\\n* Addresses which control the greatest amount of preferred-representation are\\neligible to join the dCERT group (up the _maximum number of dCERT members_).\\nIf the dCERT group is already full and new member is admitted, the existing\\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\\n* In the split situation where the dCERT group is full but a vying candidate\\nhas the same amount of vote as an existing dCERT member, the existing\\nmember should maintain its position.\\n* In the split situation where somebody must be kicked out but the two\\naddresses with the smallest number of votes have the same number of votes,\\nthe address with the smallest sequence number maintains its position.\\n* A stabilization period can be optionally included to reduce the\\n\"flip-flopping\" of the dCERT membership tail members. If a stabilization\\nperiod is provided which is greater than 0, when members are kicked due to\\ninsufficient support, a queue entry is created which documents which member is\\nto replace which other member. While this entry is in the queue, no new entries\\nto kick that same dCERT member can be made. When the entry matures at the\\nduration of the  stabilization period, the new member is instantiated, and old\\nmember kicked.\\n### Staking\/Slashing\\nAll members of the dCERT group must stake tokens _specifically_ to maintain\\neligibility as a dCERT member. These tokens can be staked directly by the vying\\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\\nbenefits for doing so). This staking mechanism should use the existing global\\nunbonding time of tokens staked for network validator security. A dCERT member\\ncan _only be_ a member if it has the required tokens staked under this\\nmechanism. If those tokens are unbonded then the dCERT member must be\\nautomatically kicked from the group.\\nSlashing of a particular dCERT member due to soft-contract breach should be\\nperformed by governance on a per member basis based on the magnitude of the\\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\\nby the dCERT group prior to being slashed by governance.\\nMembership suspension by the dCERT group takes place through a voting procedure\\nby the dCERT group members. After this suspension has taken place, a governance\\nproposal to slash the dCERT member must be submitted, if the proposal is not\\napproved by the time the rescinding member has completed unbonding their\\ntokens, then the tokens are no longer staked and unable to be slashed.\\nAdditionally in the case of an emergency situation of a colluding and malicious\\ndCERT group, the community needs the capability to disband the entire dCERT\\ngroup and likely fully slash them. This could be achieved though a special new\\nproposal type (implemented as a general governance proposal) which would halt\\nthe functionality of the dCERT group until the proposal was concluded. This\\nspecial proposal type would likely need to also have a fairly large wager which\\ncould be slashed if the proposal creator was malicious. The reason a large\\nwager should be required is because as soon as the proposal is made, the\\ncapability of the dCERT group to halt message routes is put on temporarily\\nsuspended, meaning that a malicious actor who created such a proposal could\\nthen potentially exploit a bug during this period of time, with no dCERT group\\ncapable of shutting down the exploitable message routes.\\n### dCERT membership transactions\\nActive dCERT members\\n* change of the description of the dCERT group\\n* circuit break a message route\\n* vote to suspend a dCERT member.\\nHere circuit-breaking refers to the capability to disable a groups of messages,\\nThis could for instance mean: \"disable all staking-delegation messages\", or\\n\"disable all distribution messages\". This could be accomplished by verifying\\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\\n`baseapp\/baseapp.go`).\\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\\nmeaning that no capability to unbreak a message route on a live chain is\\nrequired.\\nNote also, that if there was a problem with governance voting (for instance a\\ncapability to vote many times) then governance would be broken and should be\\nhalted with this mechanism, it would be then up to the validator set to\\ncoordinate and hard-fork upgrade to a patched version of the software where\\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\\nthey should all be severely slashed.\\n","tokens":292,"id":829,"Predictions":"Decision (A). dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.nC. dCERT should be created to handle sensitive information in an emergency scenario.nD. dCERT should be created to handle sensitive information in an emergency scenario.nA. dCERT should be created to handle sensitive information in an emergency scenario.nB. dCERT should be created to handle sensitive information in an emergency scenario.n"}
{"File Name":"cosmos-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```text\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts (sybil attack), so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```text\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will get slashed at the sum 10% amount.\\nHowever in practice, we likely don't want a linear relation between amount of stake at fault, and the percentage of stake to slash. In particular, solely 5% of stake double signing effectively did nothing to majorly threaten security, whereas 30% of stake being at fault clearly merits a large slashing factor, due to being very close to the point at which Tendermint security is threatened. A linear relation would require a factor of 6 gap between these two, whereas the difference in risk posed to the network is much larger. We propose using S-curves (formally [logistic functions](https:\/\/en.wikipedia.org\/wiki\/Logistic_function) to solve this). S-Curves capture the desired criterion quite well. They allow the slashing factor to be minimal for small values, and then grow very rapidly near some threshold point where the risk posed becomes notable.\\n#### Parameterization\\nThis requires parameterizing a logistic function. It is very well understood how to parameterize this. It has four parameters:\\n1) A minimum slashing factor\\n2) A maximum slashing factor\\n3) The inflection point of the S-curve (essentially where do you want to center the S)\\n4) The rate of growth of the S-curve (How elongated is the S)\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Griefing\\nGriefing, the act of intentionally getting oneself slashed in order to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker also gets equally impacted by the grief as the victim, so it would not provide much benefit to the griefer.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occurred within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occurred within the last `jail period`.\\n```go\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nValidatorVotingPercent      sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `ValidatorVotingPercent` to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occurred, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":830,"Predictions":"Decision 1). The delegators should be able to delegate to large validators. 2). The delegators should be able to delegate to large validators. 3) The delegators should be able to delegate to large validators. 4) The delegators should be able to delegate to large validators. 5) The delegators should be able to delegate to large validators. 6) The delegators should be able to delegate to large validators. 7) The delegators should be able to delegate to large validators. 8) The delegators should be able to delegate to large validators. 9) The delegators should be able to delegate to large validators. 10) The delegators should be able to delegate to large validators. 10) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators. ) The delegators should be able to delegate to large validators."}
{"File Name":"cosmos-sdk\/adr-041-in-place-store-migrations.md","Context":"## Context\\nWhen a chain upgrade introduces state-breaking changes inside modules, the current procedure consists of exporting the whole state into a JSON file (via the `simd genesis export` command), running migration scripts on the JSON file (`simd genesis migrate` command), clearing the stores (`simd unsafe-reset-all` command), and starting a new chain with the migrated JSON file as new genesis (optionally with a custom initial block height). An example of such a procedure can be seen [in the Cosmos Hub 3->4 migration guide](https:\/\/github.com\/cosmos\/gaia\/blob\/v4.0.3\/docs\/migration\/cosmoshub-3.md#upgrade-procedure).\\nThis procedure is cumbersome for multiple reasons:\\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6936)).\\n","Decision":"We propose a migration procedure based on modifying the KV store in-place without involving the JSON export-process-import flow described above.\\n### Module `ConsensusVersion`\\nWe introduce a new method on the `AppModule` interface:\\n```go\\ntype AppModule interface {\\n\/\/ --snip--\\nConsensusVersion() uint64\\n}\\n```\\nThis methods returns an `uint64` which serves as state-breaking version of the module. It MUST be incremented on each consensus-breaking change introduced by the module. To avoid potential errors with default values, the initial version of a module MUST be set to 1. In the Cosmos SDK, version 1 corresponds to the modules in the v0.41 series.\\n### Module-Specific Migration Functions\\nFor each consensus-breaking change introduced by the module, a migration script from ConsensusVersion `N` to version `N+1` MUST be registered in the `Configurator` using its newly-added `RegisterMigration` method. All modules receive a reference to the configurator in their `RegisterServices` method on `AppModule`, and this is where the migration functions should be registered. The migration functions should be registered in increasing order.\\n```go\\nfunc (am AppModule) RegisterServices(cfg module.Configurator) {\\n\/\/ --snip--\\ncfg.RegisterMigration(types.ModuleName, 1, func(ctx sdk.Context) error {\\n\/\/ Perform in-place store migrations from ConsensusVersion 1 to 2.\\n})\\ncfg.RegisterMigration(types.ModuleName, 2, func(ctx sdk.Context) error {\\n\/\/ Perform in-place store migrations from ConsensusVersion 2 to 3.\\n})\\n\/\/ etc.\\n}\\n```\\nFor example, if the new ConsensusVersion of a module is `N` , then `N-1` migration functions MUST be registered in the configurator.\\nIn the Cosmos SDK, the migration functions are handled by each module's keeper, because the keeper holds the `sdk.StoreKey` used to perform in-place store migrations. To not overload the keeper, a `Migrator` wrapper is used by each module to handle the migration functions:\\n```go\\n\/\/ Migrator is a struct for handling in-place store migrations.\\ntype Migrator struct {\\nBaseKeeper\\n}\\n```\\nMigration functions should live inside the `migrations\/` folder of each module, and be called by the Migrator's methods. We propose the format `Migrate{M}to{N}` for method names.\\n```go\\n\/\/ Migrate1to2 migrates from version 1 to 2.\\nfunc (m Migrator) Migrate1to2(ctx sdk.Context) error {\\nreturn v2bank.MigrateStore(ctx, m.keeper.storeKey) \/\/ v043bank is package `x\/bank\/migrations\/v2`.\\n}\\n```\\nEach module's migration functions are specific to the module's store evolutions, and are not described in this ADR. An example of x\/bank store key migrations after the introduction of ADR-028 length-prefixed addresses can be seen in this [store.go code](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/36f68eb9e041e20a5bb47e216ac5eb8b91f95471\/x\/bank\/legacy\/v043\/store.go#L41-L62).\\n### Tracking Module Versions in `x\/upgrade`\\nWe introduce a new prefix store in `x\/upgrade`'s store. This store will track each module's current version, it can be modelized as a `map[string]uint64` of module name to module ConsensusVersion, and will be used when running the migrations (see next section for details). The key prefix used is `0x1`, and the key\/value format is:\\n```text\\n0x2 | {bytes(module_name)} => BigEndian(module_consensus_version)\\n```\\nThe initial state of the store is set from `app.go`'s `InitChainer` method.\\nThe UpgradeHandler signature needs to be updated to take a `VersionMap`, as well as return an upgraded `VersionMap` and an error:\\n```diff\\n- type UpgradeHandler func(ctx sdk.Context, plan Plan)\\n+ type UpgradeHandler func(ctx sdk.Context, plan Plan, versionMap VersionMap) (VersionMap, error)\\n```\\nTo apply an upgrade, we query the `VersionMap` from the `x\/upgrade` store and pass it into the handler. The handler runs the actual migration functions (see next section), and if successful, returns an updated `VersionMap` to be stored in state.\\n```diff\\nfunc (k UpgradeKeeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {\\n\/\/ --snip--\\n-   handler(ctx, plan)\\n+   updatedVM, err := handler(ctx, plan, k.GetModuleVersionMap(ctx)) \/\/ k.GetModuleVersionMap() fetches the VersionMap stored in state.\\n+   if err != nil {\\n+       return err\\n+   }\\n+\\n+   \/\/ Set the updated consensus versions to state\\n+   k.SetModuleVersionMap(ctx, updatedVM)\\n}\\n```\\nA gRPC query endpoint to query the `VersionMap` stored in `x\/upgrade`'s state will also be added, so that app developers can double-check the `VersionMap` before the upgrade handler runs.\\n### Running Migrations\\nOnce all the migration handlers are registered inside the configurator (which happens at startup), running migrations can happen by calling the `RunMigrations` method on `module.Manager`. This function will loop through all modules, and for each module:\\n* Get the old ConsensusVersion of the module from its `VersionMap` argument (let's call it `M`).\\n* Fetch the new ConsensusVersion of the module from the `ConsensusVersion()` method on `AppModule` (call it `N`).\\n* If `N>M`, run all registered migrations for the module sequentially `M -> M+1 -> M+2...` until `N`.\\n* There is a special case where there is no ConsensusVersion for the module, as this means that the module has been newly added during the upgrade. In this case, no migration function is run, and the module's current ConsensusVersion is saved to `x\/upgrade`'s store.\\nIf a required migration is missing (e.g. if it has not been registered in the `Configurator`), then the `RunMigrations` function will error.\\nIn practice, the `RunMigrations` method should be called from inside an `UpgradeHandler`.\\n```go\\napp.UpgradeKeeper.SetUpgradeHandler(\"my-plan\", func(ctx sdk.Context, plan upgradetypes.Plan, vm module.VersionMap)  (module.VersionMap, error) {\\nreturn app.mm.RunMigrations(ctx, vm)\\n})\\n```\\nAssuming a chain upgrades at block `n`, the procedure should run as follows:\\n* the old binary will halt in `BeginBlock` when starting block `N`. In its store, the ConsensusVersions of the old binary's modules are stored.\\n* the new binary will start at block `N`. The UpgradeHandler is set in the new binary, so will run at `BeginBlock` of the new binary. Inside `x\/upgrade`'s `ApplyUpgrade`, the `VersionMap` will be retrieved from the (old binary's) store, and passed into the `RunMigrations` function, migrating all module stores in-place before the modules' own `BeginBlock`s.\\n","tokens":247,"id":831,"Predictions":"Decision (A). The current procedure consists of exporting the whole state into a JSON file (via the simd genesis export command), running migration scripts on the JSON file (simd genesis migrate command), clearing the stores (simd unsafe-reset-all command), and starting a new chain with the migrated JSON file as new genesis (optionally with a custom initial block height).nIt can take hours to run the export command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the export command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to run the initChain command, plus some additional hours to run InitChain on the fresh chain using the migrated JSON file.nIt can take more time to"}
{"File Name":"cosmos-sdk\/adr-012-state-accessors.md","Context":"## Context\\nCosmos SDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\\nmediocre.\\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\\nwhich are repetitive and hard to maintain.\\nSecond, this makes it harder to align with the object capability theorem: the right to access the\\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\\nFinally, because the getter\/setter functions are defined as methods of a module's `Keeper`, the reviewers\\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\\nThere is no static way to know which part of the state that the function is accessing (and which is not).\\n","Decision":"We will define a type named `Value`:\\n```go\\ntype Value struct {\\nm   Mapping\\nkey []byte\\n}\\n```\\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\\nspace it will access and `Value.key` defines the exact key for the reference.\\nWe will define a type named `Mapping`:\\n```go\\ntype Mapping struct {\\nstoreKey sdk.StoreKey\\ncdc      *codec.LegacyAmino\\nprefix   []byte\\n}\\n```\\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\\nWe will define the following core methods for the `Value` type:\\n```go\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Value) Get(ctx Context, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\\n\/\/ Get stored data as raw byte slice\\nfunc (Value) GetRaw(ctx Context) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Value) Set(ctx Context, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Value) Exists(ctx Context) bool {}\\n\/\/ Delete a raw value value\\nfunc (Value) Delete(ctx Context) {}\\n```\\nWe will define the following core methods for the `Mapping` type:\\n```go\\n\/\/ Constructs key-value pair reference corresponding to the key argument in the Mapping space\\nfunc (Mapping) Value(key []byte) Value {}\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\\n\/\/ Get stored data as raw byte slice\\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\\n\/\/ Delete a raw value value\\nfunc (Mapping) Delete(ctx Context, key []byte) {}\\n```\\nEach method of the `Mapping` type that is passed the arguments `ctx`, `key`, and `args...` will proxy\\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\\nIn addition, we will define and provide a common set of types derived from the `Value` type:\\n```go\\ntype Boolean struct { Value }\\ntype Enum struct { Value }\\ntype Integer struct { Value; enc IntEncoding }\\ntype String struct { Value }\\n\/\/ ...\\n```\\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\\nin core methods are replaced by explicit return types.\\nFinally, we will define a family of types derived from the `Mapping` type:\\n```go\\ntype Indexer struct {\\nm   Mapping\\nenc IntEncoding\\n}\\n```\\nWhere the `key` argument in core method is typed.\\nSome of the properties of the accessor types are:\\n* State access happens only when a function which takes a `Context` as an argument is invoked\\n* Accessor type structs give rights to access the state only that the struct is referring, no other\\n* Marshalling\/Unmarshalling happens implicitly within the core methods\\n","tokens":250,"id":832,"Predictions":"Decision (A) The state of the module should be a StoreKey state.nB) The state of the module should be a StoreKey state.nC) The state of the module should be a StoreKey state.nD) The state of the module should be a StoreKey state.nAnswer:A.nB.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.nC.n"}
{"File Name":"cosmos-sdk\/adr-023-protobuf-naming.md","Context":"## Context\\nProtocol Buffers provide a basic [style guide](https:\/\/developers.google.com\/protocol-buffers\/docs\/style)\\nand [Buf](https:\/\/buf.build\/docs\/style-guide) builds upon that. To the\\nextent possible, we want to follow industry accepted guidelines and wisdom for\\nthe effective usage of protobuf, deviating from those only when there is clear\\nrationale for our use case.\\n### Adoption of `Any`\\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\\ninterface types (as opposed to `oneof`) makes package naming a central part\\nof the encoding as fully-qualified message names now appear in encoded\\nmessages.\\n### Current Directory Organization\\nThus far we have mostly followed [Buf's](https:\/\/buf.build) [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout)\\nwhich although being convenient for developing code comes with the warning\\nfrom Buf that:\\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\\n### Adoption of gRPC Queries\\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\\npath. In the future, gRPC queries may be allowed from within persistent scripts\\nby technologies such as CosmWasm and these query routes would be stored within\\nscript binaries.\\n","Decision":"The goal of this ADR is to provide thoughtful naming conventions that:\\n* encourage a good user experience for when users interact directly with\\n.proto files and fully-qualified protobuf names\\n* balance conciseness against the possibility of either over-optimizing (making\\nnames too short and cryptic) or under-optimizing (just accepting bloated names\\nwith lots of redundant information)\\nThese guidelines are meant to act as a style guide for both the Cosmos SDK and\\nthird-party modules.\\nAs a starting point, we should adopt all of the [DEFAULT](https:\/\/buf.build\/docs\/lint-checkers#default)\\ncheckers in [Buf's](https:\/\/buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https:\/\/buf.build\/docs\/lint-checkers#file_layout),\\nexcept:\\n* [PACKAGE_VERSION_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n* [SERVICE_SUFFIX](https:\/\/buf.build\/docs\/lint-checkers#service_suffix)\\nFurther guidelines to be described below.\\n### Principles\\n#### Concise and Descriptive Names\\nNames should be descriptive enough to convey their meaning and distinguish\\nthem from other names.\\nGiven that we are using fully-qualifed names within\\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\\nkeep names concise, without going overboard. The general rule of thumb should\\nbe if a shorter name would convey more or else the same thing, pick the shorter\\nname.\\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\\nSuch conciseness makes names both more pleasant to work with and take up less\\nspace within transactions and on the wire.\\nWe should also resist the temptation to over-optimize, by making names\\ncryptically short with abbreviations. For instance, we shouldn't try to\\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\\nThe goal is to make names **_concise but not cryptic_**.\\n#### Names are for Clients First\\nPackage and type names should be chosen for the benefit of users, not\\nnecessarily because of legacy concerns related to the go code-base.\\n#### Plan for Longevity\\nIn the interests of long-term support, we should plan on the names we do\\nchoose to be in usage for a long time, so now is the opportunity to make\\nthe best choices for the future.\\n### Versioning\\n#### Guidelines on Stable Package Versions\\nIn general, schema evolution is the way to update protobuf schemas. That means that new fields,\\nmessages, and RPC methods are _added_ to existing schemas and old fields, messages and RPC methods\\nare maintained as long as possible.\\nBreaking things is often unacceptable in a blockchain scenario. For instance, immutable smart contracts\\nmay depend on certain data schemas on the host chain. If the host chain breaks those schemas, the smart\\ncontract may be irreparably broken. Even when things can be fixed (for instance in client software),\\nthis often comes at a high cost.\\nInstead of breaking things, we should make every effort to evolve schemas rather than just breaking them.\\n[Buf](https:\/\/buf.build) breaking change detection should be used on all stable (non-alpha or beta) packages\\nto prevent such breakage.\\nWith that in mind, different stable versions (i.e. `v1` or `v2`) of a package should more or less be considered\\ndifferent packages and this should be last resort approach for upgrading protobuf schemas. Scenarios where creating\\na `v2` may make sense are:\\n* we want to create a new module with similar functionality to an existing module and adding `v2` is the most natural\\nway to do this. In that case, there are really just two different, but similar modules with different APIs.\\n* we want to add a new revamped API for an existing module and it's just too cumbersome to add it to the existing package,\\nso putting it in `v2` is cleaner for users. In this case, care should be made to not deprecate support for\\n`v1` if it is actively used in immutable smart contracts.\\n#### Guidelines on unstable (alpha and beta) package versions\\nThe following guidelines are recommended for marking packages as alpha or beta:\\n* marking something as `alpha` or `beta` should be a last resort and just putting something in the\\nstable package (i.e. `v1` or `v2`) should be preferred\\n* a package _should_ be marked as `alpha` _if and only if_ there are active discussions to remove\\nor significantly alter the package in the near future\\n* a package _should_ be marked as `beta` _if and only if_ there is an active discussion to\\nsignificantly refactor\/rework the functionality in the near future but not remove it\\n* modules _can and should_ have types in both stable (i.e. `v1` or `v2`) and unstable (`alpha` or `beta`) packages.\\n_`alpha` and `beta` should not be used to avoid responsibility for maintaining compatibility._\\nWhenever code is released into the wild, especially on a blockchain, there is a high cost to changing things. In some\\ncases, for instance with immutable smart contracts, a breaking change may be impossible to fix.\\nWhen marking something as `alpha` or `beta`, maintainers should ask the questions:\\n* what is the cost of asking others to change their code vs the benefit of us maintaining the optionality to change it?\\n* what is the plan for moving this to `v1` and how will that affect users?\\n`alpha` or `beta` should really be used to communicate \"changes are planned\".\\nAs a case study, gRPC reflection is in the package `grpc.reflection.v1alpha`. It hasn't been changed since\\n2017 and it is now used in other widely used software like gRPCurl. Some folks probably use it in production services\\nand so if they actually went and changed the package to `grpc.reflection.v1`, some software would break and\\nthey probably don't want to do that... So now the `v1alpha` package is more or less the de-facto `v1`. Let's not do that.\\nThe following are guidelines for working with non-stable packages:\\n* [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix)\\n(ex. `v1alpha1`) _should_ be used for non-stable packages\\n* non-stable packages should generally be excluded from breaking change detection\\n* immutable smart contract modules (i.e. CosmWasm) _should_ block smart contracts\/persistent\\nscripts from interacting with `alpha`\/`beta` packages\\n#### Omit v1 suffix\\nInstead of using [Buf's recommended version suffix](https:\/\/buf.build\/docs\/lint-checkers#package_version_suffix),\\nwe can omit `v1` for packages that don't actually have a second version. This\\nallows for more concise names for common use cases like `cosmos.bank.Send`.\\nPackages that do have a second or third version can indicate that with `.v2`\\nor `.v3`.\\n### Package Naming\\n#### Adopt a short, unique top-level package name\\nTop-level packages should adopt a short name that is known to not collide with\\nother names in common usage within the Cosmos ecosystem. In the near future, a\\nregistry should be created to reserve and index top-level package names used\\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\\n[ICS](https:\/\/github.com\/cosmos\/ics) specifications could consider a\\nshort top-level package like `ics23` based upon the standard number.\\n#### Limit sub-package depth\\nSub-package depth should be increased with caution. Generally a single\\nsub-package is needed for a module or a library. Even though `x` or `modules`\\nis used in source code to denote modules, this is often unnecessary for .proto\\nfiles as modules are the primary thing sub-packages are used for. Only items which\\nare known to be used infrequently should have deep sub-package depths.\\nFor the Cosmos SDK, it is recommended that we simply write `cosmos.bank`,\\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\\ntypes can go straight in the `cosmos` package or we can introduce a\\n`cosmos.base` package if needed. Note that this naming _will not_ change\\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\\n`x\/bank`.\\n### Message Naming\\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\\ntypes which are used in transactions will retain the `Msg` prefix as that provides\\nhelpful context.\\n### Service and RPC Naming\\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\\nimplement a gRPC query service. We should consider the principle of conciseness\\nfor query service and RPC names as these may be called from persistent script\\nmodules such as CosmWasm. Also, users may use these query paths from tools like\\n[gRPCurl](https:\/\/github.com\/fullstorydev\/grpcurl). As an example, we can shorten\\n`\/cosmos_sdk.x.bank.v1.QueryService\/QueryBalance` to\\n`\/cosmos.bank.Query\/Balance` without losing much useful information.\\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`\/\\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\\non the `Query` service, the request and response types would be `QueryBalanceRequest`\\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\\nand `BalanceResponse`.\\n#### Use just `Query` for the query service\\nInstead of [Buf's default service suffix recommendation](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6033),\\nwe should simply use the shorter `Query` for query services.\\nFor other types of gRPC services, we should consider sticking with Buf's\\ndefault recommendation.\\n#### Omit `Get` and `Query` from query service RPC names\\n`Get` and `Query` should be omitted from `Query` service names because they are\\nredundant in the fully-qualified name. For instance, `\/cosmos.bank.Query\/QueryBalance`\\njust says `Query` twice without any new information.\\n","tokens":330,"id":833,"Predictions":"Decision (A) The following are the steps that should be taken to implement gRPC queries:n### Adoption of Anynn### Adoption of google.protobuf.Anyn### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries. The full gRPC service path thus becomes a key part of ABCI querynpath.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobufnnative queries.n### Decision (A) The following steps should be taken to implement gRPC queries:n### Adoption of gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC QueriesnIn [ADR 021](adr-021-protobuf-query-encoding.md]"}
{"File Name":"cosmos-sdk\/adr-038-state-listening.md","Context":"## Context\\nCurrently, KVStore data can be remotely accessed through [Queries](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/build\/building-modules\/02-messages-and-queries.md#queries)\\nwhich proceed either through Tendermint and the ABCI, or through the gRPC server.\\nIn addition to these request\/response queries, it would be beneficial to have a means of listening to state changes as they occur in real time.\\n","Decision":"We will modify the `CommitMultiStore` interface and its concrete (`rootmulti`) implementations and introduce a new `listenkv.Store` to allow listening to state changes in underlying KVStores. We don't need to listen to cache stores, because we can't be sure that the writes will be committed eventually, and the writes are duplicated in `rootmulti.Store` eventually, so we should only listen to `rootmulti.Store`.\\nWe will introduce a plugin system for configuring and running streaming services that write these state changes and their surrounding ABCI message context to different destinations.\\n### Listening\\nIn a new file, `store\/types\/listening.go`, we will create a `MemoryListener` struct for streaming out protobuf encoded KV pairs state changes from a KVStore.\\nThe `MemoryListener` will be used internally by the concrete `rootmulti` implementation to collect state changes from KVStores.\\n```go\\n\/\/ MemoryListener listens to the state writes and accumulate the records in memory.\\ntype MemoryListener struct {\\nstateCache []StoreKVPair\\n}\\n\/\/ NewMemoryListener creates a listener that accumulate the state writes in memory.\\nfunc NewMemoryListener() *MemoryListener {\\nreturn &MemoryListener{}\\n}\\n\/\/ OnWrite writes state change events to the internal cache\\nfunc (fl *MemoryListener) OnWrite(storeKey StoreKey, key []byte, value []byte, delete bool) {\\nfl.stateCache = append(fl.stateCache, StoreKVPair{\\nStoreKey: storeKey.Name(),\\nDelete:   delete,\\nKey:      key,\\nValue:    value,\\n})\\n}\\n\/\/ PopStateCache returns the current state caches and set to nil\\nfunc (fl *MemoryListener) PopStateCache() []StoreKVPair {\\nres := fl.stateCache\\nfl.stateCache = nil\\nreturn res\\n}\\n```\\nWe will also define a protobuf type for the KV pairs. In addition to the key and value fields this message\\nwill include the StoreKey for the originating KVStore so that we can collect information from separate KVStores and determine the source of each KV pair.\\n```protobuf\\nmessage StoreKVPair {\\noptional string store_key = 1; \/\/ the store key for the KVStore this pair originates from\\nrequired bool set = 2; \/\/ true indicates a set operation, false indicates a delete operation\\nrequired bytes key = 3;\\nrequired bytes value = 4;\\n}\\n```\\n### ListenKVStore\\nWe will create a new `Store` type `listenkv.Store` that the `rootmulti` store will use to wrap a `KVStore` to enable state listening.\\nWe will configure the `Store` with a `MemoryListener` which will collect state changes for output to specific destinations.\\n```go\\n\/\/ Store implements the KVStore interface with listening enabled.\\n\/\/ Operations are traced on each core KVStore call and written to any of the\\n\/\/ underlying listeners with the proper key and operation permissions\\ntype Store struct {\\nparent    types.KVStore\\nlistener  *types.MemoryListener\\nparentStoreKey types.StoreKey\\n}\\n\/\/ NewStore returns a reference to a new traceKVStore given a parent\\n\/\/ KVStore implementation and a buffered writer.\\nfunc NewStore(parent types.KVStore, psk types.StoreKey, listener *types.MemoryListener) *Store {\\nreturn &Store{parent: parent, listener: listener, parentStoreKey: psk}\\n}\\n\/\/ Set implements the KVStore interface. It traces a write operation and\\n\/\/ delegates the Set call to the parent KVStore.\\nfunc (s *Store) Set(key []byte, value []byte) {\\ntypes.AssertValidKey(key)\\ns.parent.Set(key, value)\\ns.listener.OnWrite(s.parentStoreKey, key, value, false)\\n}\\n\/\/ Delete implements the KVStore interface. It traces a write operation and\\n\/\/ delegates the Delete call to the parent KVStore.\\nfunc (s *Store) Delete(key []byte) {\\ns.parent.Delete(key)\\ns.listener.OnWrite(s.parentStoreKey, key, nil, true)\\n}\\n```\\n### MultiStore interface updates\\nWe will update the `CommitMultiStore` interface to allow us to wrap a `Memorylistener` to a specific `KVStore`.\\nNote that the `MemoryListener` will be attached internally by the concrete `rootmulti` implementation.\\n```go\\ntype CommitMultiStore interface {\\n...\\n\/\/ AddListeners adds a listener for the KVStore belonging to the provided StoreKey\\nAddListeners(keys []StoreKey)\\n\/\/ PopStateCache returns the accumulated state change messages from MemoryListener\\nPopStateCache() []StoreKVPair\\n}\\n```\\n### MultiStore implementation updates\\nWe will adjust the `rootmulti` `GetKVStore` method to wrap the returned `KVStore` with a `listenkv.Store` if listening is turned on for that `Store`.\\n```go\\nfunc (rs *Store) GetKVStore(key types.StoreKey) types.KVStore {\\nstore := rs.stores[key].(types.KVStore)\\nif rs.TracingEnabled() {\\nstore = tracekv.NewStore(store, rs.traceWriter, rs.traceContext)\\n}\\nif rs.ListeningEnabled(key) {\\nstore = listenkv.NewStore(store, key, rs.listeners[key])\\n}\\nreturn store\\n}\\n```\\nWe will implement `AddListeners` to manage KVStore listeners internally and implement `PopStateCache`\\nfor a means of retrieving the current state.\\n```go\\n\/\/ AddListeners adds state change listener for a specific KVStore\\nfunc (rs *Store) AddListeners(keys []types.StoreKey) {\\nlistener := types.NewMemoryListener()\\nfor i := range keys {\\nrs.listeners[keys[i]] = listener\\n}\\n}\\n```\\n```go\\nfunc (rs *Store) PopStateCache() []types.StoreKVPair {\\nvar cache []types.StoreKVPair\\nfor _, ls := range rs.listeners {\\ncache = append(cache, ls.PopStateCache()...)\\n}\\nsort.SliceStable(cache, func(i, j int) bool {\\nreturn cache[i].StoreKey < cache[j].StoreKey\\n})\\nreturn cache\\n}\\n```\\nWe will also adjust the `rootmulti` `CacheMultiStore` and `CacheMultiStoreWithVersion` methods to enable listening in\\nthe cache layer.\\n```go\\nfunc (rs *Store) CacheMultiStore() types.CacheMultiStore {\\nstores := make(map[types.StoreKey]types.CacheWrapper)\\nfor k, v := range rs.stores {\\nstore := v.(types.KVStore)\\n\/\/ Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\\n\/\/ set same listeners on cache store will observe duplicated writes.\\nif rs.ListeningEnabled(k) {\\nstore = listenkv.NewStore(store, k, rs.listeners[k])\\n}\\nstores[k] = store\\n}\\nreturn cachemulti.NewStore(rs.db, stores, rs.keysByName, rs.traceWriter, rs.getTracingContext())\\n}\\n```\\n```go\\nfunc (rs *Store) CacheMultiStoreWithVersion(version int64) (types.CacheMultiStore, error) {\\n\/\/ ...\\n\/\/ Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\\n\/\/ set same listeners on cache store will observe duplicated writes.\\nif rs.ListeningEnabled(key) {\\ncacheStore = listenkv.NewStore(cacheStore, key, rs.listeners[key])\\n}\\ncachedStores[key] = cacheStore\\n}\\nreturn cachemulti.NewStore(rs.db, cachedStores, rs.keysByName, rs.traceWriter, rs.getTracingContext()), nil\\n}\\n```\\n### Exposing the data\\n#### Streaming Service\\nWe will introduce a new `ABCIListener` interface that plugs into the BaseApp and relays ABCI requests and responses\\nso that the service can group the state changes with the ABCI requests.\\n```go\\n\/\/ baseapp\/streaming.go\\n\/\/ ABCIListener is the interface that we're exposing as a streaming service.\\ntype ABCIListener interface {\\n\/\/ ListenFinalizeBlock updates the streaming service with the latest FinalizeBlock messages\\nListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error\\n\/\/ ListenCommit updates the steaming service with the latest Commit messages and state changes\\nListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []*StoreKVPair) error\\n}\\n```\\n#### BaseApp Registration\\nWe will add a new method to the `BaseApp` to enable the registration of `StreamingService`s:\\n```go\\n\/\/ SetStreamingService is used to set a streaming service into the BaseApp hooks and load the listeners into the multistore\\nfunc (app *BaseApp) SetStreamingService(s ABCIListener) {\\n\/\/ register the StreamingService within the BaseApp\\n\/\/ BaseApp will pass BeginBlock, DeliverTx, and EndBlock requests and responses to the streaming services to update their ABCI context\\napp.abciListeners = append(app.abciListeners, s)\\n}\\n```\\nWe will add two new fields to the `BaseApp` struct:\\n```go\\ntype BaseApp struct {\\n...\\n\/\/ abciListenersAsync for determining if abciListeners will run asynchronously.\\n\/\/ When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\\n\/\/ When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\\nabciListenersAsync bool\\n\/\/ stopNodeOnABCIListenerErr halts the node when ABCI streaming service listening results in an error.\\n\/\/ stopNodeOnABCIListenerErr=true must be paired with abciListenersAsync=false.\\nstopNodeOnABCIListenerErr bool\\n}\\n```\\n#### ABCI Event Hooks\\nWe will modify the `FinalizeBlock` and `Commit` methods to pass ABCI requests and responses\\nto any streaming service hooks registered with the `BaseApp`.\\n```go\\nfunc (app *BaseApp) FinalizeBlock(req abci.RequestFinalizeBlock) abci.ResponseFinalizeBlock {\\nvar abciRes abci.ResponseFinalizeBlock\\ndefer func() {\\n\/\/ call the streaming service hook with the FinalizeBlock messages\\nfor _, abciListener := range app.abciListeners {\\nctx := app.finalizeState.ctx\\nblockHeight := ctx.BlockHeight()\\nif app.abciListenersAsync {\\ngo func(req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) {\\nif err := app.abciListener.FinalizeBlock(blockHeight, req, res); err != nil {\\napp.logger.Error(\"FinalizeBlock listening hook failed\", \"height\", blockHeight, \"err\", err)\\n}\\n}(req, abciRes)\\n} else {\\nif err := app.abciListener.ListenFinalizeBlock(blockHeight, req, res); err != nil {\\napp.logger.Error(\"FinalizeBlock listening hook failed\", \"height\", blockHeight, \"err\", err)\\nif app.stopNodeOnABCIListenerErr {\\nos.Exit(1)\\n}\\n}\\n}\\n}\\n}()\\n...\\nreturn abciRes\\n}\\n```\\n```go\\nfunc (app *BaseApp) Commit() abci.ResponseCommit {\\n...\\nres := abci.ResponseCommit{\\nData:         commitID.Hash,\\nRetainHeight: retainHeight,\\n}\\n\/\/ call the streaming service hook with the Commit messages\\nfor _, abciListener := range app.abciListeners {\\nctx := app.deliverState.ctx\\nblockHeight := ctx.BlockHeight()\\nchangeSet := app.cms.PopStateCache()\\nif app.abciListenersAsync {\\ngo func(res abci.ResponseCommit, changeSet []store.StoreKVPair) {\\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\\napp.logger.Error(\"ListenCommit listening hook failed\", \"height\", blockHeight, \"err\", err)\\n}\\n}(res, changeSet)\\n} else {\\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\\napp.logger.Error(\"ListenCommit listening hook failed\", \"height\", blockHeight, \"err\", err)\\nif app.stopNodeOnABCIListenerErr {\\nos.Exit(1)\\n}\\n}\\n}\\n}\\n...\\nreturn res\\n}\\n```\\n#### Go Plugin System\\nWe propose a plugin architecture to load and run `Streaming` plugins and other types of implementations. We will introduce a plugin\\nsystem over gRPC that is used to load and run Cosmos-SDK plugins. The plugin system uses [hashicorp\/go-plugin](https:\/\/github.com\/hashicorp\/go-plugin).\\nEach plugin must have a struct that implements the `plugin.Plugin` interface and an `Impl` interface for processing messages over gRPC.\\nEach plugin must also have a message protocol defined for the gRPC service:\\n```go\\n\/\/ streaming\/plugins\/abci\/{plugin_version}\/interface.go\\n\/\/ Handshake is a common handshake that is shared by streaming and host.\\n\/\/ This prevents users from executing bad plugins or executing a plugin\\n\/\/ directory. It is a UX feature, not a security feature.\\nvar Handshake = plugin.HandshakeConfig{\\nProtocolVersion:  1,\\nMagicCookieKey:   \"ABCI_LISTENER_PLUGIN\",\\nMagicCookieValue: \"ef78114d-7bdf-411c-868f-347c99a78345\",\\n}\\n\/\/ ListenerPlugin is the base struct for all kinds of go-plugin implementations\\n\/\/ It will be included in interfaces of different Plugins\\ntype ABCIListenerPlugin struct {\\n\/\/ GRPCPlugin must still implement the Plugin interface\\nplugin.Plugin\\n\/\/ Concrete implementation, written in Go. This is only used for plugins\\n\/\/ that are written in Go.\\nImpl baseapp.ABCIListener\\n}\\nfunc (p *ListenerGRPCPlugin) GRPCServer(_ *plugin.GRPCBroker, s *grpc.Server) error {\\nRegisterABCIListenerServiceServer(s, &GRPCServer{Impl: p.Impl})\\nreturn nil\\n}\\nfunc (p *ListenerGRPCPlugin) GRPCClient(\\n_ context.Context,\\n_ *plugin.GRPCBroker,\\nc *grpc.ClientConn,\\n) (interface{}, error) {\\nreturn &GRPCClient{client: NewABCIListenerServiceClient(c)}, nil\\n}\\n```\\nThe `plugin.Plugin` interface has two methods `Client` and `Server`. For our GRPC service these are `GRPCClient` and `GRPCServer`\\nThe `Impl` field holds the concrete implementation of our `baseapp.ABCIListener` interface written in Go.\\nNote: this is only used for plugin implementations written in Go.\\nThe advantage of having such a plugin system is that within each plugin authors can define the message protocol in a way that fits their use case.\\nFor example, when state change listening is desired, the `ABCIListener` message protocol can be defined as below (*for illustrative purposes only*).\\nWhen state change listening is not desired than `ListenCommit` can be omitted from the protocol.\\n```protobuf\\nsyntax = \"proto3\";\\n...\\nmessage Empty {}\\nmessage ListenFinalizeBlockRequest {\\nRequestFinalizeBlock  req = 1;\\nResponseFinalizeBlock res = 2;\\n}\\nmessage ListenCommitRequest {\\nint64                block_height = 1;\\nResponseCommit       res          = 2;\\nrepeated StoreKVPair changeSet    = 3;\\n}\\n\/\/ plugin that listens to state changes\\nservice ABCIListenerService {\\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\\n}\\n```\\n```protobuf\\n...\\n\/\/ plugin that doesn't listen to state changes\\nservice ABCIListenerService {\\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\\n}\\n```\\nImplementing the service above:\\n```go\\n\/\/ streaming\/plugins\/abci\/{plugin_version}\/grpc.go\\nvar (\\n_ baseapp.ABCIListener = (*GRPCClient)(nil)\\n)\\n\/\/ GRPCClient is an implementation of the ABCIListener and ABCIListenerPlugin interfaces that talks over RPC.\\ntype GRPCClient struct {\\nclient ABCIListenerServiceClient\\n}\\nfunc (m *GRPCClient) ListenFinalizeBlock(goCtx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n_, err := m.client.ListenDeliverTx(ctx, &ListenDeliverTxRequest{BlockHeight: ctx.BlockHeight(), Req: req, Res: res})\\nreturn err\\n}\\nfunc (m *GRPCClient) ListenCommit(goCtx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n_, err := m.client.ListenCommit(ctx, &ListenCommitRequest{BlockHeight: ctx.BlockHeight(), Res: res, ChangeSet: changeSet})\\nreturn err\\n}\\n\/\/ GRPCServer is the gRPC server that GRPCClient talks to.\\ntype GRPCServer struct {\\n\/\/ This is the real implementation\\nImpl baseapp.ABCIListener\\n}\\nfunc (m *GRPCServer) ListenFinalizeBlock(ctx context.Context, req *ListenFinalizeBlockRequest) (*Empty, error) {\\nreturn &Empty{}, m.Impl.ListenFinalizeBlock(ctx, req.Req, req.Res)\\n}\\nfunc (m *GRPCServer) ListenCommit(ctx context.Context, req *ListenCommitRequest) (*Empty, error) {\\nreturn &Empty{}, m.Impl.ListenCommit(ctx, req.Res, req.ChangeSet)\\n}\\n```\\nAnd the pre-compiled Go plugin `Impl`(*this is only used for plugins that are written in Go*):\\n```go\\n\/\/ streaming\/plugins\/abci\/{plugin_version}\/impl\/plugin.go\\n\/\/ Plugins are pre-compiled and loaded by the plugin system\\n\/\/ ABCIListener is the implementation of the baseapp.ABCIListener interface\\ntype ABCIListener struct{}\\nfunc (m *ABCIListenerPlugin) ListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\\n\/\/ send data to external system\\n}\\nfunc (m *ABCIListenerPlugin) ListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\\n\/\/ send data to external system\\n}\\nfunc main() {\\nplugin.Serve(&plugin.ServeConfig{\\nHandshakeConfig: grpc_abci_v1.Handshake,\\nPlugins: map[string]plugin.Plugin{\\n\"grpc_plugin_v1\": &grpc_abci_v1.ABCIListenerGRPCPlugin{Impl: &ABCIListenerPlugin{}},\\n},\\n\/\/ A non-nil value here enables gRPC serving for this streaming...\\nGRPCServer: plugin.DefaultGRPCServer,\\n})\\n}\\n```\\nWe will introduce a plugin loading system that will return `(interface{}, error)`.\\nThis provides the advantage of using versioned plugins where the plugin interface and gRPC protocol change over time.\\nIn addition, it allows for building independent plugin that can expose different parts of the system over gRPC.\\n```go\\nfunc NewStreamingPlugin(name string, logLevel string) (interface{}, error) {\\nlogger := hclog.New(&hclog.LoggerOptions{\\nOutput: hclog.DefaultOutput,\\nLevel:  toHclogLevel(logLevel),\\nName:   fmt.Sprintf(\"plugin.%s\", name),\\n})\\n\/\/ We're a host. Start by launching the streaming process.\\nenv := os.Getenv(GetPluginEnvKey(name))\\nclient := plugin.NewClient(&plugin.ClientConfig{\\nHandshakeConfig: HandshakeMap[name],\\nPlugins:         PluginMap,\\nCmd:             exec.Command(\"sh\", \"-c\", env),\\nLogger:          logger,\\nAllowedProtocols: []plugin.Protocol{\\nplugin.ProtocolNetRPC, plugin.ProtocolGRPC},\\n})\\n\/\/ Connect via RPC\\nrpcClient, err := client.Client()\\nif err != nil {\\nreturn nil, err\\n}\\n\/\/ Request streaming plugin\\nreturn rpcClient.Dispense(name)\\n}\\n```\\nWe propose a `RegisterStreamingPlugin` function for the App to register `NewStreamingPlugin`s with the App's BaseApp.\\nStreaming plugins can be of `Any` type; therefore, the function takes in an interface vs a concrete type.\\nFor example, we could have plugins of `ABCIListener`, `WasmListener` or `IBCListener`. Note that `RegisterStreamingPluing` function\\nis helper function and not a requirement. Plugin registration can easily be moved from the App to the BaseApp directly.\\n```go\\n\/\/ baseapp\/streaming.go\\n\/\/ RegisterStreamingPlugin registers streaming plugins with the App.\\n\/\/ This method returns an error if a plugin is not supported.\\nfunc RegisterStreamingPlugin(\\nbApp *BaseApp,\\nappOpts servertypes.AppOptions,\\nkeys map[string]*types.KVStoreKey,\\nstreamingPlugin interface{},\\n) error {\\nswitch t := streamingPlugin.(type) {\\ncase ABCIListener:\\nregisterABCIListenerPlugin(bApp, appOpts, keys, t)\\ndefault:\\nreturn fmt.Errorf(\"unexpected plugin type %T\", t)\\n}\\nreturn nil\\n}\\n```\\n```go\\nfunc registerABCIListenerPlugin(\\nbApp *BaseApp,\\nappOpts servertypes.AppOptions,\\nkeys map[string]*store.KVStoreKey,\\nabciListener ABCIListener,\\n) {\\nasyncKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIAsync)\\nasync := cast.ToBool(appOpts.Get(asyncKey))\\nstopNodeOnErrKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIStopNodeOnErrTomlKey)\\nstopNodeOnErr := cast.ToBool(appOpts.Get(stopNodeOnErrKey))\\nkeysKey := fmt.Sprintf(\"%s.%s.%s\", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIKeysTomlKey)\\nexposeKeysStr := cast.ToStringSlice(appOpts.Get(keysKey))\\nexposedKeys := exposeStoreKeysSorted(exposeKeysStr, keys)\\nbApp.cms.AddListeners(exposedKeys)\\napp.SetStreamingManager(\\nstoretypes.StreamingManager{\\nABCIListeners: []storetypes.ABCIListener{abciListener},\\nStopNodeOnErr: stopNodeOnErr,\\n},\\n)\\n}\\n```\\n```go\\nfunc exposeAll(list []string) bool {\\nfor _, ele := range list {\\nif ele == \"*\" {\\nreturn true\\n}\\n}\\nreturn false\\n}\\nfunc exposeStoreKeys(keysStr []string, keys map[string]*types.KVStoreKey) []types.StoreKey {\\nvar exposeStoreKeys []types.StoreKey\\nif exposeAll(keysStr) {\\nexposeStoreKeys = make([]types.StoreKey, 0, len(keys))\\nfor _, storeKey := range keys {\\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\\n}\\n} else {\\nexposeStoreKeys = make([]types.StoreKey, 0, len(keysStr))\\nfor _, keyStr := range keysStr {\\nif storeKey, ok := keys[keyStr]; ok {\\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\\n}\\n}\\n}\\n\/\/ sort storeKeys for deterministic output\\nsort.SliceStable(exposeStoreKeys, func(i, j int) bool {\\nreturn exposeStoreKeys[i].Name() < exposeStoreKeys[j].Name()\\n})\\nreturn exposeStoreKeys\\n}\\n```\\nThe `NewStreamingPlugin` and `RegisterStreamingPlugin` functions are used to register a plugin with the App's BaseApp.\\ne.g. in `NewSimApp`:\\n```go\\nfunc NewSimApp(\\nlogger log.Logger,\\ndb dbm.DB,\\ntraceStore io.Writer,\\nloadLatest bool,\\nappOpts servertypes.AppOptions,\\nbaseAppOptions ...func(*baseapp.BaseApp),\\n) *SimApp {\\n...\\nkeys := sdk.NewKVStoreKeys(\\nauthtypes.StoreKey, banktypes.StoreKey, stakingtypes.StoreKey,\\nminttypes.StoreKey, distrtypes.StoreKey, slashingtypes.StoreKey,\\ngovtypes.StoreKey, paramstypes.StoreKey, ibchost.StoreKey, upgradetypes.StoreKey,\\nevidencetypes.StoreKey, ibctransfertypes.StoreKey, capabilitytypes.StoreKey,\\n)\\n...\\n\/\/ register streaming services\\nstreamingCfg := cast.ToStringMap(appOpts.Get(baseapp.StreamingTomlKey))\\nfor service := range streamingCfg {\\npluginKey := fmt.Sprintf(\"%s.%s.%s\", baseapp.StreamingTomlKey, service, baseapp.StreamingPluginTomlKey)\\npluginName := strings.TrimSpace(cast.ToString(appOpts.Get(pluginKey)))\\nif len(pluginName) > 0 {\\nlogLevel := cast.ToString(appOpts.Get(flags.FlagLogLevel))\\nplugin, err := streaming.NewStreamingPlugin(pluginName, logLevel)\\nif err != nil {\\ntmos.Exit(err.Error())\\n}\\nif err := baseapp.RegisterStreamingPlugin(bApp, appOpts, keys, plugin); err != nil {\\ntmos.Exit(err.Error())\\n}\\n}\\n}\\nreturn app\\n```\\n#### Configuration\\nThe plugin system will be configured within an App's TOML configuration files.\\n```toml\\n# gRPC streaming\\n[streaming]\\n# ABCI streaming service\\n[streaming.abci]\\n# The plugin version to use for ABCI listening\\nplugin = \"abci_v1\"\\n# List of kv store keys to listen to for state changes.\\n# Set to [\"*\"] to expose all keys.\\nkeys = [\"*\"]\\n# Enable abciListeners to run asynchronously.\\n# When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\\n# When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\\nasync = false\\n# Whether to stop the node on message deliver error.\\nstop-node-on-err = true\\n```\\nThere will be four parameters for configuring `ABCIListener` plugin: `streaming.abci.plugin`, `streaming.abci.keys`, `streaming.abci.async` and `streaming.abci.stop-node-on-err`.\\n`streaming.abci.plugin` is the name of the plugin we want to use for streaming, `streaming.abci.keys` is a set of store keys for stores it listens to,\\n`streaming.abci.async` is bool enabling asynchronous listening and `streaming.abci.stop-node-on-err` is a bool that stops the node when true and when operating\\non synchronized mode `streaming.abci.async=false`. Note that `streaming.abci.stop-node-on-err=true` will be ignored if `streaming.abci.async=true`.\\nThe configuration above support additional streaming plugins by adding the plugin to the `[streaming]` configuration section\\nand registering the plugin with `RegisterStreamingPlugin` helper function.\\nNote the that each plugin must include `streaming.{service}.plugin` property as it is a requirement for doing the lookup and registration of the plugin\\nwith the App. All other properties are unique to the individual services.\\n#### Encoding and decoding streams\\nADR-038 introduces the interfaces and types for streaming state changes out from KVStores, associating this\\ndata with their related ABCI requests and responses, and registering a service for consuming this data and streaming it to some destination in a final format.\\nInstead of prescribing a final data format in this ADR, it is left to a specific plugin implementation to define and document this format.\\nWe take this approach because flexibility in the final format is necessary to support a wide range of streaming service plugins. For example,\\nthe data format for a streaming service that writes the data out to a set of files will differ from the data format that is written to a Kafka topic.\\n","tokens":93,"id":834,"Predictions":"Decision (A) The KVStore data can be remotely accessed through [Queries](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/build\/building-modules\/02-messages-and-queries.md#queries)nwhich proceed either through Tendermint and the ABCI, or through the gRPC server.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time.nIt would be beneficial to have a means of listening to state changes as they occur in real time."}
{"File Name":"cosmos-sdk\/adr-027-deterministic-protobuf-serialization.md","Context":"### Context\\nFor signature verification in Cosmos SDK, the signer and verifier need to agree on\\nthe same serialization of a `SignDoc` as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization.\\nCurrently, for block signatures we are using a workaround: we create a new [TxRaw](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L30)\\ninstance (as defined in [adr-020-protobuf-transaction-encoding](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-020-protobuf-transaction-encoding.md#transactions))\\nby converting all [Tx](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L13)\\nfields to bytes on the client side. This adds an additional manual\\nstep when sending and signing transactions.\\n### Decision\\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n","Decision":"The following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n","tokens":284,"id":835,"Predictions":"Decision based on the information in the text.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-031-msg-service.md","Context":"## Context\\nCurrently `Msg` handlers in the Cosmos SDK do have return values that are placed in the `data` field of the response.\\nThese return values, however, are not specified anywhere except in the golang handler code.\\nIn early conversations [it was proposed](https:\/\/docs.google.com\/document\/d\/1eEgYgvgZqLE45vETjhwIw4VOqK-5hwQtZtjVbiXnIGc\/edit)\\nthat `Msg` return types be captured using a protobuf extension field, ex:\\n```protobuf\\npackage cosmos.gov;\\nmessage MsgSubmitProposal\\noption (cosmos_proto.msg_return) = \u201cuint64\u201d;\\nstring delegator_address = 1;\\nstring validator_address = 2;\\nrepeated sdk.Coin amount = 3;\\n}\\n```\\nThis was never adopted, however.\\nHaving a well-specified return value for `Msg`s would improve client UX. For instance,\\nin `x\/gov`,  `MsgSubmitProposal` returns the proposal ID as a big-endian `uint64`.\\nThis isn\u2019t really documented anywhere and clients would need to know the internals\\nof the Cosmos SDK to parse that value and return it to users.\\nAlso, there may be cases where we want to use these return values programmatically.\\nFor instance, https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7093 proposes a method for\\ndoing inter-module Ocaps using the `Msg` router. A well-defined return type would\\nimprove the developer UX for this approach.\\nIn addition, handler registration of `Msg` types tends to add a bit of\\nboilerplate on top of keepers and is usually done through manual type switches.\\nThis isn't necessarily bad, but it does add overhead to creating modules.\\n","Decision":"We decide to use protobuf `service` definitions for defining `Msg`s as well as\\nthe code generated by them as a replacement for `Msg` handlers.\\nBelow we define how this will look for the `SubmitProposal` message from `x\/gov` module.\\nWe start with a `Msg` `service` definition:\\n```protobuf\\npackage cosmos.gov;\\nservice Msg {\\nrpc SubmitProposal(MsgSubmitProposal) returns (MsgSubmitProposalResponse);\\n}\\n\/\/ Note that for backwards compatibility this uses MsgSubmitProposal as the request\\n\/\/ type instead of the more canonical MsgSubmitProposalRequest\\nmessage MsgSubmitProposal {\\ngoogle.protobuf.Any content = 1;\\nstring proposer = 2;\\n}\\nmessage MsgSubmitProposalResponse {\\nuint64 proposal_id;\\n}\\n```\\nWhile this is most commonly used for gRPC, overloading protobuf `service` definitions like this does not violate\\nthe intent of the [protobuf spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#services) which says:\\n> If you don\u2019t want to use gRPC, it\u2019s also possible to use protocol buffers with your own RPC implementation.\\nWith this approach, we would get an auto-generated `MsgServer` interface:\\nIn addition to clearly specifying return types, this has the benefit of generating client and server code. On the server\\nside, this is almost like an automatically generated keeper method and could maybe be used instead of keepers eventually\\n(see [\\#7093](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7093)):\\n```go\\npackage gov\\ntype MsgServer interface {\\nSubmitProposal(context.Context, *MsgSubmitProposal) (*MsgSubmitProposalResponse, error)\\n}\\n```\\nOn the client side, developers could take advantage of this by creating RPC implementations that encapsulate transaction\\nlogic. Protobuf libraries that use asynchronous callbacks, like [protobuf.js](https:\/\/github.com\/protobufjs\/protobuf.js#using-services)\\ncould use this to register callbacks for specific messages even for transactions that include multiple `Msg`s.\\nEach `Msg` service method should have exactly one request parameter: its corresponding `Msg` type. For example, the `Msg` service method `\/cosmos.gov.v1beta1.Msg\/SubmitProposal` above has exactly one request parameter, namely the `Msg` type `\/cosmos.gov.v1beta1.MsgSubmitProposal`. It is important the reader understands clearly the nomenclature difference between a `Msg` service (a Protobuf service) and a `Msg` type (a Protobuf message), and the differences in their fully-qualified name.\\nThis convention has been decided over the more canonical `Msg...Request` names mainly for backwards compatibility, but also for better readability in `TxBody.messages` (see [Encoding section](#encoding) below): transactions containing `\/cosmos.gov.MsgSubmitProposal` read better than those containing `\/cosmos.gov.v1beta1.MsgSubmitProposalRequest`.\\nOne consequence of this convention is that each `Msg` type can be the request parameter of only one `Msg` service method. However, we consider this limitation a good practice in explicitness.\\n### Encoding\\nEncoding of transactions generated with `Msg` services do not differ from current Protobuf transaction encoding as defined in [ADR-020](.\/adr-020-protobuf-transaction-encoding.md). We are encoding `Msg` types (which are exactly `Msg` service methods' request parameters) as `Any` in `Tx`s which involves packing the\\nbinary-encoded `Msg` with its type URL.\\n### Decoding\\nSince `Msg` types are packed into `Any`, decoding transactions messages are done by unpacking `Any`s into `Msg` types. For more information, please refer to [ADR-020](.\/adr-020-protobuf-transaction-encoding.md#transactions).\\n### Routing\\nWe propose to add a `msg_service_router` in BaseApp. This router is a key\/value map which maps `Msg` types' `type_url`s to their corresponding `Msg` service method handler. Since there is a 1-to-1 mapping between `Msg` types and `Msg` service method, the `msg_service_router` has exactly one entry per `Msg` service method.\\nWhen a transaction is processed by BaseApp (in CheckTx or in DeliverTx), its `TxBody.messages` are decoded as `Msg`s. Each `Msg`'s `type_url` is matched against an entry in the `msg_service_router`, and the respective `Msg` service method handler is called.\\nFor backward compatibility, the old handlers are not removed yet. If BaseApp receives a legacy `Msg` with no corresponding entry in the `msg_service_router`, it will be routed via its legacy `Route()` method into the legacy handler.\\n### Module Configuration\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), we introduced a method `RegisterQueryService`\\nto `AppModule` which allows for modules to register gRPC queriers.\\nTo register `Msg` services, we attempt a more extensible approach by converting `RegisterQueryService`\\nto a more generic `RegisterServices` method:\\n```go\\ntype AppModule interface {\\nRegisterServices(Configurator)\\n...\\n}\\ntype Configurator interface {\\nQueryServer() grpc.Server\\nMsgServer() grpc.Server\\n}\\n\/\/ example module:\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ntypes.RegisterQueryServer(cfg.QueryServer(), keeper)\\ntypes.RegisterMsgServer(cfg.MsgServer(), keeper)\\n}\\n```\\nThe `RegisterServices` method and the `Configurator` interface are intended to\\nevolve to satisfy the use cases discussed in [\\#7093](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7093)\\nand [\\#7122](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/7421).\\nWhen `Msg` services are registered, the framework _should_ verify that all `Msg` types\\nimplement the `sdk.Msg` interface and throw an error during initialization rather\\nthan later when transactions are processed.\\n### `Msg` Service Implementation\\nJust like query services, `Msg` service methods can retrieve the `sdk.Context`\\nfrom the `context.Context` parameter method using the `sdk.UnwrapSDKContext`\\nmethod:\\n```go\\npackage gov\\nfunc (k Keeper) SubmitProposal(goCtx context.Context, params *types.MsgSubmitProposal) (*MsgSubmitProposalResponse, error) {\\nctx := sdk.UnwrapSDKContext(goCtx)\\n...\\n}\\n```\\nThe `sdk.Context` should have an `EventManager` already attached by BaseApp's `msg_service_router`.\\nSeparate handler definition is no longer needed with this approach.\\n","tokens":386,"id":836,"Predictions":"Decision (A) The Msg return type should be captured using a protobuf extension field.nB) The Msg return type should be captured using a protobuf extension field.nC) The Msg return type should be captured using a protobuf extension field.nD) The Msg return type should be captured using a protobuf extension field.nAnswer:A.nB.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.nB.nC.n"}
{"File Name":"cosmos-sdk\/adr-029-fee-grant-module.md","Context":"## Context\\nIn order to make blockchain transactions, the signing account must possess a sufficient balance of the right denomination\\nin order to pay fees. There are classes of transactions where needing to maintain a wallet with sufficient fees is a\\nbarrier to adoption.\\nFor instance, when proper permissions are setup, someone may temporarily delegate the ability to vote on proposals to\\na \"burner\" account that is stored on a mobile phone with only minimal security.\\nOther use cases include workers tracking items in a supply chain or farmers submitting field data for analytics\\nor compliance purposes.\\nFor all of these use cases, UX would be significantly enhanced by obviating the need for these accounts to always\\nmaintain the appropriate fee balance. This is especially true if we wanted to achieve enterprise adoption for something\\nlike supply chain tracking.\\nWhile one solution would be to have a service that fills up these accounts automatically with the appropriate fees, a better UX\\nwould be provided by allowing these accounts to pull from a common fee pool account with proper spending limits.\\nA single pool would reduce the churn of making lots of small \"fill up\" transactions and also more effectively leverages\\nthe resources of the organization setting up the pool.\\n","Decision":"As a solution we propose a module, `x\/feegrant` which allows one account, the \"granter\" to grant another account, the \"grantee\"\\nan allowance to spend the granter's account balance for fees within certain well-defined limits.\\nFee allowances are defined by the extensible `FeeAllowanceI` interface:\\n```go\\ntype FeeAllowanceI {\\n\/\/ Accept can use fee payment requested as well as timestamp of the current block\\n\/\/ to determine whether or not to process this. This is checked in\\n\/\/ Keeper.UseGrantedFees and the return values should match how it is handled there.\\n\/\/\\n\/\/ If it returns an error, the fee payment is rejected, otherwise it is accepted.\\n\/\/ The FeeAllowance implementation is expected to update it's internal state\\n\/\/ and will be saved again after an acceptance.\\n\/\/\\n\/\/ If remove is true (regardless of the error), the FeeAllowance will be deleted from storage\\n\/\/ (eg. when it is used up). (See call to RevokeFeeAllowance in Keeper.UseGrantedFees)\\nAccept(ctx sdk.Context, fee sdk.Coins, msgs []sdk.Msg) (remove bool, err error)\\n\/\/ ValidateBasic should evaluate this FeeAllowance for internal consistency.\\n\/\/ Don't allow negative amounts, or negative periods for example.\\nValidateBasic() error\\n}\\n```\\nTwo basic fee allowance types, `BasicAllowance` and `PeriodicAllowance` are defined to support known use cases:\\n```protobuf\\n\/\/ BasicAllowance implements FeeAllowanceI with a one-time grant of tokens\\n\/\/ that optionally expires. The delegatee can use up to SpendLimit to cover fees.\\nmessage BasicAllowance {\\n\/\/ spend_limit specifies the maximum amount of tokens that can be spent\\n\/\/ by this allowance and will be updated as tokens are spent. If it is\\n\/\/ empty, there is no spend limit and any amount of coins can be spent.\\nrepeated cosmos_sdk.v1.Coin spend_limit = 1;\\n\/\/ expiration specifies an optional time when this allowance expires\\ngoogle.protobuf.Timestamp expiration = 2;\\n}\\n\/\/ PeriodicAllowance extends FeeAllowanceI to allow for both a maximum cap,\\n\/\/ as well as a limit per time period.\\nmessage PeriodicAllowance {\\nBasicAllowance basic = 1;\\n\/\/ period specifies the time duration in which period_spend_limit coins can\\n\/\/ be spent before that allowance is reset\\ngoogle.protobuf.Duration period = 2;\\n\/\/ period_spend_limit specifies the maximum number of coins that can be spent\\n\/\/ in the period\\nrepeated cosmos_sdk.v1.Coin period_spend_limit = 3;\\n\/\/ period_can_spend is the number of coins left to be spent before the period_reset time\\nrepeated cosmos_sdk.v1.Coin period_can_spend = 4;\\n\/\/ period_reset is the time at which this period resets and a new one begins,\\n\/\/ it is calculated from the start time of the first transaction after the\\n\/\/ last period ended\\ngoogle.protobuf.Timestamp period_reset = 5;\\n}\\n```\\nAllowances can be granted and revoked using `MsgGrantAllowance` and `MsgRevokeAllowance`:\\n```protobuf\\n\/\/ MsgGrantAllowance adds permission for Grantee to spend up to Allowance\\n\/\/ of fees from the account of Granter.\\nmessage MsgGrantAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\ngoogle.protobuf.Any allowance = 3;\\n}\\n\/\/ MsgRevokeAllowance removes any existing FeeAllowance from Granter to Grantee.\\nmessage MsgRevokeAllowance {\\nstring granter = 1;\\nstring grantee = 2;\\n}\\n```\\nIn order to use allowances in transactions, we add a new field `granter` to the transaction `Fee` type:\\n```protobuf\\npackage cosmos.tx.v1beta1;\\nmessage Fee {\\nrepeated cosmos.base.v1beta1.Coin amount = 1;\\nuint64 gas_limit = 2;\\nstring payer = 3;\\nstring granter = 4;\\n}\\n```\\n`granter` must either be left empty or must correspond to an account which has granted\\na fee allowance to fee payer (either the first signer or the value of the `payer` field).\\nA new `AnteDecorator` named `DeductGrantedFeeDecorator` will be created in order to process transactions with `fee_payer`\\nset and correctly deduct fees based on fee allowances.\\n","tokens":247,"id":837,"Predictions":"Decision (A).nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A."}
{"File Name":"cosmos-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n* `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n* `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n* `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n* `LockedCoins(ctx Context, addr AccAddress) Coins`\\n* `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```go\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":838,"Predictions":"Decision (A) The user should be able to withdraw the coins from the account.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"Predictions":"Decision (A).nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/hackatom\/x\/delegation).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group].nIt is not possible to implement the module without the support of the [Gaian's team at Hackatom Berlin 2019](https:\/\/git"}
{"File Name":"cosmos-sdk\/adr-022-custom-panic-handling.md","Context":"## Context\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give Cosmos SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#recovery-middleware)).\\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\\nin order to customize it.\\n","Decision":"### Design\\n#### Overview\\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\\ncan be found [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6053).\\n#### Implementation details\\n##### Recovery handler\\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\\n`recover()` from the `builtin` package.\\n```go\\ntype RecoveryHandler func(recoveryObj interface{}) error\\n```\\nHandler should type assert (or other methods) an object to define if object should be handled.\\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\\nAn example:\\n```go\\nfunc exampleErrHandler(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(error)\\nif !ok { return nil }\\nif someSpecificError.Is(err) {\\npanic(customPanicMsg)\\n} else {\\nreturn nil\\n}\\n}\\n```\\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\\n##### Recovery middleware\\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\\nchain processing.\\n```go\\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\\nif err := handler(recoveryObj); err != nil {\\nreturn nil, err\\n}\\nreturn next, nil\\n}\\n}\\n```\\nFunction receives a `recoveryObj` object and returns:\\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\\n`OutOfGas` middleware example:\\n```go\\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\\nif !ok { return nil }\\nreturn errorsmod.Wrap(\\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\\n\"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\\n),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, next)\\n}\\n```\\n`Default` middleware example:\\n```go\\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nreturn errorsmod.Wrap(\\nsdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, nil)\\n}\\n```\\n##### Recovery processing\\nBasic chain of middlewares processing would look like:\\n```go\\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\\nif middleware == nil { return nil }\\nnext, err := middleware(recoveryObj)\\nif err != nil { return err }\\nif next == nil { return nil }\\nreturn processRecovery(recoveryObj, next)\\n}\\n```\\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\\n`default` handler which must return an `error`.\\n##### BaseApp changes\\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\\n```go\\ntype BaseApp struct {\\n\/\/ ...\\nrunTxRecoveryMiddleware recoveryMiddleware\\n}\\nfunc NewBaseApp(...) {\\n\/\/ ...\\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\\n}\\nfunc (app *BaseApp) runTx(...) {\\n\/\/ ...\\ndefer func() {\\nif r := recover(); r != nil {\\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\\nerr, result = processRecovery(r, recoveryMW), nil\\n}\\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\\n}()\\n\/\/ ...\\n}\\n```\\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\\n```go\\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\\nfor _, h := range handlers {\\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\\n}\\n}\\n```\\nThis method would prepend handlers to an existing chain.\\n","tokens":338,"id":840,"Predictions":"Decision (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (B) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (C) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (D) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (B) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (C) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (D) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery. (A) The current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery."}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"Predictions":"We suggest a simplest form of validator consensus key rotation implementation.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a consensus key rotation of a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a consensus key to another.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement of a validator.nWe consider that a validator is simply a replacement"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"Predictions":"Decision (A). nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the Cosmos SDK.nThe proposed method will be used to consume the event driven events in the"}
{"File Name":"cosmos-sdk\/adr-037-gov-split-vote.md","Context":"## Context\\nCurrently, an address can cast a vote with only one options (Yes\/No\/Abstain\/NoWithVeto) and use their full voting power behind that choice.\\nHowever, often times the entity owning that address might not be a single individual.  For example, a company might have different stakeholders who want to vote differently, and so it makes sense to allow them to split their voting power.  Another example use case is exchanges.  Many centralized exchanges often stake a portion of their users' tokens in their custody.  Currently, it is not possible for them to do \"passthrough voting\" and giving their users voting rights over their tokens.  However, with this system, exchanges can poll their users for voting preferences, and then vote on-chain proportionally to the results of the poll.\\n","Decision":"We modify the vote structs to be\\n```go\\ntype WeightedVoteOption struct {\\nOption string\\nWeight sdk.Dec\\n}\\ntype Vote struct {\\nProposalID int64\\nVoter      sdk.Address\\nOptions    []WeightedVoteOption\\n}\\n```\\nAnd for backwards compatibility, we introduce `MsgVoteWeighted` while keeping `MsgVote`.\\n```go\\ntype MsgVote struct {\\nProposalID int64\\nVoter      sdk.Address\\nOption     Option\\n}\\ntype MsgVoteWeighted struct {\\nProposalID int64\\nVoter      sdk.Address\\nOptions    []WeightedVoteOption\\n}\\n```\\nThe `ValidateBasic` of a `MsgVoteWeighted` struct would require that\\n1. The sum of all the Rates is equal to 1.0\\n2. No Option is repeated\\nThe governance tally function will iterate over all the options in a vote and add to the tally the result of the voter's voting power * the rate for that option.\\n```go\\ntally() {\\nresults := map[types.VoteOption]sdk.Dec\\nfor _, vote := range votes {\\nfor i, weightedOption := range vote.Options {\\nresults[weightedOption.Option] += getVotingPower(vote.voter) * weightedOption.Weight\\n}\\n}\\n}\\n```\\nThe CLI command for creating a multi-option vote would be as such:\\n```shell\\nsimd tx gov vote 1 \"yes=0.6,no=0.3,abstain=0.05,no_with_veto=0.05\" --from mykey\\n```\\nTo create a single-option vote a user can do either\\n```shell\\nsimd tx gov vote 1 \"yes=1\" --from mykey\\n```\\nor\\n```shell\\nsimd tx gov vote 1 yes --from mykey\\n```\\nto maintain backwards compatibility.\\n","tokens":168,"id":843,"Predictions":"Decision Makers should consider the following: nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"crispy-dragon\/OPM-1-Decision-Tracking.md","Context":"## Context\\nA microservices architecture is complex and we'll need to make many decisions.\\nWe'll need a way to keep track of the important decision we make, so that we can revisit and re-evalute them in the future.\\nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.\\n","Decision":"We've decided to use [Michael Nygard's lightweight architectural decision record (LADR)](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) format.\\nLADR is text based and is lightweight enough to meet our needs.\\nWe'll keep each LADR record in its own text file and manage the files like code.\\nWe also considered the following alternative solutions:\\n* Project management tooling (not selected, because we didn't want to install tools)\\n* Informal or \"word of mouth\" record keeping (not reliable)\\n","tokens":75,"id":845,"Predictions":"We'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.nWe'd prefer to use a lightweight"}
{"File Name":"crispy-dragon\/ADR-1-Serverless-Framework.md","Context":"## Context\\nThe resources, database, storage and API endpoints needs to be managed as a unit.\\nWhen build our application with the Serverless framework we get CloudFormation templates that manages our infrastructure in AWS.\\n","Decision":"To help package the individual AWS resource into a serverless application we will use [Serverless](https:\/\/www.serverless.com\/) framework.\\nWe also considered the following alternative solutions:\\n* Manual creation of resource in AWS throurgh the AWS Console.\\n* AWS Serverless Application Model (AWS SAM)\\n","tokens":43,"id":846,"Predictions":"Decision (A) The resource, database, storage and API endpoints needs to be managed as a unit.nWhen build our application with the Serverless framework we get CloudFormation templates that manages our infrastructure in AWS.n Decision (B) The resource, database, storage and API endpoints needs to be managed as a unit.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"Predictions":"We need a frontend that can be used by modern devices.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"sw360portal\/adr-001.md","Context":"### Context\\nComponents and Projects pages display entire contents of the database in one table. Reading data, generating HTML\\npage, and rendering the table (which is paginated by datatables on the client) all take a long time with thousands of\\nentities.\\nDatatables supports server-side processing for displaying and paginating data. For that, the server must be able to\\ntell datatables how many rows are in the table in total and to load a page of data starting from given index.\\nHowever, CouchDB cookbook strongly discourages loading of data starting from some index because of performance\\nconcerns. Instead, loading data starting from a specific key should be used. This is incompatible with what datatables\\nrequires and also makes going to previous pages highly complicated.\\nTo support sorting of the table by multiple columns would require creating a CouchDB view per column.\\nIn addition, projects are filtered by visibility in backend after loading from CouchDB. This filtering cannot be\\nimplemented in CouchDB. This complicates the matters even further with regards to pagination of projects table in\\nbackend.\\n### Decision\\nWe will not use datatables' server-side processing as it's not worth the effort.\\nWe will load only some number of latest components by default and let the user increase that number up to all\\navailable components. We will make this choice sticky between sessions.\\nWe will not change the projects table for now as the users have the option of loading only the projects from their\\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\\n### Status\\nAccepted\\n### Consequences\\nUsers will not see some components that are already created in the system and may try to create the \"missing\"\\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\\nthe total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n","Decision":"We will not use datatables' server-side processing as it's not worth the effort.\\nWe will load only some number of latest components by default and let the user increase that number up to all\\navailable components. We will make this choice sticky between sessions.\\nWe will not change the projects table for now as the users have the option of loading only the projects from their\\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\\n### Status\\nAccepted\\n### Consequences\\nUsers will not see some components that are already created in the system and may try to create the \"missing\"\\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\\nthe total number of components in the system.\\nLoading time of unfiltered projects table will still be slow with thousands of projects.\\n","tokens":444,"id":848,"Predictions":"We will not use datatables' server-side processing as it's not worth the effort. We will load only some number of latest components by default and let the user increase that number up to allnavailable components. We will make this choice sticky between sessions. We will not change the projects table for now as the users have the option of loading only the projects from theirngroup and are not disturbed much by the performance of the page when all projects are displayed.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents. Quick Filter will not help them find the component as it works on client-side only.nTo really make sure that a component is not in the system, users will have to use Advanced Search.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents. We will make this choice sticky between sessions.n### DecisionnWe will not use datatables' server-side processing as it's not worth the effort.nWe will load only some number of latest components by default and let the user increase that number up to allnavailable components. We will make this choice sticky between sessions.nWe will not change the projects table for now as the users have the option of loading only the projects from theirngroup and are not disturbed much by the performance of the page when all projects are displayed.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents. Quick Filter will not help them find the component as it works on client-side only.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents. We will make this choice sticky between sessions.n### DecisionnWe will not change the projects table for now as the users have the option of loading only the projects from theirngroup and are not disturbed much by the performance of the page when all projects are displayed.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents. We will make this choice sticky between sessions.n### DecisionnWe will not change the projects table for now as the users have the option of loading only the projects from theirngroup and are not disturbed much by the performance of the page when all projects are displayed.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and may try to create the \"missing\"ncomponents.n### StatusnAcceptedn### ConsequencesnUsers will not see some components that are already created in the system and"}
{"File Name":"SAP-library\/cx-server-in-container.md","Context":"## Context\\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\\nRecently, we added a \"companion\" Docker image which is used by `cx-server` to run scripts.\\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\\n","Decision":"We move the bash script inside the `s4sdk\/cxserver-companion` Docker image.\\nThe old `cx-server` script just delegates the command to the script inside the companion container.\\nA new `cx-server.bat` script is added, doing the same for Windows.\\nWe don't use PowerShell to increase compatibility with Windows.\\n","tokens":175,"id":849,"Predictions":"Decision (A). The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nD. The Cx Server should be a virtual machine.nA. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.nC. The Cx Server should be a virtual machine.nB. The Cx Server should be a virtual machine.n"}
{"File Name":"SAP-library\/automatic-release.md","Context":"## Context\\nWe provide a CD Pipeline for SAP Cloud Platform applications, which adheres to the \"build once\" principle.\\nStill, the pipeline does each build step, like building the artifact, running tests or static code checks in separate stages.\\nWe use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\nFor this document, the term \"release\" (noun) refers to a uniquely identifiable version of software.\\nThis includes the source code version (commit or tag) from which the artifacts are built, and the build artifacts themselves.\\nThe verb \"to release\" refers to the process of creating a new release.\\nPart of this process is to determine the version number of the release candidate.\\nThe release candidate becomes a release, when its build pipeline succeeded, and the build artifact is deployed to the Cloud Platform and the artifact repository.\\nWith Maven, this is usually facilitated with the [Maven Release Plugin](http:\/\/maven.apache.org\/maven-release\/maven-release-plugin\/).\\nUsing this plugin does not satisfy our requirements as described below.\\nThe pipeline automatically uploads build artifacts to an artifact repository and deploys the app to the Cloud Platform.\\nArtifact uploads and deployments happen only for commits on the so co called \"productive branch\" (`master` by default).\\nMaven's versioning schema appends `SNAPSHOT` suffix to versions which are not released.\\nA version like `1.0.2-SNAPSHOT` does not say from which commit this was built.\\nArtifact repositories might delete `SNAPSHOT` versions after some time, because those are not releases.\\n","Decision":"We implement an automated versioning schema, in which each commit to the productive branch is equivalent to a new release.\\nThis feature is enabled by default, but can be disabled.\\nThe version number shall contain a human readable _build_ time stamp (ISO 8601, without colons for file-name compatibility on Windows, always `UTC`) and the git commit id of the most recent commit to `master`, for example `2.7.3-2018-03-02T114757UTC_ff46bb0f00a663018f3efea697b2fb5e86fe6d41`.\\nAn auto-created release does not imply creating a tag in the repository.\\nCreating tags may be done manually to mark noteworthy versions by the developer.\\n### Reasoning\\n* Each commit on `master` is a new release: We assume the work happens in feature branches, which are merged once they implement a feature and meet the team's definition of done.\\nMerging to `master` is implicitly approval for release.\\n* Feature can be disabled: You might still have builds which don't follow this release approach.\\nFor those, it must be possible to disable automatic versioning.\\n* _Build_ instead of _commit_ time stamp: This implies that multiple builds of the same commit have a different version number.\\nThis avoids conflicts, when uploading a second build of a commit to a artifact repository.\\n* Always ISO 8601 date-time format: Can be sorted in lexical order which results in a chronological list.\\n* Always `UTC`: Most simple solution, avoids daylight saving time issues and is unambiguous for teams working distributed in multiple time zones.\\n* Don't create git tags: The version number contains the commit id, which is sufficient to check out this particular version.\\nIf we created tags automatically for each version, tags would be cluttered very quickly.\\nTags still can be used to mark a version on purpose, with semantic versioning if desired.\\n","tokens":336,"id":850,"Predictions":"Decision (A). The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nC. The build pipeline should be used to build the app.nD. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nC. The build pipeline should be used to build the app.nD. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nC. The build pipeline should be used to build the app.nD. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nC. The build pipeline should be used to build the app.nD. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nC. The build pipeline should be used to build the app.nD. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nC. The build pipeline should be used to build the app.nD. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nC. The build pipeline should be used to build the app.nD. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nB. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app.nA. The build pipeline should be used to build the app."}
{"File Name":"banner-server\/001-test-names.md","Context":"## Context\\nWhen deciding on a naming scheme for unit test methods, the following criteria were most important to us:\\n* **Readability** - The test method names should read like English sentences, with clear word boundaries\\n* **Flexibility** - We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n","Decision":"We use `snake_case` for method names in unit tests. We adapt the coding style settings to ignore the deviation from our usual `camelCase` convention.\\nIf it makes sense, we use sentences containing the words `given`, `when` and `then`. To give each section a clear boundary, when we use `given` or `when`, then we also use `then`.\\nGood:\\ntest_given_first_time_visitor_then_return_main_banner\\nBad:\\ntest_given_first_time_visitor_return_main_banner\\nWe keep in mind that the sentences always refer to the system-under-test (SUT) and don't unnecessarily repeat its class name.\\n","tokens":77,"id":852,"Predictions":"Decision based on the following criteria: *Readability *Flexibility *Flexibility == a naming scheme for unit test methods.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"infra\/0007-service-dns-patterns.md","Context":"## Context\\nOur current dns naming follows a couple of very similar patterns. Sometimes using 'gcp', 'frankfurt', 'oregon-b' as ways to separate different environments.  We should have one pattern and stick to it the best we can.\\nThings the pattern needs to solve for:\\n* Should be 'the same' for all of the meao services (for example: nucleus\/bedrock\/snippets).\\n* should allow for multiple 'environments' of a service to be deployed in the same region ('prod'|'stg'|'dev')\\n* should allow for multiple regions\/deployments of the same service + environment ('or', 'fr', 'ia')\\n* should also have a good 'user facing' pattern, that is not the same as the above pattern. (www.mozilla.org -> 'bedrock' 'prod' 'or' && 'bedrock' 'prod' 'fr' with some mechanism for choosing between the two deployments.)\\n","Decision":"For the backend deployments follow this pattern: 'service'.'environment'.'region'.'domain'. An incomplete list of each of examples of values for those variables:\\n| Service  |\\n|----------|\\n| bedrock  |\\n| nucleus  |\\n| snippets |\\n| prom     |\\n| Environments |\\n|--------------|\\n| dev          |\\n| stg          |\\n| prod         |\\n| demo1        |\\n| Region | Description           |\\n|--------|-----------------------|\\n| or     | oregon eks cluster    |\\n| fr     | frankfurt eks cluster |\\n| ia     | iowa gcp cluster      |\\n| Domain     |\\n|------------|\\n| moz.works  |\\n| mozmar.org |\\n| ramzom.org |\\nThis leads to a few examples:\\n| Examples                 |\\n|--------------------------|\\n| bedrock.dev.or.moz.works |\\n| prom.prod.fr.mozmar.org  |\\n| nucleus.stg.ia.moz.works |\\nNote that these are for 'internal' use primarily.  The user facing domains will stay as they are.  A few examples, nucleus.mozilla.org (prod) and nucleus.allizom.org (stg), www.mozilla.org (bedrock prod) www.allizom.org (bedrock stg).  The connection between the new dns entries and the user facing will stay the same. (If we're using a r53 traffic policy now, we will continue to after this change, if we're just using cname\/alias records we will again after this change,etc, including cloudflare vs cloudfront etc.)\\n","tokens":205,"id":853,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"infra\/0005-vpc-and-network-design.md","Context":"## Context\\nChanging networking can be hard.  It usually requires a full redeploy of all services and all infrastructure to make it 'real'.  Our current network has overlaps, which makes it more difficult to setup vpns, usually requring syncing of live IP addresses to their desired state.\\n","Decision":"Netops will reserve a \/16 block of ips for mozmeao. For each VPC make a block of \/20 ipv4 addresses.  Where VPC maps to a region within a cloud provider we use.  Divide that into \/24 subnets, where we'll have just one subnet per AZ.\\nOur \/16 is - 10.154.0.0\/16\\nFor example, in oregon, the network would look like:\\n| Label           | CIDR          | Range Start  | Range End      | Description                     |\\n|-----------------|---------------|--------------|----------------|---------------------------------|\\n| Oregon VPC      | 10.154.0.0\/20 | 10.154.0.1   | 10.154.15.254  | A large block for the whole VPC |\\n| Oregon Subnet A | 10.154.0.0\/24 | 10.154.0.1   | 10.154.0.254   | Subnet for oregon-a az          |\\n| Oregon Subnet B | 10.154.1.0\/24 | 10.154.1.1   | 10.154.1.254   | Subnet for oregon-b az          |\\n| Oregon Subnet C | 10.154.2.0\/24 | 10.154.2.1   | 10.154.3.254   | Subnet for oregon-c az          |\\nand Frankfurt would be:\\n| Label              | CIDR           | Range Start   | Range End       | Description                        |\\n|--------------------|----------------|---------------|-----------------|------------------------------------|\\n| Frankfurt VPC      | 10.154.16.0\/20 | 10.154.31.1   | 10.154.255.254  | A large block for the whole VPC    |\\n| Frankfurt Subnet A | 10.154.16.0\/24 | 10.154.16.1   | 10.154.16.254   | Subnet for frankfurt-a az          |\\n| Frankfurt Subnet B | 10.154.17.0\/24 | 10.154.17.1   | 10.154.17.254   | Subnet for frankfurt-b az          |\\n| Frankfurt Subnet C | 10.154.18.0\/24 | 10.154.18.1   | 10.154.18.254   | Subnet for frankfurt-c az          |\\nThe next few vpc blocks would be 10.154.32.0\/20, 19.154.48.0\/20, 19.154.128.0\/20\\nIn oregon we could continue with 10.154.3 and 10.154.4 until 15 for the subnets.  Essentially the same for frankfurt 10.154.19, 10.154.20.\\n","tokens":60,"id":854,"Predictions":"Decision (A) To switch to a new network. (B) To switch to a new network. (C) To switch to a new network. (D) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (B) To switch to a new network. (C) To switch to a new network. (D) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (B) To switch to a new network. (C) To switch to a new network. (D) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (B) To switch to a new network. (C) To switch to a new network. (D) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (A) To switch to a new network. (B) To switch to a new network. (C) To switch to a new network. (D) To switch to a new network. (A) To switch to a new network."}
{"File Name":"infra\/0003-use-voyager-for-service-ingress.md","Context":"## Context\\nWe need to have load balancers between our CDNs and our services, in order to deal with k8s node failures (and for handing traffic, and as a place to log stuff, and as a way to block bad actors if needed).  We'd prefer to only pay AWS\/GCP for a single load balancer thing, while still having the ability to generate unique dns addresses for each of our services.  They must be able to host certs correctly.\\n","Decision":"We're currently managing our ELBs in aws with some out of band terraform that connects the nodes of each k8s cluster to a load balancer. Since this is out of band, upgrading clusters or services implies doing a bunch of k8s stuff, and then also running terraform.  We could possibly simplify the whole experience by moving the full definition of the load balancer, and dns, and certs to objects inside k8s.  Voyager + External Ingress seem like the most common way to do this.  Deploying one 'ingress' object per group of services you want to have behind an ALB, and listing all the DNS to point at those services solves the problem outlined above.\\nThe primary advantage of doing this work, is that it allows dynamic things (deployments created in response to events, such as pull requests and ephemeral 'demo' branches) to be created simply by writing the yaml and deploying it to kubernetes.\\n","tokens":98,"id":855,"Predictions":"Decision (A) The CDNs should be able to host certificates correctly. (B) The CDNs should be able to host certificates correctly. (C) The CDNs should be able to host certificates correctly. (D) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (B) The CDNs should be able to host certificates correctly. (C) The CDNs should be able to host certificates correctly. (D) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (B) The CDNs should be able to host certificates correctly. (C) The CDNs should be able to host certificates correctly. (D) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (A) The CDNs should be able to host certificates correctly. (B) The CDNs should be able to host certificates correctly. (C) The CDNs should be able to host certificates correctly."}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"Predictions":"We need to invest in kubernetes to avoid drift.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"infra\/0004-prometheus-and-alertmanager-monitoring-and-alerting.md","Context":"## Context\\nWe want to have a flexible, easy to run, but not too expensive alerting and monitoring solution.  Since we are primarily kubernetes based, something built around that seems ideal.  We are in a little bit of a rush given that our current monitoring solution has yearly renewals, and that renewal would be coming up at the end of the month.\\nPossible options include, influx cloud (v1, v2), influx we host (rejected mostly because v2 is not yet ready for primetime, and team has no influx familiarity).  New Relic, DataDog, HoneyComb are all great products, but are a bit expensive for us (saas prices for our data load seem to be too much).  Tools like Nagios are hard to run in the cloud because they assume mostly assume 100% network reliability.  Given all that, and our team's familiarity with prometheus, it seems like the best choice.\\n","Decision":"Run prometheus and alertmanager, on our clusters.  Run one prom per k8s cluster to collect metrics. Run one prom\/grafana\/alertmanager deployment to collect and display the information all in one place.  Send alerts to slack and\/or pagerduty depending on severity.  Monitor this stack with dead man's snitch or other negative alerting services.\\n","tokens":194,"id":857,"Predictions":"Decision based on the information in this passage.nAnswer:A.nAnswer:A.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:A.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:A.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:A.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:B.nAnswer:C.nAnswer:D.nAnswer:B.nAnswer:C.nAnswer:D."}
{"File Name":"infra\/0006-aws-vpc-public-and-private-subnets.md","Context":"## Context\\nMozilla IT SREs have built a VPC module at https:\/\/github.com\/mozilla-it\/itsre-deploy\/tree\/master\/modules\/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public\/private subnets. That difference will be described in this document.\\n","Decision":"We will create two subnets per az. One 'public' and one 'private'. In general this is mostly by convention, rather than something that is strictly enforced.  We may later validate instances in private are not accessible to the general public with automated auditing solution. But, for now, we'll just do our best to only assign public things to the public subnet, and everything else can go in private.\\nThe one functional item for the subnets, 'public ips' will be assigned by default in the public subnets, but not the private subnets.\\n","tokens":66,"id":858,"Predictions":"Decision (A) The VPC module is a good choice for the public\/private subnets.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"infra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":859,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"odh-manifests\/0000-odh-release-policy.md","Context":"## Context and Problem Statement\\nThe Open Data Hub itself consists of many components all having their own release cycles. ODH users might decide to\\nupdate individual components such as container images used by JupyterHub. Nevertheless it is required to create\\nreleases of ODH as a whole.\\n","Decision":"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\\nproject\/product level while maintianing freedom of others to update.\\n### Positive Consequences <!-- optional -->\\n* Operators of ODH have a clear base line of versions, these versions have been tested with each other and have\\nundergone ODH integration testing.\\n* Operators of ODH can update individual components, they could maintain a repository analog to odh-manifests declaring\\nthe exact versions (container image tags, git repository tags) of components they want to deploy.\\n* Operators can mix in their builds of container images following the method mentioned above.\\n### Negative Consequences <!-- optional -->\\n* An ODH release (a tag in the odh-manifests repository) might not contain the latest versions of components, for example\\nsecurity updates might have forced the build of a S2I image used with JupyterHub component of ODH.\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":59,"id":860,"Predictions":"Decision (A) To update the components of the Open Data Hub. (B) To update the components of the Open Data Hub. (C) To update the components of the Open Data Hub. (D) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (B) To update the components of the Open Data Hub. (C) To update the components of the Open Data Hub. (D) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (B) To update the components of the Open Data Hub. (C) To update the components of the Open Data Hub. (D) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (A) To update the components of the Open Data Hub. (B) To update the components of the Open Data Hub. (C) To update the components of the Open Data Hub. (D) To update the components of the Open Data Hub."}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"Predictions":"Decision (A) The k8s operator should use Golang. (B) The k8s operator should use Rust. (C) The k8s operator should use Python. (D) The k8s operator should use Python. (A) The k8s operator should use Python. (A) The k8s operator should use Python. (B) The k8s operator should use Python. (C) The k8s operator should use Python. (D) The k8s operator should use Python. (A) The k8s operator should use Python. (A) The k8s operator should use Python. (B) The k8s operator should use Python. (C) The k8s operator should use Python. (D) The k8s operator should use Python. (A) The k8s operator should use Python. (A) The k8s operator should use Python. (B) The k8s operator should use Python. (C) The k8s operator should use Python. (D) The k8s operator should use Python. (A) The k8s operator should use Python. (A) The k8s operator should use Python. (B) The k8s operator should use Python. (C) The k8s operator should use Python. (D) The k8s operator should use Python."}
{"File Name":"nada-kafkarator\/0005-use-the-fastapi-web-framework.md","Context":"## Context\\nOur application needs to supply some web endpoints for healthchecks, so we need to select a web framework.\\nThere are a multitude to choose from, but we have some preferences:\\n- We want something that is well maintained and has a community\\n- We want something that has decent performance\\n- As a learning experience, we want to use one of the many async frameworks\\nThe front contenders seems to be Sanic, FastAPI, Tornado, Vibora and Quart.\\nTornado seems to be somewhat dated, as it was created before Python had async built in.\\nVibora is a relatively new project, with uncertain community adoption.\\nQuart is API compatible with Flask, which is the most common non-async web framework.\\nSanic seems to be the most popular of the async frameworks.\\nFastAPI is by far the most performant of them all.\\n","Decision":"We will use FastAPI, because of the performance, and some of the interesting features.\\n","tokens":183,"id":862,"Predictions":"We want to choose a web framework.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance.nWe want to choose a web framework that has decent performance."}
{"File Name":"nada-kafkarator\/0007-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":72,"id":863,"Predictions":"Decision (A) The Kafka cluster should be hosted by Aiven.nB) The Kafka cluster should be hosted by Kafkarator.nC) The Kafka cluster should be hosted by Kafkarator.nD) The Kafka cluster should be hosted by Aiven.nAnswer (A) None of the above choices.nB) The Kafka cluster should be hosted by Aiven.nC) The Kafka cluster should be hosted by Kafkarator.nD) The Kafka cluster should be hosted by Aiven.nAnswer (A) None of the above choices.nB) The Kafka cluster should be hosted by Aiven.nC) The Kafka cluster should be hosted by Aiven.nD) The Kafka cluster should be hosted by Aiven.nAnswer (A) None of the above choices.nB) The Kafka cluster should be hosted by Aiven.nAnswer (A) None of the above choices.nB) The Kafka cluster should be hosted by Aiven.nAnswer (A) None of the above choices.nB) The Kafka cluster should be hosted by Aiven.nAnswer (A) None of the above choices."}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"Predictions":"Decision (A) To isolate Python projects from the system Python. (B) To use a tool to manage dependencies installed in that virtualenv. (C) To isolate Python projects from the system Python. (D) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (B) To isolate Python projects from the system Python. (C) To isolate Python projects from the system Python. (D) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (B) To isolate Python projects from the system Python. (C) To isolate Python projects from the system Python. (D) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (B) To isolate Python projects from the system Python. (C) To isolate Python projects from the system Python. (D) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (A) To isolate Python projects from the system Python. (B) To isolate Python projects from the system Python. (C) To isolate Python projects from the system Python."}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"Predictions":"Decision (A).n1n1n2n3n4n5n6n7n8n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9n9nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nada-kafkarator\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":867,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nada-kafkarator\/0006-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decicion, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- Piggybacking on the existing NAIS CRDs are not encouraged and should be avoided\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"- We will define two new CRD objects (see github issues #3 and #12)\\n- App developers will create these in the cluster when deploying their application\\n- Kafkarator will watch these two CRDs and take needed actions\\n","tokens":172,"id":868,"Predictions":"We need to make a decicion about how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.nWe have a few findings that guide our decision:n- Piggybacking on the existing NAIS CRDs are not encouraged and should be avoidedn- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the clustern- We have two separate concerns that needs two separate configurationsn ## Decision 0 of 4. nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe decision is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the following:nThe final outcome is based on the"}
{"File Name":"profiles-etl-combiner\/0002-update-running-database-from-etl-output.md","Context":"## Context\\nGP Data changes on a daily basis.\\nNightly ETLs are run to obtain Syndication and POMI data.\\nCurrently [profiles-db](https:\/\/github.com\/nhsuk\/profiles-db) requires manually updating from the ETL outputs, and re-deploying to refresh the data.\\nNo automated validation of data is performed during the update.\\n","Decision":"To avoid a new release of [profiles-db](https:\/\/github.com\/nhsuk\/profiles-db) each time data changes, data will be inserted into a running mongodb instance on a daily schedule.\\nThe output from the nightly [gp-data-etl](https:\/\/github.com\/nhsuk\/gp-data-etl) and [pomi-data-etl](https:\/\/github.com\/nhsuk\/pomi-data-etl) will be validated and combined.\\nThe contents of the existing database will only be replaced if the new data is of a comparable size.\\n","tokens":78,"id":869,"Predictions":"Decision (A) The data is not valid.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"profiles-etl-combiner\/0003-remove-mongodb-update-and-write-to-azure.md","Context":"## Context\\nThe merge of the data sources is a common operation from which the generated asset can be used by several different processes.\\nCurrently the generated asset is only available for the immediately following process within this application i.e. updating MongoDB.\\nThere is an immediate need to use the same generated asset for updating Elasticsearch.\\nThe [mongodb-updater](https:\/\/github.com\/nhsuk\/mongodb-updater) service is able to update a MongoDB database from a JSON file available at a URL.\\n","Decision":"The `gp-data-merged.json` file will be written to the team's preferred cloud hosting platform, enabling the merged data to be used as a\\nsource for both the `mongodb-updater` and the forthcoming `elasticsearch-updater`.\\n","tokens":100,"id":870,"Predictions":"Decision (A) The generated asset can be used by several different processes.nThere is an immediate need to use the same generated asset for updating Elasticsearch.nThe [mongodb-updater](https:\/\/github.com\/nhsuk\/mongodb-updater) service is able to update a MongoDB database from a JSON file available at a URL.nAnswer (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"Predictions":"Decision (A).nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A.nAnswer:A."}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"Predictions":"Decision (A) The pointer events on Safari are unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n- The pointer events on desktop Safari are also unsupported.n"}
{"File Name":"python-library-project-generator\/0002-use-gnu-make-to-centralise-all-tasks.md","Context":"## Context and Problem Statement\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\n* In the CI\/CD pipelines.\\n## Decision Drivers\\n* Must have a significant user base and community.\\n* Must not require significant installation.\\n* Must be sufficiently simple to learn and use.\\n","Decision":"* Must have a significant user base and community.\\n* Must not require significant installation.\\n* Must be sufficiently simple to learn and use.\\nChosen option: `GNU Make`, because compared to the other evaluated tools (see\\n[Pros and Cons](#pros-and-cons-of-the-options)), it fits the bill where as:\\n* `Invoke` is not well maintained, nor well documented, nor a de facto standard, nor has\\na sufficient community.\\n* `Rake` required to install and learn Ruby.\\n* `SCons` is more a build tool and seems difficult to apprehend\/get to grips with.\\n","tokens":82,"id":874,"Predictions":"Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* In the CI\/CD pipelines.n## Decision Driversn* Must have a significant user base and community.n* Must not require significant installation.n* Decision Makersn* Must be sufficiently simple to learn and use.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* In the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* In the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* In the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* In the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* In the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n* During the build cycle.n* During the CI\/CD pipelines.n## Decision Makersn* Must be able to centralise in a single tool all tasks to be called:n* During the development cycle.n"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"archivematica-architectural-decisions\/0003-black-code-formatter.md","Context":"## Context and problem statement\\nArchivematica code style is inconsistent and controversial. Formatting is a\\ndistraction from content and a constant dilemma in our code review process.\\nThe style guide described by [PEP 8][0] is not specific enough and Python is too\\nflexible. Every project ends up with its own style and memorizing the rules is\\nhard.\\nWith the recent rise of auto formatters such *gofmt* or *Prettier*, we see the\\nvalue of ceding control to an opinionated tool.\\n","Decision":"Chosen option: \"black\", because:\\n* It prioritizes consistency over style.\\n* It can check that the reformatted code still produces a valid AST that is\\nequivalent to the original.\\n* Zero configuration, sane defaults.\\n* Adopted by popular projects such [responder][5] or [pytest][6].\\n","tokens":115,"id":877,"Predictions":"Decision (A). Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nC. Formatting should be a constant dilemma in our code review process.nD. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nC. Formatting should be a constant dilemma in our code review process.nD. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nC. Formatting should be a constant dilemma in our code review process.nD. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nC. Formatting should be a constant dilemma in our code review process.nD. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nA. Formatting should be a constant dilemma in our code review process.nB. Formatting should be a constant dilemma in our code review process.nA."}
{"File Name":"archivematica-architectural-decisions\/0006-use-bagit-to-package-transfers.md","Context":"## Context and problem statement\\nThe Simon Fraser University Archives is a long-time Archivematica user. They\\nrecognized that there is often a long-period between receiving digital materials\\nfrom donors and an archivist creating a Submission Information Package (SIP) for\\nlong-term archival storage. SFU Archives wanted the ability to perform minimal\\ndigital preservation tasks, such as those provided by Archivematica's Transfer\\nfunctionality, and then return to the creation of SIPs from backlog at an\\nundetermined future time, perhaps several years in the future.\\nThis meant that transfer backlogs are used for long-term storage of content and\\nthat users should expect it to be as durable as Archival Information Package (AIP)\\nstorage for maintaining accurate metadata over pipeline upgrades, migrations or\\nre-indexes. To enable this, the decision was made to package backlog transfers as\\nBagIt packages, supported by a verbose METS file, as is already done for AIPs.\\n","Decision":"* Preservation of original content and metadata over long-term gaps in\\nprocessing\\n* Re-use of existing standards and protocols\\nChosen option: Option 2 was chosen because SFU Archives wanted the ability to\\ncreate multiple SIPs from multiple transfers. In their use cases, one transfer\\ndoes not automatically equal one SIP and one AIP. By sending transfers to\\nbacklog they are able to use the Appraisal tab functionality where users can\\ncreate SIPs by combining files from different transfers in the Archivematica\\nbacklog.\\nHowever, this creates a new expectation, namely, that transfer\\nbacklogs can be used for long-term storage of content and that users should\\nexpect it to be as durable as AIP storage for maintaining accurate metadata\\nover pipeline upgrades, migrations or re-indexes. However, prior to release\\n1.10, transfers stored using Archivematica's backlog functionality could only be\\naccessed by using the same pipeline that the transfer was created in. If the\\npipeline was accidently lost or decomissioned, you would be unable to reindex\\nand repopulate the backlogged material in a new Archivematica pipeline. The\\nreliance on maintaining this backlog information in the pipeline's transactional\\ndatabase (MCPServer) and search index (Elasticsearch) is a significant\\npreservation risk if the files are kept in backlog for an indefinitely long\\nperiod of time, as is the case for SFU Archives and an increasing number of\\nArchivematica users.\\n![before_archivematica_backlog](\/assets\/before_archivematica_backlog.png)\\nThe decision was made to convert the materials in transfer backlogs to mirror\\nArchivematica's AIPs, namely to use the Bagit and METS standards to\\npackage and describe the transfer materials. As materials go through transfer\\nprocessing, event and object information is written to the transfer METS file\\nwherever feasible. Other relevant information is captured in a logs subdirectory\\nin an IETF-compliant Bagit package. Functionality was then added in Archivematica\\nrelease 1.10 to rebuild backlogs from these self-describing packages in the event\\nof a lost or destroyed pipeline.\\n![after_archivematica_backlog](\/assets\/after_archivematica_backlog.png)\\n### Positive consequences\\nArchivematica users that rely on transfer backlog for long-term storage of\\ndigital materials can now be assured that they can rebuild their backlogs from\\nthe transfer Bagit packages that Archivematica creates as of release 1.10. This\\nsignificantly reduces the risk of loss of materials during the transfer backlog\\nphase.\\n### Negative consequences\\nSome major changes were introduced to the Archivematica codebase to enable it\\nto restore from backlog using transfer Bagit packages. Previously, the\\ninformation used to accomplish this was maintained primarily in the MCPServer\\ndatabase and ElasticSearch index. There is a risk that not all previous\\nfunctionality to restore from backlog is fully enabled and that some\\nregressions may have been introduced as a result of this architectural change.\\n","tokens":202,"id":878,"Predictions":"Decision (A). The Simon Fraser University Archives should package backlog transfers as BagIt packages, supported by a verbose METS file, as is already done for AIPs.nBagIt packages are more durable than AIPs.nBagIt packages are more expensive.nBagIt packages are more secure.nBagIt packages are more reliable.nBagIt packages are more expensive.nBagIt packages are more durable.nBagIt packages are more expensive.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.nBagIt packages are more durable.n"}
{"File Name":"archivematica-architectural-decisions\/0009-unification-of-api-handling-in-am.md","Context":"## Context and problem statement\\nArtefactual have been working on a client application to provide easier access\\nto the functions exposed by the Archivematica Application Programming Interface\\n(API). This application is called [AMClient][amclient-1]. The application also\\nacts as a Python library which makes it easier to make Archivematica specific\\nAPI calls in code. The library was released as a [Python package][amclient-2]\\nearly in 2019.\\n","Decision":"* Desire to simplify development processes in Archivematica and the Storage\\nService.\\n* Desire to make it easier to maintain components of Archivematica downstream\\nand push the effects upstream (modularity).\\nOption 2. The AMClient library helps us to create consistency across the\\nArchivematica code-base. It is already widely used in the default Automation\\nTools, and Archivematica Automated Acceptance Tests (AMAUAT).\\n### Positive consequences\\n* As the AMClient library is adapted as a Python package its use on the command\\nline is extended and so it becomes more useful to those embedding it outside\\nof Archivematica.\\n* As AMClient is extended, tests are added to the package itself, providing a\\ngreater level of lower-level testing, thusly the Archivematica API. The focus\\nof testing in Archivematica can be on manipulating well understood AMClient\\nresponses where this is appropriate for the unit being tested.\\n* The number of lines of code required to talk to the AMClient package versus\\nthat of creating lower-level API calls via a HTTP requests library should be\\nfewer.\\n* Missing functionality, e.g. as described by issue [#905][amclient-3] can be\\nadded as it is required by Archivematica and the Storage Service. As an\\nexternal package, and something used inside Archivematica, the two take on a\\nsymbiotic relationship.\\n### Negative consequences\\n* The release candidate for version 1.0.0 of AMClient does not yet adhere to\\nmore traditional Python best practices i.e. it might not be considered to be\\nentirely \"Pythonic\" in how it exposes errors to the calling code. This is\\ndescribed in issue [#488][amclient-4]. This effect of not being 'Pythonic'\\nmay travel upstream to Archivematica and is something for maintainers to be\\naware of. Work on an AMClient 2.0.0 should replace these patterns and then\\ncascading those changes down to Archivematica should still be easier than\\nworking with and replacing the calls that exist in Archivematica using a HTTP\\nrequests library.\\n","tokens":101,"id":879,"Predictions":"Decision (A) The client application should be a [AMClient][amclient-1] and the [AMClient-2] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]; and [AMClient][amclient-1] should be a [AMClient][amclient-2]"}
{"File Name":"archivematica-architectural-decisions\/0010-archivematica-original-design-principles.md","Context":"## Context and problem statement\\nThere is no open-source software system capable of implementing the functional\\nrequirements of the OAIS reference model (ISO 14721:2003). Digital preservation\\nspecialists must use multiple tools, which can be difficult to install and use,\\nto perform discrete preservation actions. Those tools produce metadata that do\\nnot conform to digital preservation and exchange standards and schemas, and do\\nnot provide a way to automatically generate standardized, system-independent,\\nself-documenting Archival Information Packages (AIPs) that package content and\\nPreservation Description Information (PDI) as described by OAIS. Repository\\napplications such as Fedora are capable of performing some but not all OAIS\\npreservation actions, and tend to be complex to develop and maintain, posing a\\nrisk to future retrieval and readability of the AIPs. In fact, any middleware\\nrepository or database software that is required to access and read AIPs is\\ninherently a risk to their long-term usability.\\n","Decision":"Artefactual designed an open-source, web-based archival description and access\\nsystem called ICA-AtoM (Access To Memory) that has a broad user base around the\\nworld. ICA-AtoM does not provide digital preservation functionality as described\\nby OAIS. It would benefit ICA-AtoM users to be able to integrate with a back-end\\nsystem designed to preserve digital objects that are linked to access copies in\\nICA-AtoM. The system should also be usable on its own or in conjunction with\\nother access tools.\\nDesign principles:\\n1. The application will perform a set of configurable preservation actions on\\ningested digital objects, using the file system as the focal point of\\npreservation action operations. Making the file system the focal point of\\nmicro-services operations is noteworthy as a long-term preservation strategy\\nbecause it provides users with the option of direct, unmediated access to\\narchival storage. This might be necessary one day because the various layers\\nand generations of digital preservation system components are just as\\nsusceptible to the risk of technology obsolescence and incompatibility as the\\ndigital objects they are attempting to preserve.\\n2. The information packages ingested by the application will be moved from one\\nmicro-service to the next using the Unix pipeline pattern.\\n3. Micro-service functionality will be provided by one or more of the\\nopen-source software utilities and applications bundled into the application.\\nWhere necessary, these will be supplemented by integration code written as\\nPython scripts.\\n4. The application will provide a graphical user interface so that the end user\\ncan determine and control the status of digital objects moving through the\\npipeline.\\n5. The application will generate AIPs that are system-independent,\\nself-documenting and self-contained. The AIPs will contain PDI that conforms\\nto recognized standards and schemas, such as PREMIS, METS and Dublin Core.\\n6. The file-based AIP is the canonical source of the preserved digital objects\\nand metadata. The preservation application may be supported by databases and\\nindexes that are derived and updated from this source, but these are not\\nessential to long-term preservation of the AIP.\\n7. The application will be designed to integrate with diverse external systems\\ncapable of serving as sources of digital objects to be ingested.\\n8. The application will be designed to integrate with diverse storage systems\\nand protocols for deposit and long-term preservation of AIPs.\\n9. The application will be designed to integrate with diverse external systems\\ncapable of providing search, browse and display capabilities for\\nDissemination Information Packages (DIPs) generated by the application.\\n","tokens":206,"id":880,"Predictions":"Decision (A). The OAIS reference model is not compatible with the OAIS reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nC. The OAIS reference model is compatible with the Fedora reference model.nD. The OAIS reference model is compatible with the Fedora reference model.nA. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nC. The OAIS reference model is compatible with the Fedora reference model.nD. The OAIS reference model is compatible with the Fedora reference model.nA. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nC. The OAIS reference model is compatible with the Fedora reference model.nD. The OAIS reference model is compatible with the Fedora reference model.nA. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nC. The OAIS reference model is compatible with the Fedora reference model.nD. The OAIS reference model is compatible with the Fedora reference model.nA. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nC. The OAIS reference model is compatible with the Fedora reference model.nD. The OAIS reference model is compatible with the Fedora reference model.nA. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nC. The OAIS reference model is compatible with the Fedora reference model.nD. The OAIS reference model is compatible with the Fedora reference model.nA. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.nB. The OAIS reference model is compatible with the Fedora reference model.n"}
{"File Name":"archivematica-architectural-decisions\/0005-new-api-endpoint-for-csv-validation.md","Context":"## Context and problem statement\\nArchivematica users rely on spreadsheets created in a specific way to perform\\ntasks within or after Archivematica. Documentations and examples can be found\\n[here](https:\/\/www.archivematica.org\/en\/docs\/archivematica-1.9\/user-manual\/transfer\/import-metadata\/)\\nbut there is no clear validation as can be performed by a machine. As one\\nexample, the metadata.csv and rights.csv files are \"special\" and are utilized by\\nArchivematica to add metadata or rights metadata into the AIP's METS file.\\nAnother example is the Avalon Media System having a specific Manifest.csv file\\nthat is used to recreate hierarchical information and additional metadata, which\\nis used after a DIP is created from a stored AIP. It would be beneficial if this\\nmanifest could be validated prior to going through the preservation process.\\nBoth of these examples would benefit from a validation service that a user (or\\nautomated system) could access prior to ingest into Archivematica.\\n","Decision":"* More closely integrate two open source projects: Archivematica and Avalon\\n* [Feature request](https:\/\/github.com\/archivematica\/Issues\/issues\/563) for\\nvalidation of Archivematica-specific CSVs within Archivematica\\n* Creation of something for a specific purpose that can later be extended to\\nsuit many use cases\\nChosen option: \"1. API endpoint for pre-ingest CSV validation\", because it is\\nflexible, it lays the groundwork for future work around CSV validation as a step\\nto be taken by Archivematica. It doesn't have the complications of the long-term\\nmaintenance and testing of a GUI component. It can be more easily automated. The\\nsolution allows for custom or institutionally-specific CSV to be used\/added.\\nTechnically, this would live in the Archivematica codebase and be a new endpoint\\nin the [Archivematica API](https:\/\/wiki.archivematica.org\/Archivematica_API)\\nProposed endpoint below:\\n```yaml\\nURL: \/api\/transfer\/validate_csv\\nVerb: POST\\nValidates local CSV with validator service Python script\\nParameters: CSV\\ninput: Path to the CSV\\nvalidator: Name of service CSV should be checked against, i.e. \"avalon\"\\nor \"rights\"\\nResponse: JSON\\nmessage: Approval or non-approval, depending on service output\\n```\\n### Positive consequences\\n* Fulfills requirement of Avalon\/Archivematica integration work.\\n* Relatively small feature with big potential benefits.\\n* Lays foundation for more development in the future.\\n### Negative consequences\\n* Code must be maintained and tested over time.\\n* User would need API expertise to benefit from this feature.\\n* Implementation -- timeouts on very big CSVs?\\n","tokens":221,"id":881,"Predictions":"It would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.nIt would be beneficial if this metadata could be validated prior to going through the preservation process.n"}
{"File Name":"archivematica-architectural-decisions\/0002-json-encoded-workflow.md","Context":"## Context and problem statement\\nThe core of Archivematica is a server application that coordinates work across\\nmultiple workers. The collection of related tasks (also known as *workflow\\ndata*) is stored in the application database using a relation model.\\nIntroducing changes to the workflow data is hard because the model is clumsy and\\nconvoluted. Changes to the data are delivered as data and schema migrations\\nwhich are difficult to implement and they add risk to the process of upgrading\\nthe software.\\nAdditionally, we want to add full internationalisation support in order to make\\nthe system more attractive to a wider range of users and increase adoption. We\\nneed a simpler mechanism to deliver translations and other workflow data changes\\nin general.\\n","Decision":"* Archivematica 1.9 will implement the proposed solution replacing the existing\\ndatabase tables.\\n* We do not currently intend to expose the ability to modify the workflow data\\nto Archivematica users - but this may change in the future as the solution\\nmatures and becomes interoperable.\\n* The visualization and manipulation of the workflow data will be facilitated by\\na new tool that we're calling amflow.\\n","tokens":150,"id":883,"Predictions":"We need to add full internationalisation support to the system.n1n2n3n4n5n6n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n7n8n8n7n8n8n7n8n8n7n8n8n7n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"archivematica-architectural-decisions\/0008-remove-quarantine.md","Context":"## Context and problem statement\\nQuarantine is an optional workflow step in Archivematica that places digital\\nobjects in a separate space for a specified period of time for virus definitions\\nto be updated. It adds complexity to the workflow and is arguably unnecessary\\nbecause ingested digital objects generally speaking pre-date the available virus\\ndefinitions, which are updated daily. This proposal is to remove quarantine\\nto simplify Archivematica's workflow and ease maintenance and testing.\\n","Decision":"* Desire to simplify Archivematica workflows\\n* Need to fix known bugs related to quarantine if it remains supported\\n* Little evidence of need for or use of quarantine in the user community\\nOption 1, because there is no evidence of use in the community to justify the\\ncontinued maintenance of the feature and the positive consequences of the\\nremoval of the code outweigh the benefits of the functionality. The virus\\nscanning micro-service remains unchanged by this decision.\\n### Positive consequences\\n* Simplified, easier to maintain workflow\\n* Removes two processing configurations that users are required to make but\\nwhich are rarely implemented.\\n* Resolution of the following issues:\\n* DSpace workflow issue: [#426][1]\\n* Virus scan not triggered after quarantine: [691][2]\\n[1]: https:\/\/github.com\/archivematica\/Issues\/issues\/426\\n[2]: https:\/\/github.com\/archivematica\/Issues\/issues\/691\\n","tokens":99,"id":884,"Predictions":"Decision (A).nQuarantine should be removed from Archivematica.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specified period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.nIt should be replaced with a separate space for a specific period of time for virus definitions.n"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"Predictions":"Decision (A) We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>We need to use asynchronous communication in the form of event-driven architecture.nbr>"}
{"File Name":"modular-monolith-with-ddd\/0003-use_dotnetcore_and_csharp.md","Context":"## Context\\nAs it is monolith, only one language (or platform) must be selected for implementation.\\n","Decision":"I decided to use:\\n- .NET Core platform - it is new generation multi-platform, fully supported by Microsoft and open-source community, optimized and designed to replace old .NET Framework\\n- C# language - most popuplar language in .NET ecosystem, I have 12 years commercial experience\\n- F# will not be used, I don't have commercial experience with it\\n","tokens":23,"id":886,"Predictions":"Decision Makers should select only one language for implementation.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0014-event-driven-communication-between-modules.md","Context":"## Context\\nEach module should be autonomous. However, communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\\n","Decision":"Solution number 2 - Event-driven (asynchronous)<\/br>\\nWe want to achieve the maximum level of autonomy and loose coupling between modules. Moreover, we don't want dependencies between modules. We allow direct calls in the future, but this should be an exception, not a rule.\\n","tokens":36,"id":887,"Predictions":"Decision Makers should consider the following options: a). The modules should be autonomous; b). The modules should be integrated; c). The modules should be autonomous; d). The modules should be integrated; e. The modules should be autonomous; f). The modules should be integrated; g). The modules should be integrated; h). The modules should be integrated; ii. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; j. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules should be integrated; k. The modules"}
{"File Name":"modular-monolith-with-ddd\/0007-use-cqrs-architectural-style.md","Context":"## Context\\nOur application should handle 2 types of requests - reading and writing. <\/br>\\nFor now, it looks like:<\/br>\\n- for reading, we need data model in relational form to return data in tabular\/flattened way (tables, lists, dictionaries).\\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\\n","Decision":"We applied the CQRS architectural style\/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\\n","tokens":87,"id":888,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0002-use_modular-monolith-system-architecture.md","Context":"## Context\\nAn advanced example of Modular Monolith architecture and tactical DDD implementation in .NET is missing on the internet.\\n","Decision":"I decided to create nontrivial application using Modular Monolith architecture and Domain-Driven Design tactical patterns.\\n","tokens":26,"id":889,"Predictions":"The DDD implementation in.NET is not a problem.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0008-allow-return-result-after-command-processing.md","Context":"## Context\\nThe theory of the CQRS and the CQS principle says that we should not return any information as the result of Command processing. The result should be always \"void\". However, sometimes we need to return some data immediately as part of the same request.\\n","Decision":"We decided to allow in some cases return results after command processing. Especially, when we create something and we need to return the ID of created object or don't know if request is Command or Query (like Authentication).\\n","tokens":55,"id":890,"Predictions":"Decision (A) The result should be always \"void\" and the result should be \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\".nbr>The result should be always \"void\"."}
{"File Name":"modular-monolith-with-ddd\/0004-divide-the-system-into-4-modules.md","Context":"## Context\\nThe MyMeetings domain contains 4 main subdomains: Meetings (core domain), Administration (supporting subdomain), Payments (supporting subdomain) and User Access (generic domain).\\nWe use Modular Monolith architecture so we need to implement one application which solves all requirements from all domains listed above.\\nWe need to modularize our system.\\n2. Create 4 modules based on Bounded Contexts which in this scenario maps 1:1 to domains. This solution is more difficult at the beginning. We need to set modules boundaries, communication strategy between modules and have more advanced infrastructure code. It is a more complex solution. On the other hand, it supports autonomy, maintainability, readability. We can develop our Domain Models in all of the Bounded Contexts independently.\\n","Decision":"Solution 2.\\nWe created 4 modules: Meetings, Administration, Payments, User Access. The key factor here is module autonomy and maintainability. We want to develop each module independently. This is more cleaner solution. It involves more work at the beginning but we want to invest.\\n","tokens":164,"id":891,"Predictions":"Decision (A) The MyMeetings domain should be modularized.n2 The MyMeetings domain should be modularized.n3 The MyMeetings domain should be modularized.n4 The MyMeetings domain should be modularized.n5 The MyMeetings domain should be modularized.n6 The MyMeetings domain should be modularized.n7 The MyMeetings domain should be modularized.n8 The MyMeetings domain should be modularized.n9 The MyMeetings domain should be modularized.n10 The MyMeetings domain should be modularized.n11 The MyMeetings domain should be modularized.n12 The MyMeetings domain should be modularized.n13 The MyMeetings domain should be modularized.n14 The MyMeetings domain should be modularized.n15 The MyMeetings domain should be modularized.n16 The MyMeetings domain should be modularized.n17 The MyMeetings domain should be modularized.n18 The MyMeetings domain should be modularized.n19 The MyMeetings domain should be modularized.n20 The MyMeetings domain should be modularized.n20 The MyMeetings domain should be modularized.n20 The MyMeetings domain should be modularized.n21 The MyMeetings domain should be modularized.n22 The MyMeetings domain should be modularized.n23 The MyMeetings domain should be modularized.n24 The MyMeetings domain should be modularized.n25 The MyMeetings domain should be modularized.n26 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n28 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n28 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n30 The MyMeetings domain should be modularized.n27 The MyMeetings domain should be modularized.n"}
{"File Name":"modular-monolith-with-ddd\/0005-create-one-rest-api-module.md","Context":"## Context\\nWe need to expose the API of our application to the outside world. For now, we expect one client of our application - FrontEnd SPA application.\\n","Decision":"Solution 1.\\nCreating separate API projects for each module will add complexity and little value. Grouping endpoints for a particular business module in a special directory is enough. Another layer on top of the module is unnecessary.\\n","tokens":34,"id":892,"Predictions":"We will need to make the API public.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0006-create-facade-between-api-and-business-module.md","Context":"## Context\\nOur API layer should communicate with business modules to fulfill client requests. To support the maximum level of autonomy, each module should expose a minimal set of operations (the module API\/contract\/interface).\\n","Decision":"Each module will provide implementation for one interface with 3 methods:<\/br>\\n```csharp\\nTask<TResult> ExecuteCommandAsync<TResult>(ICommand<TResult> command);\\nTask ExecuteCommandAsync(ICommand command);\\nTask<TResult> ExecuteQueryAsync<TResult>(IQuery<TResult> query);\\n```\\nThis interface will act as a fa\u00e7ade (Fa\u00e7ade pattern) between API and module. Only Commands, Queries and returned objects (which are part of this interface) should be visible to the API. Everything else should be hidden behind the fa\u00e7ade (module encapsulation).\\n","tokens":42,"id":893,"Predictions":"Decision 1). The business modules should communicate with business modules to fulfill client requests. 2). The business modules should communicate with business modules to fulfill client requests. 3) The business modules should communicate with business modules to fulfill client requests. 4) The business modules should communicate with business modules to fulfill client requests. 5) The business modules should communicate with business modules to fulfill client requests. 6) The business modules should communicate with business modules to fulfill client requests. 7) The business modules should communicate with business modules to fulfill client requests. 8) The business modules should communicate with business modules to fulfill client requests. 9) The business modules should communicate with business modules to fulfill client requests. 10) The business modules should communicate with business modules to fulfill client requests. 10) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with business modules to fulfill client requests. ) The business modules should communicate with"}
{"File Name":"modular-monolith-with-ddd\/0017-implement-archictecture-tests.md","Context":"## Context\\nIn some cases it is not possible to enforce the application architecture, design or established conventions using compiler (compile-time). For this reason, code implementations can diverge from the original design and architecture. We want to minimize this behavior, not only by code review.\\n","Decision":"We decided to implement Unit Tests for our architecture. <\/br>\\nWe will implement tests for each module separately and one tests library for general architecture. We will use _NetArchTest_ library which was created exactly for this purpose.\\n","tokens":56,"id":894,"Predictions":"Decision (A) To minimize the behavior of code implementations.nbr>nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0013-protect-business-invariants-using-exceptions.md","Context":"## Context\\nAggregates should check business invariants. When the invariant is broken, we should stop processing and return an error immediately to the client.\\n","Decision":"Solution number 1 - Use exceptions. <\/br>\\nPerformance cost of throwing an exception is irrelevant, we don't want too many if\/else statements in entities, more familiar with exceptions approach.\\n","tokens":32,"id":895,"Predictions":"We should stop processing and return an error immediately to the client.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0011-create-rich-domain-models.md","Context":"## Context\\nWe need to create Domain Models for all of the modules. Each Domain Model should represent a solution that solves a particular set of Domain problems (implements business logic).\\n","Decision":"Solution number 3 - Rich Domain Model<\/br>\\n1 - no, because the procedural style of coding will not be enough. We want to focus on behavior, not on the data.<\/br>\\n2 - no, keeping business logic in the database is not a good idea in that case, object-oriented programming is better than T-SQL to model our domain and we don't have performance architectural drivers to resign from OOD.<\/br>\\nWe expect complex business logic with different rules, calculations and processing so we want to get as much as possible from Object-Oriented Design principles like abstraction, encapsulation, polymorphism. We want to mutate the state of our objects only through methods (abstraction) to encapsulate all logic and hide implementation details from the client (the Application Service Layer and Unit Tests).<\/br>\\n","tokens":36,"id":896,"Predictions":"Decision Makers should create Domain Models for all of the modules.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0009-use-2-layered-architectural-style-for-reads.md","Context":"## Context\\nWe applied the CQRS style (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), now we need to decide how to handle reading (querying) requests.\\n","Decision":"We will use 2 layered architecture to handle queries: API layer and Application Service layer. As we applied the CQRS and created a separated read model, querying should be straightforward so 2 layers are enough. The API layer is responsible for Query creation based on HTTP request and the module Application layer is responsible for query handling.\\n","tokens":50,"id":897,"Predictions":"Decision (A) The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture.nbr>The CQRS style should be applied to the architecture"}
{"File Name":"modular-monolith-with-ddd\/0012-use-domain-driven-design-tactical-patterns.md","Context":"## Context\\nWe decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)) for each module. We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.\\n","Decision":"We decided to use **Domain-Driven Design** tactical patterns. They focus on the Domain Model implementation. Especially we will use the following building blocks:\\n- Command - public method on Aggregate (behavior)\\n- Domain Event - the immutable class which represents important fact occurred on a special point of time (behavior)\\n- Entity - class with identity (identity cannot change) with mutable attributes which represents concept from domain\\n- Value Object - immutable class without an identity which represents concept from domain\\n- Aggregate - cluster of domain objects (Entities, Value Objects) with one class entry point (Entity as Aggregate Root) which defines the boundary of transaction\/consistency and protects business rules and invariants\\n- Repository - collection-like abstraction to persist and load particular Aggregate\\n- Domain Service - stateless service to execute some business logic which does not belong to any of Entity\/Value Object\\n","tokens":70,"id":898,"Predictions":"We need to define or use some construction elements \/ building blocks to implement our architecture and business logic.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"Predictions":"Decision Objects are resolved by a graph. The graph is a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph. The objects are a graph."}
{"File Name":"modular-monolith-with-ddd\/0001-record-architecture-decisions.md","Context":"## Context\\nAs the project is an example of a more advanced monolith architecture, it is necessary to save all architectural decisions in one place.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":30,"id":900,"Predictions":"Decision Makers should save all architectural decisions in one place.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"modular-monolith-with-ddd\/0010-use-clean-architecture-for-writes.md","Context":"## Context\\nWe applied the CQRS style (see [ADR #7](0007-use-cqrs-architectural-style.md)), now we need to decide how to handle writing operations (Commands).\\n","Decision":"We will use **Clean Architecture** to handle commands with 4 layers: **API layer**, **Application Service layer**, **Infrastructure layer** and **Domain layer**. <\/br>\\nWe need to add Domain layer because domain logic will be complex and we want to isolate this logic from other stuff like infrastructure or API. Isolation of domain logic supports testing, maintainability and readability.\\n","tokens":43,"id":901,"Predictions":"Decision (A) The CQRS style should be applied.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"konduit-serving\/0001-Extend_vertx_launcher_for_CLI.md","Context":"## Context\\nCurrently, we have a main class called [KonduitServingMain](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/45af79d15abe4912ccd81e78c9d215306472036e\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingMain.java) that is the entrypoint for a konduit-serving application to run. The main command line arguments are defined inside [KonduitServingNodeConfigurer](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/e791741b80721980f8b66a35ed42f20b30612d5c\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingNodeConfigurer.java) class. We also have an additional [Python CLI](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/7965965b58217f2b4d983fd41aaea013264491ee\/python\/cli.py) that can be just implemented in Java. Vert.x Launcher supports the ability to start, stop and list running verticles out of the box.\\n","Decision":"- Extend `KonduitServingNodeConfigurer` from [Vert.x Launcher](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_the_vert_x_launcher) class.\\n- Write all the initialization\/tear-down logic inside the lifecycle methods of Vert.x Launcher which is present inside both of the above classes (KonduitServingMain and KonduitServingNodeConfigurer).\\n- Extend the CLI and write separate classes for every command we want to add to the CLI (see the documentation [here](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_extending_the_vert_x_launcher)).\\n- Depending on the final decided API, register or unregister commands in Vert.x Launcher.\\n### Example CLI:\\nThe following CLI will be the start for how this will be look:\\n#### Starting Verticles\\nEvery verticle will be named some sensible value instead of the full classpath so that the verticles can be started with a simple name. For example:\\nkonduit serve inference-server --config '{}' # With a JSON string\\nkonduit serve inference-server --config config.json # With a JSON file\\nkonduit serve inference-server --config config.json --name=mnist-prediction-server # With a server name. If no names are given then a random name\\n# is generated.\\nThe CLI can also have options for setting up deployment options like:\\nkonduit serve inference-server --config config.json --instances 3 # For running 3 instances of a Verticle. Usually it runs on a single port with\\n# round robin fashion requests transfer for load balancing\\n#### Stopping Verticles\\nkonduit stop inference --name=mnist-prediction-server # stops the server by the name of \"mnist-prediction-server\"\\n#### Listing Verticles\\nkonduit list # Lists all of the running verticle services with their names, host, port, configuration\\n#### Inspecting Verticles\\nPossible details will include:\\n- configuration\\n- host and port\\n- current resource usage\\n```bash\\nkonduit inspect # Give details of all the running verticles\\nkonduit inspect --name=mnist-prediction-server # Details of a specific verticle\\n```\\n#### Running Predictions\\n#### Predict with JSON (application\/json)\\nkonduit predict --name=mnist-prediction-server --input '{\"key\":\"value\"}' # With JSON string\\nkonduit predict --name=mnist-prediction-server --input input.json # With JSON file\\n#### Predict with Files (multipart\/form-data)\\nkonduit predict --name=mnist-prediction-server --input file.npy # Numpy\\nkonduit predict --name=mnist-prediction-server --input file.zip # DL4J\\nkonduit predict --name=mnist-prediction-server --input image.jspg # Image\\nkonduit predict --name=mnist-prediction-server --input file1.zip,file2.zip # DL4J with multiple inputs\\n","tokens":247,"id":902,"Predictions":"The main class of the konduit-serving application is [KonduitServingMain](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/45af79d15abe4912ccd81e78c9d215306472036e\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingMain.java) that is the entrypoint for a konduit-serving application to run. The main command line arguments are defined inside [KonduitServingNodeConfigurer](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/e791741b80721980f8b66a35ed42f20b30612d5c\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingNodeConfigurer.java) that can be just implemented in Java.n22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222"}
{"File Name":"konduit-serving\/0008-URIs_and_remote_resources.md","Context":"## Context\\nCurrently, the CLI allows users to launch a pipeline based on a configuration file that is hosted locally, such as:\\n```text\\nkonduit serve -c \/some\/path\/to\/config.json\\n```\\nHowever, in addition to serving local files, we want to support serving of pipelines that are hosted remotely:\\n```text\\nkonduit serve -c https:\/\/www.somewebsite.com\/pipelines\/config.json\\nkonduit serve -c s3:\/\/s3-eu-west-1.amazonaws.com\/myaccount\/models\/myconfig.yaml\\nkonduit serve -c wasb:\/\/mycontainer@myaccount.blob.core.windows.net\/directory\/myconfig.json\\n```\\nIn addition to launching a server from a configuration hosted at a remote location, we also want the ability to load models\\nthat are hosted at some non-local URI.\\nThat is: when loading any model type, the location of the model is specified in terms of a URI in the configuration\\n(such as the `modelUri` field in DL4JModelPipelineStep and TensorFlowPipelineStep) which needs to be downloaded (and perhaps cached).\\nConsider the following simple use case:\\n```text\\n$ pip install konduit-serving\\n$ konduit serve -c https:\/\/serving.konduit.ai\/pipelines\/images\/classification\/mobilenetv2.json\\n```\\nAssume the model specified in mobilenetv2.json is hosted remotely also at say `https:\/\/serving.konduit.ai\/models\/mobilenetv2.pb`.\\nThis use case (remote configuration and remote model) should \"just work\".\\nOverall, we will aim to support the following URI types for both configurations and model locations:\\n* HTTP\/HTTPS, HTTPS + basic authentication, and _maybe_ other options like HTTPS + OAuth 2.0?\\n* FTP, FTP + basic authentication\\n* HDFS (Hadoop filesystem, hdfs:\/\/)\\n* Amazon S3 (s3:\/\/)\\n* Azure (Storage account \/ blob storage - wasb:\/\/, wasbs:\/\/)\\n* Google Cloud Storage (gs:\/\/)\\nFor all of these, we simply need the ability to download a single file given the URI. At this point, uploading, directory\\nlisting, etc is not required.\\nThe supported types of URIs should be extensible also - i.e., users should be able to add other types if needed.\\n","Decision":"We utilize standard Java APIs for this: URI, URL, URLConnection and InputStream:\\n```java\\nURI u = URI.create(\"hdfs:\/\/somepath:12345\/pipelines\/config.json\");\\nURL url = u.toURL();\\nURLConnection connection = url.openConnection();\\ntry(InputStream is = connection.getInputStream()){\\n\/\/Read data\\n}\\n```\\nThere are three issues we need to resolve in this approach.\\n1. URI --> URL - i.e., we need protocol handlers for each protocol type\\nA reminder: URIs describe a resource; URLs additionally provide a mechanism to access the resource.\\n2. Ensuring the protocol handlers are on the classpath when required\\nHaving every protocol handler always on the classpath is a simple solution\/option here, though doing so will pollute\\nthe classpath with a bunch of additional dependencies most users won't need, and risks introducing  dependency divergence\\n(version) issues.\\n3. Authentication - how do we deal with authentication (which may be optionally present or absent in many URI types)\\nNote also we may need multiple sets of different credentials for launching the one pipeline.\\nFor example, we may require one set of credentials for `https:\/\/somewebsite.com\/myconfig.json` and an entirely different\\nset of credentials for `https:\/\/othersite.com\/mymodel.pb`.\\n### URI --> URL + Protocol Handlers\\nFor the \"standard\" protocol types such as HTTPS and FTP, these will be built into `konduit-serving-pipeline`.\\nThe other protocols (requiring 3rd party dependencies - `hdfs:\/\/`, `s3:\/\/` etc) will be placed into separate modules with\\nnames `konduit-serving-protocol\/konduit-serving-<protocol_name>` i.e., all protocols are in their own module under the\\n`konduit-serving-protocol` parent module, with name (for example, for Azure `wasb:\/\/` and `wasbs:\/\/`) `konduit-serving-azure`.\\nIn each of these modules (and also in konduit-serving-pipeline) we will provide the following:\\n* `SomeURLStreamHandler extends java.net.URLStreamHandler`\\n* `SomeURLConnection extends java.net.URLConnection`\\nSome of the underlying dependencies (S3 or Azure APIs for example) may provide these already.\\nFinally, we need to actually register these 2 new classes so they are used when we attempt to do the URI --> URL conversion.\\nApparently there are 2 ways to do this: https:\/\/accu.org\/index.php\/journals\/1434\\n* Register a factory\\n* Register via system property\\nThe system property approach is somewhat ugly, but might be the easiest solution. If possible, we will set it at runtime\\nusing System.setProperty before anything that would attempt to read a URL would be used.\\nFor the content of that system property (i.e., the fully qualified class names), we can likely simply collect all available\\nhandlers using a mechanism similar to what konduit-serving-meta already uses for collecting all available pipeline step types.\\nThe alternative is to use URL.setURLStreamHandlerFactory but this has the unfortunate design of being callable only once\\nfor the entire JVM: https:\/\/docs.oracle.com\/javase\/7\/docs\/api\/java\/net\/URL.html#setURLStreamHandlerFactory(java.net.URLStreamHandlerFactory)\\ni.e., the second call to URL.setURLStreamHandlerFactory will throw an exception.\\nThis is also an option but risks being incompatible with any 3rd party dependencies that also call this method.\\nSee also: https:\/\/www.oreilly.com\/library\/view\/learning-java\/1565927184\/apas02.html\\n### Authentication and Credentials\\nUnfortunately, credentials are likely going to be protocol-specific, with different ways of setting the credentials\\nfor different protocols. For example, Azure may require environment variables so set for example an access key, whereas\\nother protocols may require system properties or some other mechanism.\\nWe will need to look into the available authentication methods for each of the protocols we want to support.\\nFor now, where system properties or environment variables can be used, we'll provide a mechanism to do so when launching\\na server (or calling the CLI).\\n### Protocols for CLI\\nUnless the dependencies for the other protocol types (s3, azure, hdfs, etc) are especially large (file size) or are problematic\\n(dependency problems) we will simply include all of the protocol types by default in the CLI.\\nIf there are problems with this \"always available in CLI\" design, we will switch to an alternative approach:\\n* Only the standard protocols (http, ftp etc) are available by default\\n* If another protocol is required (s3, wasbs, hdfs etc) we download the appropriate module dependencies using the build tool\\n* Each module has an entry point used for downloading files, which we run to download the file\\nThat is: we launch a new, temporary process from the CLI that simply calls the module's main method in order to download\\nthe configuration file and then exit\\nNote however that we'll probably need this \"alternative approach\" in the future anyway to enable us to support custom protocols\\nvia the CLI.\\n### Protocols for Runtime (Model Servers)\\nThere are two use cases here, as discussed in the ADR 0006-CI_Launching_vs_Build.md.\\nFirst is the \"immediate CLI launch case\", and second is the \"deployment artifact\" launch case.\\nFor both cases, we will start by always including all protocol modules as per the CLI.\\nAnd again, if this proves problematic, we will consider an alternative.\\nAlternative approach, to be implemented if necessary: We will use the same approach that we use already for determining\\nmodules and dependencies for servers via the build tool.\\nThat is: We will look at the provided configuration (JSON\/YAML) and find any relevant URIs. From that, we will determine\\nthe modules needed. For example, if the configuration contains `\"modelUri\" : \"s3:\/\/some\/path\/to\/model.pb` then we know\\nwe need to include the `konduit-serving-s3` module when launching the server.\\n### Extensibility - Custom Protocol Types\\nThe likely approach for custom protocols is to use the (soon to be added) `additionalDependencies` \/ `additionalJars` options\\nfor the CLI and build tool. By setting one of these, a user can make a protocol handler for their custom type available\\nboth for the CLI and at runtime.\\n### File Caching\\nAn additional consideration here is file caching.\\nSuppose I'm launching a server from a remote URI. I shut down and then immediately restart the server.\\nUpon that restart: should I download the model again or not?\\nFor large models, this could be a significant problem - we don't want to redundantly download a model every time we launch\\na server.\\nTo account for this, we will cache downloaded models in say `~\/.konduit_cache`.\\nThen, when launching servers, we will:\\n1. Check this cache. If no file exists, download as normal\\n2. If a file does exist, we will open a URLConnection and check that both of the values returned by `URLConnection.getLastModified()`\\nand `URLConnection.getContentLength()`. If either differs, we will delete the old file and download from scratch.\\nNote that if opening the URL fails after say 3 retries (hence we can't check if the cached file is the latest): by default\\nwe will provide a warning and use the cached version anyway. This has the downside of potentially using an old model,\\nfor a server, but reduces the probability of intermittent network problems causing a server launch failure even when\\nthe model we've got cached is totally fine.\\nHowever, this \"use anyway\" default behaviour should be switchable to \"fail if we can't validate\" via some system property\\nor environment variable - perhaps `konduit.serving.cache.validation.warnonly` set to true by default, false if \"exception\\non validation failure\" is preferred instead.\\nTo implement this, when downloading a model into the cache, we need to record (in a separate file) the content length and\\nlast modified values. We should also record the most recent time a cache entry was accessed, which will allow us to delete\\nold entries (for example, those that haven't been used for say 60 days or more by default). We will thus implement a method\\nto check and clear the cache once per JVM launch of a server, after the cache has been used\/accessed.\\nTo implement this caching for downloading models, we will add a `URIResolver` class to `konduit-serving-pipeline` with\\n`public static File getFile(URI uri)` and `getFile(String uri)` methods that handles the downloading, caching, etc.\\nThis URIResolver class should then be used within each of the pipeline step runners to launch models.\\nNote that when no scheme is provided for the `getFile(String)` method, URIResolver should assume it's a local file (and\\nhence add the `file:\/\/` scheme to the provided path). Users shouldn't get obscure URI syntax exceptions when trying to\\nlaunch from a local file if they forget to add `file:\/\/` to the start of every single path.\\nFinally, as a workaround for potentially stale cache entries, we will implement a system property `konduit.serving.cache.clear`\\nthat, when set to true, will clear the cache before launching. This should be rarely needed, but will be available if the\\nlastModified and contentLength checks fail to identify an updated remote file for some reason.\\n","tokens":496,"id":904,"Predictions":"Decision (A) To support the use of URIs for configurations and model locations. (B) To support the use of URIs for configurations and model locations. (C) To support the use of URIs for configurations and model locations. (D) To support the use of URIs for configurations and model locations. (E) To support the use of URIs for configurations and model locations. (A) To support the use of URIs for configurations and model locations. (A) To support the use of URIs for configurations and model locations. (B) To support the use of URIs for configurations and model locations. (C) To support the use of URIs for configurations and model locations. (D) To support the use of URIs for configurations and model locations. (A) To support the use of URIs for configurations and model locations. (A) To support the use of URIs for configurations and model locations. (A) To support the use of URIs for configurations and model locations. (B) To support the use of URIs for configurations and model locations. (C) To support the use of URIs for configurations and model locations. (D) To support the use of URIs for configurations and model locations. (A) To support the use of URIs for configurations and model locations. (A) To support the use of URIs for configurations and model locations."}
{"File Name":"konduit-serving\/0004-Graph_pipelines.md","Context":"## Context\\nIn the past, Konduit Serving pipelines have been essentially a 'stack' of pipeline steps - each connected to only to the next: `A -> B -> C` and so on.\\nHowever, some use cases require a more complex structure, allowing parallel and\/or conditional execution of pipeline steps.\\nExamples of use cases:\\n* \"Select one of N models\" as part of a pipeline\\n- A\/B split testing (i.e., test different models for different inputs\/users)\\n- One model per X, selected dynamically (per region, language, time of day, etc)\\n- \"Select N of M models\" might occasionally be useful in some cases (like sensor fusion: image + text + sound in, but these are all optional\/unreliable inputs)\\n* Parallel branches\\n- Ensembles of ML models\\n- Parallelization: Execute slow steps in parallel to reduce overall pipeline execution time (database access, network communication, etc)\\n* Fallback models (i.e., \"RNN has 100ms to provide a response otherwise we return 'X'\"; or \"if model fails, return Y\")\\nThis ADR proposed the GraphPipeline, which will enable these use cases and more.\\nThe existing SequencePipeline functionality (i.e., \"stack of steps\" approach) would not be changed by this proposal.\\nFor GraphPipeline, there are a two considerations here:\\n* Functionality to provide\\n* API for providing that functionality\\n","Decision":"### Functionality\\nGraphPipeline will, like SequencePipeline, have a single Data input and a single Data output. This allows for a number of things including:\\n* Embedding a GraphPipeline within another Pipeline (SequencePipeline or GraphPipeline)\\n* Same API and serving methods and serving code for SequencePipeline and GraphPipeline\\n- i.e., users don't\\nInternally, GraphPipeline can have any amount of branching, parallelism, etc.\\nThis single input, single output restriction should not be cause any usability problems due to the fact that Data instances can contain any number of values (i.e., any number of (key,value) pairs). Hence anything we can do with a multi-Data input design can be achieved by combining and splitting Data instances.\\nIt is proposed that we provide support for directed acyclic graphs only - no loops are allowed within graph pipelines.\\nWe provide 5 types of \"graph steps\"\\n* Standard (1-to-1): single input, single output - a normal PipelineStep in a graph\\n* Switch operation (1 to 1-of-N): 1 Data instance in, which is routed to one of N outputs, based on some criteria (value in Data instance, or otherwise)\\n- Example use case: A\/B testing (switch op selects which model step to use)\\n- Merge (N-to-1): simply copy the content of all input Data instances into one output Data instance\\n- Any (N-to-1): simply forwards on the first (possibly only) available Data instance\\n- Typically used in conjunction with a switch step, where only one of N branches is executed\\n- Combine function (N-to-1): An arbitrary N-to-1 function, with or without all the inputs being available first. In addition to allowing custom Java\/Python UDFs, we will provide a small number of built-in functions for:\\n- Ensemble: allows weighted averaging, etc\\n- an integer aggregation selection (`inIdx = argMax(in[i][\"score\"])`)\\n- a timeout condition (\"return X if we get the value within N ms, otherwise return Y\")\\nInternally (but not for JSON) Merge and Any will probably be implemented as special cases of CombineFn - so we have really have just 3 types from an implementation perspective (1->1, 1->(1-of-N), N->1).\\nExamples use cases:\\n- Any: conditional execution with 2 branches: `in -> Switch(a,b), a->X, b->Y, Merge[Any](X,Y) -> output`. We either execute the left branch (`in->a->X->Merge(ANY)->out`) or the right branch (`in->b->Y->Merge(ANY)->out`)\\n- CombineFn: Select and return the predictions of the model with the highest probability\\nWe could also introduce a \"split\" operation, that does 1-to-N by splitting up a single Data instance; in practice we can do this simply by a number of simple \"subset\" pipeline steps in parallel. For example, `in -> SubsetPipelineStep -> A`, `in -> SubsetPipelineStep -> B`, where SubsetPipelineStep simply copies a subset of the input Data (key,value) pairs to the output Data instance.\\nNote that routing an input to \"N of M\" outputs is not easily supported in this proposal (where N changes on each inference step). If needed, it can be added later, or it can be approximated by a series of switch, no-op pipeline steps (returns empty Data), and merge operations.\\n### API (Java)\\nThe goal of the API is to make it as easy as passible to create graph pipelines, that do exactly (and unambiguously) what users expect.\\nAt least two options exist:\\n* Functional-style API\\n* Builder style API (like DL4J ComputationGraphConfiguration)\\nIt is propesd to use a semi-functional API, as follows:\\n```java\\nGraphBuilder b = new GraphBuilder();\\nGraphStep input = b.input();\\n\/\/Standard PipelineStep:\\nGraphStep myStep = input.then(\"myStep\", new SomePipelineStep());    \/\/Always require a name\\n\/\/Merge:\\nGraphStep merged = myStep.mergeWith(\"myMerged\", input);             \/\/Name is optional\\n\/\/Any:\\nGraphStep any = b.any(step1, step2);                                \/\/Name is optional\\n\/\/Combine\\nCombineFn fn = ...\\nGraphStep combined = b.combine(fn, step1, step2);                   \/\/Name is optional\\n\/\/Switch: note exact API here is TBD, but it's essentially a Function<List<Data>,Integer> with a numOutputs() method\\nSwitchFn sf = ...\\nGraphStep[] switched = b.switch(fn, myStep)\\n\/\/Construct the final GraphPipeline\\nPipeline p = b.build(combined);                                     \/\/Build method takes the final output step\\n```\\nAssuming we go with the functional-style design, there's not too many design decisions here, mainly related to naming:\\n* The method name for adding a step - \"then\", \"call\", \"followedBy\", \"inputTo\", and probably a lot more are possible\\n* The method name for merging: \"merge\", \"mergeWith\", etc\\n* The method name for any: \"any\", \"merge\", \"first\", etc\\n* The method name for combine: \"combine\", \"combineFn\", \"combineFunction\", \"aggregate\", etc\\nThere's also the concern that \"merge\" and \"combine\" are too close in name\/meaning, to confuse people. (Suggestions here are welcome)\\n### API (Python)\\nIn Python, it will be almost idestical, other than being a true functional interface for pipeline steps\\n```python\\nb = GraphBuilder()\\ninput = b.input()\\n# Stardard PipelineStep:\\nmyStep = input(\"myStep\", SomePipelineStep())\\n# Merge:\\nmerged = myStep.mergeWith(\"myMerged\", input)\\n# Any:\\nany = b.any(step1, step2)\\n# CombineFn\\nfn = ...\\ncombined = b.combine(fn, step1, step2)\\n# Switch\\nsf = ...\\nswitched = b.switch(sf, myStep)\\n# Construct final GraphPipeline:\\np = b(combined)                     #OR: b.build(combined)?\\n```\\n### JSON\\nWe should consider JSON part of the public API also - we want people to be able to write graph steps using JSON\/YAML by hand.\\nFor SequencePipeline, defining steps is simple: Users just provide an array\/list of steps, like so:\\n```json\\n{\\n\"steps\" : [\\n{\\n\"@type\" : \"<step type>\",\\n\"config1\" : \"value1\",\\n\"config2\" : \"value2\"\\n},\\n{\\n\"@type\" : \"<step type>\",\\n\"config1\" : \"value1\",\\n\"config2\" : \"value2\"\\n},\\n]\\n}\\n```\\nFor Graph pipelines, we have to encode extra information:\\n* Graph structure - i.e., names and inputs\\n* Graph components other than just PipelineSteps\\nProposal: We stay as close to the SequnencePipeline representation as possible, changing only the following:\\n* \"steps\": Becomes an object (map) not a list. Object keys are step names.\\n* We add an \"@input\" (alias: \"@inputs\") field within each pipeline step\\n* For pipeline steps: can take a single value, or a size 1 list\\n* For Merge\/Any\/CombineFn - take a list\/array\\nBy calling the field `@input` \/ `@inputs` we avoid to avoid ambiguity \/ clashing names when it comes to JSON serialization time for arbitrary configuration classes. That is, if we called it `input` and the user had a field called `input` in their configuration class, we have a problem.\\nExample JSON for graph pipelines, with 1 pipeline step (connected to pipeline input), and one merge step (connected to input and the step \"myStep\")\\n```json\\n{\\n\"steps\" : {\\n\"myStep\" : {\\n\"@input\" : \"input\",\\n\"@type\" : \"<step type>\",\\n\"config1\" : \"value1\",\\n\"config2\" : \"value2\"\\n},\\n\"myMerged\" : {\\n\"@input\" : [\"input\", \"myStep\"],\\n\"@type\" : \"MERGE\",\\n}\\n}\\n}\\n```\\n","tokens":301,"id":905,"Predictions":"Decision : The GraphPipeline should be implemented as a parallel branch of the pipeline.nOptions:AnBnCnDnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnAnA"}
{"File Name":"konduit-serving\/0006-CI_Launching_vs_Build.md","Context":"## Context\\nThere are multiple ways a user might want to use Konduit Serving to serve a model.\\nFor the sake of this ADR, I'll refer to two use cases as:\\n1. Immediate deployment case: \"Deploy a server for this pipeline right now on this machine via the CLI\"\\n2. Deployment artifact case: \"Create an artifact (JAR, docker image, etc) for deployment somewhere else\"\\nThe ADR 0005-Packaging_System_Design.md dealt with the \"create deployment artifact\" (2) case above.\\nHowever, the build system in its current form it does well not serve the needs of the immediate deployment use case\\nparticularly well, mainly due to its uber-jar design. This causes a few usability problems:\\n1. Deploying a pipeline with different modules (dl4j vs. samediff vs. TF, or CPU vs. GPU) requires building an uber-jar\\n2. Uber-jar builds can take a long time (30-120+ seconds, plus download time)\\n3. Either we always rebuild (slow launches) or we have an uber-JAR cache (potentially lots of unnecessary disk space used)\\nThe old API CLI dependency\/launching approach for this use case also had\/has the following problems:\\n1. Requiring the user to build a JAR ahead of time and manually include the modules they need\\n2. Not being able to launch (for example) both CPU and GPU servers simultaneously without rebuilding the whole Konduit\\nServing uber-jar in between launching the different servers\\nFor the CLI-based \"deploy right now\" use case, an alternative is proposed.\\n","Decision":"For the \"immediate deployment via CLI\" scenario, we will not create an uber-jar; instead, we will use the Konduit Serving\\nbuild tool to work out the dependencies we need, download them, and return a list of all dependencies (i.e., a list of JAR file\\npaths) that the Konduit Serving server will be launched with.\\nThis is similar to how IDEs like IntelliJ work. Consider for example the command IntelliJ uses when launching unit tests etc: (some parts omitted)\\n```\\n\"C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.242.08-hotspot\\bin\\java.exe\" -ea -Dfile.encoding=UTF-8 -classpath \"C:\\Program Files\\JetBrains\\IntelliJ IDEA Community Edition 2019.3.3\\lib\\idea_rt.jar;...;C:\\DL4J\\git\\konduit-serving\\konduit-serving-models\\konduit-serving-deeplearning4j\\target\\test-classes;...;C:\\Users\\Alex\\.m2\\repository\\org\\nd4j\\nd4j-api\\1.0.0-SNAPSHOT\\nd4j-api-1.0.0-SNAPSHOT.jar;C:\\Users\\Alex\\.m2\\repository\\com\\jakewharton\\byteunits\\byteunits\\0.9.1\\byteunits-0.9.1.jar;C:\\Users\\Alex\\.m2\\repository\\com\\google\\flatbuffers\\flatbuffers-java\\1.10.0\\flatbuffers-java-1.10.0.jar;C:\\Users\\Alex\\.m2\\repository\\org\\nd4j\\protobuf\\1.0.0-SNAPSHOT\\protobuf-1.0.0-SNAPSHOT.jar;C:\\Users\\Alex\\.m2\\repository\\commons-net\\commons-net\\3.1\\commons-net-3.1.jar... \" com.intellij.rt.junit.JUnitStarter -ideVersion5 -junit4 ai.konduit.serving.deeplearning4j.TestDL4JStep\\n```\\nNote the `-classpath \"<list of JAR paths>\"` component here.\\nIn practice, we won't pass a list of JAR file paths directly due to constraints on the maximum command line length on Windows.\\nInstead, we will use a small JAR containing a manifest file, which will list the absolute path of all dependencies:\\nhttps:\/\/www.baeldung.com\/java-jar-manifest\\nThis single manifest JAR is then passed via the `-classpath <path>` arg during launch.\\nNote that this exact Manifest JAR approach is also used by IntelliJ as an option for command line shortening to work around\\nthe maximum command line length problem.\\nThe key aspects of this design:\\n**1 - Static CLI JAR**\\nThe CLI JAR is a static JAR without any modules\/dependencies that are needed to run pipeline steps; it never gets modified,\\nrebuilt, etc no matter what type of pipeline is being launched.\\n**2 - Konduit Serving Build Tool - Downloads and Resolves Dependencies**\\nWhen a user launches a server based on some Konduit Serving pipeline configuration, the following occurs:\\n1. The CLI calls the build tool\\n2. The build tool resolves all dependencies that need to be included to run that pipeline\\n3. All direct and transitive dependencies are downloaded as normal via Gradle and stored in their usual location\\n4. The build tool creates the required manifest JAR\\n**3 - We introduce a concept of \"device profiles\" in the CLI**\\nIn practice, this will allow users to switch between different targets when launching (CPU vs. CUDA, and x86 instead of x86_AVX2 if needed\\nfor some reason). Specifically:\\n1. On the first run of the Konduit Serving CLI, we automatically create a set of appropriate device profiles based on\\nhardware and software available on this system. We will also set the default device profile to the CUDA profile if present.\\nIn practice, this will usually be just a CPU profile (highest level supported - avx2, avx512, etc) and a CUDA profile\\nif a CUDA device is present on the system. For the CUDA profile, we'll also detect if the CUDA is installed (if not,\\nwe'll use the JavaCPP presets CUDA redist binaries to provide it at runtime, avoiding the need for a manual install)\\n2. When running, we'll use the default profile unless the user passes a `-p <profile_name>` during launch. That is:\\n`konduit serve -c config.json -p CPU`\\nIn practice most users won't need to worry about device profiles, unless they need:\\n(a) to run on CPU only for a GPU-enabled device, or\\n(b) (very rarely, if ever) need to downgrade the target (for example, x86 instead of x86_avx2 on an avx2 compatible system)\\nas a workaround to some issue with an avx2 or higher binary.\\n### Example Workflow: Launch Locally\\nSuppose a user wants to deploy a server for inference, on system without a Konduit Serving installation.\\nHere's what that could look like:\\n```text\\n$ pip install konduit-serving           #Or any other easy installation method - apt, yum, etc etc\\n$ konduit serve -c config.json\\nKonduit Serving\\n--- Konduit Serving First Run Initialization ---\\nDetecting hardware... done\\nCPU:                 ARM64 (aarch64) - 4 core\\nCUDA GPU:            <device name>, 4GB\\nCUDA installation:   Found CUDA 10.2 (\/usr\/local\/cuda)\\nCreating device profiles...\\nProfile 1: \"CUDA_10.2\" - ARM64, CUDA 10.2 installed\\nProfile 2: \"CPU\"       - ARM64, CPU execution\\nCreating device profiles complete\\nSetting default profile: \"CUDA_10.2\"\\nUse <some_command> to set default profile or pass \"-p <profile>\" when launching to override\\n--- First run initialization complete ---\\nLaunching server using default device profile \"CUDA_10.2\"\\nAcquiring dependencies... done\\n<usual Konduit Serving launch info>\\n```\\nNote that users need only 2 lines here to go from a brand new system (no KS install) to hosting a model server using the\\noptimal hardware\/configuration for that device (i.e., CUDA, or highest supported AVX level for x86 systems, etc).\\nFurthermore there is no slow \"build uberjar\" step that delays the launching of the server by 30-120 seconds, on top of\\ndependency downloading.\\n### Launching for the \"Deployment Artifact\" case\\nThis \"manifest JAR\" approach can likely be used in other situations:\\n* Docker: Could use either uber-JAR or switching to an assembly-JAR style (i.e., embed the original\/unmodified dependency\\nJARs instead of an uberjar)\\n* RPM and DEB: As per docker\\n* Stand-alone .exe: Continue to use uber-JAR approach\\nIf we decide an assembly-JAR style approach is useful for these deployment artifacts, we can implement that at a later date.\\nAlso, in principle we can add extra dependencies on top of an uberjar... it may not be an especially elegant design, but\\ncombining uber-jars with 'extra' classpath dependencies may be possible if we ever really need it. However that won't\\nbe something we support for now\\n### Detecting Hardware and Creating Profiles\\nDetecting the CPU details should be straightforward, at least on x86-based systems using a library such as OSHI:\\nhttps:\/\/github.com\/oshi\/oshi\\nHow well OSHI supports ARM-based platforms is something we need to explore, though Raspberry Pi (armhf) support does\\nseem to be available: https:\/\/github.com\/oshi\/oshi\/issues\/864\\nFalling back on a system utility (`cat \/proc\/cpuinfo` or similar) is also a possibility here.\\nDetecting the presence or absence of a compatible CUDA GPU may be harder. When CUDA is installed and available on the\\npath, this becomes easier (CUDA install -> assume CUDA device present, or parse output of `nvidia-smi`).\\nFor the \"no CUDA installed but CUDA GPU available\" case, OSHI may show it up, or there may be a command line based approach\\nto find it (like cpuinfo).\\nIn principle this is a solveable problem but additional work is required to find a robust solution for detecting hardware\\n(including CUDA GPUs, and maybe other GPUs in the future) that will work across all devices and operating systems we expect\\nto deploy on in practice.\\n","tokens":341,"id":906,"Predictions":"Decision (A) The CLI-based \"deploy right now\" use case is a better choice.nB) The CLI-based \"deploy right now\" use case is a better choice.nC) The CLI-based \"deploy right now\" use case is a better choice.nD) The CLI-based \"deploy right now\" use case is a better choice.nAnswer:A.B.C.nB.C.nC.nB.C.nC.nB.C.nC.nB.C.nC.nB.C.nC.nB.C.nC.nB.C.nC.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.C.nB.n.nB.C.nB.n.nB.C.n.nB.C.n.nB.C.n.nB.C.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n.n"}
{"File Name":"konduit-serving\/0002-Use_swagger_annotations_for_generating_client_APIs.md","Context":"## Context\\nConsidering only inferences, we have two endpoints that are responsible to take inputs for prediction.\\n- application\/json\\n- \/:predictionType\/:inputDataFormat # For JSON inputs\\n- multipart\/form-data\\n- \/:predictionType\/:inputDataFormat # For multipart inputs (FILES)\\nGiven those two endpoints we have done a ton of work on creating a python client that (as of right now) needs more proper documentation, examples and maintenance planning for making the APIs more adaptable.\\nSince, we're planning to have APIs in multiple languages (python, java, C# and others), it might get difficult to maintain and document them separately in the future.\\n","Decision":"### Using swagger annotations to generate and document client APIs.\\n[Swagger annotations](https:\/\/github.com\/swagger-api\/swagger-core\/wiki\/Swagger-2.X---Annotations) are a quick and easy way to generate openapi specifications from the source code. Annotations apply to classes, methods and arguments. Using this way, it's easier to get rid of client APIs maintenance (generation, documentation and packaging) in different languages.\\n#### How it works?\\nBy refactoring konduit-serving source code into classes that contain APIs for different types of verticles. For example:\\n```java\\n@Path(\"\/\")\\n@Produces(MediaType.APPLICATION_JSON)\\npublic class InferenceApi {\\n@GET\\n@Path(\"\/config\")\\npublic InferenceConfiguration getConfig() {\\nreturn new InferenceConfiguration();\\n}\\n@POST\\n@Path(\"\/{predictionType}\/{inputDataFormat}\")\\n@Consumes(MediaType.MULTIPART_FORM_DATA)\\n@Operation(summary = \"Get inference result with multipart data.\",\\ntags = {\"inference\"},\\ndescription = \"You can send multipart data for inference where the input names will be the names of the inputs of a transformation process. \" +\\n\"or a model input and the corresponding files containing the data for each input.\",\\nresponses = {\\n@ApiResponse(description = \"Batch output data\",\\nresponseCode = \"200\",\\ncontent = @Content(schema = @Schema(oneOf = {\\nClassifierOutput.class,\\nRegressionOutput.class,\\nDetectedObjectsBatch.class,\\nManyDetectedObjects.class\\n}))\\n),\\n}\\n)\\npublic BatchOutput predict(@PathParam(\"predictionType\") Output.PredictionType predictionType,\\n@PathParam(\"inputDataFormat\") Input.DataFormat inputDataFormat,\\n@Parameter(description = \"An array of files to upload.\") File[] multipartInput) {\\nreturn new ClassifierOutput();\\n}\\n}\\n```\\nThis will require similar refactoring for the other verticles and their respective routers. Currently, the following classes will have to be refactored based on the above details:\\n- PipelineRouteDefiner\\n- MemMapRouteDefiner\\n- ConverterInferenceVerticle\\n- ClusteredVerticle\\n#### How it would look at the end?\\nHaving an API that looks like the class above will generate an API specification that will look like:\\n```yaml\\nopenapi: 3.0.1\\ninfo:\\ntitle: Konduit Serving REST API\\ndescription: RESTful API for various operations inside konduit-serving\\ncontact:\\nname: Konduit AI\\nurl: https:\/\/konduit.ai\/contact\\nemail: hello@konduit.ai\\nlicense:\\nname: Apache 2.0\\nurl: https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/master\/LICENSE\\nversion: 0.1.0-SNAPSHOT\\nexternalDocs:\\ndescription: Online documentation\\nurl: https:\/\/serving.oss.konduit.ai\\ntags:\\n- name: inference\\ndescription: Tag for grouping inference server operations\\n- name: convert\\ndescription: Tag for grouping converter operations\\n- name: memmap\\ndescription: Tag for grouping memory mapping operations\\npaths:\\n\/config:\\nget:\\noperationId: getConfig\\nresponses:\\ndefault:\\ndescription: default response\\ncontent:\\napplication\/json:\\nschema:\\n$ref: '#\/components\/schemas\/InferenceConfiguration'\\n\/{predictionType}\/{inputDataFormat}:\\npost:\\ntags:\\n- inference\\nsummary: Get inference result with multipart data.\\ndescription: You can send multipart data for inference where the input names\\nwill be the names of the inputs of a transformation process. or a model input\\nand the corresponding files containing the data for each input.\\noperationId: predict\\nparameters:\\n- name: predictionType\\nin: path\\nrequired: true\\nschema:\\ntype: string\\nenum:\\n- CLASSIFICATION\\n- YOLO\\n- SSD\\n- RCNN\\n- RAW\\n- REGRESSION\\n- name: inputDataFormat\\nin: path\\nrequired: true\\nschema:\\ntype: string\\nenum:\\n- NUMPY\\n- JSON\\n- ND4J\\n- IMAGE\\n- ARROW\\nrequestBody:\\ndescription: An array of files to upload.\\ncontent:\\nmultipart\/form-data:\\nschema:\\ntype: array\\nitems:\\ntype: string\\nformat: binary\\nresponses:\\n\"200\":\\ndescription: Batch output data\\ncontent:\\napplication\/json:\\nschema:\\noneOf:\\n- $ref: '#\/components\/schemas\/ClassifierOutput'\\n- $ref: '#\/components\/schemas\/RegressionOutput'\\n- $ref: '#\/components\/schemas\/DetectedObjectsBatch'\\n- $ref: '#\/components\/schemas\/ManyDetectedObjects'\\ncomponents:\\nschemas:\\nInferenceConfiguration:\\ntype: object\\nproperties:\\nsteps:\\ntype: array\\nitems:\\n$ref: '#\/components\/schemas\/PipelineStep'\\nservingConfig:\\n$ref: '#\/components\/schemas\/ServingConfig'\\nmemMapConfig:\\n$ref: '#\/components\/schemas\/MemMapConfig'\\nMemMapConfig:\\ntype: object\\nproperties:\\narrayPath:\\ntype: string\\nunkVectorPath:\\ntype: string\\ninitialMemmapSize:\\ntype: integer\\nformat: int64\\nworkSpaceName:\\ntype: string\\nPipelineStep:\\ntype: object\\nproperties:\\ninput:\\n$ref: '#\/components\/schemas\/PipelineStep'\\noutput:\\n$ref: '#\/components\/schemas\/PipelineStep'\\noutputColumnNames:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\ninputColumnNames:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\ninputSchemas:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- String\\n- Integer\\n- Long\\n- Double\\n- Float\\n- Categorical\\n- Time\\n- Bytes\\n- Boolean\\n- NDArray\\n- Image\\noutputSchemas:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- String\\n- Integer\\n- Long\\n- Double\\n- Float\\n- Categorical\\n- Time\\n- Bytes\\n- Boolean\\n- NDArray\\n- Image\\noutputNames:\\ntype: array\\nitems:\\ntype: string\\ninputNames:\\ntype: array\\nitems:\\ntype: string\\nServingConfig:\\ntype: object\\nproperties:\\nhttpPort:\\ntype: integer\\nformat: int32\\nlistenHost:\\ntype: string\\noutputDataFormat:\\ntype: string\\nenum:\\n- NUMPY\\n- JSON\\n- ND4J\\n- ARROW\\nuploadsDirectory:\\ntype: string\\nlogTimings:\\ntype: boolean\\nincludeMetrics:\\ntype: boolean\\nmetricTypes:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- CLASS_LOADER\\n- JVM_MEMORY\\n- JVM_GC\\n- PROCESSOR\\n- JVM_THREAD\\n- LOGGING_METRICS\\n- NATIVE\\n- GPU\\nClassifierOutput:\\ntype: object\\nproperties:\\ndecisions:\\ntype: array\\nitems:\\ntype: integer\\nformat: int32\\nprobabilities:\\ntype: array\\nitems:\\ntype: array\\nitems:\\ntype: number\\nformat: double\\nlabels:\\ntype: array\\nitems:\\ntype: string\\nbatchId:\\ntype: string\\nRegressionOutput:\\ntype: object\\nproperties:\\nvalues:\\ntype: array\\nitems:\\ntype: array\\nitems:\\ntype: number\\nformat: double\\nbatchId:\\ntype: string\\nDetectedObjectsBatch:\\ntype: object\\nproperties:\\ncenterX:\\ntype: number\\nformat: float\\ncenterY:\\ntype: number\\nformat: float\\nwidth:\\ntype: number\\nformat: float\\nheight:\\ntype: number\\nformat: float\\npredictedClassNumbers:\\ntype: array\\nitems:\\ntype: integer\\nformat: int32\\npredictedClasses:\\ntype: array\\nitems:\\ntype: string\\nconfidences:\\ntype: array\\nitems:\\ntype: number\\nformat: float\\nbatchId:\\ntype: string\\nManyDetectedObjects:\\ntype: object\\nproperties:\\ndetectedObjectsBatches:\\ntype: array\\nitems:\\n$ref: '#\/components\/schemas\/DetectedObjectsBatch'\\nbatchId:\\ntype: string\\nwriteOnly: true\\nBatchOutput:\\ntype: object\\nproperties:\\nbatchId:\\ntype: string\\nwriteOnly: true\\n```\\nAnd from this yaml the clients will be generated using [openapi-generator](https:\/\/github.com\/OpenAPITools\/openapi-generator). For example:\\n```bash\\njava -jar openapi-generator-cli.jar generate -i openapi.yaml -g python -o python_api_client\\n```\\nThe above command will generate the python clients for us and the related docs for using the API in python.\\nThis will be a similar process for other languages as well.\\n","tokens":138,"id":907,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"konduit-serving\/0009-Custom_Endpoints.md","Context":"## Context\\nCurrently, we have the \"\/predict\" endpoint for performing inference - this takes a Konduit Serving `Data` instance in,\\nreturns a `Data` instance out, in either JSON or binary (protobuf) format. We also have the ability ot create Grafana dashboards\\nbased on our metrics functionality\/endpoints.\\nGrafana dashboards are totally fine for some use cases (simple passive visualization). However, in we might want:\\n(a) more flexibility and control than Grafana is able to provide for displaying predictions\/metrics (Grafana doesn't really\\nsupport images or video, other than maybe via custom plugins),\\n(b) The ability to provide interactive dashboards, that can also take input, not just show outputs\\n(c) the ability to provide input\/output in some \"application specific\" format (for example, allow users to post a raw PDF\\nto a custom endpoint, instead of having to do preprocessing on the client side post create image\/text via the Konduit\\nServing Data).\\nAdditionally, for developing real apps we might need to (for example) provide a configuration endpoint, or a debugging\\nendpoint, etc that hosts an actual HTML page the user can view and interact with.\\n","Decision":"This proposal suggests the addition of custom endpoint functionality for Konduit Serving.\\nThe idea: to add a custom endpoint (get\/post etc, with or without inteference), users need to:\\n1. Provide a class that implements an interface - i.e., `MyCustomEndpoint implements HttpEndpoint`\\n2. Provide the fully qualified class name in their pipeline configuration - i.e., `\"customEndpoint\" : \"com.company.MyCustomEndpoint`\\n3. When launching the server, they provide a JAR with their custom code (using the \"additionalDependencies\" and \"additionalClasspath\"\\nmechanism already proposed for the CLI \/ build tool) - i.e., the JAR containing the custom endpoint code is provided\\nvia a JAR location, or via GAV coordinates\\nThe `HttpEndpoint` interface is defined with just one method:\\n`endpoints(Pipeline, PipelineExecutor) : List<Endpoint>`\\nThis gives the endpoint access to the pipeline\/executor for use in the endpoints\\nThe `Endpoint` interface then has the following methods:\\n`type() : io.vertx.core.http.HttpMethod` - i.e., GET, POST, etc\\n`path() : String` - i.e., \"\/myEndpoint\" - may optionally include path parameters etc\\n`consumes() : List<String>` - MIME types for the input\\n`produces() : List<String>` - MIME types of the response\\n`handler() : Handler<RoutingContext>` - i.e., the actual endpoint-handling code.\\nNote that the endpoints don't necessarily have to call the underlying pipeline \/ pipeline executor, though some will.\\nFor example, two use cases supported by this design include:\\n(a) Static HTML page serving - that may have the option to call another endpoint (example: a form that allows uploading\\nof an image to another endpoint)\\n(b) POST endpoints for proving for example a PDF, that does conversion, calls the executor, and returns a `Data` instance\\nor some other type\\n","tokens":252,"id":908,"Predictions":"We want to provide interactive dashboards that can also take input, not just show outputsnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"konduit-serving\/0010-Async_Pipelines.md","Context":"## Context\\nIn some machine learning deployments, we need to perform inference periodically, regardless of whether anyone is actively\\nquerying the pipeline from a REST\/GRPC etc endpoint.\\nExamples 1: When processing video, we want to perform inference on each frame, or N times per second, etc regardless of\\nwhether someone calls a REST endpoint or not.\\nExample 2: When processing video, we want the number of pipeline executions to be independent of the number of clients\\nquerying the REST endpoint (i.e., we should run at 20 FPS and return the latest output\/prediction available, regardless of\\nwhether there is 1 or 100 clients querying per second).\\nExample 3: anomaly detection use case without any sort of REST\/GRPC\/MQTT etc server. Imagine we want to perform\\ninference every second, and (conditionally, if a fault is detected) execute a \"HTTP post\" pipeline step to report the fault.\\nExample 4: \"I want to perform inference at most 5 times per second. If no requests occur, we shouldn't perform any\\ninference. If more than 5 requests per second occur, we want to return the last cached result instead\"\\nExample 5: \"periodic push based\" pipeline execution (microservices style). Suppose we want to perform inference on a camera\\nfeed at 5 FPS and for every frame post the prediction (for example, predicted class or detected objects) to a HTTP endpoint,\\na Kafka queue, an MQTT endoint, or simply writing to a file (where these are implemented as pipeline steps).\\nThese use cases aren't yet supported in Konduit Serving.\\n","Decision":"We introduce an additional Pipeline type, `AsyncPipeline`. It implements the `Pipeline` interface (same as `SequencePipeline`\\nand `GraphPipeline`) so from an API and execution point of view it is the same as the other Pipeline types.\\nThis AsyncPipeline is a Decorator\/Wrapper pattern - i.e.,\\n```java\\nPipeline p = SequencePipeline.builder()... .build();\\nPipeline asyncPipeline = new AsyncPipeline(p, Trigger);\\n```\\nThe AsyncPipeline does two things:\\n* Performs execution of the underlying Pipeline based on some sort of trigger\\n* (Usually) stores the last output of the underlying pipeline, and returns it when query() is called\\nConsider an AsyncPipeline set to perform inference of the underlying pipeline once per second. If we query the AsyncPipeline\\n100 times per second (for example, by 100 difference users all querying the same REST endpoint), we get the same result\\nreturned 100 times, not 100 independent (potentially redundant) execution of the underlying pipeline.\\nThe \"trigger\" for the is configurable. It allows different ways of performing inference on the underlying model.\\nThe `Trigger` interface would have the following API:\\n```text\\nquery(Data) : Data                       - Called when the AsyncPipelineExecutor.exec(...) is called. Returns either a\\ncached Data instance, or optionally blocks for perform a new inference.\\nsetCallback(Function<Data,Data>) : void  - The function provided here is used by the Trigger to perform execution of the\\nunderlying Pipeline whenever the Trigger wants to (with the provided Data),\\nirrespective of whether there is a query() call or not\\n```\\nThe idea is the Trigger would call the function whenever it wants inference to be performed, whether or not the external\\nPipeline\/PipelineExecutor has been called or not (i.e., irrespective of whether query(Data) is called or not).\\nNote that the Trigger instances should be thread safe.\\nBuilt-in implementations would initially include:\\n* `SimpleLoopTrigger`: performs inference in a loop as fast as possible, unless an optional configuration `frequencyMs`\\noption is set (in which case, it calls the underlying pipeline every `frequencyMs` milliseconds).\\n* `TimeLoopTrigger`: calls every N `TimeUnit`s, with some offset. For example, \"Every hour, at the start of the hour\", or\\n\"3 hours past the start of the day, every day\", etc\\n* `CachingTrigger`: performs inference \"at most every N milliseconds\". For example, if we say \"at most once per 1000ms\",\\nand we get a query(Data) call at T=0, we block and call the `Function<Data,Data>`. For all subsequent queries up to T=1000ms,\\nwe return the cached value from the T=0 call. The next call immediately after T=1000ms results in another blocking call\\nand an update of the cached value (until T=2000ms, and so on).\\n### JSON Format\\nThe JSON format for SequencePipeline and GraphPipeline is something that users are supposed to be able to understand, edit\\nand potentially even write from scratch if they so desire. It is also (with only a few exceptions) programming language\\nindependent.\\nThe SequencePipeline vs. GraphPipeline is differentiated by the form of the \"steps\" field: if it's a list, it's a SequencePipeline;\\nif it's an object\/map, it's a GraphPipeline.\\nWe can do a simple extension to this idea: Again noting that the AsyncPipeline has a decorator pattern, we either have a\\nAsyncPipeline(SequencePipeline) or a AsyncPipeline(GraphPipeline).\\nIn either case, we can simply add a new field  - `@AsyncTrigger`, and otherwise leave the existing JSON format unchanged.\\ni.e., the presence of this field means that we have a AsyncPipeline decorator.\\n","tokens":343,"id":909,"Predictions":"Decision (A) To perform inference periodically, regardless of whether someone is activelynquerying the pipeline from a REST\/GRPC etc endpoint.nExamples 1: When processing video, we want to perform inference on each frame, or N times per second, etc regardless of whether someone calls a REST endpoint or not.nExample 2: When processing video, we want to perform inference on each frame, or N times per second, etc regardless of whether someone calls a REST endpoint or not.nExample 3: An anomaly detection use case without any sort of REST\/GRPC\/MQTT etc server. Imagine we want to perform inference every second, and (conditionally, if a fault is detected) execute a \"HTTP post\" pipeline step to report the fault.nExample 4: \"I want to perform inference at most 5 times per second. If no requests occur, we shouldn't perform anyninference. If more than 5 requests per second occur, we want to return the last cached result instead\"nExample 5: \"period push based\" pipeline execution (microservices style).nAnswer:A.B.C.B.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C.C"}
{"File Name":"konduit-serving\/0007-Point_Data_Type.md","Context":"## Context\\nThere are multiple ways one can represent a point with the existing data types. However, all of those ways can be classified as a workaround. A bounding box with zero width and height, a list of doubles, or an NDArray all don't communicate the intent of their contents well.\\nBecause communicating intent is important when building maintainable systems, a Point data type was suggested.\\n","Decision":"We create a Point datatype that is explicitly meant to be used for point based data. The Point datatype can represent points with any number of dimensions. It is **not** limited to just 2 or 3 dimensional points.\\nIn order to make conversion between bounding boxes and points easier, points like bounding boxes will also provide optional \"label\" and \"probability\" fields.\\nTo access a specific dimension of a point, a direct `.get(dimension)` method is provided. Because we anticipate that 2 and 3 dimensional points are going to be used very often, the typical x, y, z based notation will also be allowed trough the use of `.x()`, `.y()` and `.z()` methods which internally call `.get(0)`, `.get(1)` or `.get(2)` respectively.\\nThe value of a point will *usually* fall between 0 and 1, i.e. will be a relative measure. This is especially useful when used with other data types like image. However, it is not required to be within this range meaning absolute values are also allowed - interpreting the meaning of a point value is up to the user.\\nThe point data type is going to be implemented with an interface \/ implementation split, however only a single implementation that can take n-dimensional points will be used. If this should ever become a problem, we should be able to provide specialized implementations without breaking existing code.\\n","tokens":80,"id":910,"Predictions":"Decision Makers made a recommendation to represent a point with the existing data types.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Extended\/adr-0001-geometry-shaders.md","Context":"## Context and Problem Statement\\nShould `Extended` support geometry shaders?\\n## Decision Drivers <!-- optional -->\\n* Metal does not support geometry shaders.\\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\\n","Decision":"* Metal does not support geometry shaders.\\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\\nNone taken yet.\\n","tokens":160,"id":911,"Predictions":"Decision Makers!-- optional -->n* MonoGame does not support geometry shaders.n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.n* Decision Makers!-- optional -->n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.n* Decision Makers!-- optional -->n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.n* Decision Makers!-- optional -->n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.n* Decision Makers!-- optional -->n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.n* Decision Makers!-- optional -->n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.n"}
{"File Name":"Ais.Net\/0002-missing-fragment-handling.md","Context":"## Context\\nSome AIS data sources include NMEA sentences that are not correctly formed. We sometimes see truncated messages, for example. This causes problems when a corrupt message was a fragment. At some point we need to let go of the fragment because its fellow fragments are never going to arrive.\\n","Decision":"We default to abandoning fragmented messages if 8 other messages have arrived since the first fragment. This is configurable, but we chose 8 because there's a limit of 9 fragmented messages in progress at any one time if you use the AIVDM-level group identifiers. It is configurable because systems that provide tag block group identifiers often use larger group IDs (e.g., 4 digits are common), making it possible for more to be in flight at once. But in practice, most message fragments are adjacent, so a large window is usaully unnecessary.\\n","tokens":59,"id":912,"Predictions":"We need to let go of the fragment because its fellow fragments are never going to arrive.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"Ais.Net\/0001-error-handling-strategy.md","Context":"## Context\\nNMEA messages containing AIS data are sometimes malformed. The messages are often transferred from transponder devices to computer equipment over a serial port, which have limited error detection and correction capabilities, making it possible for messages to lose characters or contain garbage characters.\\n","Decision":"We deal with malformed NMEA messages at two levels.\\nThe `NmeaLineParser` detects failures to conform to the basic structure required of an NMEA message. It reports these errors by throwing an `ArgumentException` with a text message describing the way in which the message structure does not match the specification. For example, if the normal `!` start character is missing, the error message is `Invalid data. Expected '!' at sentence start`. In cases where the message is well formed but with features that Ais.Net cannot currently parse, we throw a `NotSupportedException` exception.\\nCode that processes large numbers of messages will typically not construct the `NmeaLineParser` itself, and instead relies on the high-throughput methods offered by `NmeaStreamParser`. Code working this way passes in an implementation of either `INmeaLineStreamProcessor` or `INmeaAisMessageStreamProcessor` depending on whether it wants to process each line in the file directly, or it wants `NmeaStreamParser` to handle the task of reassembling AIS payloads that have been fragmented across multiple NMEA message lines. Both of these interfaces require an `OnError` method that looks like this:\\n```csharp\\nvoid OnError(in ReadOnlySpan<byte> line, Exception error, int lineNumber);\\n```\\nWhen the `NmeaLineParser` constructed by the `NmeaStreamParser` reports an error in the manner described above, the exception it throws will be passed to this `OnError` method, along with a span providing access to the problematic line's full content, and the line number within the stream at which the error was detected.\\nThere are also errors that can occur with fragmented messages. While each individual fragment might be well-formed, they might be wrong in combination. For example, it is not permitted for a fragmented message to have padding at the end of the first fragment. In these cases, which will be detected when working at the message level, we report errors through the same mechanism.\\n","tokens":53,"id":913,"Predictions":"Decision (A) The NMEA messages containing AIS data are sometimes malformed.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"Predictions":"The system will be a model for the care system.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"Predictions":"Decision (A) The Childrens Social Care Placement Service should be in the [Alpha] phase of delivery.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"csc-swr-architecture\/001-Use-GitHub-Workflow-for-ADRs.html.md.erb","Context":"## Context\\nArchitectural Decisions are not made in isolation and need to be discussed and agreed amongst the Service Team.\\n","Decision":"Utilise [GitHub Workflow](https:\/\/guides.github.com\/introduction\/flow\/) to manage the acceptance, or not, of ADRs by the Service Team. The Pull Request review process, prior to merging into `main`, provides suitable mechanisms for tracking comments, amendments, approvals and so on.\\n","tokens":26,"id":916,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"csc-swr-architecture\/005-Monolith-First.html.md.erb","Context":"## Context\\nThe CSC Placement service is taking an [Evolutionary Architecture] approach.\\n","Decision":"Since we are in a highly uncertain phase of the Service, and we want to optimise around developer productivity and feature prototyping, the adoption of a [Monolith First] architecture is considered best.\\nAlthough Martin Fowler uses this as a precursor for a [Microservice Architecture], it is not definitely regarded as a transition to Microservices here, but rather a reasonable first step before deciding future architectural quanta.\\n","tokens":19,"id":917,"Predictions":"The CSC Placement service is taking an [Evolutionary Architecture] approach.nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nbr>nb"}
{"File Name":"csc-swr-architecture\/006-Postcode-Lookup.html.md.erb","Context":"## Context\\nPostcode Validation\/Verification, Address & Geocoding searches is a requirement in\\nthe Diary Logging Service and more particularly in the Placement Shortlisting Service.\\nCurrently there is no adopted DfE Architectural pattern for postcode searches, address\\nsearches & Geocoding searches.\\nThere is a Common Components Initiate (CCI) currently in development by DfE where several\\nSaaS offerings are being considered.  Postcode, address & geocoding is part of this initiative,\\nbut this will not be approved and an adopted practise for some time. For further information see\\n[here](https:\/\/dfe-digital.github.io\/architecture\/common-components\/#postcode-lookup).\\nCCI will also include Geospatial Mapping and the following products and service will\\nbe available across the department:\\n* [Azure Maps]\\n* [Google Maps]\\n* [Bing Maps]\\nEach product provides a slightly different set of features and capabilities that\\nfit different use cases.\\n","Decision":"This Alpha is working to build out a simple API postcode service, in the first utilising\\n[Postcodes.io].\\nAddress validation and geocoding is not required during the alpha phase and\\nthis decision will be replaced by a substantive revised decision, including address lookup\\nand geocoding when the detail of the data model is clear, and the project moves to the Beta phase.\\nA summary of the availability statistics for the services are available on this link:\\n* [Ideal Postcodes Status Page]\\n","tokens":205,"id":918,"Predictions":"Decision (A) The Postcode Validation\/Verification, Address & Geocoding searches will be implemented in the Diary Logging Service and more particularly in the Placement Shortlisting Service.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"csc-swr-architecture\/002-Progamming-Language-and-Framework.html.md.erb","Context":"## Context\\nThis decision is being made during Alpha. It also comes at a time that the Digital organisation within DfE are building developer capability and want the members of the developer community to readily switch between services. Hence a default preferred language and framework is desired, Digital are coalescing around Ruby and the associated Rails framework.\\nRails is a development tool which gives web developers a framework, providing structure for all the code they write. The Rails framework helps developers to build websites and applications, because it abstracts and simplifies common repetitive tasks.\\nRails is written in Ruby, the programming language which is also used alongside Rails. Ruby is to Rails as PHP is to Symfony and Zend, or as Python is to Django. The appeal of Ruby to developers lies in the elegance and terseness of the language.\\n","Decision":"The decision is to align with the central Digital preference for Ruby and Rails.\\nKey reasons for choosing Ruby and Rails are:-\\n* Alignment with a large portion of the Government Digital Service sites and services, including cross service development skills within DfE. [Here](https:\/\/dfe-digital.github.io\/technical-guidance\/guides\/default-technology-stack\/#the-ruby-stack) is the DfE Technical Guidance around this.\\n* Optimisation around developer productivity and lower costs.\\n* It was created specifically for building web applications.\\n* Numerous code libraries (Gems) which provide free, open-source code for developers to fulfil specific needs.\\n* A very strong community with a great sense of collaboration and support.\\n* It is robust and high-quality. All Ruby developers are encouraged to follow a set of community-supported coding conventions and standards, which in turn helps produce better code, and thereby high quality digital products and services.\\n* It\u2019s conventions make it much easier for developers to move between Rails services, which will tend to use the same structure and coding practices.\\n* A strong focus on testing, with many good testing frameworks.\\n* It emphasises RESTful application design. REST (REpresentational State Transfer) is a style of software architecture based around a client-server relationship. It encourages a logical structure which can be exposed as an API (Application Programing Interface) which can be used to expose platform functionality (as desired for this service).\\n* Is an intuitive and easy-to-read language. It uses fewer characters than other languages allowing for intuitive, natural language, coding rather than having to use lots of special characters. The flip side is that reading code is made simpler, allowing for quick on-boarding to a service and peer reviewing for example.\\n* Enables rapid development. Due to many of the above points, such as readily available code libraries specifically designed for websites and apps, an active community which promotes high-quality development, and code that is simple to read and write then using Rails can achieve rapid development with high-quality results.\\nReasons why you might not choose Ruby and Rails are:-\\n* Not all hosting can support Rails - but since we are choosing where to host our service we can ensure this isn\u2019t an issue.\\n* Java and PHP are more widely used and there are more developers in these languages. However there is still a strong market in Ruby developers and the very extensive open-source libraries available mean a lot can be achieved by a few developers.\\n* Performance and Scalability. Rails is not as fast as Java or C, but it is fast enough for this service and will horizontally scale out when needed. There are plenty of high-profile organisations relying on Rails, examples include AirBnB, Yellow Pages, Groupon, Channel 5, GitHub, Shopify and Gov.uk.\\n","tokens":164,"id":919,"Predictions":"Decision based on the information in the record.nThe following are the possible outcomes of the Alpha decision:nThe following are the possible outcomes of the Ruby decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:nThe following are the possible outcomes of the Rails decision:"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"Predictions":"Decision (A) The service should be in Alpha.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"universal-remote-control-skill\/0002-overall-system-architecture.md","Context":"## Context\\nTo put down an architecture for the system as a whole.\\n","Decision":"The system consists of the following components:\\n* Global components:\\n* Alexa Smart Home Skill\\n* AWS Lambda\\n* AWS API Gateway websocket endpoint\\n* Per user-components:\\n* Google Account containing an automatically set-up Google Spreadsheet\\n* Universal Remote Controller (URC) device\\n### Alexa Smart Home Skill\\n* Takes care of activation and hands off control to AWS Lambda\\n* Links to user's Google Account\\n### AWS Lambda\\n* Receives and keeps websocket connection from each URC and sends commands to the URCs\\n* Reads configuration data from user's Google Spreadsheet\\nTODO: this needs to be worked out further, following\\n### Google Account with Spreadsheet\\nThe Spreadsheet contains all necessary configuration data to control the user's specific environment:\\n* URCID\\n* IR\/RF transmission sequences grouped by appliance\\n* mappings of commands to transmission sequences, e.g. \"Turn On TV\" --> specific IR transmission sequence to turn on TV set\\n### Universal Remote Controller\\n* ESP8266 system with IR and RF transmitter\\n* has a baked-in unique URCID (from DS2401)\\n* upon startup establishes websocket connection to AWS API Gateway (passing its URCID), and keeping that connection permanently alive\\n* receives and executes commands that AWS Lambda sends via AWS API Gateway\\n","tokens":16,"id":921,"Predictions":"Decision 121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212"}
{"File Name":"universal-remote-control-skill\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":922,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"Predictions":"We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.nbr>We will move to AWS.n"}
{"File Name":"terraform\/OS-Patching.md","Context":"## Context\\nAny operating systems that we use are likely to be patched for bug-fixes or security reasons at least once per month. Ideally we would pick up the latest stable release all the time, however this implies a great deal of churn. Such change would also come in spikes of activity e.g. around Windows patch tuesdays.\\n","Decision":"We will not patch our operating systems in AWS. Instead we will pin every instance to a specific version of an operating system and move the pin (via a PR) only when necessary.\\n","tokens":69,"id":924,"Predictions":"Decision (A) The patching schedule should be flexible. (B) The patching schedule should be flexible. (C) The patching schedule should be flexible. (D) The patching schedule should be flexible. (A) The patching schedule should be flexible. (A) The patching schedule should be flexible. (B) The patching schedule should be flexible. (C) The patching schedule should be flexible. (D) The patching schedule should be flexible. (A) The patching schedule should be flexible. (A) The patching schedule should be flexible. (A) The patching schedule should be flexible. (B) The patching schedule should be flexible. (C) The patching schedule should be flexible. (D) The patching schedule should be flexible. (A) The patching schedule should be flexible. (A) The patching schedule should be flexible. (B) The patching schedule should be flexible. (C) The patching schedule should be flexible. (D) The patching schedule should be flexible. (A) The patching schedule should be flexible. (A) The patching schedule should be flexible. (B) The patching schedule should be flexible. (C) The patching schedule should be flexible. (D) The patching schedule should be flexible. (A) The patching schedule should be flexible. (A) The patching schedule should be flexible. (B) The patching schedule should be flexible. (C) The patching schedule should be flexible."}
{"File Name":"terraform\/Scale-and-consolidate.md","Context":"## Context\\nWe are moving from platforms such as VMWare, where instances are kept small and are encouraged to scale out. If we were to scale up instead, then we would require either\\n1. A workload that is able to make efficient use of multi-threading.\\n1. Workloads that are able to co-exist peacefully.\\nIt cannot be assumed that the workloads we are moving into AWS will fit neatly into either category.\\n","Decision":"Our preference for enterprise applications is to consolidate onto a smaller number of instances.\\n","tokens":91,"id":925,"Predictions":"Decision (A). The workloads we are moving into AWS will fit neatly into either category.n1n2n3n4n5n6n7n8n9n9n9n8n9n9n8n9n9n8n9n8n9n8n9n8n9n8n9n8n9n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8n8nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"terraform\/Network-Controls.md","Context":"## Context\\n- Windows Defender is active by default in the Windows Server 2019 AMIs shipped by AWS. Although there is a good deal of overlap between the functionality of AWS Security Groups and Windows Defender, AWS has decided to keep Defender active for one specific reason, namely; \"the Windows firewall gives you audit details about packet drops, which may be important to meet your security policy or compliance requirements\". There is currently no such requirement in RI Tech.\\n- Windows Defender is difficult, but not impossible, to configure in code.\\n- AWS Security Groups are relatively easy to configure in code.\\n- When Defender is in use, the best way to work out what it is doing, is to log into the instance as Administrator and use the Defender GUI, but logging in as Administrator is strongly discouraged.\\n","Decision":"Windows Defender is to be de-activated on all Windows Servers in AWS.\\n","tokens":163,"id":926,"Predictions":"Decision (A).n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n- The AWS Security Groups are not compatible with Windows Defender.n"}
{"File Name":"terraform\/SSH-and-RDP.md","Context":"## Context\\nWe currently use RDP for Windows Server and SSH for Linux to connect to the servers to be migrated, when we have server administration tasks to perform. However, if EC2 instances are stateless, then we would not need to log into these servers, if they are sick, then then can be terminated.\\n","Decision":"RDP and SSH are allowed for all non-production EC2 istances, but are left blocked for production EC2 instances.\\n","tokens":66,"id":927,"Predictions":"Decision (A). The EC2 instances should be terminated.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"terraform\/Amazon-Linux.md","Context":"## Context\\nWe are migrating from several different versions of Windows and Linux in DXC to AWS. The easiest migration path would be to move applications and databases to exactly the same platform in AWS. The lowest TCO for the target solution would be provided by migrating onto a small number of consolidated Linux platforms (assuming we can't go serverless for everything). These two options are mutually exclusive, so we need a decision on the way forward.\\n","Decision":"Our strong preference is to go serverless. Where we can't go serverless we prefer to use the latest AWS Linux, but accept that the choice of OS is often dictated by the application and\/or database layers.\\n","tokens":89,"id":928,"Predictions":"Decision based on the information in this passage.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"terraform\/Automation.md","Context":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"Predictions":"Decision (A) To handle the maintenance of mature services. (B) To handle the maintenance of mature services. (C) To handle the maintenance of mature services. (D) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (B) To handle the maintenance of mature services. (C) To handle the maintenance of mature services. (D) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (B) To handle the maintenance of mature services. (C) To handle the maintenance of mature services. (D) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (B) To handle the maintenance of mature services. (C) To handle the maintenance of mature services. (D) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (B) To handle the maintenance of mature services. (C) To handle the maintenance of mature services. (D) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (A) To handle the maintenance of mature services. (B) To handle the maintenance of mature services. (C) To handle the maintenance of mature services."}
{"File Name":"postcode-checker\/0001-use-postgres-database.md","Context":"## Context\\nOur postcode checker uses a database to store valid service areas and allowed post codes.\\nCurrently our requirements involve looking up a postcode using the [postcodes.io](https:\/\/postcodes.io).\\nIn future we may want to build features that involve more sophisticated geolocation capabilities. Most databases do not support geolocation natively.\\nPostgres is the most geolocation capable SQL database. In future we can enable the PostGIS extension.\\n","Decision":"We will use Postgres for our database.\\n","tokens":91,"id":930,"Predictions":"Decision (A) To enable PostGIS extension. (B) To enable PostGIS extension. (C) To enable PostGIS extension. (D) To enable PostGIS extension. (A) To enable PostGIS extension. (A) To enable PostGIS extension. (A) To enable PostGIS extension. (B) To enable PostGIS extension. (C) To enable PostGIS extension. (D) To enable PostGIS extension. (A) To enable PostGIS extension. (A) To enable PostGIS extension. (A) To enable PostGIS extension. (B) To enable PostGIS extension. (C) To enable PostGIS extension. (D) To enable PostGIS extension. (A) To enable PostGIS extension. (A) To enable PostGIS extension. (A) To enable PostGIS extension. (B) To enable PostGIS extension. (A) To enable PostGIS extension. (B) To enable PostGIS extension. (C) To enable PostGIS extension. (D) To enable PostGIS extension. (A) To enable PostGIS extension. (A) To enable PostGIS extension. (B) To enable PostGIS extension. (A) To enable PostGIS extension. (B) To enable PostGIS extension. (C) To enable PostGIS extension. (D) To enable PostGIS extension."}
{"File Name":"inato-anticovid\/2020_04_02_environments_management.md","Context":"## Context\\nThroughout the process of development and release, the deployed components, codebase and data may vary in stability and quality (despite all used effort by the teams \ud83d\ude1b). Also, since several teams with different use cases work on the same project, potentially at the same time, there might be collision(s) in development and integration.\\n","Decision":"In order to ensure that only stable and quality code is shipped to the public viewer (app visitor), we will use several environments. This will allow siloting the code updates in a controlled manner.\\nWe will use the following environment:\\n- **Production**: Visitor facing environment. **It must be stable**, i.e. code and data shipped to production must be of the best quality and validated by product team.\\n- **Staging**: Internal purposed environment. _Components, code and data therein might be unstable_ and thus could potentially not reflect what would be deployed to production. This is the environment used by the product team to validate new improvements, fix and so... Since there is \"only\" one environment of that kind, several updates might concurrently live there.\\n- **Local**: Development purposed environment. Each developer (independently of its team) might use such an environment while developing. Since our stack involves components on GCP and Algolia, the following has been decided: GCP CloudFunctions can be ran locally (refer to `README.md` for usage); Algolia indices can only exist in Algolia (i.e. SAAS), thus one must create its own (i.e. developer bound) index (refer to `functions\/README.md` for usage) and refer to it locally by updating `algolia.index` in the `functions\/.runtimeconfig.json` file.\\n","tokens":70,"id":931,"Predictions":"Decision (A).nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tech-team\/0009-supported-python-versions-containers.md","Context":"## Context\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nWe previously have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\nPython official images make it easy to support a new Python version without custom PPAs.\\n","Decision":"We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\nWe will use Python 3.6 as our default supported version for any containerized project that solely uses or supports Python 3.\\nWe will use Python 3.5 as our default supported version for any project that is not containerized at the moment.\\n","tokens":98,"id":932,"Predictions":"We will support the new Python version of the project.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tech-team\/0003-rds-postgresql-permissions.md","Context":"## Context\\nAWS RDS are managed database servers.\\neLife uses RDS to alleviate the task of database management tasks like\\ndistribution, fault tolerance, monitoring.\\neLife uses PostgreSQL RDS instances for several projects.\\neLife provisions these RDS instances, their databases and database users using\\nCloudformation.\\nBecause RDS is a *managed* database server, there are constraints in it's usage.\\nCommonly encountered constraints are:\\n* no ssh access to the machine(s) hosting the database server\\n* no 'root' user access within the database\\n* [a similar-to-but-not-really 'root' user called 'rds_superuser'](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/UsingWithRDS.MasterAccounts.html)\\nDuring the life of an application, the database provisioned by Cloudformation\\nmay be deleted and re-created, losing the permissions the original database was\\ncreated and successfully provisioned with.\\nIf the new owner of the database is not the 'root' user or a member of the 'rds_superuser' role, some non-application activities such as backups and testing may fail with permission errors.\\nRe-instating permissions in a PostgreSQL database is an involved process.\\n","Decision":"The RDS *root* user provisioned shall always be the `owner` of the application database.\\nThe RDS *application* user will have enough permissions to read and write to the application database.\\nThe RDS *application* user will not have permission to drop\/re-create the application database *in continuumtest, end2end and prod environments*.\\n","tokens":252,"id":933,"Predictions":"Decision (A).nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the architecture of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRevision of the RDS instance.nRe"}
{"File Name":"tech-team\/0008-civi.md","Context":"## Context\\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\\n","Decision":"We will push responsibilities away from CiviCRM, favoring [Separate Ways](http:\/\/culttt.com\/2014\/11\/26\/strategies-integrating-bounded-contexts\/) or API-based integration of an external microservice over developing inside CiviCRM.\\n","tokens":80,"id":934,"Predictions":"The system is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.nCiviCRM is not suitable for the purpose of this passage.n"}
{"File Name":"tech-team\/0004-supported-python-versions.md","Context":"## Context\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nWe have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\n","Decision":"We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\nWe will use Python 3.5 as our default supported version for any project that solely uses or supports Python 3.\\n","tokens":79,"id":935,"Predictions":"We will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Python programming language](https:\/\/www.python.org\/).nWe will support the [Py"}
{"File Name":"tech-team\/0005-supported-php-versions.md","Context":"## Context\\neLife has several microservices (and larger projects) written in PHP.\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nThere is an exception, `crm` not being upgraded on PHP 7 (using 5.6 instead) but supporting it.\\nStaying up to date with the major and minor versions of PHP is important:\\n- to be able to use new features and libraries (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\nAll infrastructure is currently based on:\\n- Ubuntu 14.04 (doesn't have PHP 7.x by default)\\n- Ubuntu 16.04 (does have PHP 7.0 by default)\\n- a popular [PPA](https:\/\/launchpad.net\/~ondrej\/+archive\/ubuntu\/php) filling in the blanks, supporting 7.0, 7.1, and 7.2.\\n- official [PHP Docker images](https:\/\/hub.docker.com\/_\/php\/) supporting 5.6, 7.0, 7.1, and 7.2.\\nPHP 7.0 has ceased active support, but has [security support](http:\/\/php.net\/supported-versions.php) until 2018-12-03.\\n","Decision":"We will use PHP 7.0 on all existing and new PHP projects bar exceptions that do not support it (`crm` if needed).\\nWe will upgrade to PHP 7.1, PHP 7.2 and similar minor versions as a concerted effort on all libraries and projects, before the end of the security support.\\n","tokens":287,"id":936,"Predictions":"The project will be supported on PHP 7.0 and 5.6.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tech-team\/0002-use-containers-for-foreign-languages.md","Context":"## Context\\neLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.\\nThere are tools that are peculiar to our infrastructure, such as [goaws](https:\/\/github.com\/p4tin\/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).\\nThere are also tools that were originally written in another language but are being adopted by us, like [INK](https:\/\/gitlab.coko.foundation\/INK\/ink-api) for document conversion, written in Ruby.\\nThese tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.\\nSome tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.\\n","Decision":"We will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.\\n","tokens":183,"id":937,"Predictions":"Decision (A) To use Java as source code. (B) To use Java as binaries. (C) To use Java as binaries. (D) To use Java as binaries. (A) To use Java as binaries. (A) To use Java as binaries. (B) To use Java as binaries. (C) To use Java as binaries. (D) To use Java as binaries. (A) To use Java as binaries. (A) To use Java as binaries. (A) To use Java as binaries. (B) To use Java as binaries. (C) To use Java as binaries. (D) To use Java as binaries. (A) To use Java as binaries. (A) To use Java as binaries. (B) To use Java as binaries. (C) To use Java as binaries. (D) To use Java as binaries. (A) To use Java as binaries. (A) To use Java as binaries. (B) To use Java as binaries. (A) To use Java as binaries. (B) To use Java as binaries. (C) To use Java as binaries. (D) To use Java as binaries. (A) To use Java as binaries. (A) To use Java as binaries. (B) To use Java as binaries. (C) To use Java as binaries."}
{"File Name":"tech-team\/0001-pull-request-workflow.md","Context":"## Context\\nSoftware is built incrementally as an accumulation of changes.\\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\\nShort-lived pull requests allow:\\n- visibility of who is changing what.\\n- discussion and review from other people in the team.\\n- automated testing and other kind of checks to run, offloading work from humans to machines.\\n","Decision":"We will open short-lived pull requests as the primary means for deploying a change.\\n","tokens":85,"id":938,"Predictions":"Decision (A) To allow short-lived pull requests. (B) To allow automated testing and other kind of checks to run. (C) To allow automated testing and other kind of checks to run. (D) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (B) To allow automated testing and other kind of checks to run. (C) To allow automated testing and other kind of checks to run. (D) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (B) To allow automated testing and other kind of checks to run. (C) To allow automated testing and other kind of checks to run. (D) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (A) To allow automated testing and other kind of checks to run. (B) To allow automated testing and other kind of checks to run. (C) To allow automated testing and other kind of checks to run."}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"Predictions":"Decision (A) To support the major and minor versions of Node. (B) To support the major and minor versions of Node. (C) To support the minor and major versions of Node. (D) To support the major and minor versions of Node. (E) To support the minor and minor versions of Node. (F) To support the major and minor versions of Node. (G) To support the minor and minor versions of Node. (H) To support the major and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the major and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the major and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the major and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the major and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the minor and minor versions of Node. (H) To support the minor and minor versions of Node."}
{"File Name":"summits\/0003-use-java-newer-than-java-8.md","Context":"## Context\\nWe use Java 8 in all projects so far. However the most current version of Java as of now is version 13.\\n","Decision":"Use Java 13 to start the Java version of these services.\\n","tokens":30,"id":940,"Predictions":"The most current version of Java is 13.nbr>nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>Java 8 is the most current version of Java as of now.nbr>"}
{"File Name":"summits\/0002-use-docker-as-runtime-for-services.md","Context":"## Context\\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\\nDocker is the de-facto standard today and is the system used by us for all projects.\\n","Decision":"Use docker to deploy all services.\\n","tokens":53,"id":941,"Predictions":"Decide whether to use Docker or Docker.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"summits\/0004-move-services-to-top-level.md","Context":"## Context\\nInitially the directory structure was to have the language as a top folder and have services below. However this makes it harder to use different language versions of the services together.\\n","Decision":"Keep a monorepo for easier handling the project (it will never be grow too large and will not be maintained by different teams).\\nMove the services to top level. And have language versions below each service.\\n","tokens":37,"id":942,"Predictions":"The directory structure was not a good fit for the language.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"summits\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":943,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"edgex-docs\/0001-Registy-Refactor.md","Context":"- [Context](#context)\\n- [Proposed Design](#proposed-design)\\n- [Decision](#decision)\\n- [Consequences](#consequences)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [Consequences](#consequences)\\n- [References](#references)\\n<!--te-->\\nIt was decided to move forward with the above design\\nAfter initial ADR was approved, it was decided to retain the `-r\/--registry` command-line flag and not add the `Enabled` field in the Registry provider configuration.\\n","tokens":51,"id":944,"Predictions":"Decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ##"}
{"File Name":"edgex-docs\/0015-in-cluster-tls.md","Context":"## Context\\nThis ADR seeks to define the EdgeX direction on using encryption\\nto secure \"in-cluster\" EdgeX communications, that is,\\ninternal microservice-to-microservice communication.\\nThis ADR will seek to clarify the EdgeX direction\\nin several aspects with regard to:\\n- EdgeX services communicating within a single host\\n- EdgeX services communicating across multiple hosts\\n- Using encryption for confidentiality or integrity in communication\\n- Using encryption for authentication between microservices\\nThis ADR will be used to triage EdgeX feature requests in this space.\\n","Decision":"At this time, EdgeX is primarily a single-node IoT application framework.\\nShould this position change, this ADR should be revisited.\\nBased on the single-node assumption:\\n- TLS will not be used for confidentiality and integrity of internal on-host microservice communication.\\n- TLS will be avoided as an authentication mechanism of peer microservices.\\n- Integrity and confidentiality of microservice communcations crossing host boundaries is required to secure EdgeX, but are an EdgeX customer responsibility.\\n- EdgeX customers are welcome to add extra security to their own EdgeX deployments.\\n","tokens":118,"id":945,"Predictions":"Decision (A).nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A.nThe ADR will be used to triage EdgeX feature requests in this space.nAnswer:A."}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"Predictions":"Decision ## Final decision - [Existing Behavior] (#existing-behavior) - [Device Services] (#device-services) - [Registry Client Interface Usage] (#registry-client-interface-usage) - [Core and Support Services] (#core-and-support-services) - [Security Proxy Setup] (#security-proxy-setup) - [History] (#history) - [Problem Statement] (#problem-statement) - [Decision] (#decision) - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#references) - [!--te-->n ## Decision - [References] (#re"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"Predictions":"Decision ## Final: The Secret Store for non-secure mode is not a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is a secret. The Secret Store for non-secure mode is "}
{"File Name":"edgex-docs\/0017-consul-security.md","Context":"## Context\\nThis ADR defines the motiviation and approach used to secure access\\nto the Consul component in the EdgeX architecture\\nfor *security-enabled configurations only*.\\nNon-secure configuations continue to use Consul in\\nanonymous read-write mode.\\nAs this Consul security feature requires Vault to function,\\nif `EDGEX_SECURITY_SECRET_STORE=false` and Vault is not present,\\nthe legacy behavior (unauthenticated Consul access) will be preserved.\\nConsul provides several services for the EdgeX architecture:\\n- Service registry (see ADR in references below)\\n- Service health monitoring\\n- Mutable configuration data\\nUse of the services provided by Consul is optional on a service-by-service basis.\\nUse of the registry is controlled by the `-r` or `--registry` flag provided to an EdgeX service.\\nUse of mutable configuration data is controlled by the `-cp` or `--configProvider` flag provided to an EdgeX service.\\nWhen Consul is enabled as a configuration provider,\\nthe `configuration.toml` is parsed into individual settings\\nand seeded into the Consul key-value store on the first start of a service.\\nConfiguration reads and writes are then done to Consul if it is specified as the configuration provider,\\notherwise the static `configuration.toml` is used.\\nWrites to the `[Writable]` section in Consul trigger per-service callbacks\\nnotifying the application of the changed data.\\nUpdates to non-`[Writable]` sections are parsed only once at startup\\nand require a service restart to take effect.\\nSince configuration data can affect the runtime behavior of services,\\ncompensating controls must be introduced in order to mitigate the risks introduced\\nby moving configuration from a static file into to an HTTP-accessible service with mutable state.\\nThe current practice is that Consul is exposed via unencrypted HTTP in anonymous read\/write mode\\nto all processes and EdgeX services running on the host machine.\\n","Decision":"Consul will be configured with access control list (ACL) functionality enabled,\\nand each EdgeX service will utilize a Consul access token to authenticate to Consul.\\nConsul access tokens will be requested from the Vault Consul secrets engine\\n(to avoid introducing additional bootstrapping secrets).\\nDNS will be disabled via configuration as it is not used in EdgeX.\\n**Consul Access Via API Gateway**\\nIn security enabled EdgeX, the API gateway will be configured to\\nproxy the Consul service over the `\/consul` path,\\nusing the `request-transformer` plugin\\nto add the global management token to incoming requests\\nvia the `X-Consul-Token` HTTP header.\\nThus, ability to access remote APIs also grants the ability\\nto modify Consul's key-value store.\\nAt this time, service access via API gateway is all-or-nothing,\\nbut this does not preclude future fine-grained authorization\\nat the API gateway layer to specific microservices, including Consul.\\nProxying of the Consul UI is problematic and there is no current solution,\\nwhich would involve proper balacing of the externally-visible URL,\\nthe path-stripping effect (or not) of the proxy,\\nConsul's `ui_content_path`,\\nand UI authentication\\n(the `request-transfomer` does not work on the UI).\\n","tokens":409,"id":949,"Predictions":"Decision (A) To allow Consul access to the EdgeX architecture.nB) To allow Consul access to the EdgeX architecture.nC) To allow Consul access to the EdgeX architecture.nD) To allow Consul access to the EdgeX architecture.nAnswer:A.B.C.2A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3A.B.C.3"}
{"File Name":"edgex-docs\/0011-DeviceService-Rest-API.md","Context":"## Context\\nThis ADR details the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier \"Device Service Functional Requirements\" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.\\n","Decision":"### Common endpoints\\nThe DS should provide the REST endpoints that are expected of all EdgeX microservices, specifically:\\n* *config*\\n* *metrics*\\n* *ping*\\n* *version*\\n### Callback\\n| Endpoint | Methods\\n| --- | ---\\n| *callback\/device* | `PUT` and `POST`\\n| *callback\/device\/name\/{name}* | `DELETE`\\n| *callback\/profile* | `PUT`\\n| *callback\/watcher* | `PUT` and `POST`\\n| *callback\/watcher\/name\/{name}* | `DELETE`\\n| parameter | meaning\\n| --- | ---\\n| *{name}* | the name of the device or watcher\\nThese endpoints are used by the Core Metadata service to inform the device service of metadata updates. Endpoints are defined for each of the objects of interest to a device service, ie Devices, Device Profiles and Provision Watchers. On receipt of calls to these endpoints the device service should update its internal state accordingly. Note that the device service does not need to be informed of the creation or deletion of device profiles, as these operations may only occur where no devices are associated with the profile. To avoid stale profile entries the device service should delete a profile from its cache when the last device using it is deleted.\\n#### Object deletion\\nWhen an object is deleted, the Metadata service makes a `DELETE` request to the relevant *callback\/{type}\/name\/{name}* endpoint.\\n#### Object creation and updates\\nWhen an object is created or updated, the Metadata service makes a `POST` or `PUT` request respectively to the relevant *callback\/{type}* endpoint. The payload of the request is the new or updated object, ie one of the Device, DeviceProfile or ProvisionWatcher DTOs.\\n### Device\\n| Endpoint | Methods\\n| --- | ---\\n| *device\/name\/{name}\/{command}* | `GET` and `PUT`\\n| parameter | meaning\\n| --- | ---\\n| *{name}* | the name of the device\\n| *{command}* | the command name\\nThe command specified must match a deviceCommand or deviceResource name in the device's profile\\n**body** (for `PUT`): An `application\/json` SettingRequest, which is a set of key\/value pairs where the keys are valid deviceResource names, and the values provide the command argument for that resource. Example: `{\"AHU-TargetTemperature\": \"28.5\", \"AHU-TargetBand\": \"4.0\"}`\\n| Return code | Meaning\\n| --- | ---\\n| **200** | the command was successful\\n| **404** | the specified device does not exist, or the command\/resource is unknown\\n| **405** | attempted write to a read-only resource\\n| **423** | the specified device is locked (admin state) or disabled (operating state)\\n| **500** | the device driver is unable to process the request\\n**response body**: A successful `GET` operation will return a JSON-encoded EventResponse object, which contains one or more Readings. Example: `{\"apiVersion\":\"v2\",\"deviceName\":\"Gyro\",\"origin\":1592405201763915855,\"readings\":[{\"deviceName\":\"Gyro\",\"name\":\"Xrotation\",\"value\":\"124\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"deviceName\":\"Gyro\",\"name\":\"Yrotation\",\"value\":\"-54\",\"origin\":1592405201763915855,\"valueType\":\"int32\"},{\"deviceName\":\"Gyro\",\"name\":\"Zrotation\",\"value\":\"122\",\"origin\":1592405201763915855,\"valueType\":\"int32\"}]}`\\nThis endpoint is used for obtaining readings from a device, and for writing settings to a device.\\n#### Data formats\\nThe values obtained when readings are taken, or used to make settings, are expressed as strings.\\n| Type | EdgeX types | Representation\\n| --- | --- | ---\\n| Boolean | `Bool` | \"true\" or \"false\"\\n| Integer | `Uint8-Uint64`, `Int8-Int64` | Numeric string, eg \"-132\"\\n| Float | `Float32`, `Float64` | Decimal with exponent, eg \"1.234e-5\"\\n| String | `String` | string\\n| Binary | `Bytes` | octet array\\n| Array | `BoolArray`, `Uint8Array-Uint64Array`, `Int8Array-Int64Array`, `Float32Array`, `Float64Array` | JSON Array, eg \"[\"1\", \"34\", \"-5\"]\"\\nNotes:\\n- The presence of a Binary reading will cause the entire Event to be encoded using CBOR rather than JSON\\n- Arrays of String and Binary data are not supported\\n#### Readings and Events\\nA Reading represents a value obtained from a deviceResource. It contains the following fields\\n| Field name | Description\\n| --- | ---\\n| *deviceName* | The name of the device\\n| *profileName* | The name of the Profile describing the Device\\n| *resourceName* | The name of the deviceResource\\n| *origin* | A timestamp indicating when the reading was taken\\n| *value* | The reading value\\n| *valueType* | The type of the data\\nOr for binary Readings, the following fields\\n| Field name | Description\\n| --- | ---\\n| *deviceName* | The name of the device\\n| *profileName* | The name of the Profile describing the Device\\n| *resourceName* | The name of the deviceResource\\n| *origin* | A timestamp indicating when the reading was taken\\n| *binaryValue* | The reading value\\n| *mediaType* | The MIME type of the data\\nAn Event represents the result of a `GET` command. If the command names a deviceResource, the Event will contain a single Reading. If the command names a deviceCommand, the Event will contain as many Readings as there are deviceResources listed in the deviceCommand.\\nThe fields of an Event are as follows:\\n| Field name | Description\\n| --- | ---\\n| *deviceName* | The name of the Device from which the Readings are taken\\n| *profileName* | The name of the Profile describing the Device\\n| *origin* | The time at which the Event was created\\n| *readings* | An array of Readings\\n#### Query Parameters\\nCalls to the device endpoints may include a Query String in the URL. This may be used to pass parameters relating to the request to the device service. Individual device services may define their own parameters to control specific behaviors. Parameters beginning with the prefix `ds-` are reserved to the Device SDKs and the following parameters are defined for GET requests:\\n| Parameter | Valid Values      | Default | Meaning\\n| --- |-------------------|---------| ---\\n| *ds-pushevent* | \"true\" or \"false\" | \"false\" | If set to true, a successful `GET` will result in an event being pushed to the EdgeX system\\n| *ds-returnevent* | \"true\" or \"false\" | \"true\"  | If set to false, there will be no Event returned in the http response\\n!!! edgey \"EdgeX 3.0\"\\nThe valid values of **ds-pushevent** and **ds-returnevent** is changed to `true\/false` instead of `yes\/no` in EdgeX 3.0.\\n#### Device States\\nA Device in EdgeX has two states associated with it: the Administrative state and the Operational state. The Administrative state may be set to `LOCKED` (normally `UNLOCKED`) to block access to the device for administrative reasons. The Operational state may be set to `DOWN` (normally `UP`) to indicate that the device is not currently working. In either case access to the device via this endpoint will be denied and HTTP 423 (\"Locked\") will be returned.\\n#### Data Transformations\\nA number of simple data transformations may be defined in the deviceResource. The table below shows these transformations in the order in which they are applied to outgoing data, ie Readings. The transformations are inverted and applied in reverse order for incoming data.\\n| Transform | Applicable reading types | Effect\\n| --- | --- | ---\\n**mask** | Integers | The reading is masked (bitwise-and operation) with the specified value.\\n**shift** | Integers | The reading is bit-shifted by the specified value. Positive values indicate right-shift, negative for left.\\n**base** | Integers and Floats | The reading is replaced by the specified value raised to the power of the reading.\\n**scale** | Integers and Floats | The reading is multiplied by the specified value.\\n**offset** | Integers and Floats | The reading is increased by the specified value.\\nThe operation of the **mask** transform on incoming data (a setting) is that the value to be set on the resource is the existing value bitwise-anded with the complement of the mask, bitwise-ored with the value specified in the request.\\nie, `new-value = (current-value & !mask) | request-value`\\nThe combination of mask and shift can therefore be used to access data contained in a subdivision of an octet.\\nIt is possible that following the application of the specified transformations, a value may exceed the range that may be represented by its type. Should this occur on a set operation, a suitable error should be logged and returned, along with the `Bad Request` http code 400. If it occurs as part of a get operation, the Reading's value should be set to the String `\"overflow\"` and its valueType to `String`.\\n#### Assertions and Mappings\\nAssertions are another attribute in a device resource's PropertyValue, which specify a string which the reading value is compared against. If the comparison fails, then the http request returns a string of the form *\"Assertion failed for device resource: \\<name>, with value: \\<value>\"*, this also has a side-effect of setting the device operatingstate to `DISABLED`. A 500 status code is also returned. Note that the error response and status code should be returned regardless of the `ds-returnevent` setting.\\nAssertions are also checked where an event is being generated due to an AutoEvent, or asynchronous readings are pushed. In these cases if the assertion is triggered, an error should be logged and the operating state should be set as above.\\nAssertions are not checked for settings, only for readings.\\nMappings may be defined in a deviceCommand. These allow Readings of string type to be remapped. Mappings are applied after assertions are checked, and are the final transformation before Readings are created. Mappings are also applied, but in reverse, to settings (`PUT` request data).\\n#### lastConnected timestamp\\nEach Device has as part of its metadata a timestamp named `lastConnected`, this\\nindicates the most recent occasion when the device was successfully interacted\\nwith. The device service should update this timestamp every time a GET or PUT\\noperation succeeds, unless it has been configured not to do so (eg for\\nperformance reasons).\\n### Discovery\\n| Endpoint | Methods\\n| --- | ---\\n| *discovery* | `POST`\\nA call to this endpoint triggers the device discovery process, if enabled. See\\n[Discovery Design](..\/..\/legacy-design\/device-service\/discovery.md) for details.\\n","tokens":72,"id":950,"Predictions":"Decision (A) The REST API should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.nbr>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n\/br>n"}
{"File Name":"edgex-docs\/0016-docker-image-guidelines.md","Context":"## Context\\nWhen deploying the EdgeX Docker containers some security measures are recommended to ensure the integrity of the software stack.\\n","Decision":"When deploying Docker images, the following flags should be set for heightened security.\\n- To avoid escalation of privileges each docker container should use the `no-new-privileges` option in their Docker compose file (example below). More details about this flag can be found [here](https:\/\/docs.docker.com\/engine\/reference\/run\/#security-configuration). This follows Rule #4 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-4-add-no-new-privileges-flag).\\n```docker\\nsecurity_opt:\\n- \"no-new-privileges:true\"\\n```\\n> NOTE: Alternatively an AppArmor security profile can be used to isolate the docker container. More details about apparmor profiles can be found [here](https:\/\/docs.docker.com\/engine\/security\/apparmor\/)\\n```docker\\nsecurity_opt:  [ \"apparmor:unconfined\" ]\\n```\\n- To further prevent privilege escalation attacks the user should be set for the docker container using the `--user=<userid>` or `-u=<userid>` option in their Docker compose file (example below). More details about this flag can be found [here](https:\/\/docs.docker.com\/engine\/reference\/run\/#user). This follows Rule #2 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\\n```docker\\nservices:\\ndevice-virtual:\\nimage: ${REPOSITORY}\/docker-device-virtual-go${ARCH}:${DEVICE_VIRTUAL_VERSION}\\nuser: $CONTAINER-PORT:$CONTAINER-PORT # user option using an unprivileged user\\nports:\\n- \"127.0.0.1:49990:49990\"\\ncontainer_name: edgex-device-virtual\\nhostname: edgex-device-virtual\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-virtual\\ndepends_on:\\n- consul\\n- data\\n- metadata\\n```\\n> NOTE: exception\\nSometimes containers will require root access to perform their fuctions. For example the System Management Agent requires root access to control other Docker containers. In this case you would allow it run as default root user.\\n- To avoid a faulty or compromised containers from consuming excess amounts of the host of its resources `resource limits` should be set for each container. More details about `resource limits` can be found [here](https:\/\/docs.docker.com\/config\/containers\/resource_constraints\/). This follows Rule #7 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-7-limit-resources-memory-cpu-file-descriptors-processes-restarts).\\n```docker\\nservices:\\ndevice-virtual:\\nimage: ${REPOSITORY}\/docker-device-virtual-go${ARCH}:${DEVICE_VIRTUAL_VERSION}\\nuser: 4000:4000 # user option using an unprivileged user\\nports:\\n- \"127.0.0.1:49990:49990\"\\ncontainer_name: edgex-device-virtual\\nhostname: edgex-device-virtual\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-virtual\\ndepends_on:\\n- consul\\n- data\\n- metadata\\ndeploy:  # Deployment resource limits\\nresources:\\nlimits:\\ncpus: '0.001'\\nmemory: 50M\\nreservations:\\ncpus: '0.0001'\\nmemory: 20M\\n```\\n- To avoid attackers from writing data to the containers and modifying their files the `--read_only` flag should be set. More details about this flag can be found [here](https:\/\/docs.docker.com\/compose\/compose-file\/#domainname-hostname-ipc-mac_address-privileged-read_only-shm_size-stdin_open-tty-user-working_dir). This follows Rule #8 for Docker security found [here](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html#rule-8-set-filesystem-and-volumes-to-read-only).\\n```docker\\ndevice-rest:\\nimage: ${REPOSITORY}\/docker-device-rest-go${ARCH}:${DEVICE_REST_VERSION}\\nports:\\n- \"127.0.0.1:49986:49986\"\\ncontainer_name: edgex-device-rest\\nhostname: edgex-device-rest\\nread_only: true # read_only option\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-rest\\ndepends_on:\\n- data\\n- command\\n```\\n> NOTE: exception\\nIf a container is required to have write permission to function, then this flag will not work. For example, the vault needs to run setcap in order to lock pages in memory. In this case the `--read_only` flag will not be used.\\nNOTE: Volumes\\nIf writing persistent data is required then a volume can be used. A volume can be attached to the container in the following way\\n```docker\\ndevice-rest:\\nimage: ${REPOSITORY}\/docker-device-rest-go${ARCH}:${DEVICE_REST_VERSION}\\nports:\\n- \"127.0.0.1:49986:49986\"\\ncontainer_name: edgex-device-rest\\nhostname: edgex-device-rest\\nread_only: true # read_only option\\nnetworks:\\n- edgex-network\\nenv_file:\\n- common.env\\nenvironment:\\nSERVICE_HOST: edgex-device-rest\\ndepends_on:\\n- data\\n- command\\nvolumes:\\n- consul-config:\/consul\/config:z\\n```\\n> NOTE: alternatives\\nIf writing non-persistent data is required (ex. a config file) then a temporary filesystem mount can be used to accomplish this goal while still enforcing `--read_only`. Mounting a `tmpfs` in Docker gives the container a temporary location in the host systems memory to modify files. This location will be removed once the container is stopped. More details about `tmpfs` can be found [here](https:\/\/docs.docker.com\/storage\/tmpfs\/)\\nfor additional docker security rules and guidelines please check the Docker security [cheatsheet](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Docker_Security_Cheat_Sheet.html)\\n","tokens":25,"id":952,"Predictions":"Decision based on the information in this document.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"edgex-docs\/013-Device-Service-Events-Message-Bus.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n* [Which Message Bus implementations?](#which-message-bus-implementations)\\n* [Go Device SDK](#go-device-sdk)\\n* [C Device SDK](#c-device-sdk)\\n* [Core Data and Persistence](#core-data-and-persistence)\\n* [V2 Event DTO](#v2-event-dto)\\n+ [Validation](#validation)\\n* [Message Envelope](#message-envelope)\\n* [Application Services](#application-services)\\n* [MessageBus Topics](#messagebus-topics)\\n* [Configuration](#configuration)\\n+ [Device Services](#device-services)\\n- [[MessageQueue]](#messagequeue)\\n+ [Core Data](#core-data)\\n- [[MessageQueue]](#messagequeue)\\n+ [Application Services](#application-services)\\n- [[MessageBus]](#messagebus)\\n- [[Binding]](#binding)\\n* [Secure Connections](#secure-connections)\\n- [Consequences](#consequences)\\n","Decision":"* [Which Message Bus implementations?](#which-message-bus-implementations)\\n* [Go Device SDK](#go-device-sdk)\\n* [C Device SDK](#c-device-sdk)\\n* [Core Data and Persistence](#core-data-and-persistence)\\n* [V2 Event DTO](#v2-event-dto)\\n+ [Validation](#validation)\\n* [Message Envelope](#message-envelope)\\n* [Application Services](#application-services)\\n* [MessageBus Topics](#messagebus-topics)\\n* [Configuration](#configuration)\\n+ [Device Services](#device-services)\\n- [[MessageQueue]](#messagequeue)\\n+ [Core Data](#core-data)\\n- [[MessageQueue]](#messagequeue)\\n+ [Application Services](#application-services)\\n- [[MessageBus]](#messagebus)\\n- [[Binding]](#binding)\\n* [Secure Connections](#secure-connections)\\n- [Consequences](#consequences)\\n### Which Message Bus implementations?\\nMultiple Device Services may need to be publishing Events to the MessageBus concurrently.  `ZMQ` will not be a valid option if multiple Device Services are configured to publish. This is because `ZMQ` only allows for a single publisher. `ZMQ` will still be valid if only one Device Service is publishing Events. The `MQTT` and `Redis Streams` are valid options to use when multiple Device Services are required, as they both support multiple publishers. These are the only other implementations currently available for Go services. The C base device services do not yet have a MessageBus implementation.  See the [C Device SDK](#c-device-sdk) below for details.\\n> *Note: Documentation will need to be clear when `ZMQ` can be used and when it can not be used.*\\n### Go Device SDK\\nThe Go Device SDK will take advantage of the existing `go-mod-messaging` module to enable use of the EdgeX MessageBus. A new bootstrap handler will be created which initializes the MessageBus client based on configuration. See [Configuration](#configuration) section below for details.  The Go Device SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\\n### C Device SDK\\nThe C Device SDK will implement its own MessageBus abstraction similar to the one in `go-mod-messaging`.  The first implementation type (MQTT or Redis Streams) is TBD. Using this abstraction allows for future implementations to be added when use cases warrant the additional implementations.  As with the Go SDK, the C SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\\n### Core Data and Persistence\\nWith this design, Events will be sent directly to Application Services w\/o going through Core Data and thus will not be persisted unless changes are made to Core Data. To allow Events to optionally continue to be persisted, Core Data will become an additional or secondary (and optional) subscriber for the Events from the MessageBus. The Events will be persisted when they are received. Core Data will also retain the ability to receive Events via HTTP, persist them and publish them to the MessageBus as is done today. This allows for the flexibility to have some device services to be configured to POST Events and some to be configured to publish Events while we transition the Device Services to all have the capability to publishing Events. In the future, once this new `Publish` approach has been proven, we may decide to remove POSTing Events to Core Data from the Device SDKs.\\nThe existing `PersistData` setting will be ignored by the code path subscribing to Events since the only reason to do this is to persist the Events.\\nThere is a race condition for `Marked As Pushed` when Core Data is persisting Events received from the MessageBus. Core Data may not have finished persisting an Event before the Application Service has processed the Event and requested the Event be `Marked As Pushed`. It was decided to remove `Mark as Pushed` capability and just rely on time based scrubbing of old Events.\\n### V2 Event DTO\\nAs this development will be part of the Ireland release all Events published to the MessageBus will use the V2 Event DTO. This is already implemented in Core Data for the V2 AddEvent API.\\n#### Validation\\nServices receiving the Event DTO from the MessageBus will log validation errors and stop processing the Event.\\n### Message Envelope\\nEdgeX Go Services currently uses a custom Message Envelope for all data that is published to the MessageBus. This envelope wraps the data with metadata, which is `ContentType` (JSON or CBOR), `Correlation-Id` and the obsolete `Checksum`. The `Checksum` is used when the data is CBOR encoded to identify the Event in V1 API to be mark it as pushed. This checksum is no longer needed as the V2 Event DTO requires the ID be set by the Device Services which will always be used in the V2 API to mark the Events as pushed. The Message Envelope will be updated to remove this property.\\nThe C SDK will recreate this Message Envelope.\\n### Application Services\\nAs part of the V2 API consumption work in Ireland the App Services SDK will be changed to expect to receive V2 Event DTOs rather than the V1 Event model. It will also be updated to no longer expect or use the `Checksum` currently on the  Message Envelope. Note these changes must occur for the V2 consumption and are not directly tied to this effort.\\nThe App Service SDK will be enhanced for the secure MessageBus connection described below. See **[Secure Connections](#secure-connections)** for details\\n### MessageBus Topics\\n> *Note: The change recommended here is not required for this design, but it provides a good opportunity to adopt it.*\\nCurrently Core Data publishes Events to the simple `events` topic. All Application Services running receive every Event published, whether they want them or not. The Events can be filtered out using the `FilterByDeviceName` or `FilterByResourceName` pipeline functions, but the Application Services still receives every Event and process all the Events to some extent. This could cause load issues in a deployment with many devices and large volume of Events from various devices or a very verbose device that the Application Services is not interested in.\\n> *Note: The current `FilterByDeviceName` is only good if the device name is known statically and the only instance of the device defined by the `DeviceProfileName`. What we really need is `FilterByDeviceProfileName` which allows multiple instances of a device to be filtered for, rather than a single instance as it it now. The V2 API will be adding `DeviceProfileName` to the Events, so in Ireland this  filter will be possible.*\\nPub\/Sub systems have advanced topic schema, which we can take advantage of from Application Services to filter for just the Events the Application Service actual wants. Publishers of Events must add the `DeviceProfileName`, `DeviceName` and `SourceName` to the topic in the form `edgex\/events\/<device-profile-name>\/<device-name>\/<source-name>`. The `SourceName` is the `Resource` or `Command` name used to create the Event. This allows Application Services to filter for just the Events from the device(s) it wants by only subscribing to those `DeviceProfileNames` or the specific `DeviceNames` or just the specific `SourceNames`  Example subscribe topics if above schema is used:\\n- **edgex\/events\/#**\\n- All Events\\n- Core Data will subscribe using this topic schema\\n- **edgex\/events\/Random-Integer-Device\/#**\\n- Any Events from devices created from the **Random-Integer-Device** device profile\\n- **edgex\/events\/Random-Integer-Device\/Random-Integer-Device1**\\n- Only Events from the **Random-Integer-Device1** Device\\n- **edgex\/events\/Random-Integer-Device\/#\/Int16**\\n- Any Events with Readings from`Int16` device resource from devices created from the **Random-Integer-Device** device profile.\\n- **edgex\/events\/Modbus-Device\/#\/HVACValues\\n- Any Events with Readings from `HVACValues` device command from devices created from the **Modbus-Device** device profile.\\nThe MessageBus abstraction allows for multiple subscriptions, so an Application Service could specify to receive data from multiple specific device profiles or devices by creating multiple subscriptions. i.e.  `edgex\/Events\/Random-Integer-Device\/#` and  `edgex\/Events\/Random-Boolean-Device\/#`. Currently the App SDK only allows for a single subscription topic to be configured, but that could easily be expanded to handle a list of subscriptions. See [Configuration](#configuration) section below for details.\\nCore Data's existing publishing of Events would also need to be changed to use this new topic schema. One challenge with this is Core Data doesn't currently know the `DeviceProfileName` or `DeviceName` when it receives a CBOR encoded event. This is because it doesn't decode the Event until after it has published it to the MessageBus. Also, Core Data doesn't know of `SourceName` at all. The V2 API will be enhanced to change the AddEvent endpoint from `\/event` to `\/event\/{profile}\/{device}\/{source}` so that `DeviceProfileName`, `DeviceName`, and `SourceName` are always know no matter how the request is encoded.\\nThis new topic approach will be enabled via each publisher's `PublishTopic` having the `DeviceProfileName`, `DeviceName`and `SourceName`  added to the configured `PublishTopicPrefix`\\n```toml\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\n```\\nSee [Configuration](#configuration) section below for details.\\n### Configuration\\n#### Device Services\\nAll Device services will have the following additional configuration to allow connecting and publishing to the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopic` will include the `DeviceProfileName` and `DeviceName`.\\n##### [MessageQueue]\\nA  MessageQueue section will be added, which is similar to that used in Core Data today, but with `PublishTopicPrefix` instead of `Topic`.To enable secure connections, the `Username` & `Password` have been replaced with ClientAuth & `SecretPath`, See **[Secure Connections](#secure-connections)** section below for details. The added `Enabled` property controls whether the Device Service publishes to the MessageBus or POSTs to Core Data.\\n```toml\\n[MessageQueue]\\nEnabled = true\\nProtocol = \"tcp\"\\nHost = \"localhost\"\\nPort = 1883\\nType = \"mqtt\"\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\n[MessageQueue.Optional]\\n# Default MQTT Specific options that need to be here to enable environment variable overrides of them\\n# Client Identifiers\\nClientId =\"<device service key>\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n#### Core Data\\nCore data will also require additional configuration to be able to subscribe to receive Events from the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopicPrefix` will have `DeviceProfileName` and `DeviceName` added to create the actual Public Topic.\\n##### [MessageQueue]\\nThe `MessageQueue` section will be  changed so that the `Topic` property changes to `PublishTopicPrefix` and `SubscribeEnabled` and `SubscribeTopic` will be added. As with device services configuration, the `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details. In addition, the Boolean `SubscribeEnabled` property will be used to control if the service subscribes to Events from the MessageBus or not.\\n```toml\\n[MessageQueue]\\nProtocol = \"tcp\"\\nHost = \"localhost\"\\nPort = 1883\\nType = \"mqtt\"\\nPublishTopicPrefix = \"edgex\/events\" # \/<device-profile-name>\/<device-name>\/<source-name> will be added to this Publish Topic prefix\\nSubscribeEnabled = true\\nSubscribeTopic = \"edgex\/events\/#\"\\n[MessageQueue.Optional]\\n# Default MQTT Specific options that need to be here to enable evnironment variable overrides of them\\n# Client Identifiers\\nClientId =\"edgex-core-data\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n#### Application Services\\n##### [MessageBus]\\nSimilar to above, the Application Services `MessageBus` configuration will change to allow for secure connection to the MessageBus. The `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details.\\n```toml\\n[MessageBus.Optional]\\n# MQTT Specific options\\n# Client Identifiers\\nClientId =\"<app sevice key>\"\\n# Connection information\\nQos          =  \"0\" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\\nKeepAlive    =  \"10\" # Seconds (must be 2 or greater)\\nRetained     = \"false\"\\nAutoReconnect  = \"true\"\\nConnectTimeout = \"5\" # Seconds\\nSkipCertVerify = \"false\" # Only used if Cert\/Key file or Cert\/Key PEMblock are specified\\nClientAuth = \"none\" # Valid values are: `none`, `usernamepassword` or `clientcert`\\nSecretpath = \"messagebus\"  # Path in secret store used if ClientAuth not `none`\\n```\\n##### [Binding]\\nThe `Binding` configuration section will require changes for the subscribe topics scheme described in the [MessageBus Topics](#messagebus-topics) section above to filter for Events from specific device profiles or devices. `SubscribeTopic` will change from a string property containing a single topic to the `SubscribeTopics` string property containing a comma separated list of topics. This allows for the flexibility for the property to be a single topic with the `#` wild card so the Application Service receives all Events as it does today.\\nReceive only Events from the `Random-Integer-Device` and `Random-Boolean-Device` profiles\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/Random-Integer-Device, edgex\/events\/Random-Boolean-Device\"\\n```\\nReceive only Events from the  `Random-Integer-Device1` from the `Random-Integer-Device` profile\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/Random-Integer-Device\/Random-Integer-Device1\"\\n```\\nor receives all Events:\\n```toml\\n[Binding]\\nType=\"messagebus\"\\nSubscribeTopics=\"edgex\/events\/#\"\\n```\\n### Secure Connections\\nAs stated earlier,  this ADR is dependent on the  **Secret Provider for All**(Link TBD) ADR to provide a common Secret Provider for all Edgex Services to access their secrets. Once this is available, the MessageBus connection can be secured via the following configurable client authentications modes which follows similar implementation for secure MQTT Export and secure MQTT Trigger used in Application Services.\\n- **none** - No authentication\\n- **usernamepassword** - Username & password authentication.\\n- **clientcert** - Client certificate and key for authentication.\\n- The secrets specified for the above options are pulled from the `Secret Provider` using the configured `SecretPath`.\\nHow the secrets are injected into the `Secret Provider` is out of scope for this ADR and covered in the **Secret Provider for All**( Link TBD) ADR.\\n","tokens":227,"id":954,"Predictions":"[]n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "}
{"File Name":"edgex-docs\/0009-Secure-Bootstrapping.md","Context":"* [Context](#context)\\n+ [History](#history)\\n* [Decision](#decision)\\n+ [Stage-gate mechanism](#stage-gate-mechanism)\\n+ [Docker-specific service changes](#docker-specific-service-changes)\\n- [\"As-is\" startup flow](#as-is-startup-flow)\\n- [\"To-be\" startup flow](#to-be-startup-flow)\\n- [New Bootstrap\/RTR container](#new-bootstraprtr-container)\\n* [Consequences](#consequences)\\n+ [Benefits](#benefits)\\n+ [Drawbacks](#drawbacks)\\n* [Alternatives](#alternatives)\\n+ [Event-driven vs commanded staging](#event-driven-vs-commanded-staging)\\n+ [System management agent (SMA) as the coordinator](#system-management-agent-sma-as-the-coordinator)\\n+ [Create a mega-install container](#create-a-mega-install-container)\\n+ [Manual secret provisioning](#manual-secret-provisioning)\\n* [References](#references)\\n","Decision":"+ [Stage-gate mechanism](#stage-gate-mechanism)\\n+ [Docker-specific service changes](#docker-specific-service-changes)\\n- [\"As-is\" startup flow](#as-is-startup-flow)\\n- [\"To-be\" startup flow](#to-be-startup-flow)\\n- [New Bootstrap\/RTR container](#new-bootstraprtr-container)\\n* [Consequences](#consequences)\\n+ [Benefits](#benefits)\\n+ [Drawbacks](#drawbacks)\\n* [Alternatives](#alternatives)\\n+ [Event-driven vs commanded staging](#event-driven-vs-commanded-staging)\\n+ [System management agent (SMA) as the coordinator](#system-management-agent-sma-as-the-coordinator)\\n+ [Create a mega-install container](#create-a-mega-install-container)\\n+ [Manual secret provisioning](#manual-secret-provisioning)\\n* [References](#references)\\n### Stage-gate mechanism\\nThe stage-gate mechanism must work in the following environments:\\n* docker-compose in Linux on a single node\/system\\n* docker-compose in Microsoft* Windows* on a single node\/system\\n* docker-compose in Apple* MacOS* on a single node\/system\\nStartup sequencing will be driven by two primary mechanisms:\\n1. Use of entrypoint scripts to:\\n- Block on stage-gate and service dependencies\\n- Perform first-boot initialization phase activities as noted in [Context](#context)\\nThe bootstrap container will inject entrypoint scripts into\\nthe other containers in the case where EdgeX is directly consuming\\nan upstream container.  Docker will automatically retry\\nrestarting containers if its entrypoint script is missing.\\n2. Use of open TCP sockets as semaphores to gate startup sequencing\\nUse of TCP sockets for startup sequencing is commonly used in Docker environments.\\nDue to its popularlity, there are several existing tools for this, including\\n[wait-for-it](https:\/\/github.com\/vishnubob\/wait-for-it),\\n[dockerize](https:\/\/github.com\/jwilder\/dockerize), and\\n[wait-for](https:\/\/github.com\/Eficode\/wait-for).\\nThe TCP mechanism is portable across platforms\\nand will work in distributed multi-node scenarios.\\nAt least three new ports will be added to EdgeX for sequencing purposes:\\n* `bootstrap` port. This port will be opened once first-time initialization has been completed.\\n* `tokens_ready` port. This port signals that secret-store tokens have been provisioned and are valid.\\n* `ready_to_run` port. This port will be opened once stateful services have completed initialization\\nand it is safe for the majority of EdgeX core services to start.\\nThe stateless EdgeX services should block on `ready_to_run` port.\\n### Docker-specific service changes\\n#### \"As-is\" startup flow\\nThe following diagram shows the \"as-is\" startup flow.\\n![AS-IS](.\/bootstrap-as-is.jpg)\\nThere are several components being removed via activity unrelated with this ADR.\\nThese proposed edits are shown to reduce clutter in the TO-BE diagram.\\n* secrets-setup is being eliminated through a separate ADR to eliminate TLS for single-node usage.\\n* kong-migrations is being combined with the kong service via an entrypoint script.\\n* bootstrap-redis will be incorporated into the Redis entrypoint script to set the Redis password before Redis starts to fix the time delay before a Redis password is set.\\n#### \"To-be\" startup flow\\nThe following diagram shows the \"to-be\" startup flow.\\nNote that the bootstrap flows are always processed,\\nbut can be short-circuited.\\n![TO-BE](.\/bootstrap-to-be.jpg)\\nAnother difference to note in the \"to-be\" diagram is that\\nthe Vault depdendency on Consul is reversed\\n[in order to provide better security](https:\/\/github.com\/edgexfoundry\/edgex-docs\/issues\/296).\\n#### New Bootstrap\/RTR container\\nThe purpose of this new container is to:\\n* Inject entrypoint scripts into third-party containers\\n(such as Vault, Redis, Consul, PostgreSQL, Kong)\\nin order to perform first-time initialization and\\nwait on service dependencies\\n* Raise the `bootstrap` semaphore\\n* Wait on dependent semaphores required to raise the `ready_to_run` semaphore\\n(these are the stateful components such as databases,\\nand blocking waiting for sercret store tokens to be provisioned)\\n* Raise the `ready_to_run` semaphore\\n* Wait forever (in order to leave TCP sockets open)\\n","tokens":225,"id":955,"Predictions":"Decision ## Final. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism. The stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage-gate mechanism is Stage"}
{"File Name":"edgex-docs\/0020-spiffe.md","Context":"## Context\\nIn security-enabled EdgeX,\\nthere is a component called `security-secretstore-setup`\\nthat seeds authentication tokens for Hashicorp Vault--EdgeX's secret store--into\\ndirectories reserved for each EdgeX microservice.\\nThe implementation is provided by a sub-component, `security-file-token-provider`,\\nthat works off of a static configuration file (`token-config.json`)\\nthat configures known EdgeX services,\\nand an environment variable that lists additional services that require tokens.\\nThe token provider creates a unique token for each service\\nand attaches a custom policy to each token that limits token access\\nin a manner that paritions the secret store's namespace.\\nThe current solution has some problematic aspects:\\n* These tokens have an initial TTL of one hour (1h)\\nand become invalid if not used and renewed within that time period.\\nIt is not possible to delay the start of EdgeX services until a later time\\n(that is, greater than the default token TTL),\\nas they will not be able to connect to the EdgeX secret store\\nto obtain required secrets.\\n* Transmission of the authentication token requires one or more shared file systems\\nbetween the service and `security-secretstore-setup`.\\nIn the Docker implementation,\\nthis shared file system is constructed by bind-mounting a host-based directory\\nto multiple containers.\\nThe snap implementation is similar, utilizing a content-interface between snaps.\\nIn a Kubernetes implementation limited to a single worker node,\\na CSI storage driver that provided RWO volumes would suffice.\\n* The current approach cannot support distributed services\\nwithout an underlying distributed file system to distribute tokens,\\nsuch as GlusterFS, running across the participating nodes.\\nFor Kubernetes, the requirement would be a remote shared file system\\npersistent volume (RWX volume).\\n","Decision":"EdgeX will create a new service, `security-spiffe-token-provider`.\\nThis service will be a mutual-auth TLS service that exchanges\\na [SPIFFE](https:\/\/spiffe.io\/) X.509 SVID for a secret store token.\\nAn SPIFFE identifier is a URI of the format `spiffe:\/\/trust domain\/workload identifier`.\\nFor example: `spiffe:\/\/edgexfoundry.org\/service\/core-data`.\\nA SPIFFE Verifiable Identity Document (SVID) is a cryptographically-signed\\nversion of a SPIFFE ID, typically a X.509 certificate with\\nthe SPIFFE ID encoded into the `subjectAltName` certificate extension,\\nor a JSON web token (encoded into the `sub` claim).\\nThe EdgeX implementation will use a naming convention on\\nthe path component, such as the above, in order to be\\nable to extract the requesting service from the SPIFFE ID.\\nThe SPIFFE token provider will take three parameters:\\n1. An X.509 SVID used in mutual-auth TLS for the token provider\\nand the service to cross-authenticate.\\n2. The reqested service key.  If blank, the service key will\\ndefault to the service name encoded in the SVID.\\nIf the service name follows the pattern `device-(name)`,\\nthen the service key must follow the format\\n`device-(name)` or `device-name-*`.\\nIf the service name is `app-service-configurable`,\\nthen the service key must follow the format `app-*`.\\n(This is an accomodation for the Unix workload attester\\nnot being able to distingish workloads that are launched\\nusing the same executable binary.\\nCustom app services that support multiple instances\\nwon't be supported unless they name the executable\\nthe same as the standard app service binary or\\nmodify this logic.)\\n3. A list of \"known secret\" identifiers that will allow\\nnew services to request database passwords or\\nother \"known secrets\" to be seeded into their service's\\npartition in the secret store.\\nThe `go-mod-secrets` module will be modified to enable a new mode\\nwhereby a secret store token is obtained by:\\n1. Obtaining an X.509 SVID by contacting a local SPIFFE agent's\\nworkload API on a local Unix domain socket.\\n2. Connecting to the `security-spiffe-token-provider` service\\nusing the X.509 SVID to request a secret store token.\\nThe SPIFFE authentication mode will be an opt-in feature.\\nThe SPIFFE implementation will be user-replaceable;\\nspecifically, the workload API socket will be configurable,\\nas well as the parsing of the SPIFFE ID.\\nReasons for doing so might include: changing the name of\\nthe trust domain in the SPIFFE ID, or moving the SPIFFE\\nserver out of the edge.\\nThis feature is estimated to be a \"large\" or \"extra large\"\\neffort that could be implemented in a single release cycle.\\n### Technical Architecture\\n![SPIFFE Architecture and Workflow](0020-spiffe-architecture.jpg)\\nThe work flow is as follows:\\n1. Create a root CA for the SPIFFE user to use for creation of sub-CA's.\\n1. The SPIFFE server is started.\\n1. The server creates a sub-CA for issuing new identities.\\n1. The trust bundle (certificate authority) data is exported from the SPIFFE server\\nand stored on a shared volume readable by other EdgeX microservices\\n(i.e. the existing secrets volume used for sharing secret store tokens).\\n1. A join token for the SPIFFE agent is created using `token generate`\\nand shared to the EdgeX secrets volume.\\n1. Workload entries are loaded into the SPIFFE server database,\\nusing the join-identity of the agent created in the previous step\\nas the parent ID of the workload.\\n1. The SPIFFE agent is started with the join token\\ncreated in a previous step to add it to the cluster.\\n1. Vault is started and `security-secret-store-setup`\\ninitializes it and creates an admin token for `security-spiffe-token-provider` to use.\\n1. The `security-spiffe-token-provider` service is started.\\nIt obtains an SVID from the SIFFE agent and uses it as a TLS server certificate.\\n1. An EdgeX microservice starts and obtains another SVID from the SPIFFE agent\\nand uses it as a TLS client certificate to contact the\\n`security-spiffe-token-provider` service.\\nThe EdgeX microservice uses the trust bundle as a server CA\\nto verify the TLS certificate of the remote service.\\n1. `security-spiffe-token-provider` verifies the SVID using the trust bundle as client CA\\nto verify the client,\\nextracts the service key,\\nand issues an appropriate Vault service token.\\n1. The EdgeX microservice accesses Vault as usual.\\n#### Workload Registration and Agent Sockets\\nThe server uses a workload registration Unix domain socket that allows\\nauthorization entries to be added to the authorization database.\\nThis socket is protected by Unix file system permissions to control\\nwho is allowed to add entries to the database.\\nIn this proposal, a subcommand will be added to the EdgeX `secrets-config`\\nutility to simplify the process of registering new services\\nthat uses the registration socket above.\\nThe agent uses a workload attesation Unix domain socket that\\nis open to the world.  This socket is shared via a snap content-interface\\nof via a shared host bind mount for Docker.  There is one agent per node.\\n#### Trust Bundle\\nSVID's must be traceable back to a known issuing authority (certificate authority)\\nto determine their validity.\\nIn the proposed implementation, we will generate a CA on first boot and store it persistently.\\nThis root CA will be distributed as the trust bundle.\\nThe SPIFFE server will then generate a rotating sub-CA for issuing SVIDs,\\nand the issued SVID will include both the leaf certificate and the intermediate certificate.\\nThis implementation differs from the default implementation,\\nwhich uses a transient CA that is rotated periodically\\nand that keeps a log of past CA's.\\nThe default implementation is not suitable because only the Kubernetes\\nreference implementation of the SPIRE server has a notification hook\\nthat is invoked when the CA is rotated.\\nCA rotation would just result in issuing of SVIDs that are not\\ntrusted by microservices that received only the initial CA.\\nThe SPIFFE implementation is replaceable.\\nThe user is free to replace this default implementation with\\npotentally a cloud-based SPIFFE server and a cloud-based CA.\\n#### Workload Authorization\\nWorkloads are authenticated by connecting to the `spiffe-agent`\\nvia a Unix domain socket, which is capable of identifying\\nthe process ID of the remote client.\\nThe process ID is fed into one of following workload attesters,\\nwhich gather additional metadata about the caller:\\n* The Unix workload attester gathers UID, GID, path, and SHA-256 hash of the executable.\\nThe Unix workload attester would be used native services and snaps.\\n* The Docker workload attester gathers container labels\\nthat are added by docker-compose when the container is launched.\\nThe Docker workload attester would be used for Docker-based EdgeX deployments.\\nAn example label is `docker:label:com.docker.compose.service:edgex-core-data`\\nwhere the service label is the key value in the `services` section of the `docker-compose.yml`.\\nIt is also possible to refer to labels built-in to the container image.\\n* The Kubernetes workload attester gathers a wealth of pod and container metadata.\\nOnce authenticated, the metadata is sent to the SPIFFE server\\nto authorize the workload.\\nWorkloads are authorized via an authorization database\\nconnected to the SPIFFE server.\\nSupported databases are SQLite (default), PostgreSQL, and MySQL.\\nDue to startup ordering issues, SQLite will be used.\\n(Disclaimer: SQlite, according for the\\n[Turtle book](https:\/\/thebottomturtle.io\/Solving-the-bottom-turtle-SPIFFE-SPIRE-Book.pdf)\\nis intended for development and test only.\\nWe will use SQlite anyway because because Redis is not supported.)\\nThe only service that needs to be seeded to the database as this time\\nis `security-spiffe-token-provier`.  For example:\\n```sh\\nspire-server entry create -parentID \"${local_agent_svid}\" -dns edgex-spiffe-token-provider -spiffeID \"${svid_service_base}\/edgex-spiffe-token-provider\" -selector \"docker:label:com.docker.compose.service:edgex-spiffe-token-provider\"\\n```\\nThe above command associates a SPIFFE ID with a _selector_,\\nin this case, a container label, and configures a DNS\\nsubjectAltName in the X.509 certificate for server-side TLS.\\nA snap-based installation of EdgeX would use a `unix:path` or `unix:sha256` selector instead.\\nThere are two extension mechanims for authorization additional workloads:\\n1. Inject a config file or environment variable to authorize additional workloads.\\nThe container will parse and issue `spire-server entry create` commands\\nfor each additional service.\\n2. Run the `edgex-secrets-config` utility\\n(that will wrap the `spire-server entry create` command)\\nfor ad-hoc authorization of new services.\\nThe authorization database is persistent across reboots.\\n","tokens":372,"id":956,"Predictions":"Decision (A). The current solution cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n*A. The current approach cannot support distributed services without an underlying distributed file system to distribute tokens.n"}
{"File Name":"edgex-docs\/0019-EdgeX-CLI-V2.md","Context":"## Context\\nThis ADR presents a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.\\n","Decision":"1. Use standardized command-line args\/flags\\n| Argument\/Flag      | Description |\\n| ----------- | ----------- |\\n| `-d`, `--debug`      | show additional output for debugging purposes (e.g. REST URL, request JSON, \u2026). This command-line arg will replace -v, --verbose and will no longer trigger output of the response JSON (see -j, --json).       |\\n| `-j`, `--json`   | output the raw JSON response returned by the EdgeX REST API and *nothing* else. This output mode is used for script-based usage of the client.    |\\n| `--version`   | output the version of the client and if available, the version of EdgeX installed on the system (using the version of the metadata data service)   |\\n2. Restructure the Go code hierarchy to follow the [most recent recommended guidelines](https:\/\/github.com\/golang-standards\/project-layout). For instance \/cmd should just contain the main application for the project, not an implementation for each command - that should be in \/internal\/cmd\\n3. Take full advantage of the features of the underlying command-line library, [Cobra](https:\/\/github.com\/spf13\/cobra), such as tab-completion of commands.\\n4. Allow overlap of command names across services by supporting an argument to specify the service to use: `-m\/--metadata`, `-c\/--command`, `-n\/--notification`, `-s\/--scheduler` or `--data` (which is the default). Examples:\\n- `edgex-cli ping --data`\\n- `edgex-cli ping -m`\\n- `edgex-cli version -c`\\n5. Implement all required V2 endpoints for core services\\n**Core Command**\\n- **`edgex-cli command`** `read | write | list`\\n**Core Data**\\n- **`edgex-cli event`** `add | count | list | rm | scrub**`\\n- **`edgex-cli reading`** `count | list`\\n**Metadata**\\n- **`edgex-cli device`**  `add | adminstate | list | operstate | rm | update`\\n- **`edgex-cli deviceprofile`**  `add | list | rm | update`\\n- **`edgex-cli deviceservice`** ` add | list | rm | update`\\n- **`edgex-cli provisionwatcher`**  `add | list | rm | update`\\n**Support Notifications**\\n- **`edgex-cli notification`** `add | list | rm`\\n- **`edgex-cli subscription`** `add | list | rm`\\n**Support Scheduler**\\n- **`edgex-cli interval`** `add | list | rm | update`\\n**Common endpoints in all services**\\n- **`edgex-cli version`**\\n- **`edgex-cli ping`**\\n- **`edgex-cli metrics`**\\n- **`edgex-cli status`**\\nThe commands will support arguments as appropriate. For instance:\\n- `event list` using `\/event\/all` to return all events\\n- `event list --device {name}` using `\/event\/device\/name\/{name}` to return the events sourced from the specified device.\\n6.  Currently, some commands default to always displaying GUIDs in objects when they're not really needed. Change this so that by default GUIDs aren't displayed, but add a flag which causes them to be displayed.\\n7. **scrub** may not work with Redis being secured by default. That might also apply to the top-level `db` command (used to wipe the entire db). If so, then the commands will be disabled in secure mode, but permitted in non-secure mode.\\n8. Have built-in defaults with port numbers for all core services and allow overrides, avoiding the need for static configuration file or configuration provider.\\n9. *(Stretch)* implement a `-o`\/`--output` argument which could be used to customize the pretty-printed objects (i.e. non-JSON).\\n10. *(Stretch)* Implement support for use of the client via the API Gateway, including being able to connect to a remote EdgeX instance. This might require updates in go-mod-core-contracts.\\n","tokens":44,"id":957,"Predictions":"Decision (A) The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>br>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.nbr>The ADR should be a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.n"}
{"File Name":"edgex-docs\/0002-Array-Datatypes.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n- [Consequences](#consequences)\\n<!--te-->\\n","Decision":"- [Consequences](#consequences)\\n<!--te-->\\n### DeviceProfile extension\\nThe permitted values of the `Type` field in `PropertyValue` are extended to include:\\n\"BoolArray\", \"Uint8Array\", \"Uint16Array\", \"Uint32Array\", \"Uint64Array\", \"Int8Array\", Int16Array\", \"Int32Array\", \"Int64Array\", \"Float32Array\", \"Float64Array\"\\n### Readings\\nIn the API (v1 and v2), `Reading.Value` is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie `[\"value1\", \"value2\", ...]`\\n","tokens":31,"id":959,"Predictions":"Decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ## Final decision ##"}
{"File Name":"edgex-docs\/0004-Feature-Flags.md","Context":"## Context\\nOut of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags.\\nFeature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace \u2013 i.e disabling store and forward in App Functions SDK without breaking backward compatibility.\\nIt can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending \u201cUsage Metrics\u201d of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don\u2019t provide much value on their own and they certainly don\u2019t assist in increasing velocity to help us deliver on time.\\nPutting aside one of the major value propositions listed above, feasibility of a feature flag \u201cmodule\u201d was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true\/false. This is similar to what is done today. Turning on\/off security is an example, turning on\/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as `featurepkg.Register(\u201cMyNewFeature\u201d)` or `featurepkg.IsOn(\u201cMyNewFeature\u201d)`. However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework\u2014either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized.\\n","Decision":"The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf.\\nHowever, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on\/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn\u2019t fully tested with edge cases, but doesn\u2019t impact any other functionality, should be encouraged.\\n","tokens":469,"id":960,"Predictions":"Decision Makers should consider a feature flag \u201cmodule\u201d to help address concerns around feature dependencies and compatibility.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"zero-trust-partner-admin-link\/0001-automated-zero-trust-pal.md","Context":"## Context\\nSome customers who operate in regulated industries or offer SaaS products to their end customers, require Zero Trust environments; meaning that 3rd parties (including Microsoft Partners) should not have access to any environment which contains PII, financial or medical data. This requirement is often backed by legal contracts with the customer's end-customer, who are often also regulated. Although this offers benefits to the 3rd parties around mitigating risk and liability, and limits GDPR responsibilities, it causes problems with access rights based attribution of Azure Consumed Revenue (ACR), required for partner competency consumption targets.\\nThe process for creating a Zero Trust Partner Admin Link is as follows:\\n1) The Partner shares their MPN ID with the Customer.\\n2) The Customer exports their list of subscriptions as a CSV, which then can then edited to remove any non-relevant subscriptions.\\n3) The Customer (with the relevant permissions) creates an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.\\n","Decision":"We can automate this process by encapsulating it in two PowerShell Cmdlets packaged as a module available from the [PowerShell Gallery](https:\/\/www.powershellgallery.com\/).\\n`Export-CustomerSubscriptionsAsCsvForPartnerAdminLink` - run by The Customer to generate a CSV file listing their Azure Subscriptions. This can be edited to remove non-applicable subscriptions.\\n`Set-ZeroTrustPartnerAdminLink` - run by The Customer to create a dedicated AAD Application. Then assigns this Application the \"Contributor Role\" to each of the subscriptions listed in the CSV file, and then uses the `AzManagementPartner` module to link the Application to The Partner's MPN Id to complete the Partner Admin Link.\\n","tokens":220,"id":961,"Predictions":"Decision (A) The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n2b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n3b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n4b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n5b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n6b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n7b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n8b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n9b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n9c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n10b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n11b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12b The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.n12c The customer should create an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The"}
{"File Name":"pharmacy-data-etl\/0006-full-nightly-scrape.md","Context":"## Context\\nA list of IDs retrieved from Azure Storage is used to seed the ETL scrape.\\nThe `modifiedsince` end point in syndication is used to detect when records have changed, or new records\\nhave been added, enabling an incremental scrape of data.\\nThe `modifiedsince` endpoint does not currently include records that have been removed, hidden, or unhidden.\\nRecords may also be added much later than the modification date, i.e. a record modified in January may not\\nbe published to Syndication until March. In these cases the record changes will not be reflected in the ETL.\\n","Decision":"The `modifiedsince` end point will only be used to detect newly added pharmacies, which will be added to the\\nseed ID list.\\nAll entries in the seed ID list will be refreshed overnight. If an ID has been deleted, the details will be recorded\\nin the summary file as reporting a 404 error, and the record will not be present in the output JSON.\\n","tokens":124,"id":962,"Predictions":"Decision (A) The record is not valid.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pharmacy-data-etl\/0002-store-etl-output-in-the-cloud.md","Context":"## Context\\nThe output from the ETL is only available in the container and needs to be exposed to consuming applications.\\n","Decision":"When the ETL has completed the output will be stored in an Azure blob, Azure being the current preferred cloud platform.\\nThe output file will be exposed on a publicly available URL.\\n","tokens":25,"id":963,"Predictions":"The ETL output is only available in the container and needs to be exposed to consuming applications.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pharmacy-data-etl\/0003-generate-file-containing-organisation-data.md","Context":"## Context\\nThe [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.\\nThe [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.\\n","Decision":"The NHS Organisation API will be scraped nightly to generate a file containing pharmacy data.\\n","tokens":53,"id":964,"Predictions":"Decision (A) The [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.n The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (B) The [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (D) The [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [NHS Organisation API](http:\/\/api.nhs.uk\/organisations) holds information about pharmacies.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n.n Decision (C) The [Connecting to Services](https:\/\/github.com\/nhsuk\/connecting-to-services) applications needs to make use of Pharmacy data.n"}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"Predictions":"Decision i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pharmacy-data-etl\/0005-refresh-data-on-major-version-change.md","Context":"## Context\\nData structure may change between releases of the ETL.\\nThe ETL uses a scrape of previous data to reduce unnecessary work.\\n","Decision":"The major and minor version will be included in the seed data file to identify a change of data structure.\\n","tokens":29,"id":966,"Predictions":"Decision (A) The ETL should be able to perform the scrape of previous data.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"pharmacy-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":967,"Predictions":"The architect decided to build a new building.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"golang-git-fritz\/0002-implement-with-golang.md","Context":"## Context\\nWe need to choose a language to in which to code this project.\\n","Decision":"Use Golang.\\n","tokens":18,"id":968,"Predictions":"Decided to code this project in a language that is native to the system.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"golang-git-fritz\/0003-follow-conventional-commits.md","Context":"## Context\\nThe point of the tool is to enforce good commit hygiene, which is easy to compromise while trying to just get good code out. We should pick a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility.\\n","Decision":"Use [Conventional Commits v1.0.0-beta.4]. It's similar enough to [Semantic Commits] and its inspiration [Angular Commits], but loose enough and working on being an independent standard.\\n","tokens":52,"id":969,"Predictions":"Decision Makers should consider the following: a. They should choose a standard that is popular or otherwise conventional, and not overly strict for flexibility. b. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. c. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. d. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. e. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. f. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. g. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h. They should choose a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility. h."}
{"File Name":"golang-git-fritz\/0001-record-architecture-decisions.md","Context":"## Context\\nSome records are consistently followed and should be centralized for consistency.\\n","Decision":"Use jncmaguire\/adr.\\n","tokens":16,"id":970,"Predictions":"Decision Makers should be able to easily access the records.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"operational-visualizations\/0005-about-measure-labels.md","Context":"## Context\\nThe visualization library contains a `Labels` component, which can be used in the same way as all other axial chart renderers:\\n```jsx\\n<Chart>\\n<Bars ... \/>\\n<Labels ... \/>\\n<\/Chart>\\n```\\n","Decision":"However, the user can also display labels by passing a `showLabels` flag. This solves multiple problems:\\n- In the case of stacked bars or area charts, using the above API does not take stacking into account, so the labels will be in the wrong places.\\n- The `Labels` component only works for axial charts, not for pie charts, so `PieChart` would have needed a flag anyway - this ensures consistency across all renderers.\\n- For scatter plots, the labels are now automatically offset by the radius of the dots. Any other renderer-specific styling that may be required is now also easier.\\n","tokens":54,"id":974,"Predictions":"Decision (A) The visualization library contains a Labels component, which can be used in the same way as all other axial chart renderers:njsxnChart>nBars... \/>nLabels... \/>n\/Chart>n Decision (B) The visualization library contains a Labels component, which can be used in the same way as all other axial chart renderers:njsxnChart>nBars... \/>nLabels... \/>n\/Chart>n Decision (A) The visualization library contains a Labels component, which can be used in the same way as all other axial chart renderers:njsxnChart>nBars... \/>nLabels... \/>n\/Chart>n Decision (A) The visualization library contains a Labels component, which can be used in the same way as all other axial chart renderers:njsxnChart>nBars... \/>nLabels... \/>n\/Chart>n Decision (A) The visualization library contains a Labels component, which can be used in the same way as all other axial chart renderers:njsxnChart>nBars... \/>nBars... \/>n\/Chart>n Decision (A)"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"Predictions":"Decision (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (D) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (D) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (D) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (D) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (D) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (D) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (A) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n** Decide (B) We need to provide getCursor functionality in FragmentFrame.n** Decide (C) We need to provide getCursor functionality in FragmentFrame.n**"}
{"File Name":"converge\/001-data-diff.md","Context":"## Context\\nWe need to provide a nicely ergonomic API to programs using Converge,\\nand we wanted to adhere closely to the Clojure Way (TM).  We decided\\nto use the Atom API of swap!\/reset!\/deref.\\nWhen altering the value of the ConvergentRef via swap!\/reset!, we need\\nto create a covering set of Ops to describe the change being made.  We\\nconsidered the following alternatives:\\n1. Using `clojure.data\/diff` on old-value and new-value, with metadata\\nfor Id accounting\\n2. Using [`differ`](https:\/\/github.com\/Skinney\/differ) on old-value\\nand new-value, with metadata for Id accounting\\n3. Using [`editscript`](https:\/\/github.com\/juji-io\/editscript) on\\nold-value and new-value, with metadata for Id accounting\\n4. Implementing the Map\/Vector interfaces a la\\n[Schism](https:\/\/github.com\/aredington\/schism\/blob\/master\/src\/schism\/impl\/types\/nested_map.cljc)\\nand do our tracking and accounting within those implementation\\n","Decision":"On initial analysis, it appears that #4 above is flawed, as\\nimplementing our own tracking\/accounting nested map\/list types\\nwouldn't maintain the necessary context to translate to an opset.\\nSo that left us with a strategy based on diffing\/patching. Based on\\nanalysis in [this blog\\npost](https:\/\/juji.io\/blog\/comparing-clojure-diff-libraries\/), as well\\nas our own usage testing with `clojure.data\/diff`, `differ`, and\\n`editscript`, we have decided to use\\n[`editscript`](https:\/\/github.com\/juji-io\/editscript) for creating\\nopset patches.\\n","tokens":237,"id":978,"Predictions":"Implementing the Map\/Vector interfaces a lan[Schism](https:\/\/github.com\/aredington\/schism\/blob\/master\/src\/schism\/impl\/types\/nested_map.cljc)nand do our tracking and accounting within those implementationsn ## Final answer: Implementing the Map\/Vector interfaces a lan[Schism](https:\/\/github.com\/aredington\/schism\/blob\/master\/src\/schism\/impl\/types\/nested_map.cljc)nand do our tracking and accounting within those implementationsn ## Final answer: Implementing the Map\/Vector interfaces a lan[Schism](https:\/\/github.com\/aredington\/schism\/blob\/master\/src\/schism\/impl\/types\/nested_map.cljc)nand do our tracking and accounting within those implementationsn ## Final answer: Implementing the Map\/Vector interfaces a lan[Schism](https:\/\/github.com\/aredington\/schism\/blob\/master\/src\/schism\/impl\/types\/nested_map.cljc)nand do our tracking and accounting within those implementationsn"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"xebikart-infra\/004-use-golang-as-main-langage-for-backend.md","Context":"## Context and Problem Statement\\nWe want to code a backend that will handle inbound informations through the\\nmessage broker, and expose them to the dashboard front-end.\\nWhat programming language should we use?\\n## Decision Drivers\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing\\nknowledge\\n- Library support for our usecases\\n","Decision":"- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing\\nknowledge\\n- Library support for our usecases\\nChosen option: **\"Go\"**, because:\\n- It has good library support for things such as **HTTP\/2** and **Protobuf**\\n(separately, we're not talking about gRPC  here). This will help establish\\nstrong bidirectionnal and persistant streaming with the dashboard (HTTP\/2).\\nIt will also open the way to Protobuf serialized messages over the message\\nbus that will maybe comme in addition to RabbitMQ.\\n- People on the project want to play more with it!\\n- It seems appropriate for small (micro)services like the one we're going to\\nbuild\\nHowever, we are **not excluding other languages** depending on usecases and\\nwishes of people working on it. We want people to contribute so we won't force\\nanyone to use a given language. **People are free to use the language they\\nwant**, but we'll try to have **Go as a default main language as much as\\npossible**.\\n","tokens":86,"id":980,"Predictions":"Decision nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"xebikart-infra\/006-use-password-authentication-for-rabbitmq.md","Context":"## Context and Problem Statement\\nWe are deploying a RabbitMQ cluster on GKE and people will need to connect to\\nit. We want to provide some kind of authentication so it's not totally\\nopen-bar.\\nWhat authentication mecanism should we use?\\n","Decision":"Chosen option: **\"User\/Pawword\"**, because:\\n- We want to make the RabbitMQ cluster quickly available to teams\\n- It will be easy to change to client certificate auth later on\\n- It is better than not providing auth at all\\nWe **will replace it by client certificates authentication later**.\\n","tokens":54,"id":981,"Predictions":"Decision (A).nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally open-bar.nWe want to provide some kind of authentication mechanism so it's not totally"}
{"File Name":"xebikart-infra\/005-use-kubectl-and-helm-for-k8s-deployment.md","Context":"## Context and Problem Statement\\nWe want to **deploy** pods, services, and other resources on **Kubernetes**,\\nwhich is a GKE cluster for now.\\nWhat tool should we use to do this?\\n## Decision Drivers\\n- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\n","Decision":"- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\nChosen option: \"A mix of **kubectl** and **Helm**\", because:\\n- `kubectl` is the native way of deploying to Kubernetes and it thus the most\\ndocumented, widely-used, appropriate and up-to-date.\\n- Helm fits best for complex stacks deployment and we're eager to try things\\nsuch as Helm charts repositories!\\nHelm will be used as an alternative to `kubectl` in a second part, when we'll\\nramp up things and when we will need to deploy more complex stacks. The\\nbeginning will be `kubectl` for our own services, and Helm for\\ncomponents\/middleware\/databases with existing charts such as RabbitMQ!\\nThis imply that the provided Kubernetes cluster(s) for this project will need\\nto be **provisionned with Helm requirements such as Tiller**.\\n","tokens":155,"id":982,"Predictions":"Decision Makersn- Ease of usen- Ease of usen- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn- Reliabilityn"}
