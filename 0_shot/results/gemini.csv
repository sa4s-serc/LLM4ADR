Context,Decision,Prediction
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe PriveXMl results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.\n\n## Decision\n","We increase complexity during insertion by including all necessary tenant information as part of a PriveXML event and also provide event objects which could only just contain specific changes to client information within the same topic. This then enforces us to think of this topic as a general topic for ETL which may contain an event to process a new PriveXML result or process tenant data change.\nMore so, the topics for each tenant must be tenant specific due to necessary policies and requirements to create seperation of data on physical levels.\n",Implement a Historical Tenant record with valid from / valid to date range for each Tenant in the Tenant table.  PriveXML will be related based on the Historical TenantId.  Historical Tenant will be updated during ETL tenant Migrations to ensure there is no loss in data and no change of PrvieXML above date change.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently have certain specific feeds being parsed using pentaho, these are legacy feeds which transform data feed files into PriveXMl format. These feed parsers will require migration into new CDH ETL feeds parsers based on defined standard library.\n\n## Decision\n","Due to limited resources and time constraints, we will defer the feed migration from pentaho to the CDH ETL till adequate human resources are available, more so an indept understanding of exactly how the parsers work is required, which will require talking with Christopher on this.\n",Migrate the legacy feed parsers from Pentaho to the new CDH ETL feed parsers based on the defined standard library.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, we move files around in S3 from Archive folders into bulkupload folders which is cumbersome and error prone,\nmore so this is being done to allow us trigger data feed processing from the dev admin.\nWe need a means of reducing the flow steps to go from files to processing and reduce surface level access for the ETL team.\n\n## Decision\n","- ETL will have a dedicated database tables which will serve the need to both register and retrieve data feed files in the\nmost optimized manner necessary on a per feed basis. This database tables will be appropriately updated by the ETL service\nbased on update events from either the uploader service, or the windows-specific uploader jars.\nThe following operations then becomes possible:\n1. Ability to query for specific files based on upload date (the date it was uploaded to S3).\n2. Ability to query for a specific batch of files related to specific data feeds.\n3. Ability to get data files specific to a given owner and/or provider.\n4. Ability to get data files specific to a given provider.\n- Due to we creating a database to optimize a query to retrieve and manage data feed files, then we require a way to keep\nthe ETL database tables up to date with new files from uploader service. Therefore, the ETL system will listen for events coming\nfrom both a lambda function which will be called by S3 when new files are added to the specific bucket, and the new uploader\nservice which will house necessary logic for retrieving such data feed files from their sources. Once all uploading logic\nhave being migrated to the uploader, we will de-commission the lambda function and directly have the uploader service inform the\nETL service as regards new files.\nSee\n![Data Feed Delivery](../assets/images/workflows/image8.png)\n","Implement a Lambda function that will be triggered by new files/objects created in the Archive folder in S3. This Lambda function will then move the file to the bulkupload folder, triggering the data feed processing automatically. This will reduce the number of manual steps required to process files and reduce the surface level access for the ETL team.\n\n**Benefits:**\n- Reduced manual effort and error-prone steps\n- Automated data feed processing\n- Reduced surface level access for the ETL team\n\n**Drawbacks:**\n- Additional cost for running the Lambda function\n- Potential for Lambda function failures"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMonolith will undergo segmentation where we plan to pull out the Client Data Hub out of the monolith as a separate service for better scaling and API management. The functionality of the client data hub to provided tenant, transaction and position related data will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](./etl.md).\nThe client data hub provides the following services:\n1. Delivery of tenant information (Accounts, Company, and Client data).\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\n4. Processing of reconciliation request on tenant accounts and positions.\nThis requires us to consider migration procedures for moving existing data from the monolith into the new CDH and ETL related databases.\n\n## Decision\n",Create a specific services (ETL and CDH) where an ETL service and CDH service will be responsible for the processing of delivered PriveXML for consumption and delivery of client transactions + positions into data tables. Both services will communicate across a pubsub event bus.\nThe following is expected:\n- CDH will expose an API by which it will handle all tenant and transaction related requests from monolith.\n- CDH will listen on specified event topic on message bus from which all tenant update requests will be received as events\n- CDH will publish to the monolith events on updated tenant data by which monolith will update it's records.\n- CDH will have all tenant data moved from monolith into it's control (to be discussed in CDH Tech).\n- CDH will consume all PriveXML events to update it's records of transactions + tickers + positions.\n![CDH Architecture](../assets/images/aab_workshop/aab_5.jpg)\n,"**Decision:**\n\nThe Client Data Hub (CDH) and [ETL Service](./etl.md) should be developed as separate services using Java Spring Boot and Docker.\n\n**Rationale:**\n\n* **Scalability:** Separating the CDH into a separate service will allow it to be scaled independently, ensuring better performance and reliability.\n* **API Management:** Moving the API management functionality to the CDH will provide more control and flexibility over the exposure of tenant data.\n* **Data Feed Parsing Optimization:** Offloading data feed parsing to the ETL Service will improve the efficiency of the CDH by focusing it on its core data delivery functions.\n* **Proven Technology Stack:** Java Spring Boot and Docker are well-established and widely-used technologies, ensuring stability and support.\n* **Data Migration Considerations:** The data migration process will need to be carefully planned and executed to ensure data integrity and minimize disruption during the transition."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wish to segment into separate processes where the data feed files processing is handled by the ETL service and the CDH service is reponsible for consuming these produced output which then are materialized into records which is used in response to request to the CDH service. This means ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\n\n## Decision\n,We have chosen an event based communication where the CDH and ETL service communicate results between each other over an event queue based on specified topics (deployed onsite within geozone of CDH and ETL services).\n![Event Queue](../assets/images/workflows/image3.png)\n,"Implement a message queue broker to facilitate communication between the ETL service and the CDH service. This message queue broker will act as an intermediary, allowing the ETL service to produce messages containing the processed data feed files and the CDH service to consume these messages and materialize the records accordingly. This approach ensures loose coupling and independence between the two services, enabling them to operate autonomously."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the parsing process for datafeeds is still part of the existing monolith codebase, which brings it's own share of problems with scaling and feed parsing managemnent. Monolith also has a hard dependency on AWS S3 which must be broken out to allow us specfically move these dependencies outside as they are not relatively core to the ETL processing architecture but are generally how we organize and move input into desired place for access and delivery.\nA hard requirement for the ETL service is the ability to ensure processed datafeed consistently have relational information with their tenants as specific feeds\nhave specific constraints on how they are accessed and stored.\nAnother hard requirements is to organize how data feed files are accessed and retrieved from S3, this currently has issues due to the need to directly access S3, and move files and directories into the bulkupload directories for processing by the monolith. Whilst the alternative of storing both file, metadata and file binary into the database simplifies these access issues, we create more problems in the management of the database files (without using FileStream optimization in SQLServer), increasing cost of backup and replication.\n\n## Decision\n","ETL will be moved into an external service of it's own with the following responsibilities:\n- Embodiment of all parser logic.\n- Delivery of agreed parser format (currently PriveXML) into message queues.\n- Standardized library for parsing delivery and logic.\n- Standardized database tables for data feed file delivery and access.\n- Standardized database tables for tenant data (Company, Client, Accounts).\n- Creates tenant specific events for delivery for tenant specific datafeed.\nAs regards data feed file access problem\n- Manage synchronization of uploaded files events into database from uploader service.\n","**Decision:** Migrate the datafeed parsing process to a separate microservice.\n\n**Rationale:**\n\nThe current parsing process is tightly coupled with the monolith, which introduces scalability and maintenance challenges. By migrating the parsing process to a separate microservice, we can:\n\n* **Improve scalability:** The microservice can be independently scaled to handle increased data volume or traffic.\n* **Reduce complexity:** The separation of the parsing process from the monolith will make the codebase cleaner and easier to maintain.\n* **Break the dependency on AWS S3:** The microservice can be designed to access data from S3 using an API or SDK, which will decouple it from the specific implementation of S3.\n* **Ensure relational information with tenants:** The microservice can be responsible for associating processed datafeed data with the appropriate tenants.\n* **Improve organization and retrieval of data feed files from S3:** The microservice can provide a consistent and optimized way to access and retrieve data feed files from S3, eliminating the need for direct S3 access.\n* **Avoid database file management issues:** By storing only metadata and file binary in the database, we can minimize the cost of backup and replication."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\n\n## Decision\n","1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\n![Target Parser Flow](../assets/images/workflows/image1.png)\n","**Decision:**\n\nWe will adopt a **Data Feed File Parsing Framework** to handle the processing of data feed files for ETL. This framework will provide a standardized approach for defining the expected inputs and outputs of parsers and their integration into the overall parsing process.\n\n**Key Points:**\n\n* **Parser Definition:** Individual parsers will be defined using a common schema that specifies the following:\n    * Supported file formats and structures\n    * Expected data fields and their extraction rules\n    * Validation rules to check the integrity of parsed data\n* **Integration with ETL Process:** The framework will provide a mechanism to seamlessly integrate parsers into the ETL process, including:\n    * Mapping parsed data to target data structures\n    * Handling errors and exceptions during parsing\n    * Reporting on data quality and conformance\n* **Modularity and Extensibility:** The framework will be designed to be modular and extensible, allowing for easy addition of new parsers and updates to existing ones as required.\n* **Data Validation and Quality Control:** The framework will include built-in data validation mechanisms to ensure the integrity and accuracy of parsed data, including both structural and semantic checks.\n\n**Benefits:**\n\n* Improved data quality and reliability\n* Reduced development time and effort\n* Standardized parsing process across multiple data sources\n* Increased flexibility and scalability for future changes\n* Enhanced data transparency and governance"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMonolith and a few deployed jars handling delivery of data feed files into S3 for the existing parsing processes powering the monolith client data hub system, this increases cost on the monolith's systems which require more vertical scaling of resources to manage, more so, due to the monolith lock, any fix or update is locked to the monolith release SDLC.\nConsidering these functions serve to move files from source to destination we need to migrate them as external services to both CDH and the monolith, these are then bundled into a single service responsible for the delivery of new data feed files into the S3 Archives and CDH feed data stores.\n\n## Decision\n","Migration of all monolith related uploading logic into external service which is responsible for the timely retreival, delivery and storage of data feed files from their respective sources. The service is responsible for ensuring the ETL service database is always up to date, by deliverying events on file additions into the archive storage regardless of what storage is being used by event delivery.\n![Data Feed Delivery](../assets/images/workflows/image8.png)\n",Migrate the monolith and deployed jars responsible for data feed file delivery into S3 to external services for both CDH and the monolith.\n\n**Benefits:**\n\n* Reduces cost on the monolith's systems by eliminating the need for vertical scaling of resources.\n* Improves agility by decoupling data feed file delivery from the monolith release SDLC.\n* Enhances scalability by bundling the data feed file delivery functions into a single service.\n\n**Risks:**\n\n* Potential for data loss or corruption during migration.\n* Increased complexity due to the introduction of new services.\n* Requires additional resources to manage and maintain the external services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\nrequirements during onboarding and processing of feed files, the following issues where considered:\n- Will such logic require specialized implementation across feeds?\n- Are such logic generic and require one time implementation or will require continous change/update?\n- What are the benefits of moving such logic into ETL instead of CDH.\n\n## Decision\n","The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\nensuring the final result is always consistent to march what the CDH service requires.\n","Move all the logic related to onboarding and processing of feed files to ETL services, such logic is generic and does not require specialized implementation across feeds, it will require one time implementation which will be generic enough to handle all possible feed files. Moving such logic to ETL will provide the following benefits:\n- Decoupling of CDH from ETL services.\n- Centralized management of feed files processing logic.\n- Improved maintainability and extensibility of the ETL pipeline.\n- Reduced code duplication and complexity within CDH."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\n\n## Decision\n","1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\n![Target Parser Flow](../assets/images/workflows/image1.png)\n","**Decision:** Implement a data feed pipeline with the following characteristics:\n\n* **Input:** Data feed files in CSV or JSON format, conforming to a predefined schema.\n* **Parsing:** Utilize vendor-specific parsers to extract data from the input files.\n* **Output:** Parsed data in a consistent intermediate format, suitable for further processing in the ETL pipeline.\n* **Validation:** Perform data quality checks and validations during parsing to ensure data integrity.\n* **Error Handling:** Handle parsing errors gracefully, providing error messages and logging for debugging purposes.\n* **Scalability:** Design the pipeline to support multiple data feed files concurrently, with the ability to scale as needed.\n* **Extensibility:** Allow for the easy integration of new data feed sources and parser implementations in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDuring the parsing of different feeds it has become clear that some feed files contain partial information related to Positions, Transactions and Tickers (think DBS, BOS, Credit Suisse) which end up creating inconsistent and incomplete PriveXML/JSON result files with partial information which if handled in CDH will break decision made in [0002 CDH ETL Logic Boundaries](./0002-cdh-etl-logic-boundaries.md).\nFrom existing processing of feed files, such inconsistency is resolved by reviewing previous results based on time ranges back into the past which allows us reconcile such inconsistencies, by creating a clear picture due to information on Tickers, Positions and Transactions as required.\nProblems:\n- Each parser falling into such use case will escalate processing of a batch to process more files per requests.\n- Increased processing time for processing a batch of files due to the need to reprocess (clean, parse, normalize) feed data.\n- We must scale vertically as needed to meet resource requirements for constraints of reprocessing more files per feed request (if necessary only).\n2. Reuse previous parsing results\nSince previous feed files were already processed to create their corresponding PriveXML/JSON which contains the cleaned and normalized results of such a batch of feed files for each time range (t-1, t-2,..., etc). If these are stored within an appropriate store (S3, MySQL, Redis) which will allow immediate access as such result do not change, then we can skip reprocessing of the original source files and use this as the source of the reconciliation processing during feed parsing of new feed batches.\nBenefits:\n- Each parser still only ever processing the provided batch (same memory and cpu usage)\n- Reprocessing of source files only ever occurs when such result does not yet exists (saving cpu cycles per feed)\n- Each parser retrieves previous results based on specific critierais with a single query to retrive as many t-n time range.\nProblems:\n- If relation database table is used then we create a shared resource across horizontal scaling of ETL service.\n(We can mitigate this by using a scalable store (e.g s3) that can handle as much request for files as needed).\n- We must implement more code to provide cleaner abstractions for retrieving/querying previous results (setup cost)\n- We must still fall back to processing source files if the result does not yet exists in db (so why not do it all the time?).\n**TradeOffs: Performance over Code Complexity**\nBecause if we choose option 1 then we increase complexity in execution time as each parsing data can requests multiple files for reprocessing to reconcile inconsistency, while option 2 means we must manage another database if going on a relational store to persist such processing results for later read-heavy querying.\n\n## Decision\n","We chosen to following the solution prescribed in Option 2, using a relation database table to store records to such PriveXML/JSON results which will be based on processing date and feed category for later retrieval. This can be stored as blobbed or have the contents stored externally (on S3) where the table maintains necessary pointers to location for retrieval through a library.\n","**Reuse previous parsing results**\n\n**Rationale**\n\nThe benefits of reusing previous parsing results outweigh the drawbacks. By reusing previous results, we can skip the time-consuming process of reprocessing source files, which will improve the performance of the ETL process. Additionally, reusing previous results will reduce the amount of code that needs to be written and maintained, which will make the ETL process more efficient and easier to manage.\n\n**Trade-offs**\n\nThe main trade-off is that we will need to implement more code to provide cleaner abstractions for retrieving/querying previous results. However, this trade-off is outweighed by the benefits of reusing previous results.\n\n**Consequences**\n\nBy reusing previous parsing results, we will improve the performance of the ETL process and reduce the amount of code that needs to be written and maintained."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\nAll account types have a set of balances that are composed of `Coins`. The `x/bank` module keeps\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\napplication.\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\noperations in the Cosmos SDK work off of units of 10^6.\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\nas a result the total amount of asset types increases. We propose to have `x/bank` additionally keep\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\nUX and remove the requirement for making any assumptions on the unit of denomination.\n\n## Decision\n","The `x/bank` module will be updated to store and index metadata by `denom`, specifically the ""base"" or\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\nMetadata may also include a non-zero length list of denominations. Each entry containts the name of\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\nand on exchanges. The values in those fields link to an entry in the list of denominations.\nThe list in `denom_units` and the `display` entry may be changed via governance.\nAs a result, we can define the type as follows:\n```protobuf\nmessage DenomUnit {\nstring denom    = 1;\nuint32 exponent = 2;\nrepeated string aliases = 3;\n}\nmessage Metadata {\nstring description = 1;\nrepeated DenomUnit denom_units = 2;\nstring base = 3;\nstring display = 4;\n}\n```\nAs an example, the ATOM's metadata can be defined as follows:\n```json\n{\n""description"": ""The native staking token of the Cosmos Hub."",\n""denom_units"": [\n{\n""denom"": ""uatom"",\n""exponent"": 0,\n""aliases"": [\n""microatom""\n],\n},\n{\n""denom"": ""matom"",\n""exponent"": 3,\n""aliases"": [\n""milliatom""\n]\n},\n{\n""denom"": ""atom"",\n""exponent"": 6,\n}\n],\n""base"": ""uatom"",\n""display"": ""atom"",\n}\n```\nGiven the above metadata, a client may infer the following things:\n- 4.3atom = 4.3 * (10^6) = 4,300,000uatom\n- The string ""atom"" can be used as a display name in a list of tokens.\n- The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\nan explicit decision to choose a different representation.\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\nas the base framework for this already exists in the Cosmos SDK.\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x/bank` module which is also\nindexed by the base `denom`.\n```go\ntype GenesisState struct {\nSendEnabled   bool        `json:""send_enabled"" yaml:""send_enabled""`\nBalances      []Balance   `json:""balances"" yaml:""balances""`\nSupply        sdk.Coins   `json:""supply"" yaml:""supply""`\nDenomMetadata []Metadata  `json:""denom_metadata"" yaml:""denom_metadata""`\n}\n```\n",Add metadata to the `denom` field in the `Coins` type to provide a richer UX for clients and apps built atop Cosmos-SDK-based chains. This metadata will help clients and apps understand the unit of denomination for each `denom` and remove the requirement for making any assumptions on the unit of denomination.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis idea was first conceived of in order to fulfill the use case of the\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\nmembers would be elected by a governing community and would fulfill the role of\ncoordinating the community under emergency situations. This thinking\ncan be further abstracted into the conception of ""blockchain specialization\ngroups"".\nThe creation of these groups are the beginning of specialization capabilities\nwithin a wider blockchain community which could be used to enable a certain\nlevel of delegated responsibilities. Examples of specialization which could be\nbeneficial to a blockchain community include: code auditing, emergency response,\ncode development etc. This type of community organization paves the way for\nindividual stakeholders to delegate votes by issue type, if in the future\ngovernance proposals include a field for issue type.\n\n## Decision\n","A specialization group can be broadly broken down into the following functions\n(herein containing examples):\n- Membership Admittance\n- Membership Acceptance\n- Membership Revocation\n- (probably) Without Penalty\n- member steps down (self-Revocation)\n- replaced by new member from governance\n- (probably) With Penalty\n- due to breach of soft-agreement (determined through governance)\n- due to breach of hard-agreement (determined by code)\n- Execution of Duties\n- Special transactions which only execute for members of a specialization\ngroup (for example, dCERT members voting to turn off transaction routes in\nan emergency scenario)\n- Compensation\n- Group compensation (further distribution decided by the specialization group)\n- Individual compensation for all constituents of a group from the\ngreater community\nMembership admittance to a specialization group could take place over a wide\nvariety of mechanisms. The most obvious example is through a general vote among\nthe entire community, however in certain systems a community may want to allow\nthe members already in a specialization group to internally elect new members,\nor maybe the community may assign a permission to a particular specialization\ngroup to appoint members to other 3rd party groups. The sky is really the limit\nas to how membership admittance can be structured. We attempt to capture\nsome of these possiblities in a common interface dubbed the `Electionator`. For\nits initial implementation as a part of this ADR we recommend that the general\nelection abstraction (`Electionator`) is provided as well as a basic\nimplementation of that abstraction which allows for a continuous election of\nmembers of a specialization group.\n``` golang\n// The Electionator abstraction covers the concept space for\n// a wide variety of election kinds.\ntype Electionator interface {\n// is the election object accepting votes.\nActive() bool\n// functionality to execute for when a vote is cast in this election, here\n// the vote field is anticipated to be marshalled into a vote type used\n// by an election.\n//\n// NOTE There are no explicit ids here. Just votes which pertain specifically\n// to one electionator. Anyone can create and send a vote to the electionator item\n// which will presumably attempt to marshal those bytes into a particular struct\n// and apply the vote information in some arbitrary way. There can be multiple\n// Electionators within the Cosmos-Hub for multiple specialization groups, votes\n// would need to be routed to the Electionator upstream of here.\nVote(addr sdk.AccAddress, vote []byte)\n// here lies all functionality to authenticate and execute changes for\n// when a member accepts being elected\nAcceptElection(sdk.AccAddress)\n// Register a revoker object\nRegisterRevoker(Revoker)\n// No more revokers may be registered after this function is called\nSealRevokers()\n// register hooks to call when an election actions occur\nRegisterHooks(ElectionatorHooks)\n// query for the current winner(s) of this election based on arbitrary\n// election ruleset\nQueryElected() []sdk.AccAddress\n// query metadata for an address in the election this\n// could include for example position that an address\n// is being elected for within a group\n//\n// this metadata may be directly related to\n// voting information and/or privileges enabled\n// to members within a group.\nQueryMetadata(sdk.AccAddress) []byte\n}\n// ElectionatorHooks, once registered with an Electionator,\n// trigger execution of relevant interface functions when\n// Electionator events occur.\ntype ElectionatorHooks interface {\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\nAfterMemberAccepted(addr sdk.AccAddress)\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\n}\n// Revoker defines the function required for a membership revocation rule-set\n// used by a specialization group. This could be used to create self revoking,\n// and evidence based revoking, etc. Revokers types may be created and\n// reused for different election types.\n//\n// When revoking the ""cause"" bytes may be arbitrarily marshalled into evidence,\n// memos, etc.\ntype Revoker interface {\nRevokeName() string      // identifier for this revoker type\nRevokeMember(addr sdk.AccAddress, cause []byte) error\n}\n```\nCertain level of commonality likely exists between the existing code within\n`x/governance` and required functionality of elections. This common\nfunctionality should be abstracted during implementation. Similarly for each\nvote implementation client CLI/REST functionality should be abstracted\nto be reused for multiple elections.\nThe specialization group abstraction firstly extends the `Electionator`\nbut also further defines traits of the group.\n``` golang\ntype SpecializationGroup interface {\nElectionator\nGetName() string\nGetDescription() string\n// general soft contract the group is expected\n// to fulfill with the greater community\nGetContract() string\n// messages which can be executed by the members of the group\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\n// logic to be executed at endblock, this may for instance\n// include payment of a stipend to the group members\n// for participation in the security group.\nEndBlocker(ctx sdk.Context)\n}\n```\n",To establish blockchain specialization groups within the community to enable delegated responsibilities and enhance community organization.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user’s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user/computer.\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don’t provide a native secret store.\n\n## Decision\n",We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https://github.com/99designs/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user’s machine.\n,"The decision is to use a Secret Store to store key material and metadata.\n\nA Secret Store is a secure, encrypted storage mechanism for sensitive data, such as passwords, API keys, and certificates. It provides a common abstraction layer to the many different backends and a reasonable fallback for minimal platforms that don’t provide a native secret store.\n\nUsing a Secret Store will address the security concerns raised in the context. By storing key material and metadata in an encrypted format, it will be more difficult for attackers to access and use this information.\n\nIn addition, using a Secret Store will provide a consistent way to access and manage secrets across different platforms. This will make it easier for developers to build and deploy applications that use secrets.\n\nThe following are some of the benefits of using a Secret Store:\n\n* **Improved security:** Secrets are stored in an encrypted format, making them more difficult for attackers to access.\n* **Centralized management:** Secrets can be managed from a central location, making it easier to track and control their use.\n* **Improved convenience:** Developers can access secrets without having to manage the underlying infrastructure.\n* **Increased flexibility:** Secret Stores can be used to store a variety of different types of secrets, including passwords, API keys, and certificates.\n\nOverall, using a Secret Store is a good decision for improving the security and manageability of secrets."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current ""naive"" IBC Relayer strategy currently establishes a single predetermined IBC channel atop a single connection between two clients (each potentially of a different chain).  This strategy then detects packets to be relayed by watching for `send_packet` and `recv_packet` events matching that channel, and sends the necessary transactions to relay those packets.\nWe wish to expand this ""naive"" strategy to a ""passive"" one which detects and relays both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.\nIn order to accomplish this, we propose adding more comprehensive events to expose channel metadata for each transaction sent from the `x/ibc/04-channel/keeper/handshake.go` and `x/ibc/04-channel/keeper/packet.go` modules.\nHere is an example of what would be in `ChanOpenInit`:\n```go\nconst (\nEventTypeChannelMeta = ""channel_meta""\nAttributeKeyAction = ""action""\nAttributeKeyHops = ""hops""\nAttributeKeyOrder = ""order""\nAttributeKeySrcPort = ""src_port""\nAttributeKeySrcChannel = ""src_channel""\nAttributeKeySrcVersion = ""src_version""\nAttributeKeyDstPort = ""dst_port""\nAttributeKeyDstChannel = ""dst_channel""\nAttributeKeyDstVersion = ""dst_version""\n)\n// ...\n// Emit Event with Channel metadata for the relayer to pick up and\n// relay to the other chain\n// This appears immediately before the successful return statement.\nctx.EventManager().EmitEvents(sdk.Events{\nsdk.NewEvent(\ntypes.EventTypeChannelMeta,\nsdk.NewAttribute(types.AttributeKeyAction, ""open_init""),\nsdk.NewAttribute(types.AttributeKeySrcConnection, connectionHops[0]),\nsdk.NewAttribute(types.AttributeKeyHops, strings.Join(connectionHops, "","")),\nsdk.NewAttribute(types.AttributeKeyOrder, order.String()),\nsdk.NewAttribute(types.AttributeKeySrcPort, portID),\nsdk.NewAttribute(types.AttributeKeySrcChannel, chanenlID),\nsdk.NewAttribute(types.AttributeKeySrcVersion, version),\nsdk.NewAttribute(types.AttributeKeyDstPort, counterparty.GetPortID()),\nsdk.NewAttribute(types.AttributeKeyDstChannel, counterparty.GetChannelID()),\n// The destination version is not yet known, but a value is necessary to pad\n// the event attribute offsets\nsdk.NewAttribute(types.AttributeKeyDstVersion, """"),\n),\n})\n```\nThese metadata events capture all the ""header"" information needed to route IBC channel handshake transactions without requiring the client to query any data except that of the connection ID that it is willing to relay.  It is intended that `channel_meta.src_connection` is the only event key that needs to be indexed for a passive relayer to function.\n### Handling Channel Open Attempts\nIn the case of the passive relayer, when one chain sends a `ChanOpenInit`, the relayer should inform the other chain of this open attempt and allow that chain to decide how (and if) it continues the handshake.  Once both chains have actively approved the channel opening, then the rest of the handshake can happen as it does with the current ""naive"" relayer.\nTo implement this behavior, we propose replacing the `cbs.OnChanOpenTry` callback with a new `cbs.OnAttemptChanOpenTry` callback which explicitly handles the `MsgChannelOpenTry`, usually by resulting in a call to `keeper.ChanOpenTry`.  The typical implementation, in `x/ibc-transfer/module.go` would be compatible with the current ""naive"" relayer, as follows:\n```go\nfunc (am AppModule) OnAttemptChanOpenTry(\nctx sdk.Context,\nchanKeeper channel.Keeper,\nportCap *capability.Capability,\nmsg channel.MsgChannelOpenTry,\n) (*sdk.Result, error) {\n// Require portID is the portID transfer module is bound to\nboundPort := am.keeper.GetPort(ctx)\nif boundPort != msg.PortID {\nreturn nil, sdkerrors.Wrapf(porttypes.ErrInvalidPort, ""invalid port: %s, expected %s"", msg.PortID, boundPort)\n}\n// BEGIN NEW CODE\n// Assert our protocol version, overriding the relayer's suggestion.\nmsg.Version = types.Version\n// Continue the ChanOpenTry.\nres, chanCap, err := channel.HandleMsgChannelOpenTry(ctx, chanKeeper, portCap, msg)\nif err != nil {\nreturn nil, err\n}\n// END OF NEW CODE\n// ... the rest of the callback is similar to the existing OnChanOpenTry\n// but uses msg.* directly.\n```\nHere is how this callback would be used, in the implementation of `x/ibc/handler.go`:\n```go\n// ...\ncase channel.MsgChannelOpenTry:\n// Lookup module by port capability\nmodule, portCap, err := k.PortKeeper.LookupModuleByPort(ctx, msg.PortID)\nif err != nil {\nreturn nil, sdkerrors.Wrap(err, ""could not retrieve module from port-id"")\n}\n// Retrieve callbacks from router\ncbs, ok := k.Router.GetRoute(module)\nif !ok {\nreturn nil, sdkerrors.Wrapf(port.ErrInvalidRoute, ""route not found to module: %s"", module)\n}\n// Delegate to the module's OnAttemptChanOpenTry.\nreturn cbs.OnAttemptChanOpenTry(ctx, k.ChannelKeeper, portCap, msg)\n```\nThe reason we do not have a more structured interaction between `x/ibc/handler.go` and the port's module (to explicitly negotiate versions, etc) is that we do not wish to constrain the app module to have to finish handling the `MsgChannelOpenTry` during this transaction or even this block.\n\n## Decision\n","- Expose events to allow ""passive"" connection relayers.\n- Enable application-initiated channels via such passive relayers.\n- Allow port modules to control how to handle open-try messages.\n","The passive IBC Relayer will expand upon the current ""naive"" strategy by detecting and relaying both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.  In order to accomplish this, comprehensive events will be added to expose channel metadata for each transaction sent from the `x/ibc/04-channel/keeper/handshake.go` and `x/ibc/04-channel/keeper/packet.go` modules.  These metadata events capture all the ""header"" information needed to route IBC channel handshake transactions without requiring the client to query any data except that of the connection ID that it is willing to relay.  It is intended that `channel_meta.src_connection` is the only event key that needs to be indexed for a passive relayer to function.  In the case of the passive relayer, when one chain sends a `ChanOpenInit`, the relayer should inform the other chain of this open attempt and allow that chain to decide how (and if) it continues the handshake.  Once both chains have actively approved the channel opening, then the rest of the handshake can happen as it does with the current ""naive"" relayer.  To implement this behavior, we replace the `cbs.OnChanOpenTry` callback with a new `cbs.OnAttemptChanOpenTry` callback which explicitly handles the `MsgChannelOpenTry`, usually by resulting in a call to `keeper.ChanOpenTry`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Summary\nAt launch, IBC will be a novel protocol, without an experienced user-base. At the protocol layer, it is not possible to distinguish between client expiry or misbehaviour due to genuine faults (Byzantine behavior) and client expiry or misbehaviour due to user mistakes (failing to update a client, or accidentally double-signing). In the base IBC protocol and ICS 20 fungible token transfer implementation, if a client can no longer be updated, funds in that channel will be permanently locked and can no longer be transferred. To the degree that it is safe to do so, it would be preferable to provide users with a recovery mechanism which can be utilised in these exceptional cases.\n### Exceptional cases\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\n1. The chain which the client is following has halted and is no longer producing blocks/headers, so no updates can be made to the client\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\n### Security model\nTwo-thirds of the validator set (the quorum for governance, module participation) can already sign arbitrary data, so allowing governance to manually force-update a client with a new header after a delay period does not substantially alter the security model.\n\n## Decision\n","We elect not to deal with chains which have actually halted, which is necessarily Byzantine behaviour and in which case token recovery is not likely possible anyways (in-flight packets cannot be timed-out, but the relative impact of that is minor).\n1. Require Tendermint light clients (ICS 07) to be created with the following additional flags\n1. `allow_governance_override_after_expiry` (boolean, default false)\n1. Require Tendermint light clients (ICS 07) to expose the following additional internal query functions\n1. `Expired() boolean`, which returns whether or not the client has passed the trusting period since the last update (in which case no headers can be validated)\n1. Require Tendermint light clients (ICS 07) to expose the following additional state mutation functions\n1. `Unfreeze()`, which unfreezes a light client after misbehaviour and clears any frozen height previously set\n1. Require Tendermint light clients (ICS 07) & solo machine clients (ICS 06) to be created with the following additional flags\n1. `allow_governance_override_after_misbehaviour` (boolean, default false)\n1. Add a new governance proposal type, `ClientUpdateProposal`, in the `x/ibc` module\n1. Extend the base `Proposal` with a client identifier (`string`) and a header (`bytes`, encoded in a client-type-specific format)\n1. If this governance proposal passes, the client is updated with the provided header, if and only if:\n1. `allow_governance_override_after_expiry` is true and the client has expired (`Expired()` returns true)\n1. `allow_governance_override_after_misbehaviour` is true and the client has been frozen (`Frozen()` returns true)\n1. In this case, additionally, the client is unfrozen by calling `Unfreeze()`\nNote additionally that the header submitted by governance must be new enough that it will be possible to update the light client after the new header is inserted into the client state (which will only happen after the governance proposal has passed).\nThis ADR does not address planned upgrades, which are handled separately as per the [specification](https://github.com/cosmos/ics/tree/master/spec/ics-007-tendermint-client#upgrades).\n","**Decision:** Implement a governance proposal type to allow governance to manually force-update a client with a new header after a delay period.\n\n**Rationale:**\n\n* This mechanism provides a recovery mechanism for exceptional cases where funds would otherwise be permanently locked.\n* The security model is not substantially altered, as two-thirds of the validator set can already sign arbitrary data.\n* The delay period allows for community discussion and consensus before the client is updated.\n\n**Consequences:**\n\n* Users will have a recovery mechanism for exceptional cases.\n* Governance will have the ability to manually update clients, but this will require a delay period and community consensus.\n* The security model will remain in place, as two-thirds of the validator set can already sign arbitrary data."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md) and\n[ARD 020](./adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nThis ADR continues from [ARD 020](./adr-020-protobuf-transaction-encoding.md)\nto specify the encoding of queries.\n\n## Decision\n","### Custom Query Definition\nModules define custom queries through a protocol buffers `service` definition.\nThese `service` definitions are generally associated with and used by the\nGRPC protocol. However, the protocol buffers specification indicates that\nthey can be used more generically by any request/response protocol that uses\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\nEach module with custom queries should define a service canonically named `Query`:\n```proto\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\n}\n```\n#### Handling of Interface Types\nModules that use interface types and need true polymorphism generally force a\n`oneof` up to the app-level that provides the set of concrete implementations of\nthat interface that the app supports. While app's are welcome to do the same for\nqueries and implement an app-level query service, it is recommended that modules\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\nThere is a concern on the transaction level that the overhead of `Any` is too\nhigh to justify its usage. However for queries this is not a concern, and\nproviding generic module-level queries that use `Any` does not preclude apps\nfrom also providing app-level queries that return use the app-level `oneof`s.\nA hypothetical example for the `gov` module would look something like:\n```proto\n// x/gov/types/types.proto\nimport ""google/protobuf/any.proto"";\nservice Query {\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\n}\nmessage AnyProposal {\nProposalBase base = 1;\ngoogle.protobuf.Any content = 2;\n}\n```\n### Custom Query Implementation\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https://github.com/gogo/protobuf)\ngrpc plugin, which for a service named `Query` generates an interface named\n`QueryServer` as below:\n```go\ntype QueryServer interface {\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\n}\n```\nThe custom queries for our module are implemented by implementing this interface.\nThe first parameter in this generated interface is a generic `context.Context`,\nwhereas querier methods generally need an instance of `sdk.Context` to read\nfrom the store. Since arbitrary values can be attached to `context.Context`\nusing the `WithValue` and `Value` methods, the SDK should provide a function\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\n`context.Context`.\nAn example implementation of `QueryBalance` for the bank module as above would\nlook something like:\n```go\ntype Querier struct {\nKeeper\n}\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\nreturn &balance, nil\n}\n```\n### Custom Query Registration and Routing\nQuery server implementations as above would be registered with `AppModule`s using\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\nas below:\n```go\n// x/bank/module.go\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\n}\n```\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\nquery routing table (with the routing method being described below).\nThe signature for this method matches the existing\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\nquery server implementation described above.\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\nand method name (ex. `QueryBalance`) combined with `/`s to form a full\nmethod name (ex. `/cosmos_sdk.x.bank.v1.Query/QueryBalance`). This gets translated\ninto an ABCI query as `custom/cosmos_sdk.x.bank.v1.Query/QueryBalance`. Service handlers\nregistered with `QueryRouter.RegisterService` will be routed this way.\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\n`sdk.Query` and `QueryRouter` infrastructure.\nThis basic specification allows us to reuse protocol buffer `service` definitions\nfor ABCI custom queries substantially reducing the need for manual decoding and\nencoding in query methods.\n### GRPC Protocol Support\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\nproxy server that routes requests in the GRPC protocol to ABCI query requests\nunder the hood. In this way, clients could use their host languages' existing\nGRPC implementations to make direct queries against Cosmos SDK app's using\nthese `service` definitions. In order for this server to work, the `QueryRouter`\non `BaseApp` will need to expose the service handlers registered with\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\nlaunch the proxy server on a separate port in the same process as the ABCI app\nwith a command-line flag.\n### REST Queries and Swagger Generation\n[grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway) is a project that\ntranslates REST calls into GRPC calls using special annotations on service\nmethods. Modules that want to expose REST queries should add `google.api.http`\nannotations to their `rpc` methods as in this example below.\n```proto\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balance/{address}/{denom}""\n};\n}\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balances/{address}""\n};\n}\n}\n```\ngrpc-gateway will work direcly against the GRPC proxy described above which will\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\ngenerate Swagger definitions automatically.\nIn the current implementation of REST queries, each module needs to implement\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\napproach, there will be no need to generate separate REST query handlers, just\nquery servers as described above as grpc-gateway handles the translation of protobuf\nto REST as well as Swagger definitions.\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\na separate process or the same process as the ABCI app, as well as provide a\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\nfile.\n### Client Usage\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\ninterface like:\n```go\ntype QueryClient interface {\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\n}\n```\nVia a small patch to gogo protobuf ([gogo/protobuf#675](https://github.com/gogo/protobuf/pull/675))\nwe have tweaked the grpc codegen to use an interface rather than concrete type\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\nfor ABCI client queries.\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\nthat routes calls to ABCI queries\nClients (such as CLI methods) will then be able to call query methods like this:\n```go\nclientCtx := client.NewContext()\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\nparams := &types.QueryBalanceParams{addr, denom}\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\n```\n### Testing\nTests would be able to create a query client directly from keeper and `sdk.Context`\nreferences using a `QueryServerTestHelper` as below:\n```go\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\nqueryClient := types.NewQueryClient(queryHelper)\n```\n",TBC
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to support building highly secure, robust and interoperable blockchain\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\nevidence can be submitted, evaluated and verified resulting in some agreed upon\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\nFurthermore, such a mechanism is paramount for any\n[IBC](https://github.com/cosmos/ics/blob/master/ibc/2_IBC_ARCHITECTURE.md) or\ncross-chain validation protocol implementation in order to support the ability\nfor any misbehavior to be relayed back from a collateralized chain to a primary\nchain so that the equivocating validator(s) can be slashed.\n\n## Decision\n","We will implement an evidence module in the Cosmos SDK supporting the following\nfunctionality:\n- Provide developers with the abstractions and interfaces necessary to define\ncustom evidence messages, message handlers, and methods to slash and penalize\naccordingly for misbehavior.\n- Support the ability to route evidence messages to handlers in any module to\ndetermine the validity of submitted misbehavior.\n- Support the ability, through governance, to modify slashing penalties of any\nevidence type.\n- Querier implementation to support querying params, evidence types, params, and\nall submitted valid misbehavior.\n### Types\nFirst, we define the `Evidence` interface type. The `x/evidence` module may implement\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\nIn addition, other modules may implement their own `Evidence` types in a similar\nmanner in which governance is extensible. It is important to note any concrete\ntype implementing the `Evidence` interface may include arbitrary fields such as\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\nWhen submitting evidence to the `x/evidence` module, the concrete type must provide\nthe validator's consensus address, which should be known by the `x/slashing`\nmodule (assuming the infraction is valid), the height at which the infraction\noccurred and the validator's power at same height in which the infraction occurred.\n```go\ntype Evidence interface {\nRoute() string\nType() string\nString() string\nHash() HexBytes\nValidateBasic() error\n// The consensus address of the malicious validator at time of infraction\nGetConsensusAddress() ConsAddress\n// Height at which the infraction occurred\nGetHeight() int64\n// The total power of the malicious validator at time of infraction\nGetValidatorPower() int64\n// The total validator set power at time of infraction\nGetTotalPower() int64\n}\n```\n### Routing & Handling\nEach `Evidence` type must map to a specific unique route and be registered with\nthe `x/evidence` module. It accomplishes this through the `Router` implementation.\n```go\ntype Router interface {\nAddRoute(r string, h Handler) Router\nHasRoute(r string) bool\nGetRoute(path string) Handler\nSeal()\n}\n```\nUpon successful routing through the `x/evidence` module, the `Evidence` type\nis passed through a `Handler`. This `Handler` is responsible for executing all\ncorresponding business logic necessary for verifying the evidence as valid. In\naddition, the `Handler` may execute any necessary slashing and potential jailing.\nSince slashing fractions will typically result from some form of static functions,\nallow the `Handler` to do this provides the greatest flexibility. An example could\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\nby governance. The `Evidence` type should provide all the external information\nnecessary in order for the `Handler` to make the necessary state transitions.\nIf no error is returned, the `Evidence` is considered valid.\n```go\ntype Handler func(Context, Evidence) error\n```\n### Submission\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\nhandled by the `x/evidence` module's `SubmitEvidence`.\n```go\ntype MsgSubmitEvidence struct {\nEvidence\n}\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\nreturn err.Result()\n}\n// emit events...\nreturn Result{\n// ...\n}\n}\n```\nThe `x/evidence` module's keeper is responsible for matching the `Evidence` against\nthe module's router and invoking the corresponding `Handler` which may include\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\n```go\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\nhandler := keeper.router.GetRoute(evidence.Route())\nif err := handler(ctx, evidence); err != nil {\nreturn ErrInvalidEvidence(keeper.codespace, err)\n}\nkeeper.setEvidence(ctx, evidence)\nreturn nil\n}\n```\n### Genesis\nFinally, we need to represent the genesis state of the `x/evidence` module. The\nmodule only needs a list of all submitted valid infractions and any necessary params\nfor which the module needs in order to handle submitted evidence. The `x/evidence`\nmodule will naturally define and route native evidence types for which it'll most\nlikely need slashing penalty constants for.\n```go\ntype GenesisState struct {\nParams       Params\nInfractions  []Evidence\n}\n```\n","The Cosmos SDK will implement a generic slashing mechanism in which arbitrary evidence can be submitted, evaluated and verified resulting in some agreed upon penalty for any misbehavior committed by a validator. The slashing mechanism will be designed to support a wide range of misbehavior types, including equivocation (double-voting), signing when unbonded, and signing an incorrect state transition. The slashing mechanism will also be extensible to support new misbehavior types in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\nBy default, when telemetry is enabled, the application should track and expose metrics that are\nstored in-memory. The operator may choose to enable additional sinks, where we support only\n[Prometheus](https://prometheus.io/) for now, as it's battle-tested, simple to setup, open source,\nand is rich with ecosystem tooling.\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\nmetrics may be added or removed at will and without much friction. To do this, we will use the\n[go-metrics](https://github.com/armon/go-metrics) library.\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\nwill be exposed via `/metrics?format={text|prometheus}` via the API server.\n\n## Decision\n","We will add an additional configuration block to `app.toml` that defines telemetry settings:\n```toml\n###############################################################################\n###                         Telemetry Configuration                         ###\n###############################################################################\n[telemetry]\n# Prefixed with keys to separate services\nservice-name = {{ .Telemetry.ServiceName }}\n# Enabled enables the application telemetry functionality. When enabled,\n# an in-memory sink is also enabled by default. Operators may also enabled\n# other sinks such as Prometheus.\nenabled = {{ .Telemetry.Enabled }}\n# Enable prefixing gauge values with hostname\nenable-hostname = {{ .Telemetry.EnableHostname }}\n# Enable adding hostname to labels\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\n# Enable adding service to labels\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\n```\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\n```go\n// Metrics defines a wrapper around application telemetry functionality. It allows\n// metrics to be gathered at any point in time. When creating a Metrics object,\n// internally, a global metrics is registered with a set of sinks as configured\n// by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\n// dump of formatted recent metrics will be sent to STDERR.\ntype Metrics struct {\nmemSink           *metrics.InmemSink\nprometheusEnabled bool\n}\n// Gather collects all registered metrics and returns a GatherResponse where the\n// metrics are encoded depending on the type. Metrics are either encoded via\n// Prometheus or JSON if in-memory.\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\nswitch format {\ncase FormatPrometheus:\nreturn m.gatherPrometheus()\ncase FormatText:\nreturn m.gatherGeneric()\ncase FormatDefault:\nreturn m.gatherGeneric()\ndefault:\nreturn GatherResponse{}, fmt.Errorf(""unsupported metrics format: %s"", format)\n}\n}\n```\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\nhandler accordingly.\n```go\nfunc (s *Server) Start(cfg config.Config) error {\n// ...\nif cfg.Telemetry.Enabled {\nm, err := telemetry.New(cfg.Telemetry)\nif err != nil {\nreturn err\n}\ns.metrics = m\ns.registerMetrics()\n}\n// ...\n}\nfunc (s *Server) registerMetrics() {\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\nformat := strings.TrimSpace(r.FormValue(""format""))\ngr, err := s.metrics.Gather(format)\nif err != nil {\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(""failed to gather metrics: %s"", err))\nreturn\n}\nw.Header().Set(""Content-Type"", gr.ContentType)\n_, _ = w.Write(gr.Metrics)\n}\ns.Router.HandleFunc(""/metrics"", metricsHandler).Methods(""GET"")\n}\n```\nApplication developers may track counters, gauges, summaries, and key/value metrics. There is no\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\n```go\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\ndefer metrics.MeasureSince(time.Now(), ""MintCoins"")\n// ...\n}\n```\n","To implement telemetry, we will use the go-metrics library to expose metrics from modules and other core parts of the Cosmos SDK. Metrics will be stored in-memory by default, and operators can choose to enable additional sinks, such as Prometheus. When telemetry is enabled, metrics will be exposed via `/metrics?format={text|prometheus}` via the API server. Operators can enable telemetry with specific configuration options."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a need for a scalable structure of the SDK documentation. Current documentation includes a lot of non-related SDK material, is difficult to maintain and hard to follow as a user.\nIdeally, we would have:\n- All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\n- All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\n\n## Decision\n","Re-structure the `/docs` folder of the SDK github repo as follows:\n```\ndocs/\n├── README\n├── intro/\n├── concepts/\n│   ├── baseapp\n│   ├── types\n│   ├── store\n│   ├── server\n│   ├── modules/\n│   │   ├── keeper\n│   │   ├── handler\n│   │   ├── cli\n│   ├── gas\n│   └── commands\n├── clients/\n│   ├── lite/\n│   ├── service-providers\n├── modules/\n├── spec/\n├── translations/\n└── architecture/\n```\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\n- `README`: Landing page of the docs.\n- `intro`: Introductory material. Goal is to have a short explainer of the SDK and then channel people to the resource they need. The [sdk-tutorial](https://github.com/cosmos/sdk-application-tutorial/) will be highlighted, as well as the `godocs`.\n- `concepts`: Contains high-level explanations of the abstractions of the SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\n- `clients`: Contains specs and info about the various SDK clients.\n- `spec`: Contains specs of modules, and others.\n- `modules`: Contains links to `godocs` and the spec of the modules.\n- `architecture`: Contains architecture-related docs like the present one.\n- `translations`: Contains different translations of the documentation.\nWebsite docs sidebar will only include the following sections:\n- `README`\n- `intro`\n- `concepts`\n- `clients`\n`architecture` need not be displayed on the website.\n","Move all documentation related to specific SDKs, frameworks, or tools to their respective GitHub repositories. This includes:\n\n* SDK documentation to the SDK repository\n* Hub documentation to the Hub repository\n* Lotion documentation to the Lotion repository\n\nKeep all other documentation, such as FAQs, whitepapers, and high-level material about Cosmos, on the website."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFull implementation of the [IBC specification](https://github.com/cosmos/ics) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\nas described in [ICS 5](https://github.com/cosmos/ics/tree/master/spec/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\nobject-capability keys at this time.\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L132))\nand passed to Keepers as fixed arguments ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L160)). Keepers cannot create or store capability keys during transaction execution — although they could call `NewKVStoreKey` and take the memory address\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional — were this not the case, the keys would be predictable and couldn't serve as object capabilities).\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\nThis ADR proposes such an interface & mechanism.\n\n## Decision\n","The SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\ninvoked.\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\nchain is running.\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\nThe persistent `KVStore` tracks which capability is owned by which modules.\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\na reverse mapping that map from module name, capability name to the capability index.\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\ngo-map to retrieve the capability at the original memory location.\nThe `CapabilityKeeper` will define the following types & functions:\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\na name. A `String()` method is provided for debugging.\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\n```golang\ntype Capability struct {\nindex uint64\n}\n```\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\n```golang\ntype CapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleNames   map[string]interface{}\nsealed        bool\n}\n```\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\npassed by other modules.\n```golang\ntype ScopedCapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleName    string\n}\n```\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\nIt MUST be called before `InitialiseAndSeal`.\n```golang\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\nif k.sealed {\npanic(""cannot scope to module via a sealed capability keeper"")\n}\nif _, ok := k.scopedModules[moduleName]; ok {\npanic(fmt.Sprintf(""cannot create multiple scoped keepers for the same module name: %s"", moduleName))\n}\nk.scopedModules[moduleName] = struct{}{}\nreturn ScopedKeeper{\ncdc:      k.cdc,\nstoreKey: k.storeKey,\nmemKey:   k.memKey,\ncapMap:   k.capMap,\nmodule:   moduleName,\n}\n}\n```\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\ncreation of any new `ScopedCapabilityKeeper`s.\n```golang\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\nif ck.sealed {\npanic(""capability keeper is sealed"")\n}\npersistentStore := ctx.KVStore(ck.persistentKey)\nmap := ctx.KVStore(ck.memKey)\n// initialise memory store for all names in persistent store\nfor index, value := range persistentStore.Iter() {\ncapability = &CapabilityKey{index: index}\nfor moduleAndCapability := range value {\nmoduleName, capabilityName := moduleAndCapability.Split(""/"")\nmemStore.Set(moduleName + ""/fwd/"" + capability, capabilityName)\nmemStore.Set(moduleName + ""/rev/"" + capabilityName, index)\nck.capMap[index] = capability\n}\n}\nck.sealed = true\n}\n```\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\nreference. The newly created capability is automatically persisted; the calling module need not\ncall `ClaimCapability`.\n```golang\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\n// check name not taken in memory store\nif capStore.Get(""rev/"" + name) != nil {\nreturn nil, errors.New(""name already taken"")\n}\n// fetch the current index\nindex := persistentStore.Get(""index"")\n// create a new capability\ncapability := &CapabilityKey{index: index}\n// set persistent store\npersistentStore.Set(index, Set.singleton(sck.moduleName + ""/"" + name))\n// update the index\nindex++\npersistentStore.Set(""index"", index)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to index\nmemStore.Set(sck.moduleName + ""/rev/"" + name, index)\n// set the in-memory mapping from index to capability pointer\ncapMap[index] = capability\n// return the newly created capability\nreturn capability\n}\n```\n`AuthenticateCapability` can be called by any module to check that a capability\ndoes in fact correspond to a particular name (the name can be untrusted user input)\nwith which the calling module previously associated it.\n```golang\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\n// return whether forward mapping in memory store matches name\nreturn memStore.Get(sck.moduleName + ""/fwd/"" + capability) === name\n}\n```\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\nso that future `GetCapability` calls will succeed.\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\nthey will all own it.\n```golang\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\npersistentStore := ctx.KVStore(sck.persistentKey)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to capability\nmemStore.Set(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.add(sck.moduleName + ""/"" + name)\npersistentStore.Set(capability.Index(), owners)\n}\n```\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\nThe module is not allowed to retrieve capabilities which it does not own.\n```golang\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\n// fetch the index of capability using reverse mapping in memstore\nindex := memStore.Get(sck.moduleName + ""/rev/"" + name)\n// fetch capability from go-map using index\ncapability := capMap[index]\n// return the capability\nreturn capability\n}\n```\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\nmore owners exist, the capability will be deleted globally.\n```golang\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\npersistentStore := ctx.KVStore(sck.persistentKey)\nname := capStore.Get(sck.moduleName + ""/fwd/"" + capability)\nif name == nil {\nreturn error(""capability not owned by module"")\n}\n// delete forward mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/fwd/"" + capability, name)\n// delete reverse mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.remove(sck.moduleName + ""/"" + name)\nif owners.size() > 0 {\n// there are still other owners, keep the capability around\npersistentStore.Set(capability.Index(), owners)\n} else {\n// no more owners, delete the capability\npersistentStore.Delete(capability.Index())\ndelete(capMap[capability.Index()])\n}\n}\n```\n### Usage patterns\n#### Initialisation\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\n```golang\nck := NewCapabilityKeeper(persistentKey, memoryKey)\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(""mod1""), ....)\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(""mod2""), ....)\n// other initialisation logic ...\n// load initial state...\nck.InitialiseAndSeal(initialContext)\n```\n#### Creating, passing, claiming and using capabilities\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\nModule 1 would have the following code:\n```golang\ncapability := scopedCapabilityKeeper.NewCapability(ctx, ""resourceABC"")\nmod2Keeper.SomeFunction(ctx, capability, args...)\n```\n`SomeFunction`, running in module 2, could then claim the capability:\n```golang\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\nk.sck.ClaimCapability(ctx, capability, ""resourceABC"")\n// other logic...\n}\n```\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\n```golang\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\ncapability := k.sck.GetCapability(ctx, name)\nmod1.UseResource(ctx, capability, ""resourceABC"")\n}\n```\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\n```golang\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\nif !k.sck.AuthenticateCapability(name, capability) {\nreturn errors.New(""unauthenticated"")\n}\n// do something with the resource\n}\n```\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\n","To implement the full ICS specification, the Cosmos SDK must enable the creation and authentication of object-capability keys during transaction execution.\n\nTo achieve this, the following interface will be added to the Cosmos SDK:\n\n```go\ntype CapabilityKeeper interface {\n  RegisterKey(ctx sdk.Context, key *store.KVStoreKey) error\n  GetKey(ctx sdk.Context, name string) (*store.KVStoreKey, bool)\n}\n```\n\nThe `CapabilityKeeper` interface provides methods to register and retrieve `store.KVStoreKey` objects during transaction execution.\n\nTo generate unique memory addresses for capability keys, the following mechanism will be used:\n\n1. Each `store.KVStoreKey` object will be assigned a unique identifier when it is registered with the `CapabilityKeeper`.\n2. When the application is started or restarted, the unique identifiers for all registered `store.KVStoreKey` objects will be stored in the Merkle tree.\n3. When a `store.KVStoreKey` object is needed during transaction execution, the `CapabilityKeeper` will retrieve the unique identifier for the object from the Merkle tree and generate a new memory address for the object based on the identifier.\n\nThis mechanism ensures that the memory addresses for capability keys are unique and consistent across all machines, even if the application is started or restarted.\n\nTo revert capability creation on tx failure, the following mechanism will be used:\n\n1. When a transaction is executed, the `CapabilityKeeper` will create a new map of capability keys.\n2. If the transaction succeeds, the new map of capability keys will be stored in the Merkle tree.\n3. If the transaction fails, the new map of capability keys will be discarded.\n\nThis mechanism ensures that capability keys are only created if the transaction succeeds."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nSpecifically, the client-side migration path primarily includes tx generation and\nsigning, message construction and routing, in addition to CLI & REST handlers and\nbusiness logic (i.e. queriers).\nWith this in mind, we will tackle the migration path via two main areas, txs and\nquerying. However, this ADR solely focuses on transactions. Querying should be\naddressed in a future ADR, but it should build off of these proposals.\nBased on detailed discussions ([\#6030](https://github.com/cosmos/cosmos-sdk/issues/6030)\nand [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078)), the original\ndesign for transactions was changed substantially from an `oneof` /JSON-signing\napproach to the approach described below.\n\n## Decision\n","### Transactions\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\n`sdk.Msg`s are encoding with `Any` in transactions.\nOne of the main goals of using `Any` to encode interface values is to have a\ncore set of types which is reused by apps so that\nclients can safely be compatible with as many chains as possible.\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\nformat that can serve a wide variety of use cases without breaking client\ncompatibility.\nIn order to facilitate signing, transactions are separated into `TxBody`,\nwhich will be re-used by `SignDoc` below, and `signatures`:\n```proto\n// types/types.proto\npackage cosmos_sdk.v1;\nmessage Tx {\nTxBody body = 1;\nAuthInfo auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\n// A variant of Tx that pins the signer's exact binary represenation of body and\n// auth_info. This is used for signing, broadcasting and verification. The binary\n// `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\n// becomes the ""txhash"", commonly used as the transaction ID.\nmessage TxRaw {\n// A protobuf serialization of a TxBody that matches the representation in SignDoc.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\nbytes auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\nmessage TxBody {\n// A list of messages to be executed. The required signers of those messages define\n// the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\n// Each required signer address is added to the list only the first time it occurs.\n//\n// By convention, the first required signer (usually from the first message) is referred\n// to as the primary signer and pays the fee for the whole transaction.\nrepeated google.protobuf.Any messages = 1;\nstring memo = 2;\nint64 timeout_height = 3;\nrepeated google.protobuf.Any extension_options = 1023;\n}\nmessage AuthInfo {\n// This list defines the signing modes for the required signers. The number\n// and order of elements must match the required signers from TxBody's messages.\n// The first element is the primary signer and the one which pays the fee.\nrepeated SignerInfo signer_infos = 1;\n// The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\nFee fee = 2;\n}\nmessage SignerInfo {\n// The public key is optional for accounts that already exist in state. If unset, the\n// verifier can use the required signer address for this position and lookup the public key.\nPublicKey public_key = 1;\n// ModeInfo describes the signing mode of the signer and is a nested\n// structure to support nested multisig pubkey's\nModeInfo mode_info = 2;\n// sequence is the sequence of the account, which describes the\n// number of committed transactions signed by a given address. It is used to prevent\n// replay attacks.\nuint64 sequence = 3;\n}\nmessage ModeInfo {\noneof sum {\nSingle single = 1;\nMulti multi = 2;\n}\n// Single is the mode info for a single signer. It is structured as a message\n// to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\nmessage Single {\nSignMode mode = 1;\n}\n// Multi is the mode info for a multisig public key\nmessage Multi {\n// bitarray specifies which keys within the multisig are signing\nCompactBitArray bitarray = 1;\n// mode_infos is the corresponding modes of the signers of the multisig\n// which could include nested multisig public keys\nrepeated ModeInfo mode_infos = 2;\n}\n}\nenum SignMode {\nSIGN_MODE_UNSPECIFIED = 0;\nSIGN_MODE_DIRECT = 1;\nSIGN_MODE_TEXTUAL = 2;\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\n}\n```\nAs will be discussed below, in order to include as much of the `Tx` as possible\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\nraw signatures themselves live outside of what is signed over.\nBecause we are aiming for a flexible, extensible cross-chain transaction\nformat, new transaction processing options should be added to `TxBody` as soon\nthose use cases are discovered, even if they can't be implemented yet.\nBecause there is coordination overhead in this, `TxBody` includes an\n`extension_options` field which can be used for any transaction processing\noptions that are not already covered. App developers should, nevertheless,\nattempt to upstream important improvements to `Tx`.\n### Signing\nAll of the signing modes below aim to provide the following guarantees:\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\nis signed\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\nthe final gas is fully dependent on what I am signing\nThese guarantees give the maximum amount confidence to message signers that\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\n#### `SIGN_MODE_DIRECT`\nThe ""direct"" signing behavior is to sign the raw `TxBody` bytes as broadcast over\nthe wire. This has the advantages of:\n- requiring the minimum additional client capabilities beyond a standard protocol\nbuffers implementation\n- leaving effectively zero holes for transaction malleability (i.e. there are no\nsubtle differences between the signing and encoding formats which could\npotentially be exploited by an attacker)\nSignatures are structured using the `SignDoc` below which reuses the serialization of\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\n```proto\n// types/types.proto\nmessage SignDoc {\n// A protobuf serialization of a TxBody that matches the representation in TxRaw.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\nbytes auth_info = 2;\nstring chain_id = 3;\nuint64 account_number = 4;\n}\n```\nIn order to sign in the default mode, clients take the following steps:\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\n2. Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n3. Sign the encoded `SignDoc` bytes.\n4. Build a `TxRaw` and serialize it for broadcasting.\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\nbytes encoded in `TxRaw` not based on any [""canonicalization""](https://github.com/regen-network/canonical-proto3)\nalgorithm which creates added complexity for clients in addition to preventing\nsome forms of upgradeability (to be addressed later in this document).\nSignature verifiers do:\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\n2. Create a list of required signer addresses from the messages.\n3. For each required signer:\n- Pull account number and sequence from the state.\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\n- Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n- Verify the signature at the the same list position against the serialized `SignDoc`.\n#### `SIGN_MODE_LEGACY_AMINO`\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\nsupported transaction signing. Once wallets and exchanges have had a\nchance to upgrade to protobuf based signing, this option will be disabled. In\nthe meantime, it is foreseen that disabling the current Amino signing would cause\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\nCosmos Hub and other chains may choose to disable Amino signing immediately.\nLegacy clients will be able to sign a transaction using the current Amino\nJSON format and have it encoded to protobuf using the REST `/tx/encode`\nendpoint before broadcasting.\n#### `SIGN_MODE_TEXTUAL`\nAs was discussed extensively in [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078),\nthere is a desire for a human-readable signing encoding, especially for hardware\nwallets like the [Ledger](https://www.ledger.com) which display\ntransaction contents to users before signing. JSON was an attempt at this but\nfalls short of the ideal.\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\nencoding which will replace Amino JSON. This new encoding should be even more\nfocused on readability than JSON, possibly based on formatting strings like\n[MessageFormat](http://userguide.icu-project.org/formatparse/messages).\nIn order to ensure that the new human-readable format does not suffer from\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\nto generate sign bytes.\nMultiple human-readable formats (maybe even localized messages) may be supported\nby `SIGN_MODE_TEXTUAL` when it is implemented.\n### Unknown Field Filtering\nUnknown fields in protobuf messages should generally be rejected by transaction\nprocessors because:\n- important data may be present in the unknown fields, that if ignored, will\ncause unexpected behavior for clients\n- they present a malleability vulnerability where attackers can bloat tx size\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\nnot `TxBody`)\nThere are also scenarios where we may choose to safely ignore unknown fields\n(https://github.com/cosmos/cosmos-sdk/issues/6078#issuecomment-624400188) to\nprovide graceful forwards compatibility with newer clients.\nWe propose that field numbers with bit 11 set (for most use cases this is\nthe range of 1024-2047) be considered non-critical fields that can safely be\nignored if unknown.\nTo handle this we will need a unknown field filter that:\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\nunsigned parts of `AuthInfo` if present based on the signing mode)\n- rejects unknown fields in all messages (including nested `Any`s) other than\nfields with bit 11 set\nThis will likely need to be a custom protobuf parser pass that takes message bytes\nand `FileDescriptor`s and returns a boolean result.\n### Public Key Encoding\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\nso a natural solution might be to use `Any` as we are doing for other interfaces.\nThere are, however, a limited number of public keys in existence and new ones\naren't created overnight. The proposed solution is to use a `oneof` that:\n- attempts to catalog all known key types even if a given app can't use them all\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\nEx:\n```proto\nmessage PublicKey {\noneof sum {\nbytes secp256k1 = 1;\nbytes ed25519 = 2;\n...\ngoogle.protobuf.Any any_pubkey = 15;\n}\n}\n```\nApps should only attempt to handle a registered set of public keys that they\nhave tested. The provided signature verification ante handler decorators will\nenforce this.\n### CLI & REST\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\nin the client can be interfaces, similar to how we described in [ADR 019](./adr-019-protobuf-state-encoding.md),\nthe client logic will now need to take a codec interface that knows not only how\nto handle all the types, but also knows how to generate transactions, signatures,\nand messages.\n```go\ntype AccountRetriever interface {\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\n}\ntype Generator interface {\nNewTx() TxBuilder\nNewFee() ClientFee\nNewSignature() ClientSignature\nMarshalTx(tx types.Tx) ([]byte, error)\n}\ntype TxBuilder interface {\nGetTx() sdk.Tx\nSetMsgs(...sdk.Msg) error\nGetSignatures() []sdk.Signature\nSetSignatures(...sdk.Signature)\nGetFee() sdk.Fee\nSetFee(sdk.Fee)\nGetMemo() string\nSetMemo(string)\n}\n```\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\na `Context` which should have all of these fields pre-populated.\nEach client method should then use one of the `Init` methods to re-initialize\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\ngenerate or broadcast a transaction. For example:\n```go\nimport ""github.com/spf13/cobra""\nimport ""github.com/cosmos/cosmos-sdk/client""\nimport ""github.com/cosmos/cosmos-sdk/client/tx""\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\nreturn &cobra.Command{\nRunE: func(cmd *cobra.Command, args []string) error {\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\nmsg := NewSomeMsg{...}\ntx.GenerateOrBroadcastTx(clientCtx, msg)\n},\n}\n}\n```\n",This ADR does not contain a Decision section.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion/consideration.\n\n## Decision\n","We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\nThus, we propose the following mechanism:\n### Params:\n- The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the the default voting period that all governance proposal voting periods start with.\n- There is a new gov param called `MaxVotingPeriodExtension`.\n### Mechanism\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the guage the conentiousness/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\n### Dealing with Unbonding/Redelegation\nThere is one thing that needs to be addressed.  How to deal with redelegation/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\n### Delegators\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and ""unextend"" as that would contradict the ""voting power length can only be ratcheted up"" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\n","The voting period for governance proposals will be variable, depending on the proposal's complexity and level of controversy. The governance team will assess each proposal and determine the appropriate voting period.\n\n**Pros:**\n\n* Allows for more efficient handling of non-contentious proposals\n* Provides more time for discussion and consideration of complex or contentious proposals\n* Ensures that all proposals receive the appropriate level of attention\n\n**Cons:**\n\n* May require additional effort from the governance team to assess each proposal\n* Could potentially lead to delays in the implementation of important proposals\n\n**Alternatives Considered:**\n\n* Keeping the voting period the same for all proposals\n* Implementing a tiered voting system, with different voting periods for different tiers of proposals\n\n**Decision Rationale:**\n\nThe benefits of a variable voting period outweigh the drawbacks. By allowing the voting period to vary, the governance team can ensure that each proposal receives the appropriate level of attention and consideration. This will lead to more efficient and effective governance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order for the Cosmos SDK to implement the [IBC specification](https://github.com/cosmos/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\n\n## Decision\n","The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\n```golang\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper, req abci.RequestBeginBlock) abci.ResponseBeginBlock {\ninfo := HistoricalInfo{\nHeader: ctx.BlockHeader(),\nValSet: keeper.StakingKeeper.GetAllValidators(ctx), // note that this must be stored in a canonical order\n}\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\nn := keeper.GetParamRecentHeadersToStore()\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\n// continue handling request\n}\n```\nAlternatively, the application MAY store only the hash of the validator set.\nThe application MUST make these past `n` committed headers available for querying by SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x/staking` or `x/ibc`).\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\n","The Cosmos SDK will introduce the `ConsensusState` interface that modules can implement to provide a representation of the consensus state at a given height. Modules will also implement a `ConsensusParam` interface, which will provide information about the consensus parameters such as the chain id, block gas limit, and minimum gas prices. These interfaces will be used by the IBC module to generate proofs of consensus states and consensus parameters, which can be used to verify the validity of handshakes on other chains."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the Cosmos SDK utilizes [go-amino](https://github.com/tendermint/go-amino/) for binary\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\nFrom the Amino docs:\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\n> support. See the [Proto3 spec](https://developers.google.com/protocol-buffers/docs/proto3) for more\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\n>\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\nAmino also aims to have the following goals (not a complete list):\n- Binary bytes must be decode-able with a schema.\n- Schema must be upgradeable.\n- The encoder and decoder logic must be reasonably simple.\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\nclients written in various languages while providing virtually little in the way of true backwards\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1</sup>. This is\nlargely reflected in the performance of simulations and application transaction throughput.\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\n- Language agnostic\n- Platform agnostic\n- Rich client support and thriving ecosystem\n- High performance\n- Minimal encoded message size\n- Codegen-based over reflection-based\n- Supports backward and forward compatibility\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\nmade to address client-side encoding.\n\n## Decision\n","We will adopt [Protocol Buffers](https://developers.google.com/protocol-buffers) for serializing\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\nwill provide three concrete implementations of the `Marshaler` interface: `AminoCodec`, `ProtoCodec`,\nand `HybridCodec`.\n- `AminoCodec`: Uses Amino for both binary and JSON encoding.\n- `ProtoCodec`: Uses Protobuf for or both binary and JSON encoding.\n- `HybridCodec`: Uses Amino for JSON encoding and Protobuf for binary encoding.\nUntil the client migration landscape is fully understood and designed, modules will use a `HybridCodec`\nas the concrete codec it accepts and/or extends. This means that all client JSON encoding, including\ngenesis state, will still use Amino. The ultimate goal will be to replace Amino JSON encoding with\nProtbuf encoding and thus have modules accept and/or extend `ProtoCodec`.\n### Module Codecs\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\n`Marshaler` that will be a `HybridCodec`. This migration is simple as things will just work as-is.\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\n[gogoprotobuf](https://github.com/gogo/protobuf) Value types.\nExample:\n```go\nts, err := gogotypes.TimestampProto(completionTime)\nif err != nil {\n// ...\n}\nbz := cdc.MustMarshalBinaryBare(ts)\n```\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\nto the module and will contain method contracts that know how to serialize the needed interfaces.\nExample:\n```go\n// x/auth/types/codec.go\ntype Codec interface {\ncodec.Marshaler\nMarshalAccount(acc exported.Account) ([]byte, error)\nUnmarshalAccount(bz []byte) (exported.Account, error)\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\n}\n```\n### Usage of `Any` to encode interfaces\nIn general, module-level .proto files should define messages which encode interfaces\nusing [`google.protobuf.Any`](https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/any.proto).\nAfter [extension discussion](https://github.com/cosmos/cosmos-sdk/issues/6030),\nthis was chosen as the preferred alternative to application-level `oneof`s\nas in our original protobuf design. The arguments in favor of `Any` can be\nsummarized as follows:\n* `Any` provides a simpler, more consistent client UX for dealing with\ninterfaces than app-level `oneof`s that will need to be coordinated more\ncarefully across applications. Creating a generic transaction\nsigning library using `oneof`s may be cumbersome and critical logic may need\nto be reimplemented for each chain\n* `Any` provides more resistance against human error than `oneof`\n* `Any` is generally simpler to implement for both modules and apps\nThe main counter-argument to using `Any` centers around its additional space\nand possibly performance overhead. The space overhead could be dealt with using\ncompression at the persistence layer in the future and the performance impact\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\nwith user experience as the higher order concern.\nNote, that given the SDK's decision to adopt the `Codec` interfaces described\nabove, apps can still choose to use `oneof` to encode state and transactions\nbut it is not the recommended approach. If apps do choose to use `oneof`s\ninstead of `Any` they will likely lose compatibility with client apps that\nsupport multiple chains. Thus developers should think carefully about whether\nthey care more about what is possibly a pre-mature optimization or end-user\nand client developer UX.\n### Safe usage of `Any`\nBy default, the [gogo protobuf implementation of `Any`](https://godoc.org/github.com/gogo/protobuf/types)\nuses [global type registration]( https://github.com/gogo/protobuf/blob/master/proto/properties.go#L540)\nto decode values packed in `Any` into concrete\ngo types. This introduces a vulnerability where any malicious module\nin the dependency tree could registry a type with the global protobuf registry\nand cause it to be loaded and unmarshaled by a transaction that referenced\nit in the `type_url` field.\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\nvalues into concrete types through the `InterfaceRegistry` interface which\nbears some similarity to type registration with Amino:\n```go\ntype InterfaceRegistry interface {\n// RegisterInterface associates protoName as the public name for the\n// interface passed in as iface\n// Ex:\n//   registry.RegisterInterface(""cosmos_sdk.Msg"", (*sdk.Msg)(nil))\nRegisterInterface(protoName string, iface interface{})\n// RegisterImplementations registers impls as a concrete implementations of\n// the interface iface\n// Ex:\n//  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\nRegisterImplementations(iface interface{}, impls ...proto.Message)\n}\n```\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\nto communicate the list of concrete types that satisfy an interface to clients.\nIn .proto files:\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\nmay be used via code generation, reflection &/or static linting.\nThe same struct that implements `InterfaceRegistry` will also implement an\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\n```go\ntype InterfaceUnpacker interface {\n// UnpackAny unpacks the value in any to the interface pointer passed in as\n// iface. Note that the type in any must have been registered with\n// RegisterImplementations as a concrete type for that interface\n// Ex:\n//    var msg sdk.Msg\n//    err := ctx.UnpackAny(any, &msg)\n//    ...\nUnpackAny(any *Any, iface interface{}) error\n}\n```\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\nusage of `Any`, it just introduces a security and introspection layer for\ngolang usage.\n`InterfaceRegistry` will be a member of `ProtoCodec` and `HybridCodec` as\ndescribed above. In order for modules to register interface types, app modules\ncan optionally implement the following interface:\n```go\ntype InterfaceModule interface {\nRegisterInterfaceTypes(InterfaceRegistry)\n}\n```\nThe module manager will include a method to call `RegisterInterfaceTypes` on\nevery module that implements it in order to populate the `InterfaceRegistry`.\n### Using `Any` to encode state\nThe SDK will provide support methods `MarshalAny` and `UnmarshalAny` to allow\neasy encoding of state to `Any` in `Codec` implementations. Ex:\n```go\nimport ""github.com/cosmos/cosmos-sdk/codec""\nfunc (c *Codec) MarshalEvidence(evidenceI eviexported.Evidence) ([]byte, error) {\nreturn codec.MarshalAny(evidenceI)\n}\nfunc (c *Codec) UnmarshalEvidence(bz []byte) (eviexported.Evidence, error) {\nvar evi eviexported.Evidence\nerr := codec.UnmarshalAny(c.interfaceContext, &evi, bz)\nif err != nil {\nreturn nil, err\n}\nreturn evi, nil\n}\n```\n### Using `Any` in `sdk.Msg`s\nA similar concept is to be applied for messages that contain interfaces fields.\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\nan interface:\n```protobuf\n// x/evidence/types/types.proto\nmessage MsgSubmitEvidence {\nbytes submitter = 1\n[\n(gogoproto.casttype) = ""github.com/cosmos/cosmos-sdk/types.AccAddress""\n];\ngoogle.protobuf.Any evidence = 2;\n}\n```\nNote that in order to unpack the evidence from `Any` we do need a reference to\n`InterfaceRegistry`. In order to reference evidence in methods like\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\ninterfaces before they're needed.\n### Unpacking Interfaces\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\ninterfaces wrapped in `Any` before they're needed, we create an interface\nthat `sdk.Msg`s and other types can implement:\n```go\ntype UnpackInterfacesMessage interface {\nUnpackInterfaces(InterfaceUnpacker) error\n}\n```\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\nstruct itself with a public getter `GetCachedValue() interface{}`.\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\nand stored in `cachedValue` for reference later.\nThen unpacked interface values can safely be used in any code afterwards\nwithout knowledge of the `InterfaceRegistry`\nand messages can introduce a simple getter to cast the cached value to the\ncorrect interface type.\nThis has the added benefit that unmarshaling of `Any` values only happens once\nduring initial deserialization rather than every time the value is read. Also,\nwhen `Any` values are first packed (for instance in a call to\n`NewMsgSubmitEvidence`), the original interface value is cached so that\nunmarshaling isn't needed to read it again.\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\n`GetEvidence` as follows:\n```go\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\nvar evi eviexported.Evidence\nreturn ctx.UnpackAny(msg.Evidence, *evi)\n}\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\n}\n```\n### Amino Compatibility\nOur custom implementation of `Any` can be used transparently with Amino if used\nwith the proper codec instance. What this means is that interfaces packed within\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\nhave been registered properly with Amino).\nIn order for this functionality to work:\n- **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\nnow a wrapper which properly handles `Any`**\n- **all new code should use `Marshaler` which is compatible with both amino and\nprotobuf**\n- Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\n### Why Wasn't X Chosen Instead\nFor a more complete comparison to alternative protocols, see [here](https://codeburst.io/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\n### Cap'n Proto\nWhile [Cap’n Proto](https://capnproto.org/) does seem like an advantageous alternative to Protobuf\ndue to it's native support for interfaces/generics and built in canonicalization, it does lack the\nrich client ecosystem compared to Protobuf and is a bit less mature.\n### FlatBuffers\n[FlatBuffers](https://google.github.io/flatbuffers/) is also a potentially viable alternative, with the\nprimary difference being that FlatBuffers does not need a parsing/unpacking step to a secondary\nrepresentation before you can access data, often coupled with per-object memory allocation.\nHowever, it would require great efforts into research and full understanding the scope of the migration\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\nuntrusted inputs.\n",Migrate the Cosmos SDK state machine to utilize Protobuf for state serialization.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\n\n## Decision\n","In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`’s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\nDetailed changes:\n### 1) (Un)Marshal accounts directly using amino\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\n```go\n// GenesisState - all auth state that must be provided at genesis\ntype GenesisState struct {\nParams   Params           `json:""params"" yaml:""params""`\nAccounts []GenesisAccount `json:""accounts"" yaml:""accounts""`\n}\n```\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\n```go\n// InitGenesis - Init store state from genesis data\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\nak.SetParams(ctx, data.Params)\n// load the accounts\nfor _, a := range data.Accounts {\nacc := ak.NewAccount(ctx, a) // set account number\nak.SetAccount(ctx, acc)\n}\n}\n// ExportGenesis returns a GenesisState for a given context and keeper\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\nparams := ak.GetParams(ctx)\nvar genAccounts []exported.GenesisAccount\nak.IterateAccounts(ctx, func(account exported.Account) bool {\ngenAccount := account.(exported.GenesisAccount)\ngenAccounts = append(genAccounts, genAccount)\nreturn false\n})\nreturn NewGenesisState(params, genAccounts)\n}\n```\n### 2) Register custom account types on the `auth` codec\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\nAn example custom account definition:\n```go\nimport authtypes ""github.com/cosmos/cosmos-sdk/x/auth/types""\n// Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\nfunc init() {\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, ""cosmos-sdk/ModuleAccount"")\n}\ntype ModuleAccount struct {\n...\n```\nThe `auth` codec definition:\n```go\nvar ModuleCdc *codec.LegacyAmino\nfunc init() {\nModuleCdc = codec.NewLegacyAmino()\n// register module msg's and Account interface\n...\n// leave the codec unsealed\n}\n// RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\nModuleCdc.RegisterConcrete(o, name, nil)\n}\n```\n### 3) Genesis validation for custom account types\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\n```go\ntype GenesisAccount interface {\nexported.Account\nValidate() error\n}\n```\nThen the `auth` `ValidateGenesis` function becomes:\n```go\n// ValidateGenesis performs basic validation of auth genesis data returning an\n// error for any failed validation criteria.\nfunc ValidateGenesis(data GenesisState) error {\n// Validate params\n...\n// Validate accounts\naddrMap := make(map[string]bool, len(data.Accounts))\nfor _, acc := range data.Accounts {\n// check for duplicated accounts\naddrStr := acc.GetAddress().String()\nif _, ok := addrMap[addrStr]; ok {\nreturn fmt.Errorf(""duplicate account found in genesis state; address: %s"", addrStr)\n}\naddrMap[addrStr] = true\n// check account specific validation\nif err := acc.Validate(); err != nil {\nreturn fmt.Errorf(""invalid account found in genesis state; address: %s, error: %s"", addrStr, err.Error())\n}\n}\nreturn nil\n}\n```\n### 4) Move add-genesis-account cli to `auth`\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\n### 5) Update module and vesting accounts\nUnder the new scheme, module and vesting account types need some minor updates:\n- Type registration on `auth`'s codec (shown above)\n- A `Validate` method for each `Account` concrete type\n",**Decision:** Implement account export/import in the auth keeper.\n\n**Rationale:**\n\n* **Centralized account management:** Moving account export/import to the auth keeper centralizes the logic and reduces the need for external tooling.\n* **Improved extensibility:** Allowing the auth keeper to handle all account types simplifies the process of adding new custom accounts to the SDK.\n* **Reduced maintenance burden:** Developers using custom accounts no longer have to fork and modify `genaccounts`.\n* **Enhanced security:** Exporting and importing accounts through the auth keeper ensures that the process is secure and tamper-proof.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[ICS 26 - Routing Module](https://github.com/cosmos/ics/tree/master/spec/ics-026-routing-module) defines a function [`handlePacketRecv`](https://github.com/cosmos/ics/tree/master/spec/ics-026-routing-module#packet-relay).\nIn ICS 26, the routing module is defined as a layer above each application module\nwhich verifies and routes messages to the destination modules. It is possible to\nimplement it as a separate module, however, we already have functionality to route\nmessages upon the destination identifiers in the baseapp. This ADR suggests\nto utilize existing `baseapp.router` to route packets to application modules.\nGenerally, routing module callbacks have two separate steps in them,\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\nin order to increase developer ergonomics by reducing boilerplate\nverification code.\nFor atomic multi-message transaction, we want to keep the IBC related\nstate modification to be preserved even the application side state change\nreverts. One of the example might be IBC token sending message following with\nstake delegation which uses the tokens received by the previous packet message.\nIf the token receiving fails for any reason, we might not want to keep\nexecuting the transaction, but we also don't want to abort the transaction\nor the sequence and commitment will be reverted and the channel will be stuck.\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\n\n## Decision\n","`PortKeeper` will have the capability key that is able to access only the\nchannels bound to the port. Entities that hold a `PortKeeper` will be\nable to call the methods on it which are corresponding with the methods with\nthe same names on the `ChannelKeeper`, but only with the\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\neasily construct a capability-safe `PortKeeper`. This will be addressed in\nanother ADR and we will use insecure `ChannelKeeper` for now.\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\n`Result.Code == CodeTxBreak`.\n```go\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\nmsgs := tx.GetMsgs()\n// AnteHandler\nif app.anteHandler != nil {\nanteCtx, msCache := app.cacheTxContext(ctx)\nnewCtx, err := app.anteHandler(anteCtx, tx)\nif !newCtx.IsZero() {\nctx = newCtx.WithMultiStore(ms)\n}\nif err != nil {\n// error handling logic\nreturn res\n}\nmsCache.Write()\n}\n// Main Handler\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\nresult = app.runMsgs(runMsgCtx, msgs)\n// BEGIN modification made in this ADR\nif result.IsOK() || result.IsBreak() {\n// END\nmsCache.Write()\n}\nreturn result\n}\n```\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\n`AnteDecorator` will iterate over the messages included in the transaction, type\n`switch` to check whether the message contains an incoming IBC packet, and if so\nverify the Merkle proof.\n```go\ntype ProofVerificationDecorator struct {\nclientKeeper ClientKeeper\nchannelKeeper ChannelKeeper\n}\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\nfor _, msg := range tx.GetMsgs() {\nvar err error\nswitch msg := msg.(type) {\ncase client.MsgUpdateClient:\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\ncase channel.MsgPacket:\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\ncase chanel.MsgAcknowledgement:\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\ncase channel.MsgTimeoutPacket:\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\ncase channel.MsgChannelOpenInit;\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\ndefault:\ncontinue\n}\nif err != nil {\nreturn ctx, err\n}\n}\nreturn next(ctx, tx, simulate)\n}\n```\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\nrespectively.\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\n`VerifyTimeout` will be extracted out into separated functions,\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\nwhich will be called by the application handlers after the execution.\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\nverified by the counter-party chain and increments the sequence to prevent\ndouble execution. `DeleteCommitment` will delete the commitment stored,\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\nof ordered channel.\n```go\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\n}\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\n}\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\nif channel.Ordering == types.ORDERED [\nchannel.State = types.CLOSED\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\n}\n}\n```\nEach application handler should call respective finalization methods on the `PortKeeper`\nin order to increase sequence (in case of packet) or remove the commitment\n(in case of acknowledgement and timeout).\nCalling those functions implies that the application logic has successfully executed.\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\nwhich will persist the state changes that has been already done but prevent any further\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\nincreased in the previous IBC packets(thus preventing double execution) without\nproceeding to the following messages.\nIn any case the application modules should never return state reverting result,\nwhich will make the channel unable to proceed.\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\nunder the routing module specification. Instead of define each channel handshake callback\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\n`CheckOpen` will find the correct `ChennelChecker` using the\n`PortID` and call it, which will return an error if it is unacceptable by the application.\nThe `ProofVerificationDecorator` will be inserted to the top level application.\nIt is not safe to make each module responsible to call proof verification\nlogic, whereas application can misbehave(in terms of IBC protocol) by\nmistake.\nThe `ProofVerificationDecorator` should come right after the default sybil attack\nresistent layer from the current `auth.NewAnteHandler`:\n```go\n// add IBC ProofVerificationDecorator to the Chain of\nfunc NewAnteHandler(\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\nreturn sdk.ChainAnteDecorators(\nNewSetUpContextDecorator(), // outermost AnteDecorator. SetUpContext must be called first\n...\nNewIncrementSequenceDecorator(ak),\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), // innermost AnteDecorator\n)\n}\n```\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\nExample application-side usage:\n```go\ntype AppModule struct {}\n// CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\nif channel.Ordering != UNORDERED {\nreturn ErrUncompatibleOrdering()\n}\nif channel.CounterpartyPort != ""bank"" {\nreturn ErrUncompatiblePort()\n}\nif channel.Version != """" {\nreturn ErrUncompatibleVersion()\n}\nreturn nil\n}\nfunc NewHandler(k Keeper) Handler {\nreturn func(ctx Context, msg Msg) Result {\nswitch msg := msg.(type) {\ncase MsgTransfer:\nreturn handleMsgTransfer(ctx, k, msg)\ncase ibc.MsgPacket:\nvar data PacketDataTransfer\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\nreturn err\n}\nreturn handlePacketDataTransfer(ctx, k, msg, data)\ncase ibc.MsgTimeoutPacket:\nvar data PacketDataTransfer\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\nreturn err\n}\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\n// interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\n// MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\ncase ibc.MsgChannelOpen:\nreturn handleMsgChannelOpen(ctx, k, msg)\n}\n}\n}\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\nif err != nil {\nreturn sdk.ResultFromError(err)\n}\nreturn sdk.Result{}\n}\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\nif err != nil {\n// TODO: Source chain sent invalid packet, shutdown channel\n}\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) // WriteAcknowledgement increases the sequence, preventing double spending\nreturn sdk.Result{}\n}\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\nif err != nil {\n// This chain sent invalid packet or cannot recover the funds\npanic(err)\n}\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\n// packet timeout should not fail\nreturn sdk.Result{}\n}\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\nreturn sdk.Result{}\n}\n```\n","**Decision:** Utilize the existing `baseapp.router` to route packets to application modules, and implement the verification step in the `AnteHandler` to reduce boilerplate verification code. Introduce a new `CodeType`, `CodeTxBreak`, to handle the specific case of IBC related state modification in atomic multi-message transactions, ensuring that such modifications are preserved even if the application side state change reverts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to reduce the number of parties involved with handling sensitive\ninformation in an emergency scenario, we propose the creation of a\nspecialization group named The Decentralized Computer Emergency Response Team\n(dCERT).  Initially this group's role is intended to serve as coordinators\nbetween various actors within a blockchain community such as validators,\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\naggregate and relay input from a variety of stakeholders to the developers who\nare actively devising a patch to the software, this way sensitive information\ndoes not need to be publicly disclosed while some input from the community can\nstill be gained.\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\nto ""circuit-break"" (aka. temporarily disable)  a particular message path. Note\nthat this privilege should be enabled/disabled globally with a governance\nparameter such that this privilege could start disabled and later be enabled\nthrough a parameter change proposal, once a dCERT group has been established.\nIn the future it is foreseeable that the community may wish to expand the roles\nof dCERT with further responsibilities such as the capacity to ""pre-approve"" a\nsecurity update on behalf of the community prior to a full community\nwide vote whereby the sensitive information would be revealed prior to a\nvulnerability being patched on the live network.\n\n## Decision\n","The dCERT group is proposed to include an implementation of a `SpecializationGroup`\nas defined in [ADR 007](./adr-007-specialization-groups.md). This will include the\nimplementation of:\n- continuous voting\n- slashing due to breach of soft contract\n- revoking a member due to breach of soft contract\n- emergency disband of the entire dCERT group (ex. for colluding maliciously)\n- compensation stipend from the community pool or other means decided by\ngovernance\nThis system necessitates the following new parameters:\n- blockly stipend allowance per dCERT member\n- maximum number of dCERT members\n- required staked slashable tokens for each dCERT member\n- quorum for suspending a particular member\n- proposal wager for disbanding the dCERT group\n- stabilization period for dCERT member transition\n- circuit break dCERT privileges enabled\nThese parameters are expected to be implemented through the param keeper such\nthat governance may change them at any given point.\n### Continuous Voting Electionator\nAn `Electionator` object is to be implemented as continuous voting and with the\nfollowing specifications:\n- All delegation addresses may submit votes at any point which updates their\npreferred representation on the dCERT group.\n- Preferred representation may be arbitrarily split between addresses (ex. 50%\nto John, 25% to Sally, 25% to Carol)\n- In order for a new member to be added to the dCERT group they must\nsend a transaction accepting their admission at which point the validity of\ntheir admission is to be confirmed.\n- A sequence number is assigned when a member is added to dCERT group.\nIf a member leaves the dCERT group and then enters back, a new sequence number\nis assigned.\n- Addresses which control the greatest amount of preferred-representation are\neligible to join the dCERT group (up the _maximum number of dCERT members_).\nIf the dCERT group is already full and new member is admitted, the existing\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\n- In the split situation where the dCERT group is full but a vying candidate\nhas the same amount of vote as an existing dCERT member, the existing\nmember should maintain its position.\n- In the split situation where somebody must be kicked out but the two\naddresses with the smallest number of votes have the same number of votes,\nthe address with the smallest sequence number maintains its position.\n- A stabilization period can be optionally included to reduce the\n""flip-flopping"" of the dCERT membership tail members. If a stabilization\nperiod is provided which is greater than 0, when members are kicked due to\ninsufficient support, a queue entry is created which documents which member is\nto replace which other member. While this entry is in the queue, no new entries\nto kick that same dCERT member can be made. When the entry matures at the\nduration of the  stabilization period, the new member is instantiated, and old\nmember kicked.\n### Staking/Slashing\nAll members of the dCERT group must stake tokens _specifically_ to maintain\neligibility as a dCERT member. These tokens can be staked directly by the vying\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\nbenefits for doing so). This staking mechanism should use the existing global\nunbonding time of tokens staked for network validator security. A dCERT member\ncan _only be_ a member if it has the required tokens staked under this\nmechanism. If those tokens are unbonded then the dCERT member must be\nautomatically kicked from the group.\nSlashing of a particular dCERT member due to soft-contract breach should be\nperformed by governance on a per member basis based on the magnitude of the\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\nby the dCERT group prior to being slashed by governance.\nMembership suspension by the dCERT group takes place through a voting procedure\nby the dCERT group members. After this suspension has taken place, a governance\nproposal to slash the dCERT member must be submitted, if the proposal is not\napproved by the time the rescinding member has completed unbonding their\ntokens, then the tokens are no longer staked and unable to be slashed.\nAdditionally in the case of an emergency situation of a colluding and malicious\ndCERT group, the community needs the capability to disband the entire dCERT\ngroup and likely fully slash them. This could be achieved though a special new\nproposal type (implemented as a general governance proposal) which would halt\nthe functionality of the dCERT group until the proposal was concluded. This\nspecial proposal type would likely need to also have a fairly large wager which\ncould be slashed if the proposal creator was malicious. The reason a large\nwager should be required is because as soon as the proposal is made, the\ncapability of the dCERT group to halt message routes is put on temporarily\nsuspended, meaning that a malicious actor who created such a proposal could\nthen potentially exploit a bug during this period of time, with no dCERT group\ncapable of shutting down the exploitable message routes.\n### dCERT membership transactions\nActive dCERT members\n- change of the description of the dCERT group\n- circuit break a message route\n- vote to suspend a dCERT member.\nHere circuit-breaking refers to the capability to disable a groups of messages,\nThis could for instance mean: ""disable all staking-delegation messages"", or\n""disable all distribution messages"". This could be accomplished by verifying\nthat the message route has not been ""circuit-broken"" at CheckTx time (in\n`baseapp/baseapp.go`).\n""unbreaking"" a circuit is anticipated only to occur during a hard fork upgrade\nmeaning that no capability to unbreak a message route on a live chain is\nrequired.\nNote also, that if there was a problem with governance voting (for instance a\ncapability to vote many times) then governance would be broken and should be\nhalted with this mechanism, it would be then up to the validator set to\ncoordinate and hard-fork upgrade to a patched version of the software where\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\nthey should all be severely slashed.\n","Create a specialization group called the Decentralized Computer Emergency Response Team (dCERT) to coordinate between various actors within a blockchain community during emergency scenarios, such as validators, bug-hunters, and developers. Grant the dCERT group the privilege to temporarily disable a particular message path, enabled/disabled globally with a governance parameter. Consider expanding the roles of dCERT in the future to include ""pre-approving"" security updates on behalf of the community."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\n\n## Decision\n","### Design\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\n```\nslash_amount = k * power // power is the faulting validator's voting power and k is some on-chain constant\n```\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\n```\nslash_amount = k * (power_1 + power_2 + ... + power_n) // where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\n```\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\n```\nslash_amount = k * ((power_1)^(1/r) + (power_2)^(1/r) + ... + (power_n)^(1/r))^r // where k and r are both on-chain constants\n```\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\n#### Correlation across non-sybil validators\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\n#### Parameterization\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\n#### Griefing\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\n### Implementation\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define ""recent slashes"" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define ""recent slashes"" as ones that have occured withing the last `jail period`.\n```\ntype SlashEvent struct {\nAddress                     sdk.ValAddress\nSqrtValidatorVotingPercent  sdk.Dec\nSlashedSoFar                sdk.Dec\n}\n```\nThese slash events will be pruned from the queue once they are older than their respective ""recent slash period"".\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the ""Square of Sum of Roots"" formula introduced above.\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\n","**Decision:** Implement a mechanism to charge large validators and their delegators a fee proportional to the amount of consensus power they hold.\n\n**Rationale:**\n\n* This fee aligns incentives and internalizes the negative externalities of centralization.\n* Validators and delegators who contribute to centralization will bear the associated costs, discouraging excessive concentration of consensus power.\n* The fees can be used to offset the risks associated with centralization, such as funding security audits or measures to prevent fork attacks.\n* This approach provides a transparent and fair method for distributing the costs of centralization to those who benefit from it.\n* The fee structure can be adjusted over time based on the desired level of decentralization and the amount of consensus power held by large validators."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe specification for IBC cross-chain fungible token transfers\n([ICS20](https://github.com/cosmos/ics/tree/master/spec/ics-020-fungible-token-transfer)), needs to\nbe aware of the origin of any token denomination in order to relay a `Packet` which contains the sender\nand recipient addressed in the\n[`FungibleTokenPacketData`](https://github.com/cosmos/ics/tree/master/spec/ics-020-fungible-token-transfer#data-structures).\nThe Packet relay sending works based in 2 cases (per\n[specification](https://github.com/cosmos/ics/tree/master/spec/ics-020-fungible-token-transfer#packet-relay) and [Colin Axnér](https://github.com/colin-axner)'s description):\n1. Sender chain is acting as the source zone. The coins are transferred\nto an escrow address (i.e locked) on the sender chain and then transferred\nto the receiving chain through IBC TAO logic. It is expected that the\nreceiving chain will mint vouchers to the receiving address.\n2. Sender chain is acting as the sink zone. The coins (vouchers) are burned\non the sender chain and then transferred to the receiving chain though IBC\nTAO logic. It is expected that the receiving chain, which had previously\nsent the original denomination, will unescrow the fungible token and send\nit to the receiving address.\nAnother way of thinking of source and sink zones is through the token's\ntimeline. Each send to any chain other than the one it was previously\nreceived from is a movement forwards in the token's timeline. This causes\ntrace to be added to the token's history and the destination port and\ndestination channel to be prefixed to the denomination. In these instances\nthe sender chain is acting as the source zone. When the token is sent back\nto the chain it previously received from, the prefix is removed. This is\na backwards movement in the token's timeline and the sender chain\nis acting as the sink zone.\n### Example\nAssume the following channel connections exist and that all channels use the port ID `transfer`:\n- chain `A` has channels with chain `B` and chain `C` with the IDs `channelToB` and `channelToC`, respectively\n- chain `B` has channels with chain `A` and chain `C` with the IDs `channelToA` and `channelToC`, respectively\n- chain `C` has channels with chain `A` and chain `B` with the IDs `channelToA` and `channelToB`, respectively\nThese steps of transfer between chains occur in the following order: `A -> B -> C -> A -> C`. In particular:\n1. `A -> B`: sender chain is source zone. `A` sends packet with `denom` (escrowed on `A`), `B` receives `denom` and mints and sends voucher `transfer/channelToA/denom` to recipient.\n2. `B -> C`: sender chain is source zone. `B` sends packet with `transfer/channelToA/denom` (escrowed on `B`), `C` receives `transfer/channelToA/denom` and mints and sends voucher `transfer/channelToB/transfer/channelToA/denom` to recipient.\n3. `C -> A`: sender chain is source zone. `C` sends packet with `transfer/channelToB/transfer/channelToA/denom` (escrowed on `C`), `A` receives `transfer/channelToB/transfer/channelToA/denom` and mints and sends voucher `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` to recipient.\n4. `A -> C`: sender chain is sink zone. `A` sends packet with `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` (burned on `A`), `C` receives `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom`, and unescrows and sends `transfer/channelToB/transfer/channelToA/denom` to recipient.\nThe token has a final denomination on chain `C` of `transfer/channelToB/transfer/channelToA/denom`, where `transfer/channelToB/transfer/channelToA` is the trace information.\nIn this context, upon a receive of a cross-chain fungible token transfer, if the sender chain is the source of the token, the protocol prefixes the denomination with the port and channel identifiers in the following format:\n```typescript\nprefix + denom = {destPortN}/{destChannelN}/.../{destPort0}/{destChannel0}/denom\n```\nExample: transferring `100 uatom` from port `HubPort` and channel `HubChannel` on the Hub to\nEthermint's port `EthermintPort` and channel `EthermintChannel` results in `100\nEthermintPort/EthermintChannel/uatom`, where `EthermintPort/EthermintChannel/uatom` is the new\ndenomination on the receiving chain.\nIn the case those tokens are transferred back to the Hub (i.e the **source** chain), the prefix is\ntrimmed and the token denomination updated to the original one.\n### Problem\nThe problem of adding additional information to the coin denomination is twofold:\n1. The ever increasing length if tokens are transferred to zones other than the source:\nIf a token is transferred `n` times via IBC to a sink chain, the token denom will contain `n` pairs\nof prefixes, as shown on the format example above. This poses a problem because, while port and\nchannel identifiers have a maximum length of 64 each, the SDK `Coin` type only accepts denoms up to\n64 characters. Thus, a single cross-chain token, which again, is composed by the port and channels\nidentifiers plus the base denomination, can exceed the length validation for the SDK `Coins`.\nThis can result in undesired behaviours such as tokens not being able to be transferred to multiple\nsink chains if the denomination exceeds the length or unexpected `panics` due to denomination\nvalidation failing on the receiving chain.\n2. The existence of special characters and uppercase letters on the denomination:\nIn the SDK every time a `Coin` is initialized through the constructor function `NewCoin`, a validation\nof a coin's denom is performed according to a\n[Regex](https://github.com/cosmos/cosmos-sdk/blob/a940214a4923a3bf9a9161cd14bd3072299cd0c9/types/coin.go#L583),\nwhere only lowercase alphanumeric characters are accepted. While this is desirable for native denominations\nto keep a clean UX, it presents a challenge for IBC as ports and channels might be randomly\ngenerated with special and uppercase characters as per the [ICS 024 - Host\nRequirements](https://github.com/cosmos/ics/tree/master/spec/ics-024-host-requirements#paths-identifiers-separators)\nspecification.\n\n## Decision\n","The issues outlined above, are applicable only to SDK-based chains, and thus the proposed solution\nare do not require specification changes that would result in modification to other implementations\nof the ICS20 spec.\nInstead of adding the identifiers on the coin denomination directly, the proposed solution hashes\nthe denomination prefix in order to get a consistent length for all the cross-chain fungible tokens.\nThis will be used for internal storage only, and when transferred via IBC to a different chain, the\ndenomination specified on the packed data will be the full prefix path of the identifiers needed to\ntrace the token back to the originating chain, as specified on ICS20.\nThe new proposed format will be the following:\n```golang\nibcDenom = ""ibc/"" + hash(trace path + ""/"" + base denom)\n```\nThe hash function will be a SHA256 hash of the fields of the `DenomTrace`:\n```protobuf\n// DenomTrace contains the base denomination for ICS20 fungible tokens and the source tracing\n// information\nmessage DenomTrace {\n// chain of port/channel identifiers used for tracing the source of the fungible token\nstring path = 1;\n// base denomination of the relayed fungible token\nstring base_denom = 2;\n}\n```\nThe `IBCDenom` function constructs the `Coin` denomination used when creating the ICS20 fungible token packet data:\n```golang\n// Hash returns the hex bytes of the SHA256 hash of the DenomTrace fields using the following formula:\n//\n// hash = sha256(tracePath + ""/"" + baseDenom)\nfunc (dt DenomTrace) Hash() tmbytes.HexBytes {\nreturn tmhash.Sum(dt.Path + ""/"" + dt.BaseDenom)\n}\n// IBCDenom a coin denomination for an ICS20 fungible token in the format 'ibc/{hash(tracePath + baseDenom)}'.\n// If the trace is empty, it will return the base denomination.\nfunc (dt DenomTrace) IBCDenom() string {\nif dt.Path != """" {\nreturn fmt.Sprintf(""ibc/%s"", dt.Hash())\n}\nreturn dt.BaseDenom\n}\n```\n### `x/ibc-transfer` Changes\nIn order to retrieve the trace information from an IBC denomination, a lookup table needs to be\nadded to the `ibc-transfer` module. These values need to also be persisted between upgrades, meaning\nthat a new `[]DenomTrace` `GenesisState` field state needs to be added to the module:\n```golang\n// GetDenomTrace retrieves the full identifiers trace and base denomination from the store.\nfunc (k Keeper) GetDenomTrace(ctx Context, denomTraceHash []byte) (DenomTrace, bool) {\nstore := ctx.KVStore(k.storeKey)\nbz := store.Get(types.KeyDenomTrace(traceHash))\nif bz == nil {\nreturn &DenomTrace, false\n}\nvar denomTrace DenomTrace\nk.cdc.MustUnmarshalBinaryBare(bz, &denomTrace)\nreturn denomTrace, true\n}\n// HasDenomTrace checks if a the key with the given trace hash exists on the store.\nfunc (k Keeper) HasDenomTrace(ctx Context, denomTraceHash []byte)  bool {\nstore := ctx.KVStore(k.storeKey)\nreturn store.Has(types.KeyTrace(denomTraceHash))\n}\n// SetDenomTrace sets a new {trace hash -> trace} pair to the store.\nfunc (k Keeper) SetDenomTrace(ctx Context, denomTrace DenomTrace) {\nstore := ctx.KVStore(k.storeKey)\nbz := k.cdc.MustMarshalBinaryBare(&denomTrace)\nstore.Set(types.KeyTrace(denomTrace.Hash()), bz)\n}\n```\nThe `MsgTransfer` will validate that the `Coin` denomination from the `Token` field contains a valid\nhash, if the trace info is provided, or that the base denominations matches:\n```golang\nfunc (msg MsgTransfer) ValidateBasic() error {\n// ...\nreturn ValidateIBCDenom(msg.Token.Denom)\n}\n```\n```golang\n// ValidateIBCDenom validates that the given denomination is either:\n//\n//  - A valid base denomination (eg: 'uatom')\n//  - A valid fungible token representation (i.e 'ibc/{hash}') per ADR 001 https://github.com/cosmos/cosmos-sdk/blob/master/docs/architecture/adr-001-coin-source-tracing.md\nfunc ValidateIBCDenom(denom string) error {\ndenomSplit := strings.SplitN(denom, ""/"", 2)\nswitch {\ncase strings.TrimSpace(denom) == """",\nlen(denomSplit) == 1 && denomSplit[0] == ""ibc"",\nlen(denomSplit) == 2 && (denomSplit[0] != ""ibc"" || strings.TrimSpace(denomSplit[1]) == """"):\nreturn sdkerrors.Wrapf(ErrInvalidDenomForTransfer, ""denomination should be prefixed with the format 'ibc/{hash(trace + \""/\"" + %s)}'"", denom)\ncase denomSplit[0] == denom && strings.TrimSpace(denom) != """":\nreturn sdk.ValidateDenom(denom)\n}\nif _, err := ParseHexHash(denomSplit[1]); err != nil {\nreturn Wrapf(err, ""invalid denom trace hash %s"", denomSplit[1])\n}\nreturn nil\n}\n```\nThe denomination trace info only needs to be updated when token is received:\n- Receiver is **source** chain: The receiver created the token and must have the trace lookup already stored (if necessary _ie_ native token case wouldn't need a lookup).\n- Receiver is **not source** chain: Store the received info. For example, during step 1, when chain `B` receives `transfer/channelToA/denom`.\n```golang\n// SendTransfer\n// ...\nfullDenomPath := token.Denom\n// deconstruct the token denomination into the denomination trace info\n// to determine if the sender is the source chain\nif strings.HasPrefix(token.Denom, ""ibc/"") {\nfullDenomPath, err = k.DenomPathFromHash(ctx, token.Denom)\nif err != nil {\nreturn err\n}\n}\nif types.SenderChainIsSource(sourcePort, sourceChannel, fullDenomPath) {\n//...\n```\n```golang\n// DenomPathFromHash returns the full denomination path prefix from an ibc denom with a hash\n// component.\nfunc (k Keeper) DenomPathFromHash(ctx sdk.Context, denom string) (string, error) {\nhexHash := denom[4:]\nhash, err := ParseHexHash(hexHash)\nif err != nil {\nreturn """", Wrap(ErrInvalidDenomForTransfer, err.Error())\n}\ndenomTrace, found := k.GetDenomTrace(ctx, hash)\nif !found {\nreturn """", Wrap(ErrTraceNotFound, hexHash)\n}\nfullDenomPath := denomTrace.GetFullDenomPath()\nreturn fullDenomPath, nil\n}\n```\n```golang\n// OnRecvPacket\n// ...\n// This is the prefix that would have been prefixed to the denomination\n// on sender chain IF and only if the token originally came from the\n// receiving chain.\n//\n// NOTE: We use SourcePort and SourceChannel here, because the counterparty\n// chain would have prefixed with DestPort and DestChannel when originally\n// receiving this coin as seen in the ""sender chain is the source"" condition.\nif ReceiverChainIsSource(packet.GetSourcePort(), packet.GetSourceChannel(), data.Denom) {\n// sender chain is not the source, unescrow tokens\n// remove prefix added by sender chain\nvoucherPrefix := types.GetDenomPrefix(packet.GetSourcePort(), packet.GetSourceChannel())\nunprefixedDenom := data.Denom[len(voucherPrefix):]\ntoken := sdk.NewCoin(unprefixedDenom, sdk.NewIntFromUint64(data.Amount))\n// unescrow tokens\nescrowAddress := types.GetEscrowAddress(packet.GetDestPort(), packet.GetDestChannel())\nreturn k.bankKeeper.SendCoins(ctx, escrowAddress, receiver, sdk.NewCoins(token))\n}\n// sender chain is the source, mint vouchers\n// since SendPacket did not prefix the denomination, we must prefix denomination here\nsourcePrefix := types.GetDenomPrefix(packet.GetDestPort(), packet.GetDestChannel())\n// NOTE: sourcePrefix contains the trailing ""/""\nprefixedDenom := sourcePrefix + data.Denom\n// construct the denomination trace from the full raw denomination\ndenomTrace := types.ParseDenomTrace(prefixedDenom)\n// set the value to the lookup table if not stored already\ntraceHash := denomTrace.Hash()\nif !k.HasDenomTrace(ctx, traceHash) {\nk.SetDenomTrace(ctx, traceHash, denomTrace)\n}\nvoucherDenom := denomTrace.IBCDenom()\nvoucher := sdk.NewCoin(voucherDenom, sdk.NewIntFromUint64(data.Amount))\n// mint new tokens if the source of the transfer is the same chain\nif err := k.bankKeeper.MintCoins(\nctx, types.ModuleName, sdk.NewCoins(voucher),\n); err != nil {\nreturn err\n}\n// send to receiver\nreturn k.bankKeeper.SendCoinsFromModuleToAccount(\nctx, types.ModuleName, receiver, sdk.NewCoins(voucher),\n)\n```\n```golang\nfunc NewDenomTraceFromRawDenom(denom string) DenomTrace{\ndenomSplit := strings.Split(denom, ""/"")\ntrace := """"\nif len(denomSplit) > 1 {\ntrace = strings.Join(denomSplit[:len(denomSplit)-1], ""/"")\n}\nreturn DenomTrace{\nBaseDenom: denomSplit[len(denomSplit)-1],\nTrace:     trace,\n}\n}\n```\nOne final remark is that the `FungibleTokenPacketData` will remain the same, i.e with the prefixed full denomination, since the receiving chain may not be an SDK-based chain.\n### Coin Changes\nThe coin denomination validation will need to be updated to reflect these changes. In particular, the denomination validation\nfunction will now:\n- Accept slash separators (`""/""`) and uppercase characters (due to the `HexBytes` format)\n- Bump the maximum character length to 128, as the hex representation used by Tendermint's\n`HexBytes` type contains 64 characters.\nAdditional validation logic, such as verifying the length of the hash, the  may be added to the bank module in the future if the [custom base denomination validation](https://github.com/cosmos/cosmos-sdk/pull/6755) is integrated into the SDK.\n### Positive\n- Clearer separation of the source tracing behaviour of the token (transfer prefix) from the original\n`Coin` denomination\n- Consistent validation of `Coin` fields (i.e no special characters, fixed max length)\n- Cleaner `Coin` and standard denominations for IBC\n- No additional fields to SDK `Coin`\n### Negative\n- Store each set of tracing denomination identifiers on the `ibc-transfer` module store\n- Clients will have to fetch the base denomination every time they receive a new relayed fungible token over IBC. This can be mitigated using a map/cache for already seen hashes on the client side. Other forms of mitigation, would be opening a websocket connection subscribe to incoming events.\n### Neutral\n- Slight difference with the ICS20 spec\n- Additional validation logic for IBC coins on the `ibc-transfer` module\n- Additional genesis fields\n- Slightly increases the gas usage on cross-chain transfers due to access to the store. This should\nbe inter-block cached if transfers are frequent.\n","**Decision:**\n\nTo address the problem of denomination length and character restrictions in cross-chain fungible token transfers, the following solution is proposed:\n\n**Prefix Denominations with a Hashed Trace**\n\nInstead of directly appending port and channel identifiers to the denomination, a hashed trace will be used as a prefix. The hash function will take the concatenation of the port and channel identifiers as input and produce a fixed-length hash. This hash will be prepended to the base denomination to form the final denomination.\n\n**Specifics:**\n\n* The hash function will be a collision-resistant cryptographic hash function, such as SHA256 or SHA512.\n* The hash will be truncated to a maximum length of 32 characters (for SHA256) or 64 characters (for SHA512).\n* The hashed trace will be encoded using Base64 or another suitable encoding scheme to ensure that it does not contain any special characters or uppercase letters.\n* The final denomination will have the following format:\n\n```typescript\nhashedTrace/denom\n```\n\n**Example:**\n\nTransferring `100 uatom` from port `HubPort` and channel `HubChannel` on the Hub to Ethermint's port `EthermintPort` and channel `EthermintChannel` would result in the following denomination:\n\n```typescript\nSHA256(HubPort/HubChannel)/uatom\n```\n\n**Benefits:**\n\n* This solution solves both problems identified:\n    * The hashed trace will have a fixed and manageable length, regardless of the number of cross-chain transfers.\n    * The hashed trace will not contain any special characters or uppercase letters, ensuring compatibility with the SDK's `Coin` type.\n* It maintains the ability to trace the history of a token's movements across chains.\n* It does not require any changes to the IBC protocol or the SDK's `Coin` type.\n\n**Implementation:**\n\n* The IBC module will be responsible for generating the hashed trace and prepending it to the denomination of cross-chain fungible tokens.\n* The SDK will be updated to accept denominations with the hashed trace prefix.\n\n**Testing:**\n\n* Extensive testing will be conducted to ensure the validity and robustness of the solution, including tests for:\n    * Denomination length validation\n    * Character restrictions\n    * Traceability of token history\n    * Compatibility with existing IBC implementations"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\nmediocre.\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\nwhich are repetitive and hard to maintain.\nSecond, this makes it harder to align with the object capability theorem: the right to access the\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\nFinally, because the getter/setter functions are defined as methods of a module's `Keeper`, the reviewers\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\nThere is no static way to know which part of the state that the function is accessing (and which is not).\n\n## Decision\n","We will define a type named `Value`:\n```go\ntype Value struct {\nm   Mapping\nkey []byte\n}\n```\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\nspace it will access and `Value.key` defines the exact key for the reference.\nWe will define a type named `Mapping`:\n```go\ntype Mapping struct {\nstoreKey sdk.StoreKey\ncdc      *codec.LegacyAmino\nprefix   []byte\n}\n```\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\nWe will define the following core methods for the `Value` type:\n```go\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Value) Get(ctx Context, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\n// Get stored data as raw byte slice\nfunc (Value) GetRaw(ctx Context) []byte {}\n// Marshal and set a raw value\nfunc (Value) Set(ctx Context, o interface{}) {}\n// Check if a raw value exists\nfunc (Value) Exists(ctx Context) bool {}\n// Delete a raw value value\nfunc (Value) Delete(ctx Context) {}\n```\nWe will define the following core methods for the `Mapping` type:\n```go\n// Constructs key-value pair reference corresponding to the key argument in the Mapping space\nfunc (Mapping) Value(key []byte) Value {}\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\n// Get stored data as raw byte slice\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\n// Marshal and set a raw value\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\n// Check if a raw value exists\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\n// Delete a raw value value\nfunc (Mapping) Delete(ctx Context, key []byte) {}\n```\nEach method of the `Mapping` type that is passed the arugments `ctx`, `key`, and `args...` will proxy\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\nIn addition, we will define and provide a common set of types derived from the `Value` type:\n```go\ntype Boolean struct { Value }\ntype Enum struct { Value }\ntype Integer struct { Value; enc IntEncoding }\ntype String struct { Value }\n// ...\n```\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\nin core methods are replaced by explicit return types.\nFinally, we will define a family of types derived from the `Mapping` type:\n```go\ntype Indexer struct {\nm   Mapping\nenc IntEncoding\n}\n```\nWhere the `key` argument in core method is typed.\nSome of the properties of the accessor types are:\n- State access happens only when a function which takes a `Context` as an argument is invoked\n- Accessor type structs give rights to access the state only that the struct is referring, no other\n- Marshalling/Unmarshalling happens implicitly within the core methods\n","The SDK modules should natively support a new `KVStore` interface that provides a modular and user-friendly way to access state. This interface should include methods for getting and setting values, as well as for deleting keys. The interface should also be designed in a way that makes it easy to align with the object capability theorem."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nProtocol Buffers provide a basic [style guide](https://developers.google.com/protocol-buffers/docs/style)\nand [Buf](https://buf.build/docs/style-guide) builds upon that. To the\nextent possible, we want to follow industry accepted guidelines and wisdom for\nthe effective usage of protobuf, deviating from those only when there is clear\nrationale for our use case.\n### Adoption of `Any`\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\ninterface types (as opposed to `oneof`) makes package naming a central part\nof the encoding as fully-qualified message names now appear in encoded\nmessages.\n### Current Directory Organization\nThus far we have mostly followed [Buf's](https://buf.build) [DEFAULT](https://buf.build/docs/lint-checkers#default)\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout)\nwhich although being convenient for developing code comes with the warning\nfrom Buf that:\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\n### Adoption of gRPC Queries\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\npath. In the future, gRPC queries may be allowed from within persistent scripts\nby technologies such as CosmWasm and these query routes would be stored within\nscript binaries.\n\n## Decision\n","The goal of this ADR is to provide thoughtful naming conventions that:\n* encourage a good user experience for when users interact directly with\n.proto files and fully-qualified protobuf names\n* balance conciseness against the possibility of either over-optimizing (making\nnames too short and cryptic) or under-optimizing (just accepting bloated names\nwith lots of redundant information)\nThese guidelines are meant to act as a style guide for both the SDK and\nthird-party modules.\nAs a starting point, we should adopt all of the [DEFAULT](https://buf.build/docs/lint-checkers#default)\ncheckers in [Buf's](https://buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout),\nexcept:\n* [PACKAGE_VERSION_SUFFIX](https://buf.build/docs/lint-checkers#package_version_suffix)\n* [SERVICE_SUFFIX](https://buf.build/docs/lint-checkers#service_suffix)\nFurther guidelines to be described below.\n### Principles\n#### Concise and Descriptive Names\nNames should be descriptive enough to convey their meaning and distinguish\nthem from other names.\nGiven that we are using fully-qualifed names within\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\nkeep names concise, without going overboard. The general rule of thumb should\nbe if a shorter name would convey more or else the same thing, pick the shorter\nname.\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\nSuch conciseness makes names both more pleasant to work with and take up less\nspace within transactions and on the wire.\nWe should also resist the temptation to over-optimize, by making names\ncryptically short with abbreviations. For instance, we shouldn't try to\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\nThe goal is to make names **_concise but not cryptic_**.\n#### Names are for Clients First\nPackage and type names should be chosen for the benefit of users, not\nnecessarily because of legacy concerns related to the go code-base.\n#### Plan for Longevity\nIn the interests of long-term support, we should plan on the names we do\nchoose to be in usage for a long time, so now is the opportunity to make\nthe best choices for the future.\n### Versioning\n#### Don't Allow Breaking Changes in Stable Packages\nAlways use a breaking change detector such as [Buf](https://buf.build) to prevent\nbreaking changes in stable (non-alpha or beta) packages. Breaking changes can\nbreak smart contracts/persistent scripts and generally provide a bad UX for\nclients. With protobuf, there should usually be ways to extend existing\nfunctionality instead of just breaking it.\n#### Omit v1 suffix\nInstead of using [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix),\nwe can omit `v1` for packages that don't actually have a second version. This\nallows for more concise names for common use cases like `cosmos.bank.Send`.\nPackages that do have a second or third version can indicate that with `.v2`\nor `.v3`.\n#### Use `alpha` or `beta` to Denote Non-stable Packages\n[Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix)\n(ex. `v1alpha1`) _should_ be used for non-stable packages. These packages should\nlikely be excluded from breaking change detection and _should_ generally\nbe blocked from usage by smart contracts/persistent scripts to prevent them\nfrom breaking. The SDK _should_ mark any packages as alpha or beta where the\nAPI is likely to change significantly in the near future.\n### Package Naming\n#### Adopt a short, unique top-level package name\nTop-level packages should adopt a short name that is known to not collide with\nother names in common usage within the Cosmos ecosystem. In the near future, a\nregistry should be created to reserve and index top-level package names used\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\n[ICS](https://github.com/cosmos/ics) specifications could consider a\nshort top-level package like `ics23` based upon the standard number.\n#### Limit sub-package depth\nSub-package depth should be increased with caution. Generally a single\nsub-package is needed for a module or a library. Even though `x` or `modules`\nis used in source code to denote modules, this is often unnecessary for .proto\nfiles as modules are the primary thing sub-packages are used for. Only items which\nare known to be used infrequently should have deep sub-package depths.\nFor the Cosmos SDK, it is recommended that that we simply write `cosmos.bank`,\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\ntypes can go straight in the `cosmos` package or we can introduce a\n`cosmos.base` package if needed. Note that this naming _will not_ change\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\n`x/bank`.\n### Message Naming\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\ntypes which are used in transactions will retain the `Msg` prefix as that provides\nhelpful context.\n### Service and RPC Naming\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\nimplement a gRPC query service. We should consider the principle of conciseness\nfor query service and RPC names as these may be called from persistent script\nmodules such as CosmWasm. Also, users may use these query paths from tools like\n[gRPCurl](https://github.com/fullstorydev/grpcurl). As an example, we can shorten\n`/cosmos_sdk.x.bank.v1.QueryService/QueryBalance` to\n`/cosmos.bank.Query/Balance` without losing much useful information.\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`/\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\non the `Query` service, the request and response types would be `QueryBalanceRequest`\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\nand `BalanceResponse`.\n#### Use just `Query` for the query service\nInstead of [Buf's default service suffix recommendation](https://github.com/cosmos/cosmos-sdk/pull/6033),\nwe should simply use the shorter `Query` for query services.\nFor other types of gRPC services, we should consider sticking with Buf's\ndefault recommendation.\n#### Omit `Get` and `Query` from query service RPC names\n`Get` and `Query` should be omitted from `Query` service names because they are\nredundant in the fully-qualified name. For instance, `/cosmos.bank.Query/QueryBalance`\njust says `Query` twice without any new information.\n","**Decision:** Follow the Buf style guide for Protobuf package and file naming, with the following exceptions:\n\n* **Package Naming:**\n    * For interfaces encoded using `google.protobuf.Any`, use the fully-qualified message name as the package name.\n* **File Naming:**\n    * Continue to disable the `PACKAGE_DIRECTORY_MATCH` linter check, but ensure that all proto files are organized in a logical and consistent manner.\n* **gRPC Query Paths:**\n    * For gRPC queries, use the full gRPC service path as a prefix for the ABCI query path.\n\n**Rationale:**\n\n* Following the Buf style guide for Protobuf package and file naming provides consistency and alignment with industry best practices.\n* Using the fully-qualified message name as the package name for interfaces encoded using `google.protobuf.Any` ensures that the encoded message structure is self-describing.\n* Disabling the `PACKAGE_DIRECTORY_MATCH` linter check provides flexibility in organizing proto files while still maintaining a logical structure.\n* Utilizing the full gRPC service path as a prefix for ABCI query paths makes the query routes more explicit and easier to understand."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Protobuf](https://developers.google.com/protocol-buffers/docs/proto3)\nseralization is not unique (i.e. there exist a practically unlimited number of\nvalid binary representations for a protobuf document)<sup>1</sup>. For signature\nverification in Cosmos SDK, signer and verifier need to agree on the same\nserialization of a SignDoc as defined in\n[ADR-020](./adr-020-protobuf-transaction-encoding.md) without transmitting the\nserialization. This document describes a deterministic serialization scheme for\na subset of protobuf documents, that covers this use case but can be reused in\nother cases as well.\n\n## Decision\n","The following encoding scheme is proposed to be used by other ADRs.\n### Scope\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\nserialization, such that every protobuf parser can parse it.\nNo maps are supported in version 1 due to the complexity of defining a\nderterministic serialization. This might change in future. Implementations must\nreject documents containing maps as invalid input.\n### Serialization rules\nThe serialization is based on the\n[protobuf 3 encoding](https://developers.google.com/protocol-buffers/docs/encoding)\nwith the following additions:\n1. Fields must be serialized only once in ascending order\n2. Extra fields or any extra data must not be added\n3. [Default values](https://developers.google.com/protocol-buffers/docs/proto3#default)\nmust be omitted\n4. `repeated` fields of scalar numeric types must use\n[packed encoding](https://developers.google.com/protocol-buffers/docs/encoding#packed)\nby default.\n5. Variant encoding of integers must not be longer than needed.\nWhile rule number 1. and 2. should be pretty straight forward and describe the\ndefault behaviour of all protobuf encoders the author is aware of, the 3rd rule\nis more interesting. After a protobuf 3 deserialization you cannot differentiate\nbetween unset fields and fields set to the default value<sup>2</sup>. At\nserialization level however, it is possible to set the fields with an empty\nvalue or omitting them entirely. This is a significant difference to e.g. JSON\nwhere a property can be empty (`""""`, `0`), `null` or undefined, leading to 3\ndifferent documents.\nOmitting fields set to default values is valid because the parser must assign\nthe default value to fields missing in the serialization<sup>3</sup>. For scalar\ntypes, omitting defaults is required by the spec<sup>4</sup>. For `repeated`\nfields, not serializing them is the only way to express empty lists. Enums must\nhave a first element of numeric value 0, which is the default<sup>5</sup>. And\nmessage fields default to unset<sup>6</sup>.\nOmitting defaults allows for some amount of forward compatibility: users of\nnewer versions of a protobuf schema produce the same serialization as users of\nolder versions as long as newly added fields are not used (i.e. set to their\ndefault value).\n### Implementation\nThere are three main implementation strategies, ordered from the least to the\nmost custom development:\n- **Use a protobuf serializer that follows the above rules by default.** E.g.\n[gogoproto](https://pkg.go.dev/github.com/gogo/protobuf/gogoproto) is known to\nbe compliant by in most cases, but not when certain annotations such as\n`nullable = false` are used. It might also be an option to configure an\nexisting serializer accordingly.\n- **Normalize default values before encoding them.** If your serializer follows\nrule 1. and 2. and allows you to explicitly unset fields for serialization,\nyou can normalize default values to unset. This can be done when working with\n[protobuf.js](https://www.npmjs.com/package/protobufjs):\n```js\nconst bytes = SignDoc.encode({\nbodyBytes: body.length > 0 ? body : null, // normalize empty bytes to unset\nauthInfoBytes: authInfo.length > 0 ? authInfo : null, // normalize empty bytes to unset\nchainId: chainId || null, // normalize """" to unset\naccountNumber: accountNumber || null, // normalize 0 to unset\naccountSequence: accountSequence || null, // normalize 0 to unset\n}).finish();\n```\n- **Use a hand-written serializer for the types you need.** If none of the above\nways works for you, you can write a serializer yourself. For SignDoc this\nwould look something like this in Go, building on existing protobuf utilities:\n```go\nif !signDoc.body_bytes.empty() {\nbuf.WriteUVarInt64(0xA) // wire type and field number for body_bytes\nbuf.WriteUVarInt64(signDoc.body_bytes.length())\nbuf.WriteBytes(signDoc.body_bytes)\n}\nif !signDoc.auth_info.empty() {\nbuf.WriteUVarInt64(0x12) // wire type and field number for auth_info\nbuf.WriteUVarInt64(signDoc.auth_info.length())\nbuf.WriteBytes(signDoc.auth_info)\n}\nif !signDoc.chain_id.empty() {\nbuf.WriteUVarInt64(0x1a) // wire type and field number for chain_id\nbuf.WriteUVarInt64(signDoc.chain_id.length())\nbuf.WriteBytes(signDoc.chain_id)\n}\nif signDoc.account_number != 0 {\nbuf.WriteUVarInt64(0x20) // wire type and field number for account_number\nbuf.WriteUVarInt(signDoc.account_number)\n}\nif signDoc.account_sequence != 0 {\nbuf.WriteUVarInt64(0x28) // wire type and field number for account_sequence\nbuf.WriteUVarInt(signDoc.account_sequence)\n}\n```\n### Test vectors\nGiven the protobuf definition `Article.proto`\n```protobuf\npackage blog;\nsyntax = ""proto3"";\nenum Type {\nUNSPECIFIED = 0;\nIMAGES = 1;\nNEWS = 2;\n};\nenum Review {\nUNSPECIFIED = 0;\nACCEPTED = 1;\nREJECTED = 2;\n};\nmessage Article {\nstring title = 1;\nstring description = 2;\nuint64 created = 3;\nuint64 updated = 4;\nbool public = 5;\nbool promoted = 6;\nType type = 7;\nReview review = 8;\nrepeated string comments = 9;\nrepeated string backlinks = 10;\n};\n```\nserializing the values\n```yaml\ntitle: ""The world needs change 🌳""\ndescription: """"\ncreated: 1596806111080\nupdated: 0\npublic: true\npromoted: false\ntype: Type.NEWS\nreview: Review.UNSPECIFIED\ncomments: [""Nice one"", ""Thank you""]\nbacklinks: []\n```\nmust result in the serialization\n```\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\n```\nWhen inspecting the serialized document, you see that every second field is\nomitted:\n```\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\n1: ""The world needs change \360\237\214\263""\n3: 1596806111080\n5: 1\n7: 2\n9: ""Nice one""\n9: ""Thank you""\n```\n",Use the **Canonical Protobuf encoding** for serialization in cases where unique serialization is required.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https://github.com/cosmos/cosmos-sdk/issues/5467) and [4982](https://github.com/cosmos/cosmos-sdk/issues/4982) for additional context.\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\n\n## Decision\n","Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\n### Account interface (x/auth)\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\nnow be stored in & managed by the bank module.\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\naccount balance.\nVesting accounts will continue to store original vesting, delegated free, and delegated\nvesting coins (which is safe since these cannot contain arbitrary denominations).\n### Bank keeper (x/bank)\nThe following APIs will be added to the `x/bank` keeper:\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\ncore functionality or persistence.\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\nbut retrieval of all balances for a single account is presumed to be more frequent):\n```golang\nvar BalancesPrefix = []byte(""balances"")\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\nif !balance.IsValid() {\nreturn err\n}\nstore := ctx.KVStore(k.storeKey)\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\nbz := Marshal(balance)\naccountStore.Set([]byte(balance.Denom), bz)\nreturn nil\n}\n```\nThis will result in the balances being indexed by the byte representation of\n`balances/{address}/{denom}`.\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\naccount balance by denomination found in the (un)delegation amount. As a result,\nany mutations to the account balance by will made by denomination.\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\ndirectly instead of calling `GetCoins()` / `SetCoins()` (which no longer exist).\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\naccount balances.\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\npossible as to not load the entire account balance.\n### Supply module\nThe supply module, in order to implement the total supply invariant, will now need\nto scan all accounts & call `GetAllBalances` using the `x/bank` Keeper, then sum\nthe balances and check that they match the expected total supply.\n","The decision is to store non-zero balances in a skiplist, which will allow for efficient loading and storage of large numbers of denominations. This will mitigate the potential denial-of-service concern that could be caused by too many denominations being stored in an `sdk.Coins` struct."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\n[runTx()](https://github.com/cosmos/cosmos-sdk/blob/bad4ca75f58b182f600396ca350ad844c18fc80b/baseapp/baseapp.go#L539)\nmethod. We think that this method can be more flexible and can give SDK users more options for customizations without\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\nmight be handled in a ""standard"" way (middleware) alongside the others.\nWe propose middleware-solution, which could help developers implement the following cases:\n* add external logging (let's say sending reports to external services like [Sentry](https://sentry.io));\n* call panic for specific error cases;\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#Recovery-middleware)).\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\nin order to customize it.\n\n## Decision\n","### Design\n#### Overview\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\ncan be found [here](https://github.com/cosmos/cosmos-sdk/pull/6053).\n#### Implementation details\n##### Recovery handler\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\n`recover()` from the `builtin` package.\n```go\ntype RecoveryHandler func(recoveryObj interface{}) error\n```\nHandler should type assert (or other methods) an object to define if object should be handled.\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\nAn example:\n```go\nfunc exampleErrHandler(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(error)\nif !ok { return nil }\nif someSpecificError.Is(err) {\npanic(customPanicMsg)\n} else {\nreturn nil\n}\n}\n```\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\n##### Recovery middleware\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\nchain processing.\n```go\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\nif err := handler(recoveryObj); err != nil {\nreturn nil, err\n}\nreturn next, nil\n}\n}\n```\nFunction receives a `recoveryObj` object and returns:\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\n`OutOfGas` middleware example:\n```go\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\nif !ok { return nil }\nreturn sdkerrors.Wrap(\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\n""out of gas in location: %v; gasWanted: %d, gasUsed: %d"", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\n),\n)\n}\nreturn newRecoveryMiddleware(handler, next)\n}\n```\n`Default` middleware example:\n```go\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nreturn sdkerrors.Wrap(\nsdkerrors.ErrPanic, fmt.Sprintf(""recovered: %v\nstack:\n%v"", recoveryObj, string(debug.Stack())),\n)\n}\nreturn newRecoveryMiddleware(handler, nil)\n}\n```\n##### Recovery processing\nBasic chain of middlewares processing would look like:\n```go\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\nif middleware == nil { return nil }\nnext, err := middleware(recoveryObj)\nif err != nil { return err }\nif next == nil { return nil }\nreturn processRecovery(recoveryObj, next)\n}\n```\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\n`default` handler which must return an `error`.\n##### BaseApp changes\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\n```go\ntype BaseApp struct {\n// ...\nrunTxRecoveryMiddleware recoveryMiddleware\n}\nfunc NewBaseApp(...) {\n// ...\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\n}\nfunc (app *BaseApp) runTx(...) {\n// ...\ndefer func() {\nif r := recover(); r != nil {\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\nerr, result = processRecovery(r, recoveryMW), nil\n}\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\n}()\n// ...\n}\n```\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\n```go\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\nfor _, h := range handlers {\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\n}\n}\n```\nThis method would prepend handlers to an existing chain.\n",**Decision:** Implement a middleware-based solution for customizing error handling in BaseApp.\n\n**Rationale:**\n\n* Middleware is a flexible approach that allows developers to implement a wide range of error handling customizations without the need to rewrite the entire BaseApp.\n* It provides a standardized way to handle both custom error cases and special cases like `sdk.ErrorOutOfGas`.\n* It enables the implementation of specific use cases such as external logging and panic handling for specific error cases.\n* The proposed solution addresses the specific need for customizing error handling in BaseApp to facilitate deterministic node execution in the face of communication breaks with a sidecar service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https://github.com/tendermint/tendermint/issues/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\n\n## Decision\n","### Pseudo procedure for consensus key rotation\n- create new random consensus key.\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\n- start validating with new consensus key.\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\n### Considerations\n- consensus key mapping information management strategy\n- store history of each key mapping changes in the kvstore.\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\n- the state machine does not need any historical mapping information which is past more than unbonding period.\n- key rotation costs related to LCD and IBC\n- LCD and IBC will have traffic/computation burden when there exists frequent power changes\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\n- limits\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\n- parameters can be decided by governance and stored in genesis file.\n- key rotation fee\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\n- evidence module\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\n- abci.ValidatorUpdate\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\n- new genesis parameters in `staking` module\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\n### Workflow\n1. The validator generates a new consensus keypair.\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\n```go\ntype MsgRotateConsPubKey struct {\nValidatorAddress  sdk.ValAddress\nNewPubKey         crypto.PubKey\n}\n```\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\n4. `RotateConsPubKey`\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\n- checks if the signing account has enough balance to pay `KeyRotationFee`\n- pays `KeyRotationFee` to community fund\n- overwrites `NewPubKey` in `validator.ConsPubKey`\n- deletes old `ValidatorByConsAddr`\n- `SetValidatorByConsAddr` for `NewPubKey`\n- Add `ConsPubKeyRotationHistory` for tracking rotation\n```go\ntype ConsPubKeyRotationHistory struct {\nOperatorAddress         sdk.ValAddress\nOldConsPubKey           crypto.PubKey\nNewConsPubKey           crypto.PubKey\nRotatedHeight           int64\n}\n```\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\n```go\nabci.ValidatorUpdate{\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\nPower:  0,\n}\nabci.ValidatorUpdate{\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\nPower:  v.ConsensusPower(),\n}\n```\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\n- Note : All above features shall be implemented in `staking` module.\n","Validator consensus key rotation feature will be implemented in Cosmos-SDK. The implementation will be kept as simple as possible, without considering multiple consensus keys concept."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\n- From which repo would components be published?\n- Which repos would consume components from NPM?\n- Should the Storybook live inside the Client repo?\n## Decision Drivers\nN/A\n\n## Decision\n","N/A\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally — before publishing the component to the hosted Storybook and NPM.\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\n1. Duplicate component code to Storybook repo\n2. Publish completed components to NPM\n3. Remove the original component code from the Client and import via NPM\n### Positive Consequences\n### Negative consequences\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\nIf possible, also avoid having people working simultaneously on a component that consumes / is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\n",**Decision:**\n\n* **Components will be published from the DPE Client repository.**\n* **Both the DPE Client and Storybook repositories will consume components from NPM.**\n* **The Storybook will live outside of the Client repository.**\n\n**Rationale:**\n\n* Publishing components from the DPE Client repository allows for centralized management and versioning of components.\n* Consuming components from NPM in both repositories ensures that both projects have access to the latest versions of components.\n* Keeping the Storybook outside of the Client repository allows for more flexibility and easier collaboration with other teams.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCorvus Tenancy, which provides the underpinnings of the Marain Tenancy service, has always had a concept of a root tenant (dating from its earlier, pre-open-source incarnation). This ADR captures aspects of this root tenant that are non-obvious. (We learned that it was non-obvious because some code has been written that was unaware of the special status of the root tenant.)\n\n## Decision\n","There is a special tenant known as the Root Tenant. It has a well-known id, `f26450ab1668784bb327951c8b08f347`. It is special in three respects:\n* tenants are hierarchical, and the root tenant forms the root of that hierarchy\n* the tenanted storage mechanisms will all fall back to the root tenant to find default connection settings if the tenant being used has not defined tenant-specific settings\n* within Marain services, the root tenant is always represented by a special in-memory instance of the `RootTenant` type, whereas all other tenants are managed by the tenancy service\nThat third item is there to support the second: because each service puts its own `RootTenant` into the DI service collection, as a singleton, it becomes possible for the service to attach whatever service-specific fallback settings it requires. We describe the root tenant as ""synthetic"" because each service creates its own object to represent the root tenant, whereas all other objects representing tenants are obtained via the `Marain.Tenancy` service, typically through the `ClientTenantProvider`.\nWe contemplated separating out the first two concerns (which might enable us not to need the third characteristic above) because it has been a source of confusion in the past. However, for the time being we are planning to keep it this way because that alternative approach would require us to introduce an extra mechanism to support these kinds of defaults.\n",The root tenant is a special case in Corvus. It is the only tenant that can create other tenants. It is also the only tenant that can update the configuration of the Corvus service. All other tenants are created by the root tenant and are owned by the root tenant.\n\nThe root tenant is not visible to users of the Marain Tenancy service. It is only used internally by the Corvus service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe original build of `Marain.Tenancy` provided two main things:\n- It provided an API over the existing `Corvus.Tenancy` project, allowing us to build multiple services around the same tenancy system without each service needing to be aware of the underlying persistence mechanism for tenants.\n- It provided a drop in replacement for direct use of the `Corvus.Tenancy.Abstractions.ITenantProvider`, in the form of the `ClientTenantProvider` in `Marain.Tenancy.ClientTenantProvider`.\nHowever, the service did not impose any particular tenancy model on consumers; it simply exposed the capabilities of the `Corvus.Tenancy` library.\nSubsequently, as the functionality of the remaining Marain services was extended to properly support multi-tenancy, it became necessary to define the tenancy model that would be shared across all Marain Services. This is documented in `Marain.Instance` ADRs [0005 - Multitenancy approach for Marain](https://github.com/marain-dotnet/Marain.Instance/blob/master/docs/adr/0005-multitenancy-approach-for-marain.md) and [0006 - Process for onboarding new tenants and enrolling them to use Marain services](https://github.com/marain-dotnet/Marain.Instance/blob/master/docs/adr/0006-process-for-onboarding-new-tenants.md)\nThe additional code required to support this, available in [the `Marain.TenantManagment` repository](https://github.com/marain-dotnet/Marain.TenantManagement) was built as a wrapper around the existing `Marain.Tenancy` API and is currently only exposed via a Command Line Interface.\nThis has worked reasonably well to date; the CLI and code provided by Marain.TenantManagement is in use in multiple locations to work with tenants in several different ways, but all based on the underlying model defined in the ADRs linked to above.\nHowever, it has become apparent that in Marain, the two are essentially inseperable. All Marain services (bar Marain.Tenancy) validate requests to ensure that the tenant making the request is enrolled to use the service being called, which requires use of code referenced from `Marain.TenantManagement` and `Marain.Tenancy`.\nIn addition, new requirements (such as the need for a service to be able to obtain a list of enrolled clients) are difficult to implement efficiently without further blurring the lines between the two libraries.\nThis has brought into focus the fact that `Marain.Tenancy` is something of an outlier in the Marain world; despite being the service used to manage Marain tenants, it is the only service in the Marain ecosystem that does not fully ""buy in"" to the Marain tenancy model. This means that as we expand the model to cover new scenarios (including the creation of additional tenant management tooling to simplify onboarding and offboarding, and potentially begin to integrate billing, metering, and so on) we will likely end up either continuing to add extensions to the `Marain.TenantManagement` library, or adding a separate API that acts as a wrapper around `Marain.Tenancy`.\n\n## Decision\n","To simplify our approach, we will modify `Marain.Tenancy` so that that API it exposes reflects the tenancy model used by the Marain ecosystem. This means that existing functionality provided by the `Marain.TenantManagement` extensions will become API endpoints - e.g. enrollment, client/service tenant creation, etc.\nThe existing CLIs provided with `Marain.TenantManagement` and `Marain.Tenancy` will be merged and separated out into their own project. They will then form the basis of CLI tooling required for standard tasks in all Marain services. As well as making the tooling simpler to use (by having all functionality available through one tool rather than spread across multiple), this will ensure we maintain good practice in ensuring that all of these tasks can be carried out via their relevant APIs.\n","Marain.Tenancy will be split into two distinct libraries:\n- `Marain.Tenancy`: This will be a low-level library that exposes the raw underlying functionality of the `Corvus.Tenancy` library.\n- `Marain.TenancyManagement`: This will be a higher-level library that wraps the functionality of `Marain.Tenancy` and exposes it in a way that is consistent with the Marain tenancy model. This library will also provide additional functionality such as the ability to manage tenants and enroll them to use Marain services.\nThis split will allow Marain services to continue to use the functionality of `Marain.Tenancy` without having to be aware of the underlying implementation details. It will also allow us to more easily extend the Marain tenancy model in the future, without breaking existing code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur initial design for tenancy in Corvus (which necessarily affected Marain.Tenancy) comingled read and write behaviour. The model was similar to the .NET Entity Framework: if you wanted to modify a tenant, you would first fetch an object representing that tenant, then make changes to that object, and then invoke an operation indicating that you wanted those changes to be written back.\nWe made various changes to the Property Bag system that tenancy uses to store tenant properties to disassociate the API from any particular JSON serialization framework. We had previously forced a dependency on Json.NET, but we wanted to be able to move onto `System.Text.Json`, so we wanted to introduce a Property Bag abstraction that was independent of serialization mechanism (although still with a presumption that it must be possible for the properties to be serialized as JSON).\nOne of the basic principles of efficient JSON parsing in the new world is that you don't build an object model representing the JSON unless you really need to. Ideally, you leave the JSON in its raw UTF-8 state, referred to via one or more `IMemory<byte>` values, and extract what data you need only as you need it. This can dramatically reduce GC pressure, particularly in cases where most of the data in question is not used most of the time. However, this model does not fit well with the ""modifiable entities"" approach to updates. If anything is free to modify the properties at any time, this implies an ability to edit or regenerate the JSON.\nIn practice, modification of tenant properties is the exception, not the rule. Most Marain services will only ever fetch tenant properties. Only the Marain.Tenancy service should normally directly edit these properties. So the ""modifiable entities"" approach is not really necessary, and causes problems for migration to allocation-efficient strategies.\n\n## Decision\n","Since `Corvus.Json.Abstractions` separates out read and update operations for `IPropertyBag`, and `Corvus.Tenancy` therefore does the same (since it uses property bags), Marain.Tenancy will follow suit.\nThe web API presented by Marain.Tenancy for modifying tenants uses JSON Patch. So instead of this procedure:\n* fetch a serialized representation of an ITenant from the web API\n* modify that representation to reflect the changes you wish to make\n* PUT that serialized representation of an ITenant back to the web API\nwe now use this procedure instead:\n* send a PATCH request in describing the changes required in JSON Patch format\nFor example, to rename a tenant, you would send this PATCH to the Marain.Tenancy service, using the URL representing the tenant (the same URL from which you would fetch the tenant if reading) with an `application/json-patch+json` content type:\n```json\n[{\n""path"": ""/name"",\n""op"": ""replace"",\n""value"": ""NewTenantName""\n}]\n```\nJSON Patch supports multiple changes in a single request, e.g.:\n```json\n[\n{\n""op"": ""add"",\n""path"": ""/properties/StorageConfiguration__corvustenancy"",\n""value"": {\n""AccountName"": ""mardevtenancy"",\n""Container"": null,\n""KeyVaultName"": ""mardevkv"",\n""AccountKeySecretName"": ""mardevtenancystore"",\n""DisableTenantIdPrefix"": false\n}\n},\n{\n""op"": ""add"",\n""path"": ""/properties/Foo__bar"",\n""value"": ""Some string""\n},\n{\n""op"": ""add"",\n""path"": ""/properties/Foo__spong"",\n""value"": 42\n}\n]\n```\nThe `op` can be set to `remove` to delete properties.\nClients will not typically build these PATCH requests themselves, because the `ClientTenantStore` type contains the relevant code. `ClientTenantStore` provides an implementation of `ITenantStore` that works by using the web API provided by Marain.Tenancy. So in practice, updating the name of a tenant is as simple as:\n```csharp\nawait tenantStore.UpdateTenantAsync(tenantId, name: ""NewTenantName"");\n```\nAdding or changing a property looks like this:\nawait tenantStore.UpdateTenantAsync(\ntenantId,\npropertiesToSetOrAdd: new Dictionary<string, object>()\n{\n{ ""StorageConfiguration__corvustenancy"", myStorageConfig },\n{ ""SomeOtherSetting"": 42 },\n});\n","We should change tenancy so that it represents an immutable object, with any write behavior going directly to the Property Bag abstraction."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSwitching on the C# 8.0 nullable references feature for Menes has revealed some ambiguities around whether certain properties of exceptions are meant to be nullable.\nIn many cases, the only reason for ambiguity is that we have followed a pattern of defining various ""standard constructors"", such as default constructors, exception-message-only constructors, and deserializing constructors.\n\n## Decision\n","Menes exceptions will not have any of these standard exceptions except in cases where there are no required properties (e.g., the exception's type tells you everything you need to know).\nProperties that always have non-null values in practice will declare this formally by having non-nullable types.\nWe will remove all deserializing constructors, and remove the `[Serializable]` attribute from all exceptions that have them. This has been motivated by the use of nullable references, because deserializing constructors cause some challenges there, but this is a distinct issue. Menes exceptions are all designed for use within a Menes-based service. Menes is designed to implement service boundaries, and by definition, if we ever attempt to throw a Menes-defined exception across a process boundary, we've made a mistake.\n","**The following properties of exceptions shall be made nullable using the C# 9.0 nullable reference annotation:**\n\n* `Exception.HResult`\n* `Exception.InnerException`\n* `Exception.Message`\n* `Exception.Source`\n\n**Reasoning:**\n\n* The nullability of these properties is often ambiguous, as they may be null in certain scenarios (e.g., when deserializing an exception from a JSON payload). By making them nullable, we eliminate the need for explicit null checks or default value assignments.\n* Making these properties nullable does not introduce any significant performance overhead or potential security vulnerabilities.\n* The nullability annotation provides valuable information to developers, making it clear that these properties may be null and should be handled accordingly.\n\n**Consequences:**\n\n* Code that relies on the non-nullability of these properties will need to be updated to account for the possibility of null values.\n* This may introduce additional null checks or default value assignments in some cases.\n* However, the overall benefits of improved clarity and reduced ambiguity outweigh the minor potential drawbacks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the `System.Diagnostics.CodeAnalysis` namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.\nThese attributes are not available in `netstandard2.0`. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the `Nullable` NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.\nThe problem is that you don't want to use this workaround unless you have to. Newer versions of .NET Core and .NET Standard have these attributes, so it's just a waste of space to define your own.\n\n## Decision\n","Menes will target both .NET Standard 2.0 and .NET Standard 2.1. The .NET Standard 2.0 version brings its own copies of the attributes, the .NET Standard 2.1 version relies on the ones built into the framework.\n","Use the `Nullable` NuGet package to provide nullable reference attributes for older runtimes, but only if the target runtime is `netstandard2.0`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn real-world OpenAPI schema, we have discovered that people sometimes omit the `type: object` from their object definitions. We believe that this *is* valid Open API schema.\n```yaml\nPet:\nrequired:\n- id\n- name\nproperties:\nid:\ntype: integer\nformat: int64\nname:\ntype: string\ntag:\ntype: string\n```\nHowever, there are other scenarios where you are *not* expected to supply the `type` property. Specifically, the `anyOf`, `oneOf`, `allOf` cases.\n```yaml\nsomeEntity:\nanyOf:\n- type: string\n- type: object\n- type: array\n- type: boolean\n- type: integer\n- type: number\n```\n\n## Decision\n","Menes will support these semantics. We have updated our schema validation to support this by translating the missing `type` element into the internal schema type `None`, rather than translating to `Object`.\n","The `type` field is required for OpenAPI schemas, even for object definitions. The field should be set to `object` for object definitions. For `anyOf`, `oneOf`, and `allOf`, the `type` field is not required."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nModiapersonoversikt (denne frontenden) blir utviklet som en selvstendig frontend som på sikt skal erstatte frontenden i dagens modiabrukerdialog. For å kunne levere fortløpende ny funksjonalitet til saksbehandlerene, ønsker vi å levere ofte og smått.\n\n## Decision\n",Visittkortet dras inn som en enkeltstående react-komponent inn til modiabrukerdialog.\n,Lever i små trinn for å levere ny funksjonalitet raskt og fortløpende til saksbehandlerne.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To implement an Architectural Decision Record (ADR) template and process for capturing and documenting architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/).\nGitHub Actions will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repo, which has a basic use case will provide some\nexposure to the service.\n\n## Decision\n",The decision is to replace Travis CI with GitHub Actions.\n,Utilize GitHub Actions for Continuous Integration and Delivery (CI/CD).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently used: 2.1.0\nThe latest stable version of Spring Boot is 2.2.1:\nhttps://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.2-Release-Notes\nMore frequent but smaller upgrades are recommended.\n\n## Decision\n,Spring Boot will be upgraded to 2.2.1.\nAccording to release notes no migration needed on Mokka side.\n,Upgrade Spring Boot to version 2.2.1.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMokka's stubs engine is required to provide set of new features that will allow better (wider) support of request matching,\n(i.e. headers, cookies, variety of patterns), proxying and recording interactions.\nIt has been considered if further development of Mokka stubs engine should be continued or we should research and adopt one of existing, grown up and production ready solution.\nWireMock (http://wiremock.org/) was chosen as one of the most popular mock server with strong and active open source community. It provides consistent and reach API (both: Java and HTTP JSON).\nThrough years it became one of the mostly chosen library for mocking external services in JUnit tests and also the core part of Spring Cloud Contract library.\nBy reusing WireMock standards, Mokka may benefit in easier integration with already existing solutions, products and projects.\nTable below comparises features of Mokka and WireMock in context of stubbing/mocking engine.\nPlease mind it does not considers GUI or user management features as what Mokka provides is not considered to be change.\nCompared versions:\n* WireMock version: 2.26.0<br>\n* Mokka version: 0.5.0\n| Feature | Mokka | WireMock |\n| ------- | ----  | -------- |\n| Stubs Management (CRUD) | + | + |\n| Stubs Management - Disable | + | - |\n| SOAP/HTTP Support | + <BR> BASIC: No matching by request headers/cookies, no multipart support | + |\n| REST Support | + <br> BASIC: No matching by request headers/cookies, no multipart support| + |\n| Create stubs from OpenAPI spec | - | - |\n| JMS Support | + <br> Embedded ActiveMQ | - |\n| Proxying | -  | +  |\n| Record and Playback | -  | +  |\n| Simulating Faults | - | + |\n| Response templating | + <BR> Using Groovy | + <BR> Using Handlebars |\n| Stubs stored in | Database | JSON files |\n| Stubs modifications history | + | - |\n| Interactions log | + <br> Stored in database | + <br> Stored in memory |\n| File serving | + | + |\n| Fake Payment Gateway | + <BR> BlueMedia | - |\n| Admin API | - | + <BR>JSON API, Java API |\nSo the WireMock already covers most of the requirements. The main differences:\n* ""Stubs Management - Disable"" - use Stub Mapping Metada to define ""active"" property? __TODO__\n* ""Create stubs from OpenAPI spec"" - have to implemented in Mokka and WireMock anyway. __TODO__\n* ""JMS Support"" - to be verified __TODO__\n* ""Response templating"" - using ""Handlebars"" are not necessarily a drawback. It seems to be far more elegant and easy to use than Groovy. Also safer from security point of view. __TODO__\n* ""Stubs stored in"" - we can try to provide database-based custom implementation of `MappingsSource`\n* ""Stubs modifications history"" - __TODO__??\n* ""Interactions log"" - Saving to database may be achieved by providing custom implementation of `RequestJournal`. __TODO__??\n* ""Fake Payment Gateway"" - Mokka implementation can be reused\nThe following solutions were considered:\n* continue with Mokka stubs engine\n* \+ flexibility and open architectural decisions in future\n* \- large codebase to maintain\n* \- requires high contribution to develop all missing features\n* adopt WireMock sources and maintain them to fulfill all the requirements\n* \+ large set of production-ready features provided at start\n* \+ flexibility and open architectural decisions in future\n* \- large codebase to maintain\n* \- no easy way to upgrade WireMock\n* \- no support of WireMock community\n* adopt WireMock as library and use all possible extension points\n* \+ WireMock core implementation remains untouched, new feature are provided using well designed extensions points\n* \+ small codebase to maintain\n* \+ WireMock bugs are handled by the community\n* \+ upgrading WireMock should be straightforward (as long as no breaking changes are introduced to WireMock APIs)\n* \- some future architectural decisions or feature considerations may be limited by WireMock architecture\n* \- in unknown feature WireMock license may change or community support may drop\n* \- we fully rely on WireMock development lifecycle\n\n## Decision\n",Adopt WireMock as library and use all possible extension points.\n,Adopt WireMock as library and use all possible extension points.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","To record the architectural decisions made on this project, we will use an Architectural Decision Record (ADR). ADRs are documents that capture the context, decision, and rationale for an architectural decision. They provide a single source of truth for architectural decisions and help to ensure that all stakeholders are aware of and aligned on the decisions made."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMesh Configuration Protocol (MCP) is a [protocol](https://github.com/istio/api/tree/master/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\n\n## Decision\n,"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\n",MCP uses gRPC for message serialization and transport
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEnable one way TLS between front-end and envoy per host+domain that is\nspecified via gateway config.\n\n## Decision\n,"#### Gateway Configuration\nThis is achieved by sending the following config from copilot.\n```\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\nname: mygateway\nspec:\nselector:\nistio: ingressgateway # use istio default ingress gateway\nservers:\n- port:\nnumber: 443\nname: https-httpbin\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: /etc/istio/ingressgateway-certs/tls.crt\nprivateKey: /etc/istio/ingressgateway-certs/tls.key\nhosts:\n- ""httpbin.example.com""\n- port:\nnumber: 443\nname: https-bookinfo\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: /etc/istio/ingressgateway-bookinfo-certs/tls.crt\nprivateKey: /etc/istio/ingressgateway-bookinfo-certs/tls.key\nhosts:\n- ""bookinfo.com""\n```\nIn the config above each cert and key in the array of servers represent a\nhost+domain and the path to each cert and the key is arbitrarily chosen.\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\n```\nfrontend_tls_keypairs:\nexample:\n- cert_chain: |\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nprivate_key: |\n-----BEGIN RSA PRIVATE KEY-----\n-----END RSA PRIVATE KEY-----\n- cert_chain: |\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nprivate_key: |\n-----BEGIN RSA PRIVATE KEY-----\n-----END RSA PRIVATE KEY-----\n```\n#### Cert Storage\nThe placement of the certs and keys on the envoy VM is done using a separate\nprocess specific to this purpose. This process will be in charge of knowing\nwhere the certs are located and placing the certs on the correct paths. It is\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\nare stored, and having a specific process to manage this will reduce duplication\nand mitigate skew.\n",Implement one way TLS between front-end and envoy using a wildcard certificate and SNI headers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPrevious packaging of Copilot in istio release relied on the fact that you would\nbe building copilot on the local machine (bosh pre-packaging).  This meant that\nyou could reliably fetch all of your dependencies using dep (which was included\nas a blob in the release).\nWhen we moved to get rid of pre-packaging and instead do all packaging on a bosh\nvm (just known as packaging) we ended up missing one key external dependency for\ndep to work (git). Including git as part of release would have meant adding\nanother blob and packaging step just for git.\n\n## Decision\n,We removed the .gitignore of the vendor directory and checked-in all of the\nsource code that dep was placing in that directory at build time.\n,**Decision**: Depend on a pre-installed version of git in the Go runtime.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\n\n## Decision\n,We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\n,Increase the GRPC message size to 32 MB to resolve the bottleneck between cc-route-syncer and copilot.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\nacross all diego cells at a specified time interval (at the time of writing 10\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\nimpact the performance of the BBS (large response sets coming back).\n\n## Decision\n,We want to use the [Event package](https://github.com/cloudfoundry/bbs/blob/master/doc/events.md)\nto get the event stream for each ActualLRP. We will also use a bulk sync every\n60 seconds to catch any events that were missed.\n,Upgrade diego to use an ActualLRP syncing model which fetches LRPs on a periodic basis and incrementally updates the ActualLRP cache with a REST call (HTTP POST).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Utilize an Architectural Decision Record (ADR) template to document and track architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the project, we need an API for a map: interface, search, marker placement, satellite or road map imagery. There are several options for maps, will be a primary mode of interacting with the site.\n* Google maps\n* OpenLayers\n* TomTom\n* MapBox\n* HERE\n* Mapfit\nMain factor are cost, and ease of use (documentation for the API)\nGoogle maps are highly customizable in style and appearance, and configerable for marker placement, information windows, and interface/controls.\n\n## Decision\n","Upon examining the options, Google Maps was considered the most mature, easy-to-use and well-supported option. The API has excellent documentation and example code. The interface will be familiar to the majority of site users.\n","Based on the given factors of cost and ease of use, the decision is to use OpenLayers for the project's mapping needs. OpenLayers has a relatively low cost and provides comprehensive documentation for its API, making it easy to integrate and use within the project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\n- Marker placement on maps\n- Destinations being correctly added and drawn\n- Trips being correctly written to and received from session storage\n\n## Decision\n","The testing framework chosen for automated testing is Jest. This framework is used because:\n- It has a simple installation and configuration process for Node.js\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https://testawaywego.azurewebsites.net website since it is the website used for development.\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\n","In order to ensure thorough validation of the core functionality of the website, a comprehensive testing strategy encompassing unit testing, end-to-end testing, and manual testing will be employed.\n\n**Unit Testing**\n\n* **Purpose:** To validate the functionality of individual units of code, such as functions, within back-end models and database-related code.\n* **Benefits:**\n    * Isolates and tests specific code units independently, ensuring their correctness.\n    * Provides quick feedback, enabling early detection of errors.\n\n**End-to-End Testing**\n\n* **Purpose:** To test the functionality of the website from beginning to end, simulating actual user actions.\n* **Benefits:**\n    * Verifies the correct flow and integration of different components of the website.\n    * Ensures that the website behaves as expected from a user's perspective.\n\n**Manual Testing**\n\n* **Purpose:** To perform high-level functionality assessment and confirmation, focusing on specific aspects such as:\n    * Marker placement on maps\n    * Correct addition and drawing of destinations\n    * Proper writing and retrieval of trips from session storage\n* **Benefits:**\n    * Provides a comprehensive and thorough evaluation of website functionality.\n    * Captures real-world usage scenarios and edge cases that may be missed by automated testing.\n\n**Collaboration and Communication**\n\n* Testing teams will collaborate closely with developers to ensure that test cases cover all essential functionality.\n* Clear and concise communication will be maintained throughout the testing process to facilitate timely resolution of issues.\n* Automated tests will be updated and maintained regularly to reflect any changes in the codebase."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\n\n## Decision\n","4 Sprints planned, consecutively. Will only start on 17th April, but have a ""Sprint 0"" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\n* Release will be every Tuesday, by 8pm\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\n* Friday coding sessions together from 12pm - 4pm\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\nRebecca: Product Manager\nTyson: SCRUM Master\nTheese roles will be alternated throughout the project each week.\n",Allow Thabang to work remotely from 16-25 April to ensure productivity during the major submission period.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to join a group, a potential member must be invited (it's not correct to add a person to a group without asking permission.) A person invited can either be a member of the website (have an account) or be a new user (no account registered yet). In order to cover both of these scenarios, and to avoid the website being a ""walled garden"" with a tiny set of users, and to encourage potential future growt, a mechanism to invite users could be an email, sent by an existing member, to any valid email address with an invitation to join. This could be in the form of a token with a payload, or more simply, an extra table in the DB, linking the invited person's email to a trip ID.\n\n## Decision\n","The mechanism of an external invitation with a specific link requires the ability to send an email (prefereably attractively  styled and clearly phrased, to avoid being rejected as unsolicited or junk email). The node module 'nodemailer' was selected as appropriate, for its wide support, mature development and ease of use, and 0 dependecies.\n","The mechanism for inviting new members to a group will be an email, sent by an existing member, to any valid email address with an invitation to join. The email will contain a token with a payload that includes the group ID and the email address of the invited person. When the invited person clicks on the link in the email, they will be taken to a page where they can create an account or log in if they already have one. Once they have logged in, they will be added to the group."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA list of destinations should be reorderable, not fixed\n\n## Decision\n","A trip is made up of a list of destinations. This list should be able to be reordered, on the main site or the mobile version of the site. Draggable would be the best, but a button for moving an extry up and down will also work.\n",Use a reorderable list component.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDestinations need to be entered into a trip somehow. The two most obvious choices seem to be by typing (some kind of auto-completion feature) or by clicking directly on a map, to set markers. These paradigms are the dominant ones in most existing APIs and site/map websites.\n\n## Decision\n",We will aim to support both autocomplete AND clicking on the map. This would be the most convenient for users of the site.\n,Incorporate both typing and map-select methods for specifying destinations. This will allow users to choose the most convenient method for their needs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\n\n## Decision\n",Using a well known and widely known/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login/registration page through the Google Login API\n,"**Decision:** Integrate the Google Login API for user authentication.\n\n**Rationale:**\n\n* **Convenience and efficiency:** Using the Google Login API will simplify the login process for users, reducing the need for them to create and remember separate login credentials for our application.\n* **Security enhancement:** By leveraging the Google Login API, we can utilize the security measures implemented by Google, such as two-factor authentication, to enhance the overall security of our application.\n* **Time-saving:** Integrating the Google Login API will enable us to expedite the completion of the Sprint 1 User story related to login, allowing us to allocate resources to other development tasks.\n* **User experience improvement:** Providing multiple login options, including Google Login, improves the user experience by offering flexibility and reducing barriers to entry.\n* **Data compatibility:** The Google Login API can provide a standardized format for user data, which will facilitate easier integration with our existing database and other systems."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA SCRUM-based agile devlopment workflow would benefit from a central KANBAN board to keep track of userstories that have been written, are in progress, and are complete. This will help identify the sprint backlog, and the current focus of the sprint. Labels could be used to indicate size/priority/difficuly or value to the project, to help calculate the sprint velocity and determine what can get done inside a single sprint.\n\n## Decision\n","Using the GitHib Project page with a single project for the repo, and using Issues labelled as User Stories, with columns for ""To Do"", ""In progress"", and ""Completed"".  We can leverage some of the automatic rules in Git to help automate some of the completetion of tasks ties to Milestones for each sprint:\nhttps://github.com/witseie-elen4010/2019-005-project/projects/1\n","Implement a central KANBAN board using Jira or Trello, with the following columns:\n\n* **Backlog:** User stories that have not been started yet\n* **In Progress:** User stories that are currently being worked on\n* **Code Review:** User stories that have been completed and are undergoing code review\n* **Testing:** User stories that have passed code review and are being tested\n* **Done:** User stories that have been tested and are complete\n\nLabel user stories with the following information:\n\n* **Size:** Small, Medium, Large\n* **Priority:** High, Medium, Low\n* **Difficulty:** Easy, Medium, Hard\n* **Value:** High, Medium, Low"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo perform Continual Integration and development, with weekly releases, it would be convenient and useful to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac/front-end. It is difficult to automate these front-end interface tests, and there may be factors not present in a localhost/express server that only become apparent in an online scanario.\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\n\n## Decision\n","**master** will be the release branch\n**development** will be the main development/test branch. This will also be made into the ""default"" branch for all pull requests, to avoid accidentaly PR into master\n**feature** branches must be made off development, with unique names. All pull requests for completed features to be made into ""development"".\n* All PRs must be reviewed by at least two developers to merge into ""development""\n* All PRs must be reviewed by at the three other developers to merge into ""master""\n* All PRs must pass all tests (Jest, Travis, and Coveralls) in order to be considered valid for a merge\n* Stale reviews will be automatically dismissed if a new commit is pushed to the same branch\n* Accepted PRs for completed features (User Stories) should be deleted after sucessfully merging\n",Create a `testing` branch to perform front-end interface testing before merging into the `master` branch.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving a fixed structure for a project has may advantages, limiting spread of files across multiple folders and contraining locations to known places. THere is an advantage is letting a folder strucute emerge oganically, but also a large risk, as things can break when low-level file locations change, necesitating logs of bug fixing and refactoring. Having a rigid initial structure canb lead to later restrictions, or imposed complexity.\n\n## Decision\n",The following folder strucure is adopted:\n.\n├── app\n│   ├── controllers\n│   ├── models\n│   ├── public\n│   │   ├── css\n│   │   ├── img\n│   │   └── js\n│   ├── routes\n│   └── views\n├── docs\n│   ├── adr\n│   ├── misc\n│   ├── project_artifacts\n│   └── templates\n├── node_modules\n├── test\n└── local_only\n**Update** Removed folders originally specified that were found to not be required during project development: 'log' and 'utility'\n,"**Decision:**\n\nAdopt a hybrid approach, leveraging both a defined folder structure and organic emergence:\n\n**Structured Core:**\n- Establish a well-defined folder structure for core application components, ensuring consistency and easy navigation.\n- Use this core structure for critical code and data that requires stability and control.\n\n**Organic Substructures:**\n- Allow for the organic emergence of substructures within specific application modules or feature areas.\n- Provide guidelines and best practices to encourage logical organization and prevent excessive nesting.\n- Monitor and review substructures periodically to identify potential issues and ensure alignment with the project's goals.\n\n**Benefits:**\n\n- Ensures a stable and predictable foundation for the core application.\n- Provides flexibility and adaptability for evolving requirements in specific areas.\n- Balances the advantages of both rigid and organic structures, minimizing potential risks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nPHPUnit offers a range of `@covers` annotations with the possible to enforce a strict mode or to\nenforce them. The question is when should those annotations be enforced and/or if we need to enable\nanother settings as well?\n### Decision\nSince we are using the [`symfony/phpunit-bridge`][phpunit-bridge], we decide to leverage the\n[`Symfony\Bridge\PhpUnit\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\n`@covers` annotations whilst still benefit from it.\nThis however does not allow to completely forgo its usage due to the following cases:\n- A test testing more than one class, requiring multiple `@covers` annotations\n- A test case testing a ""test class"", i.e. code reserved for testing purposes\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\nsetting `general_phpdoc_annotation_remove` has been refused.\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\n`@covers` annotation is necessary or not, no further action has been voted for automating this\nprocess.\n### Status\nAccepted ([#1060][1060])\n[code-coverage-listener]: https://symfony.com/doc/current/components/phpunit_bridge.html#code-coverage-listener\n[phpunit-bridge]: https://packagist.org/packages/symfony/phpunit-bridge\n[php-cs-fixer]: https://github.com/FriendsOfPHP/PHP-CS-Fixer\n[1060]: https://github.com/infection/infection/pull/1060\n\n## Decision\n","Since we are using the [`symfony/phpunit-bridge`][phpunit-bridge], we decide to leverage the\n[`Symfony\Bridge\PhpUnit\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\n`@covers` annotations whilst still benefit from it.\nThis however does not allow to completely forgo its usage due to the following cases:\n- A test testing more than one class, requiring multiple `@covers` annotations\n- A test case testing a ""test class"", i.e. code reserved for testing purposes\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\nsetting `general_phpdoc_annotation_remove` has been refused.\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\n`@covers` annotation is necessary or not, no further action has been voted for automating this\nprocess.\n### Status\nAccepted ([#1060][1060])\n[code-coverage-listener]: https://symfony.com/doc/current/components/phpunit_bridge.html#code-coverage-listener\n[phpunit-bridge]: https://packagist.org/packages/symfony/phpunit-bridge\n[php-cs-fixer]: https://github.com/FriendsOfPHP/PHP-CS-Fixer\n[1060]: https://github.com/infection/infection/pull/1060\n","Leverage the `Symfony\Bridge\PhpUnit\CoverageListener` in `phpunit.xml.dist` to avoid the requirement for `@covers` annotations while still benefiting from them. However, `@covers` annotations are still necessary in cases where a test covers multiple classes or a ""test class"" reserved for testing purposes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\n`self`.\nWhilst ""incorrect"", this usage does not break anything. Besides:\n- [PHUnit documentation][phpunit-doc] itself uses this by default\n- `$this` is much more widely used than `self` in this context in the community\n- all Infection code uses `$this`\nThere is not much shortcomings from using this other than the ""incorrectness"" of using a static\nmethod as a non-static one.\n### Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\nthe decision is to keep the usage of `$this` over `self` in the codebase.\n### Status\nAccepted ([#1061][1061])\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\n[1061]: https://github.com/infection/infection/pull/1061\n\n## Decision\n","Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\nthe decision is to keep the usage of `$this` over `self` in the codebase.\n### Status\nAccepted ([#1061][1061])\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\n[1061]: https://github.com/infection/infection/pull/1061\n",Keep the usage of `$this` over `self` in the codebase.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nUsing `@inheritdoc` was done inconsistently across the codebase so the decision of whether we use it\nsystematically or remove it systematically had to be done.\nA number of points:\n- [PHPDoc][phpdoc-inheritance] provides inheritance of the docblocks by default when appropriate\n- Static analysers such as PHPStan or Psalm can do without at the time of writing\nAlso it has a very limited value.\n### Decision\nDo not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\nbe removed when submitting pull requests.\n### Status\nAccepted ([#860][860])\n[phpdoc-inheritance]: https://docs.phpdoc.org/guides/inheritance.html\n[860]: https://github.com/infection/infection/issues/860\n\n## Decision\n,Do not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\nbe removed when submitting pull requests.\n### Status\nAccepted ([#860][860])\n[phpdoc-inheritance]: https://docs.phpdoc.org/guides/inheritance.html\n[860]: https://github.com/infection/infection/issues/860\n,Do not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must be removed when submitting pull requests.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\n```php\nfunction test_something(): void {\n// ...\ntry {\n// the statement that fail\n$this->fail();\n} catch (Exception $e) {\n// ...\n}\n}\n```\nOr:\n```php\nfunction test_something(): void {\n// ...\n$this->expectException($exception)\n// the statement that fail\n}\n```\n### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n\n## Decision\n","As recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n","Leverage the `expectException*()` API when possible, as recommended by Sebastian Bergmann."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGiven Dapr is going out with language specific Actor SDKs, we formally introduced an Actor API into Dapr to make Actors are first-class citizen in Dapr. The goal of this review was to ensure Dapr can provide strong support of Service Fabric stateful actors programming model so that we can offer a migration path to the majority of existing actor users.\n\n## Decision\n","### Dapr\n* A separate Actor interface is defined.\n* Actors should support multiple reminders and timers.\n* Actor state access methods are encapsulated in the Actor interface itself.\n* Actor interface shall support updating a group of key-value states in a single operation.\n* Actor interface shall support deletion of an actor. If the actor is activated when the method is called, the in-flight transaction is allowed to complete, then the actor is deactivated, deleted, with associated state removed.\n### Non-Dapr\n* Transaction across multiple API calls is left for future versions, if proven necessary. Due to single-threaded guarantee, such transaction scope might be unnecessary. However, if developer expects an Actor code to behave atomically (in an implied transaction scope), we may have to implement this.\n","**Decision**\n\nDapr will support the Service Fabric stateful actor programming model by introducing an Actor API that aligns with the Service Fabric actor model semantics. This will provide a migration path for existing Service Fabric actor users to Dapr and enable them to leverage the benefits of Dapr's distributed application runtime, including simplified actor state management, automatic service discovery, and resilient communication."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before / after PR is merged or by a scheduler.\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\n\n## Decision\n","### Test environments\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\n* **Local machine**. contributors or developers will use [Minikube](https://github.com/kubernetes/minikube) to validate their changes and run new tests before creating Pull Request.\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) before/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\n### Bring up test cluster\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https://github.com/kubernetes/test-infra/tree/master/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\n### CI/CD and test result report for tests\nMany Kubernetes-related projects use [Prow](https://github.com/kubernetes/test-infra/tree/master/prow), and [Testgrid](https://github.com/kubernetes/test-infra/tree/master/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\nInstead, Dapr will use [Azure Pipeline](https://azure.microsoft.com/en-us/services/devops/pipelines/) to run e2e tests and its [test report feature](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\n","**Decision:** To bring up a **Kubernetes cluster** with the **Dapr components** deployed, to run the **E2E tests** against the deployed components, and to report the **test results** in **JUnit XML format** using **JUnit 5**."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor cloud-edge hybrid scenarios and multie-region deployment scenarios, we need the ability to facilitate communications cross clusters. Specifically, it's desirable to have services scoped by cluster names so that a service in one cluster can address and invoke services on another trusted cluster through fully qualified names in a universal namespace, such as cluster1.serviceb.\n\n## Decision\n",We should consider adding universal namespace capabilities to Dapr.\n,Implement a cluster-aware service discovery mechanism that allows services in one cluster to discover and address services in other clusters by their fully qualified names in a universal namespace. This mechanism should be used in conjunction with a service registration mechanism that enables services to register their availability and capabilities with the discovery mechanism.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAuthenticode signing of binaries.\n\n## Decision\n,* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\n,Implement Authenticode signing for all released binaries.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\nSee https://github.com/dapr/java-sdk/issues/17\n\n## Decision\n,* Java 8 should be the minimum version supported for Dapr's Java SDK.\n* Java 11 should be used in samples and user documentation to encourage adoption.\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\n,"The minimum Java version supported by Dapr's Java SDK should be Java 8.\n\nJava 8 is still the mainly used version by the Java community in 2019. It is also the last LTS version of Java. This means that it is still supported by Oracle and will continue to receive security updates.\n\nJava 11 is the latest LTS version of Java. However, it is not yet as widely used as Java 8. This means that there may be some compatibility issues with older applications.\n\nBy supporting Java 8, Dapr's Java SDK can reach a wider audience. It can also avoid any potential compatibility issues with older applications."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\nWe needed a way to construct an accepted and constant way of naming our Docker images.\n\n## Decision\n","* An image will conform to the following format: \<namespace>/\<repository>:\<tag>\n* A valid tag conforms to the following format: \<version>-\<architecture>, or just \<version>, then arch is assumed Linux\n","As we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures, we have settled on the following naming convention for our Docker images:\n\n\n- Name: `repository-name`.`image-name`\n- Version: `major`.`minor`.`patch`\n- Architecture: `amd64` or `arm64`\n\nFor example, an image for the `front-end` repository with the `web` image, version `1.0.0`, and architecture `amd64` would be named as `front-end.web:1.0.0-amd64`.\n\nThis naming convention will help us to easily identify and manage our Docker images. It will also allow us to consistently version and architecture our images, which will be beneficial for our development and deployment processes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\nAn API change is needed to account for the requested type of operation.\n\n## Decision\n","### Naming\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\n### Types\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\nDid not provide meaningful value.\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\n### API structure\nIt was decided that the API url will be kept as: `http://localhost:<port>/v1.0/bindings/<name>`.\nThe verb for the HTTP API will remain POST/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\nThis is not a breaking change.\n### Schema and versioning\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\n",Update the API to allow for bi-directional streaming.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe reviewed storage API design for completeness and consistency.\n\n## Decision\n,"* All requests/responses use a single parameter that represents the request/response object. This allows us to extend/update request/response object without changing the API.\n* Add Delete() method\n* Support bulk operations: BulkDelete() and BulkSet(). All operations in the bulk are expected to be completed within a single transaction scope.\n* Support a generic BulkOperation() method, which is carried out as a single transaction.\n* Transaction across multiple API requests is postponed to future versions.\n* Actor state operations are moved to a new Actor interface. Please see [API-002-actor-api-design](./API-002-actor-api-design.md).\n",**Decision:** Approve the storage API design as complete and consistent.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNot adding content-type to state store, pubsub and bindings.\n\n## Decision\n","* We will not add content-type since it is a persisted metadata and it can cause problems such as:\n* Long term support since metadata persisted previously would need to be supported indefinitely.\n* Added requirement for components to implement, leading to potentially hacky implementations to persist metadata side-by-side with data.\nOriginal issue and discussion: https://github.com/dapr/dapr/issues/2026\n","**Decision:** Do not add content-type to state store, pubsub and bindings.\n\n**Rationale:**\n\n* **Simplicity:** Adding content-type introduces additional complexity and maintenance overhead.\n* **Compatibility:** Removing content-type ensures compatibility with existing systems and integrations.\n* **Performance:** Content-type adds unnecessary data to messages, potentially impacting performance.\n* **No functional impact:** The absence of content-type does not affect the functionality of the system.\n* **Protocol compliance:** PubSub and state store protocols do not require content-type for message content."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n\n## Decision\n",* We'll keep CLI versioning and runtime versioning separate.\n* CLI will pull down latest runtime binary during the *init()* command.\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\n,"Uncouple CLI versions from runtime versions.\n\n**Rationale:**\n\n* **Decouples development and deployment:** Allows CLI to evolve independently of runtime, enabling faster iteration and feature development.\n* **Enhances flexibility:** Developers can choose the latest CLI version with new features while using a stable runtime version for production.\n* **Reduces maintenance overhead:** Separate update processes simplify maintenance and minimize potential conflicts.\n* **Improves compatibility:** Enables running multiple runtime versions with a single CLI version, providing flexibility in migration scenarios.\n* **Facilitates experimentation:** Users can test newer CLI versions without committing to runtime upgrades, reducing risk and encouraging innovation."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere was a proposal to introducing versioning for HTTP App Callbacks. The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR\n\n## Decision\n,"- Introducing versioning to app callback APIs would require changes to the user applications which is not feasible\n- There would be no way for DAPR runtime to find out the app callback version before hand\nWe decided not to introduce such a versioning scheme on the app callback APIs. Post v1.0, if required, the versioning could be implemented inside the payload but not on the API itself. A missing version in the payload could imply v1.0.\n","**Decision:** Versioning for HTTP App Callbacks is not required at this time.\n\n**Rationale:**\n\n* The current implementation of DAPR is stable and there are no anticipated breaking changes for HTTP App Callbacks in the foreseeable future.\n* Adding versioning would introduce complexity and overhead to the system.\n* The benefits of versioning are outweighed by the risks and costs.\n\n**Consequences:**\n\n* HTTP App Callbacks will not be versioned.\n* If DAPR undergoes a重大版本更新 in the future, the impact on HTTP App Callbacks will need to be carefully evaluated."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\nto support multi state store with no backward compatibility.\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\nv1.0/state/storeA/\nv1.0/state/storeB/\nEarlier this breaking change, the API is v1.0/state/`<key>`\nWe have reviewed multi storage API design for completeness and consistency.\n\n## Decision\n","*  New state store API is v1.0/state/`<store-name>`/\n*  If user is using actors and like to persist the state then user must provide actorStateStore: true in the configuration yaml.\nIf the attribute is not specified or multiple actor state stores are configured, Dapr runtime will log warning.\nThe actor API to save the state will fail in both these scenarios where actorStore is not specified or multiple actor stores\nare specified.\n*  It is noted that after this breaking change, actor state store has to be specified unlike earlier where first state store is picked up by default.\n* It is noted that this breaking change will also require a CLI change to generate the state store YAML for redis with actorStateStore.\n* To provide multiple stores, user has to provide separate YAML for each store and giving unique name for the store.\n* It is noted that the param's keyPrefix represents state key prefix, it's value included ${appid} is the microservice appid, ${name} is the CRDs component's unique name, ${none} is non key prefix and the custom key prefix\nFor example, below are the 2 sample yaml files in which redis store is used as actor state store while mongodb store is not used as actor state store.\n```\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\nname: myStore1  # Required. This is the unique name of the store.\nspec:\ntype: state.redis\nmetadata:\n- name: keyPrefix\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\n- name: <KEY>\nvalue: <VALUE>\n- name: <KEY>\nvalue: <VALUE>\n- name: actorStateStore  # Optional. default: false\nvalue : true\n```\n```\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\nname: myStore2 # Required. This is the unique name of the store.\nspec:\ntype: state.mongodb\nmetadata:\n- name: keyPrefix\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\n- name: <KEY>\nvalue: <VALUE>\n- name: <KEY>\nvalue: <VALUE>\n```\nSo with the above example, the state APIs will be : v1.0/state/myStore1/`<key>`\nand v1.0/state/myStore2/`<key>`\n","**Decision:** Introduce a breaking change in the API to support multi-state store support with no backward compatibility.\n\n**Rationale:**\n\n* The previous API (v1.0/state/`<key>`) did not allow the app to target a specific state store.\n* The new API (v1.0/state/`<store-name>/`<key>`) allows the app to target a specific state store, which is necessary for supporting multiple state stores.\n* The breaking change is necessary to ensure that the new API is not compatible with the old API, which could lead to unexpected behavior.\n* The multi-storage API design has been reviewed for completeness and consistency."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\n\n## Decision\n,"### Integration build release\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\n### Official build release\n#### Pre-release build\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\n**Pre-release process**\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\n```\n$ git tag ""v0.1.0-alpha.0"" -m ""v0.1.0-alpha.0""\n$ git push --tags\n```\n3. CI creates new build and push the images with only version tag\n4. Test and validate the functionalities with the specific version\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\n7. Repeat from 4 to 6 until all bugs are fixed\n#### Release the stable version to users\nOnce all bugs are fixed, we will create the release note under [./docs/release_notes](https://github.com/dapr/dapr/tree/master/docs/release_notes) and run CI release manually in order to deliver the stable version to users.\n### Release Patch version\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\n","To safely release new Dapr binaries and corresponding configurations without interrupting users, implement the following process:\n\n**Pre-Release:**\n\n1. Announce the planned release date and changes in a blog post or other public forum.\n2. Update the Dapr website and documentation with the new version information.\n3. Test the new release internally on a test cluster to ensure stability.\n4. Make a snapshot of the Dapr repository before the release.\n\n**Release Day:**\n\n1. Release the new Dapr binaries to the public repository.\n2. Update the Dapr Helm charts to reference the new binaries.\n3. Create a release branch in the Dapr repository and cherry-pick the necessary changes.\n4. Publish the new release notes to the Dapr website and documentation.\n\n**Post-Release:**\n\n1. Monitor the release closely for any issues or blockers.\n2. Address any reported issues promptly.\n3. Update the Dapr website and documentation as needed.\n4. Merge the release branch back into the main branch."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\n\n## Decision\n","### Concurrency model\n* Dapr supports two flavors of optimistic concurrency: first-write wins and last-write wins. First-write wins is implemented through ETag.\n* User code can express concurrency intention with a *config* annotation attached to a request. See **Config annotation** for details.\n* Future version of Dapr may support call throttling through application channel.\n* We'll choose last-write wins as the default.\n### Consistency model\n* Dapr supports both eventual consistency and strong consistency.\n* Actors always use strong consistency.\n* We'll choose eventual consistency as default for services other than actors.\n### Actor Transaction\n* Dapr-compatible Actor state stores shall support ACID transaction.\n* Dapr doesn't mandate specific transaction isolation level at this point. However, when deemed necessary, we can easily add those to **Config annotation** as needed.\n### Config annotation\n* User payload can contain an optional **config** annotation/element that expresses various constraints and policies to be applied to the call, including:\n* Concurrency model: first-write or last-write\n* Consistency model: strong or eventual\n* Retry policies:\n* Interval\n* Pattern: linear, expotential\n* Circuit-breaker Timeout (before an open circuit-breaker is reset)\n### State store configuration probe\n* An Dapr-compatible state store shall provide an endpoint that answers to configuration probe and returns (among others):\n* Supported concurrency model\n* Supported consistency model\n* A state store instance shall return the specific configuration of the current instance.\n* It's considered out of scope to require state store to dynamically apply new configurations.\n### Dapr\n* Update state store API spec to reflect above decisions\n* Create backlog of issues to implement above decisions\n",Components will define behaviors in the API spec and these behaviors will be implemented in the implementation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have defined gRPC services and protobuf messages without convention, which results in the duplicated protobuf definitions and inconsistent names of services and messages. Thus, this record defines the minimum-level coding convention for Protobuf message to improve the quality of grpc/protobuf message definitions.\n\n## Decision\n","* Use `google.protobuf.Any` data field only if the message field conveys serialized protobuf message with type url. Otherwise, use the explicit data type or protobuf message.\n* Use `Request` suffix for gRPC request message name and `Response` suffix for gRPC response message name\n* Do not use `Client` and `Service` suffix for gRPC service name e.g. (x) DaprClient, DaprService\n* Avoid the duplicated protobuf message definitions by defining the messages in shared proto\n* Define and use enum type if field accepts only predefined values.\n","**Decision:**\n\nEstablish a coding convention for Protobuf message definitions to ensure consistency, reduce duplication, and improve readability.\n\n**Rationale:**\n\n* **Consistency:** A coding convention will ensure that all Protobuf messages follow the same naming conventions, avoiding confusion and inconsistencies.\n* **Reduced Duplication:** By following a convention, we can eliminate duplicate Protobuf definitions and centralize message definitions in a single location.\n* **Improved Readability:** A consistent coding style will make Protobuf messages easier to read and understand, reducing the time spent on deciphering the code.\n* **Maintainability:** A well-defined coding convention will make it easier to maintain and update Protobuf messages, ensuring long-term code quality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\n\n## Decision\n","### Dapr\n* All components use the same **Component** manifests, identified by a component **type**.\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\n","We will introduce different manifest types for State Stores, Message Buses, and Bindings.\n\n**Rationale**\n\nIntroducing different manifest types for State Stores, Message Buses, and Bindings will allow us to better organize and manage our manifests. It will also make it easier to identify the different types of resources that we are working with.\n\n**Consequences**\n\n* We will need to create new manifest types for State Stores, Message Buses, and Bindings.\n* We will need to update our tooling to support the new manifest types.\n* We will need to update our documentation to reflect the new manifest types."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https://github.com/dapr/cli/issues/411).\n\n## Decision\n,"* Calling `dapr init` will\n* Install `daprd` binary in `$HOME/.dapr/bin` for Linux/MacOS and `%USERPROFILE%\.dapr\bin` for Windows.\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\n* Create the default `components` folder in `$HOME/.dapr/bin` for Linux/MacOS or `%USERPROFILE\.dapr\bin` for Windows.\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\n* Create a default configuration file in `$HOME/.dapr/config.yaml` for Linx/MacOS and `%USERPROFILE%\.dapr\config.yaml` for Windows for enabling tracing by default.\n* Calling `dapr init --slim` will\n* Install the binaries `daprd` and `placement` in `$HOME/.dapr/bin` for Linux/MacOS and `%USERPROFILE%\.dapr\bin` for Windows.\n* Create an empty default `components` folder in `$HOME/.dapr/bin` for Linux/MacOS or `%USERPROFILE\.dapr\bin` for Windows.\n* Calling `dapr uninstall` will\n* Remove the folder containing binaries `bin` the default path `$HOME/.dapr` for Linux/MacOS and `%USERPROFILE%\.dapr` for Windows.\n* Remove the docker dapr_placement if Docker is installed.\n* Calling `dapr uninstall --all`\n* Remove the folder containing binaries `bin` the default path `$HOME/.dapr` for Linux/MacOS and `%USERPROFILE%\.dapr` for Windows.\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\n* Remove the default folder `$HOME/.dapr` in Linux/MacOS and `%USERPROFILE%\.dapr` in Windows.\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME/.dapr` for Linux/MacOS and `%USERPROFILE%\.dapr` for Windows.\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\n* The `dapr` CLI by default will expect the `daprd` in `$HOME/.dapr/bin` for Linux/MacOS and `%USERPROFILE%\.dapr\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\n","**Decision:**\n\nThe behavior of `init` and `uninstall` commands on Self Hosted mode will change to match the behavior in Hosted mode, as follows:\n\n* `init` will no longer create or modify Kubernetes resources.\n* `uninstall` will no longer delete Kubernetes resources.\n\n**Rationale:**\n\nThis change is necessary to ensure consistency between the Self Hosted and Hosted modes of Dapr. This will simplify user experience and reduce confusion.\n\n**Consequences:**\n\n* Users must manually create and delete Kubernetes resources when using Dapr in Self Hosted mode.\n* The `init` and `uninstall` commands will no longer be able to automatically create or delete Kubernetes resources."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGo community has the multiple http server implementations, such as go net/http, fasthttp, gin, to serve HTTP Restful API. This decision records describes which http server implementation uses in Dapr.\n\n## Decision\n",* Use [fasthttp server](https://github.com/valyala/fasthttp) implementation because fasthttp offers [the best performance and lowest resource usages](https://github.com/valyala/fasthttp#http-server-performance-comparison-with-nethttp) for the existing HTTP 1.1 server\n* Use [fasthttpadaptor](https://godoc.org/github.com/valyala/fasthttp/fasthttpadaptor) if you need to convert fasthttp request context to net/http context.\n,"The decision is to use **Go net/http** as the default HTTP server implementation in Dapr.\n\n**Rationale:**\n\n* **Widely adopted:** Go net/http is the standard library for HTTP server in Go, and it is widely adopted by the Go community. This makes it easy for developers to find resources and support for using net/http.\n* **Stable and mature:** Go net/http has been in development for many years and is considered stable and mature. This means that it is less likely to have bugs or security vulnerabilities.\n* **Extensible:** Go net/http is extensible, allowing developers to add custom functionality to the server. This makes it possible to use net/http to build complex and sophisticated HTTP applications.\n* **High performance:** Go net/http is a high-performance HTTP server implementation. It is able to handle a large number of concurrent requests efficiently.\n\n**Consequences:**\n\n* Developers using Dapr will need to use Go net/http to build their HTTP applications.\n* Dapr will provide support for using Go net/http in its documentation and tutorials.\n* Other HTTP server implementations may be supported in the future, but Go net/http will remain the default implementation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we keep building up Dapr features, it becomes apparent that we need to refactor the existing code base to reinforce component modularity. This will improve testability and maintainability in long run. And this refactor also lays the foundation of opening up extensible points (such as Bindings) to the community.\n\n## Decision\n","### Dapr\n* Formally separate hosting and API implementations. Hosting provides communication protocols (HTTP/gRPC) as different access heads to the same Dapr API implementation.\n* Ensure consistency between gRPC and HTTP interface.\n* Separate binding implementations to a separate repository.\n* Use smart defaults for configurable parameters.\n* Rename Dapr runtime binary from **dapr** to **daprd**.\n### Non-Dapr\n* We may consider allowing Dapr to dynamically load bindings during runtime. However, we are not going to implement this unless it's justified by customer asks.\n* A unified configuration file that includes paths to individual configuration files.\n* Provide a Discovery building block with hopefully pluggable discovery mechanisms (such as a custom DNS).\n","To refactor the Dapr code base to improve component modularity, testability, and maintainability. This refactor will also lay the foundation for opening up extensible points to the community."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDapr exposes APIs for building blocks which can be invoked over http/gRPC by the user code. Making raw http/gRPC calls from user code works but it doesn't provide a good strongly typed experience for developers.\n\n## Decision\n,"* Dapr provides language specific SDKs for developers for C#, Java, Javascript, Python, Go, Rust, C++. There may be others in the future\n- For the current release, the SDKs are auto-generated from the Dapr proto specifications using gRPC tools.\n- In future releases, we will work on creating and releasing strongly typed SDKs for the languages, which are wrappers on top of the auto-generated gRPC SDKs (e.g. C# SDK shipped for state management APIs with the 0.1.0 release.) This is the preferred approach. Creating purely handcrafted SDKs is discouraged.\n* For Actors, language specific SDKs are written as Actor specific handcrafted code is preferred since this greatly simplifies the user experience. e.g. The C# Actor SDK shipped with the 0.1.0 release.\n",Create a set of client helper libraries for each building block in each language to provide a strongly typed experience for developers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.\n\n## Decision\n,"### Dapr\n* All messaging APIs are grouped under a **messaging** namespace/package.\n* We define three distinct messaging interfaces:\n- **direct**\nOne-to-one messaging between two parties: a sender sending message to a recipient.\n- **broadcast**\nOne-to-many messaging: a sender sending message to a list of recipients.\n- **pub-sub**\nMessaging through pub-sub: a publisher publishing to a topic, to which subscribers subscribe.\n* We distinguish message and direct invocation. For messaging, we guarantee at-least-once delivery. For direct invocation, we provide best-attempt delivery.\n",We will adopt the following naming conventions for messaging interfaces:\n\n- Request interfaces will be named using the `Request` suffix.\n- Response interfaces will be named using the `Response` suffix.\n- Event interfaces will be named using the `Event` suffix.\n- Command interfaces will be named using the `Command` suffix.\n- Message interfaces will be named using the `Message` suffix.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe reviewed parity of state store APIs .\n\n## Decision\n,"* GetState APIs continue to have Single Key Get and Bulk Get APIs behaviour as current 0.10.0 version.\n* SaveState API will continue to have one SaveState API endpoint. If user wants to save single key, same save state API will be used\nfor passing single item in the bulk set.\nPotential issues arises if following new single key save state API is introduced:\n`Post : state/{storeName}/{key}`\nThis will conflict with\n- State Transaction API, if the key is ""transaction""\n- GetBulkState API, if the key is ""bulk""\nSo the decision is to continue the Save State API behaviour as current 0.10.0 version.\n* Bulk Delete API might come in future versions based on the scenarios.\n",Implement both in-memory and etcd state stores.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe now support distributed tracing across Dapr sidecars, and we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code. However, it's up to the user code to configure and implement proper tracing themselves.\n\n## Decision\n",We should consider adding a tracing endpoint that user code can call in to log traces and telemetries.\n,We will provide a set of pre-built tracing components that user code can optionally use to automatically trace incoming requests.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile we were considering how to ship the CSSK, the question of how it\nshould be organized came up several times. The crux of the issue was\nwhether the more reusable parts (for example, the event dispatch bits)\nshould be separated into a library, or whether it should all just live\nin one big pile of code. Currently, the CSSK is organized in the\nlatter manner.\nSeparating the infrastructure code out into a separate library would\nmake it more obvious which parts of the code are intended to be edited\nby the developer. Right now, it’s not immediately obvious where to\nmake changes, and what bits are there to support the development\nexperience.\nSeparating the infrastructure code out into a separate library would\nalso make it far more inconvenient to make changes to it. Developers\nwould need to update the lib, possibly creating their own fork, and\nthen ensure that the app code was using the appropriate version. This\nis extremely likely to happen because the infrastructure code is\nneither comprehensive nor mature.\n\n## Decision\n","We have decided to keep it all together, but clearly separate the\n“library” code from the “app” code by using separate directories for\nthose two types of code: “lib” and “app”.\n","**Decision:** The CSSK will be organized into a single codebase, with the more reusable parts separated into a library.\n\n**Rationale:**\n\n* **Clarity:** Separating the reusable parts into a library will make it more obvious which parts of the code are intended to be edited by the developer. This will improve the development experience and reduce the risk of unintended changes.\n* **Flexibility:** Keeping the CSSK in a single codebase will make it more flexible and easier to maintain. Developers will be able to make changes to both the reusable parts and the application code without having to manage multiple dependencies.\n* **Convenience:** Updating the reusable parts will be easier if they are part of the same codebase. Developers will not have to create their own fork or manage multiple versions of the library.\n\n**Consequences:**\n\n* **Increased maintenance overhead:** The reusable parts will need to be maintained separately from the application code. This could increase the maintenance overhead for the project.\n* **Potential for code duplication:** There is a risk that some code may be duplicated between the reusable parts and the application code. This could lead to maintenance issues down the road."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separate from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.\n\n## Decision\n",We've decided to go with pushing changes directly to GitHub via the GitHub API and using an async worker system to do it. The async system chosen was [Spinach][].\n,Push changes to GitHub as soon as they're made.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Establish an Architectural Decision Record (ADR) process and a central repository for documenting all architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe 'core mechanic' of this game is the player being in control of allied plants.  There may be other cases of player control of other in-game entities as dev continues, but this is core.\nGiving creatures and the player character pluggable command/control is more dev overhead.  As a spare time project, any choice for extra complexity deserves extra scrutiny.  On the same wave, though, the outcomes which can justify the extra effort are a larger set:  Not just important functionality, but significant learning and straight pleasure in craftsmanship can be enough reason.\nThis offers options for reuse.\nIf our guy is terrified, the player's InputCommandSource can be temporarily replaced with a FleeingCommandSource (or some such).  When the player is in charge of other entities, they get an InputCommandSource themselves, and will naturally be controlled by the player when their moments to act arrive in the schedule.\n\n## Decision\n","I'll do this.  It feels correct, and it trends toward the sort of wins and frustrations I'm hoping for in this project.  That is, where my decisions, rather than the guts of frameworks, are what I'm wrestling most often.\n",Implement an InputCommandSource system with pluggable control capabilities.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI feel that ADRs in some form may be very beneficial at Envisage, but two things make me reluctant to push the practice right now.  First, ignorance--I've never done this, so I can't talk personally about the experience.  Second, the enthusiasm of first encounter.  Both of these are addressed by actually doing it for a while.\nCopper Bend is a learning/entertainment project.  Trying new techniques and technologies is a primary goal.\nThe Envisage products are very different from this one.  What works well here won't necessarily cross over.\n\n## Decision\n","I will avoid tooling, and simply handcraft the documents.  I'll keep them in the main project repo.  I'll review periodically, trimming the present-but-useless and adding the missing-yet-promising.\nI won't worry about synchronizing the format between ADR docs.\n","**Decision:**\n\nExperiment with ADRs on the Copper Bend project to explore their benefits and drawbacks in a low-stakes environment.\n\n**Rationale:**\n\n* **Overcoming ignorance:** Hands-on experience will provide valuable insights into the practical implementation of ADRs.\n* **Mitigating the enthusiasm of first encounter:** Testing ADRs in a controlled setting will allow the team to assess their effectiveness objectively before committing to widespread adoption.\n* **Contextual relevance:** Copper Bend is a suitable environment for experimenting with ADRs due to its focus on learning and trying new techniques.\n* **Differentiation from Envisage products:** The unique nature of Copper Bend will help the team identify specific benefits and challenges of ADRs that may not be applicable to other Envisage products.\n\n**Additional Considerations:**\n\n* The team should document their experiences with ADRs, both positive and negative, to inform future decisions.\n* The experiment should be time-boxed to avoid prolonged investment in an ineffective practice.\n* If ADRs prove beneficial in Copper Bend, a gradual rollout to other Envisage products can be considered."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\n\n## Decision\n",As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\n,Implement a RESTful API in the simulator to communicate with the WebApp. The API will expose endpoints to control the simulator and to retrieve data from it.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nComponents might be more grokable if they were structured consistently. This ADR proposes conventions\n\n## Decision\n,"TL;DR:\n```\nprimer-react/\n├─ src/\n│  ├─ Breadcrumbs/\n│  │  ├─ index.ts                    // Just re-exporting?\n│  │  ├─ Breadcrumbs.tsx             // Primary component\n│  │  ├─ BreadcrumbsItem.tsx         // Subcomponent (include parent component name to increase findability in most IDEs)\n│  │  ├─ Breadcrumbs.mdx             // Documentation. Always .mdx, not .md\n│  │  ├─ Breadcrumbs.stories.tsx\n│  │  ├─ Breadcrumbs.test.tsx        // Unit tests\n│  │  ├─ Breadcrumbs.types.test.tsx  // Type tests\n│  │  ├─ Breadcrumbs.yml             // Component metadata (Possible future)\n│  │  └─ __snapshots__/\n┆  ┆\n```\n### Rules\n- Every component should have its own PascalCased directory directly under `src/`\n- Subcomponents should be properties of the exported parent component (e.g., `Breadcrumbs.Item`)\n- Replacements of existing components should use an incrementing number (e.g., `Breadcrumbs2` rather than `NewBreadcrumbs`)\n",Define component structure conventions for improved code readability.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nThroughout the last few years folks from the Design Infrastructure, Web Systems, and UI Platform teams have discussed the idea of using custom elements for behaviors in Primer React. The main goal of using custom elements in Primer React is to be able to author behaviors once and reuse them in any framework. Several experiments have been conducted which are listed above.\n### Assumptions\nDe-duplication is not our highest or only priority. Attempts at de-duplication must be weighed against changes to the maintainer, developer, and customer experience.\n### Findings\n#### Developer experience regressions\n- Custom elements rendering their own subtrees (ShadowDOM) requires polyfills for as-yet implemented specifications. This means Primer React will accumulate added complexity if we were to implement Custom Elements with ShadowDOM.\n- Implementing Custom Elements in Primer React will require a division of client side and server side code, as custom elements should only be executed in a browser environment. Currently Primer React is ""isomorphic"" - in that the code can be executed anywhere that React can be, which includes NodeJS server runtimes, as well as the client side. While not insurmountable this does mean Primer React will accumulate added complexity, which likely will be surfaced to the user.\n- While it's possible to add server side libraries to enable Custom Elements to be rendered on the Server, this adds more complexity and is antithetical to the usage patterns of custom elements.\n- As of this writing, you cannot style custom elements with styled-components[^1]. This means that if a component wants to use a custom element to get behaviors and you also want to style that component, you must use another wrapper div to apply styles. This is a bug in styled-components and should be fixed in the next release.\n#### Incompatibility with some React tools\nSome of our GitHub custom elements such as `details-dialog` and `details-menu` make assumptions about the DOM tree. For example, `details-dialog` expects a `details` element to wrap the custom element and uses this assumption[^2] to determine whether or not clicks are happening inside or outside of the dialog and closes the dialog if the click happened outside of the dialog. This makes sense in most cases and is a nice way of enforcing proper usage of the details element, but breaks down when used with [React Portals](https://reactjs.org/docs/portals.html) which are often used to ensure menus are displayed correctly in cases where a parent has an overflow: hidden applied to it, or incompatible z-index.\n#### Extensibility\nBuilding behaviors in React Hooks gives us the ability to provide things like state and state change hooks to the consumer of the component. This allows the user to build on additional behaviors to the component based on the state or other variables provided to the component consumer. Doing the same with custom elements would require listening to events on the document[^3] and reacting to them. This is certainly do-able, but goes against some of the foundational principles of React (reacting to changes in the DOM vs changes in React state).\n#### Organizational overhead\n- GitHub’s custom elements are all managed in different repos which introduces more maintenance overhead.\n- You'd need to npm link while developing if you want to test changes out with the presentational components themselves instead of making changes and seeing updates instantly. npm link usually doesn't work well with hot module reloading either.\n- You'd need to draft & publish releases to both libraries every time you want to update the behavior\n- If the behaviors are shared between github.com and Primer React, you'd need to do careful testing in both environments to make sure that changes don't create any regressions. That greatly widens the context that engineers need to keep in mind every time a change is made.\n- Reacting to changes will take a bit more time as we’ll need to orchestrate releases between custom elements and Primer React - as opposed to having behaviors already present in Primer React which can be versioned in lockstep.\n- Engineers who want to contribute to Primer React Components to build new components and behaviors would need to be familiar with both custom elements and React, two very different paradigms, and context switch between the two.\n#### Other\n- The custom element and web component API progress slower than React due to changes needing to go through the whatwg standards process.\n#### Risks of not switching to custom elements for behaviors\n- We spend extra time building behaviors in React that have already been built in our [custom elements library](https://github.github.io/web-systems-documentation/#custom-elements).\n- There are currently 19 behaviors/components listed on the custom elements documentation site. Several of these we have already implemented in React in either Primer React, Doctocat, or other React applications at GitHub which can be upstreamed (details-dialog, details-menu, clipboard-copy, text-expander, autocomplete, task-list via drag and drop hooks, tab-container, text-expander).\n- We decide not to invest further in React at GitHub and have wasted time we could have spent building more custom elements.\n- This seems unlikely as there seems to be clear consensus that we will continue to build more and more highly interactive products.\n- The React library is abandoned and becomes obsolete.\n- This is a risk with any technology that we may use, seems highly unlikely in the near term.\n- While also a possibility for custom elements, the track record demonstrates deprecations of Web APIs is extremely rare and has a long deprecation path.\n- Behaviors in github.com using custom elements and behaviors in Primer REact diverge, leading to an inconsistent experience.\n- This is probably the biggest risk we face, but moving to custom elements isn’t necessarily the only or best solution. We should explore other ways of detecting divergence such as integration tests.\n\n## Decision\n","### Custom elements\nDue to the challenges listed above and our priorities listed in the [Assumptions](#assumptions) section, we are not investing time in building out behaviors with custom elements in our Primer React library. Instead, we should spend time expanding coverage using React Hooks and focus on finding other approaches for making sure implementation of behaviors in our different stacks are consistent (such as integration tests).\n### Vanilla JavaScript behaviors\nSome behaviors can be implemented as vanilla JavaScript without introducing additional complexity to Primer React or its consumers. In cases where this is possible, behaviors will be implemented with no dependencies except the DOM and consumed within React Hooks to provide their functionality to Primer React.\nIn general, _portions of behaviors_ that affect or rely on **user interactions and events**, **shared state**, or **CSS styles** should be kept in React Hooks. Parts of the behavior that can be implemented in isolation of these concepts should be built with no dependency on React or other libraries.\n[^1]: https://codesandbox.io/s/demo-styling-custom-element-g973d?file=/src/index.tsx\n[^2]: https://github.com/github/details-dialog-element/blob/main/src/index.ts#L195\n[^3]: https://github.com/github/details-dialog-element#details-dialog-close\n",Do not use Custom Elements for behaviors in Primer React.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur component prop APIs have, at times been a bit of a mess. We've seen:\n- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)\n- Explicit plans to change some of those (e.g., the deprecation of Styled System props)\n- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)\nThis ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.\n\n## Decision\n","### 🟢 `sx`\nAll components that ultimately render to the DOM should accept an `sx` prop.\nThe `sx` prop (of type `SystemStyleObject`) should generally set styles for the root HTML element rendered by the component. An exception would be components like `<Dialog>`, whose outermost HTML element is a backdrop. In that case, it would be appropriate for `sx` styles to apply to child of the backdrop that is more likely to need styling overrides.\n### 🟢 `ref`\nAll components that ultimately render to the DOM should accept a `ref` prop. That `ref` prop should most often be passed to the root HTMLElement rendered by the component, although occasionally a different descendent node may make more sense.\nSee also: [Discussion on `ref` props (internal)](https://github.com/github/primer/discussions/131)\n### 🟡 `as`\nOnly components with a clear need for polymorphism should accept an `as` prop. Reasonable cases include:\n- Components that need functionality from the component passed to the `as` prop, like a `<Button>` that renders a React Router link.\n- Components whose accessibility are improved by using semantically appropriate HTML elements, like an ActionList\nWhen a Primer component user passes an `as` prop to a component, it should be done in a way that is consistent with the component’s intended use. In some situations we can enforce that with a narrowed type for our `as` prop.\nSee also: [Discussion on `as` props (internal)](https://github.com/github/primer/discussions/130)\n### 🟡 DOM props: Limited\nAll components that accept an `as` prop should accept props en masse for the element specified by the `as` prop (excluding props of the same name already used by the component). _Additionally_, some other elements that do _not_ accept an `as` prop should accept the props for their root HTML element when those props are fundamental to the component’s function (e.g., `<TextInput>` should accept DOM props for its underlying `<input>`).\n### 🔴 Styled System props\nComponents should not accept Styled System props (except our utility components: `Box` and `Text`)\n_Reasoning:_ Utility components are meant to provide a convenient API for writing styles (including styles that reference theme and other context managed within Primer). Non-utility components implement specific design patterns where additional styling is available for exceptional cases.\nSee also: [Discussion on the deprecation of styled-system props (internal)](https://github.com/github/primer/discussions/132)\n### 🔴 `theme`\nComponents should not accept a `theme` prop (with the exception of `ThemeProvider`).\n_Reasoning:_ The `theme` prop doesn't enable anything that can't be done with `<ThemeProvider>`, and promotes the anti-pattern of per-component theme overrides.\n### `children`\nI'm intentionally withholding advocacy about `children` prop types because I expect that topic will be covered by a future ADR.\n### Sequencing\n1. Deprecate remaining unwanted Styled System props (should be done? Let's verify.)\n1. Release an eslint rule to disallow Styled System props\n1. Release an eslint rule to disallow `theme`\n1. Migrate all usage within PRC\n1. Assist GitHub projects with migration\n1. Remove support for unwanted props\n1. Update docs to reflect the standards in this ADR\n",**Codify prop API conventions and move towards a consistent API across components**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPrimer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.\n`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.\n\n## Decision\n",We will rewrite Primer React components in TypeScript.\n,"**Migrate the Primer React components library to TypeScript**. This will enable us to:\n\n* **Maintain type definitions more easily and accurately**. TypeScript will automatically generate and maintain type definitions based on the implementation code, eliminating the need for manual maintenance and reducing the likelihood of errors.\n* **Catch type errors at compile time**. TypeScript will enforce type safety, which will help us catch type errors at compile time, improving the quality and reliability of our code.\n* **Improve the developer experience**. TypeScript will provide autocompletion, type checking, and other features that will make it easier for developers to use the Primer React components library and build applications with it."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKitsune supports many locales,\nand has content which we want to be searchable in those locales.\nElasticsearch has support for many language-specific analyzers:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.9/analysis-lang-analyzer.html\nSearch v1 used per-document analyzers,\nthat is to say, within the same index:\n```\ndoc_1: { ""content"": ""Hello world"" }\ndoc_2: { ""content"": ""Hallo Welt"" }\n```\n`doc_1.content` could be analyzed using an english analyzer,\nand `doc_2.content` could be analyzed using a german analyzer.\nWell before version 7 ES removed this feature,\nand now all fields of the same name across an index must be analyzed the same,\nso we must take a different approach with the current Search implementation.\nWe can either place separate locales in their own index,\nand set up locale-specific analyzers for the same field name across indices.\nOr we can keep separate locales within the same index,\nand define unique field names for each field which needs to be analyzed under a specific locale.\n\n## Decision\n","Heavily influenced by: https://www.elastic.co/blog/multilingual-search-using-language-identification-in-elasticsearch\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\nwith the appropriate analyzer defined on that key,\nsuch that:\n```\ndoc_1: { ""content"": { ""en-US"": ""Hello world"" }}\ndoc_2: { ""content"": { ""de"": ""Hallo Welt"" }}\n```\n`doc_1.content.en-US` is analyzed using an english analyzer,\nand `doc_2.content.de` is analyzed using a german analyzer.\n",We should place separate locales in their own index. This will allow us to set up locale-specific analyzers for the same field name across indices. This is a more flexible and scalable approach than keeping separate locales within the same index and defining unique field names for each field that needs to be analyzed under a specific locale.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we are re-implementing our search in ElasticSearch v7,\nwe must re-implement Ask a Question (AAQ) search.\nThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\nwhich we must continue to be able to do in the redesigned Search:\nsearching for an AAQ thread as a unit.\nThere are other secondary use-cases which we may want to support when storing AAQ documents in ES.\nA non-exhaustive list of these are:\n-   Searching within AAQ threads\n-   Searching within questions and their solutions\n-   Aggregating answers to create contribution data\nWe also want search to be _fast_,\nso should model our data to avoid nested fields and parent-child relationships,\nand use de-normalization wherever possible:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.9/tune-for-search-speed.html#_document_modeling\n\n## Decision\n","We will model our data in ES based on what makes most sense for our expected use-cases,\nand what will make those fast and efficient,\nrather than feeling like we must have a 1:1 copy of our data structure in our database.\nIn this vein, we will use a structure of two document ""types"" within one index,\n`QuestionDocument` and `AnswerDocument`,\nwhere a `QuestionDocument` will exist for each `Question` which exists in the database,\nand an `AnswerDocument` will exist for each `Answer` which exists in the database.\n`AnswerDocument` will be a subclass of `QuestionDocument` so will inherit all of its fields,\nand we will set the value of those fields to the value of the `Question` associated with its `Answer`.\nFor instance, if in database:\n```\nanswer.created => 2020-10-27\nanswer.question.created => 2020-11-01\n```\nin elastic:\n```\nanswer_document.created => 2020-10-27\nanswer_document.question_created => 2020-11-01\n```\n`QuestionDocument` will also have an `answer_content` field,\nwhich contains the content of all a Question's Answers.\nWe will set this to null in the `AnswerDocument`.\n",We will model the AAQ data as follows:\n-   Store all AAQ threads in a single ES document\n-   Store each question and solution in a subdocument within the thread document\n-   Store each answer in a subdocument within the question document
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** To implement an Architectural Decision Record (ADR) process to document and track architectural decisions made on the project.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to record and communicate architectural decisions.\n* They help ensure that decisions are well-informed, transparent, and traceable.\n* They facilitate collaboration and understanding among team members and stakeholders.\n* They serve as a valuable reference for future maintenance and evolution of the system.\n\n**Implementation:**\n\n* Define a standard ADR template with sections for decision context, problem, solution, alternatives considered, consequences, trade-offs, and decision.\n* Establish a central repository (e.g., a wiki or document management system) to store ADRs.\n* Assign responsibility for creating and maintaining ADRs.\n* Implement a review and approval process for ADRs.\n* Regularly review and update ADRs as the project evolves."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDirect form the [jdbc documentation](http://clojure-doc.org/articles/ecosystem/java_jdbc/home.html)\nAnother common need with SQL is for database migration libraries. Some of the more popular options are:\n* Drift\n* Migratus\n* Ragtime\n\n## Decision\n,"After a quick read of the documentation, and see that all solutions are similar, I select Migratus, by the comodity of have a lein plugin\n",Use Drift for database migrations.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTe console appication it is the more siple app that come to my mind.\n\n## Decision\n,Creates a console app and send the data via parameters\n,Use a console application to create a simple command-line interface (CLI) for the application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn clojure a normal work flow is use the repl. The problem is that when you reload the appliction the states die.\n[mount](https://github.com/tolitius/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.\nDepending on how application state is managed during development, the above three superpowers can either stay, go somewhat, or go completely.\n\n## Decision\n","Use mount libray and clojure tools space.\nThe decision of mount over component is made afer review bouth solutions. My feeling is\n* Mount is more clojure dialect oriented\n* Mount use namespace and component\nRecords, this made that the compliler control the dependencies\n* Mount is [less contagious](https://engineering.riotgames.com/news/taxonomy-tech-debt)\n",Use [mount](https://github.com/tolitius/mount) to preserve state during application reloads.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\n\n## Decision\n","We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\n","**Decision:** Invest in team training and cloud infrastructure expertise to build the required high-scalability and resilience.\n\n**Justification:**\n\n* The team needs to acquire in-depth knowledge of Clojure and cloud computing to effectively design and implement a scalable application.\n* Cloud infrastructure expertise is crucial for understanding and leveraging cloud services for autoscaling, resilience, and geographic distribution across multiple cloud providers.\n* By investing in training and expertise, the team can develop the necessary skills to build and maintain a high-performance application that meets the scalability and availability requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the final stack canditate for persistence is [cockroachdb](https://www.cockroachlabs.com/), but the windows version\n\n## Decision\n","We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\n",CockroachDB is not a suitable option for the stack due to its lack of support for Windows.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs developer, I have a good understanding of clujure language, but not the ecosystem. How persist in db, create api's, secure them, etc.\n\n## Decision\n","This will be a pet project, the goal is learning clojure ecosystem, not the aplication itself.\n","**Decision:** Adopt a framework for Clojure development, such as Luminus or Compojure, to provide a structured and well-supported approach to common architectural tasks.\n\n**Rationale:**\n\n* Frameworks provide pre-defined components and best practices for building applications, reducing the need for developers to implement complex features from scratch.\n* They offer standardized interfaces for common tasks, reducing duplication of effort and allowing for easier integration with other tools.\n* Frameworks often include support for databases, API routing, authentication and security, streamlining the development process.\n* By adopting a well-established framework, developers can leverage the experience and knowledge of the community, resulting in more robust and maintainable applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need working with command line arguments.\n\n## Decision\n,Use of [tools.cli](https://github.com/clojure/tools.cli)\n,Use the `argparse` package.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are testing how deploy in diferente databases per envirtoment, and don't find the way of do it using the pluggin\n\n## Decision\n",Develop the migration part in the code\n,"**Decision**: Use a database migration tool to manage database changes and deployments across different environments.\n\n**Rationale**: A database migration tool provides a structured and automated way to handle database schema changes and data migrations, ensuring consistency and integrity across different environments. It allows for the safe and controlled deployment of database changes, reducing the risk of data loss or corruption. Additionally, migration tools can help track and manage the history of database changes, making it easier to revert or roll back changes if necessary."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Establish an Architectural Decision Record (ADR) process to document and track architectural decisions.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture and communicate architectural decisions.\n* They help ensure that decisions are made in a thoughtful and transparent manner.\n* ADRs serve as a valuable reference for future development and maintenance efforts.\n\n**Implications:**\n\n* A template and process for creating and reviewing ADRs will be developed.\n* Developers will be responsible for creating and maintaining ADRs related to their work.\n* The ADRs will be reviewed by the architectural review board or designated decision-making body.\n* The ADRs will be stored in a central repository and made available to the project team.\n\n**Consequences:**\n\n* Improved clarity and consistency in architectural decision-making.\n* Reduced risk of architectural drift and unintended consequences.\n* Enhanced collaboration and communication among stakeholders.\n* Increased transparency and accountability for architectural decisions.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\nread a bit about how vim-plug was pretty good. And it felt like it was time for\na change.\n\n## Decision\n,[vim-plug](https://github.com/junegunn/vim-plug) will be used to manage Vim\nplugins in favour of [Vundle](https://github.com/VundleVim/Vundle.vim).\n,Switch to using vim-plug as the plugin manager for Vim.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Starting with macOS Catalina, zsh will be the default shell](https://support.apple.com/en-us/HT208050).\n\n## Decision\n",zsh should be used in place of bash.\n,Use zsh as the default shell for macOS Catalina and later.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nYCM has been a mostly positive experience over the years of using it, however,\nthere have been many occasions where time has been spent fixing issues. The\ntime has come to look at using an alternative.\n\n## Decision\n",The decision is to use [CoC](https://github.com/neoclide/coc.nvim).\n,Migrate from YCM to an alternative solution that meets YCM's functionality and provides additional features.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n","To record the architectural decisions made on this project, we will use an Architectural Decision Record (ADR). ADRs will be created for each significant architectural decision that is made, and will include the following information:\n\n* The decision that was made\n* The rationale for the decision\n* The alternatives that were considered\n* The consequences of the decision\n* The date the decision was made\n* The person(s) who made the decision\n\nADRs will be stored in a central repository, and will be made available to the project team. This will help to ensure that all stakeholders are aware of the architectural decisions that have been made, and will help to prevent the same decisions from being made multiple times."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/).\nThey are a thing that will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repository which has a very basic use case will\nprovide some exposure to the service.\n\n## Decision\n,The decision is to replace Travis CI with GitHub Actions.\n,Use GitHub Actions for CI/CD pipelines.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want Pomegranate to be a separate application from Figgy, but need some way\nfor Figgy to tell Pomegranate about new resources so that when something is\nmarked Complete in Figgy or taken down that it's reflected in Pomegranate.\n\n## Decision\n",Figgy will send create/update/delete messages to a fanout RabbitMQ Exchange.\nPomegranate will register a durable queue which listens to that exchange and\nprocess messages using [Sneakers](https://github.com/jondot/sneakers).\nThe message will contain the following information:\n* Collection slugs the object is a member of\n* Manifest URL of the object\n* change event (create / update / delete)\n,Implement a webhook endpoint that Figgy can POST to when a resource is completed or taken down.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described at https://adr.github.io/\n",Establish an Architectural Decision Record (ADR) template and process to document all architectural decisions made on the project. This template should include the following information:\n\n* Decision ID\n* Decision Title\n* Decision Statement\n* Decision Rationale\n* Decision Alternatives Considered\n* Decision Consequences\n* Decision Owners\n* Decision Date
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPUDL has [collection landing\npages](http://pudl.princeton.edu/collections/pudl0058) because it's a digital collections site which\nstaff pushed content into. PUDL required all items be in a single collection,\nand couldn't provide good search-across. Figgy is a staff back-end, so we need some sort of\napplication to provide that functionality.\nCurators also had a history of either requesting or creating ad-hoc websites to showcase\ntheir material or accompany on-site exhibits. We wanted instead to provide a CMS for\nthem to create those experiences based on material they curate which wouldn't\ncreate metadata silos and increased maintenance.\nFurther, curators often had different use cases about how metadata should\ndisplay in different contexts. For example, items with the same title in the catalog may need to be differentiated in an exhibit. It was important that certain fields display a certain\nway, but be cataloged according to best practices.\nSpotlight is an exhibit building platform that provides controlled CMS\nfunctionality and is built on Blacklight similar to our new catalog. It allows\nfor local overrides of fields.\n\n## Decision\n",We will use Spotlight to fulfill both the requirements of a Collection Landing\nPage as well as exhibits. Curators of collections will generate the collection\npages. Staff who wish to generate exhibits will be able to manage membership in\nFiggy but have the tools to create those exhibits in Pomegranate.\n,Use Spotlight as the platform for creating exhibit-specific curatorial experiences.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFiggy resources may have any of the following visibilities:\n- Open (public)\n- Princeton (netid)\n- On Campus (ip)\n- Reading Room\n- Private\nFor each of these we need a policy regarding whether it will be indexed in DPUL.\nWe used to index only public / complete items. But to support the music reserves\ncollection we need pages that would have a viewer for logged-in institutional\nusers only.\n\n## Decision\n,Resources with the following visibilities should index into DPUL:\n- Open (public)\n- Princeton (netid)\n- On Campus (ip)\nThis is implemented with a token authentication mechanism in `iiif_resource#def\nurl`\n,"**Decision**: DPUL will index **all** figgy resources, regardless of visibility, except for those that have a specific ""noindex"" metadata tag."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSpotlight uses the term ""Exhibits"" because its primary use case is generating\nmultiple exhibit sites. However, as per\n[ADR #2](./0002-build-on-spotlight.md) we want to use it as landing pages for\ncollections.\nThe term ""Collection"" was used everywhere else, including PUDL, so we decided to\nuse that instead. Originally reported in\n[#89](https://github.com/pulibrary/pomegranate/issues/89).\n\n## Decision\n","* Use ""Collection"" in the UI everywhere ""Exhibit"" is mentioned in Spotlight.\n","Resolve any instances of ""Exhibits"" in the codebase with ""Collections"" and update terminology for landing pages accordingly."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to be able to display a resource in more than one collection, because\nin Figgy a resource can be a member of multiple collections.\nAt the time of this decision, one IIIFResource could only be a member of one Exhibit.\n\n## Decision\n",We create one IIIFResource per Collection of which it is a member.\nWe map each IIIFResource to one SolrDocument.\n,"To allow a resource to appear in multiple exhibits, a resource can be instantiated for each exhibit in which it appears."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nObjects in Pomegranate need to get their metadata from Figgy, where they are\nadministered. Spotlight ships with a IIIF-based indexer. Figgy already produces\nIIIF manifests to support viewing the objects. However the metadata bucket\ndoesn't contain rich enough metadata for pomegranate use cases.\nFiggy (plum, at the time) didn't have an API at the time this decision was made. Manifests were the\nonly way to get data out. Today Figgy has a graphql API.\n\n## Decision\n","We will use the IIIF Manifests to pull data from Figgy into Pomegranate. The\nManifest gives us the manifest url (used for presenting a viewer), the thumbnail\niiif image url, and the jsonld metadata location (via seeAlso).\n",Pomegranate should use IIIF manifests (from Figgy) as the source of object metadata.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nI'm currently working on migrating from redux store to re-frame. It's a good point to define data\nstructure and API to access it.\n# Decision\nApplication DB be a hashmap with the following keys:\n- `nodes` to store hashmap where the key is the node's id and value is a node.\n- `root` to store a root node id\n- `loc` to store focused node id\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\nApplication should define the following event handlers to modify nodes:\n- `[:nodes/make node]` create a node at given loc.\n- `[:nodes/append-child child]` adds a child as a bottom child.\n- `[:nodes/insert-child child]` adds a child as a top child.\n- `[:nodes/insert-up child]` adds a sibling to up of loc.\n- `[:nodes/insert-down child]` adds a sibling to down of loc.\n- `[:nodes/remove]` removes node at loc; lot will be moved to the preceding node.\n- `[:nodes/replace node]` replaces node at loc.\nFollowing event handler to modify loc:\n- `[:loc/root]` moves loc to root.\n- `[:loc/down]` moves loc down.\n- `[:loc/up]` moves loc up.\n- `[:loc/bottom]` moves loc to bottom.\n- `[:loc/top]` moves loc to top.\n- `[:loc/right]` moves loc right.\n- `[:loc/left]` moves loc left.\nFollowing event handler to modify root:\n- `[:root/change root]` change root to a new value.\n\n## Decision\n","Application DB be a hashmap with the following keys:\n- `nodes` to store hashmap where the key is the node's id and value is a node.\n- `root` to store a root node id\n- `loc` to store focused node id\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\nApplication should define the following event handlers to modify nodes:\n- `[:nodes/make node]` create a node at given loc.\n- `[:nodes/append-child child]` adds a child as a bottom child.\n- `[:nodes/insert-child child]` adds a child as a top child.\n- `[:nodes/insert-up child]` adds a sibling to up of loc.\n- `[:nodes/insert-down child]` adds a sibling to down of loc.\n- `[:nodes/remove]` removes node at loc; lot will be moved to the preceding node.\n- `[:nodes/replace node]` replaces node at loc.\nFollowing event handler to modify loc:\n- `[:loc/root]` moves loc to root.\n- `[:loc/down]` moves loc down.\n- `[:loc/up]` moves loc up.\n- `[:loc/bottom]` moves loc to bottom.\n- `[:loc/top]` moves loc to top.\n- `[:loc/right]` moves loc right.\n- `[:loc/left]` moves loc left.\nFollowing event handler to modify root:\n- `[:root/change root]` change root to a new value.\n","The application DB should be a hashmap with the following keys:\n- `nodes` to store hashmap where the key is the node's id and value is a node.\n- `root` to store a root node id\n- `loc` to store focused node id\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\nApplication should define the following event handlers to modify nodes:\n- `[:nodes/make node]` create a node at given loc.\n- `[:nodes/append-child child]` adds a child as a bottom child.\n- `[:nodes/insert-child child]` adds a child as a top child.\n- `[:nodes/insert-up child]` adds a sibling to up of loc.\n- `[:nodes/insert-down child]` adds a sibling to down of loc.\n- `[:nodes/remove]` removes node at loc; lot will be moved to the preceding node.\n- `[:nodes/replace node]` replaces node at loc.\nFollowing event handler to modify loc:\n- `[:loc/root]` moves loc to root.\n- `[:loc/down]` moves loc down.\n- `[:loc/up]` moves loc up.\n- `[:loc/bottom]` moves loc to bottom.\n- `[:loc/top]` moves loc to top.\n- `[:loc/right]` moves loc right.\n- `[:loc/left]` moves loc left.\nFollowing event handler to modify root:\n- `[:root/change root]` change root to a new value."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI want to be able to mix and match different implementations\nof the ddd sample application, without needing to pull in\na large collection of unnecessary dependencies.\nI want all of the code to be together in one place; which\nis to say, I want to treat the entire project as a mono-repo.\nI can't be bothered to maven install/maven deploy each\nlittle piece to propagate the necessary changes between\nisolated libraries.\n\n## Decision\n",Use a maven reactor project to track the dependencies between\ndifferent libraries within the project\n,Create a monorepo with multiple modules.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI can't be bothered to be installing the citerus library into my\nrepositories all the time.\nI want to keep my copy of that closely synchronized to the common\nproject, so that any progress that is made can be incorporated\ninto my work.\nI want a convenient way to introduce fixes in the existing implementation\nas I discover the need.\nI want to keep the touch on the existing code _light_; I expect\nto be making some fairly drastic changes in approach, and I don't\nwant to be debating my choices with the maintainers.\nI'm not currently comfortable working with maven projects where\nthe elements are not rooted under a common location.\n\n## Decision\n","Add the upstream as a git submodule, so that it's clear precisely\nwhich version of the upstream is in play at any given time.\nChanges that are of general interest can be in that space.\nMost radical changes (for a _gentle_ definition of radical) should\nbe maintained elsewhere.\n",Use `gitsubmodule` to maintain a copy of the library inside your project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To use an architectural decisions record to document and track the architectural decisions made on this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently have a ""public sector domains"" table (along with an ""organisations"" table containing the owners of the domains).\nHowever, many organisations have websites that are not on their own domain but in a subfolder of another domain. Moreover, many _services_ reside in subfolders or other logical locations.\nEach of these needs to be treated as a separate ""website"" in the context of accessibility monitoring.\nThus, we either change the ""public sector domains"" table to include all websites, not just unique domains, OR we create a new table specifically for public sector websites and initially copy over those domains that we can confirm are definitely websites.\nThe domains table is useful in and of itself as a repository of registered domains. We should keep and maintain this as-is.\n\n## Decision\n","We will create and populate a new table in the PubSecDomains schema that contains:\n* url\n* name of the service\n* the site's title from its HTML <head><title> element, where given\n* the site's description from its <head><meta name=""description""> element, where given\n* last_updated timestamp\n* the website's sector (foreign key to sectors table)\n* many-to-many join to the existing Organisations table (one site can, surprisingly, come under the auspices of more than one organisation; obviously one organisation can have multiple websites for their various services)\n",Create a new table for public sector websites and initially copy over those domains that we can confirm are definitely websites.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** Implement an Architectural Decision Record (ADR) process.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to document and communicate architectural decisions.\n* They allow stakeholders to understand the rationale behind decisions and their potential implications.\n* ADRs help to avoid inconsistencies and duplication of effort by centralizing decision-making.\n\n**Implementation:**\n\n* Establish a template for ADRs that includes sections for decision statement, context, rationale, and consequences.\n* Designate a team or individual responsible for reviewing and approving ADRs.\n* Implement a process for creating, reviewing, and storing ADRs.\n* Regularly review and update ADRs as the project evolves.\n\n**Expected Benefits:**\n\n* Improved architectural decision-making and communication.\n* Reduced inconsistencies and duplication of effort.\n* Enhanced stakeholder understanding and buy-in.\n* Facilitated decision traceability and revision history."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe choice of tools that will be built and used by the accessibility monitoring team are influenced by:\n* Suitability to the task\n* Cost and ongoing-cost (open-source preference)\n* The [GDS Way](https://gds-way.cloudapps.digital/).\nAs defined in ADR002, we need to meet a number of requirements:\n1. Maintaining an organised list of public sector websites\n1. Picking websites to test from that list\n1. Triaging each site\n1. Prioritising sites for testing\n1. Tracking the progress of testing on each website\n1. Creating a report for the completed test\n1. Sending the report to the site's owner\n1. Managing and recording interaction with the site's owner\n\n## Decision\n","We will...\n### Use Zendesk\nRationale:\n* GDS have a license for Zendesk.\n* It has an extensive, well-documented API.\n* There is a lot of experience in GDS of general usage, and a fair amount in using the API.\n* We have a sandbox Zendesk environment.\nThis will be the driver of testing work. Tickets will be created in Zendesk (manually or automatically) representing websites to test.\nThey be prioritised in Zendesk and then be assigned to / picked up by Accessibility Officers.\nZendesk will also handle communication and follow-up with the site owner.\nThis satisfies items 5, 7 and 8 above.\n### Use Postgres\n* A relational database is best suited to the requirements for both a public sector domains database and the testing records\n* It is open-source\n* It is well supported and documented\n* It is available as a ""plug-and-play"" service on [GOV.UK PaaS](https://www.cloud.service.gov.uk/) (see below)\nThis satisfies item 1 and facilitates items 2, 5 and 6.\n### Use GOV.UK Platform-as-a-Service\n* Meets GDS' [cloud-first policy](https://www.gov.uk/guidance/government-cloud-first-policy)\n* VERY well supported with an extremely well-experienced and skilled team within GDS.\n* Supports all of the chosen technologies\n### Use Deque Axe\n[Deque Axe](https://github.com/dequelabs/axe-core) is a 3rd-party, open-source tool that tests the web page that you give it against a set of predefined rules.\n* It is open-source\n* It can be run from the command-line and so can probably be automated.\n* It produces results in a machine-readable format (JSON)\nThis satisfies item 3.\n### Use the GOV.UK Design System for the front-end\nThe [GOV.UK Design System](https://design-system.service.gov.uk/) is a set of components, styles and patterns created by the GOV.UK team after extensive user-research.\nNot only does this serve as a ready-made template engine, but we know that it scores extremely highly on accessibility.\n### Programming languages\n* Use node.js for Axe-integration as it is well suited to asynchronous http calls.\n* Use node.js (and nunjucks)for front-end code as that's what GOV.UK Design System is written in.\n* Use Python 3 for Zendesk integration as there is a well-maintained open-source Python library.\n### (Under review) Use PowerMapper SortSite\n[SortSite](https://www.powermapper.com/products/sortsite/) by PowerMapper is a 3rd-party commercial product that is widely used in the testing of websites, and has a thorough accessibility checking feature. It also crawls a website and catalogues its pages, a function that Axe does not perform and would be useful to us.\nHowever, it is not open-source and only runs in a Windows or MacOS environment, so would not be easy to integrate into an automated process.\nThere is a web service whereby a domain is submitted to it and it returns an HTML report, but this is not a machine-friendly format and would require screen-scraping and parsing - quite a major task, and one that's prone to breaking if/when SortSite changes its output format.\nThe site-mapping functionality of SortSite could be achieved with an existing open-source python or node library.\n","**Decision:**\n\nThe team will use the following tools for accessibility monitoring:\n\n* **WebAIM WAVE and aXe** for website testing\n* **Google Sheets** for maintaining an organized list of public sector websites, triaging sites, prioritizing sites for testing, tracking the progress of testing, and managing interactions with website owners\n* **Google Forms** for creating and sending reports to website owners\n* **A dedicated email address** for communication with website owners\n\n**Rationale:**\n\n* **Suitability to the task:** WAVE and aXe are industry-standard tools for website accessibility testing. Google Sheets and Forms provide the necessary functionality for managing and tracking the testing process.\n* **Cost and ongoing-cost (open-source preference):** WAVE and aXe are open-source tools, making them free to use. Google Sheets and Forms are part of the G Suite productivity suite, which is available to government organizations at a reduced cost.\n* **The [GDS Way](https://gds-way.cloudapps.digital/):** The tools chosen align with the GDS Way principles of being user-centered, agile, and collaborative. Google Sheets and Forms are widely used within government, facilitating collaboration and knowledge sharing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe accessibility monitoring team have to test ""a diverse, representative and geographically balanced distribution"" of public sector websites.\nIn order that no public sector website is excluded from potential scrutiny, it follows that the team will need to have a full list of websites that are in the public sector, together with the type of service (e.g. education, health, central/local government etc) and, where applicable, geographical location.\nThere are two approaches to creating such a list:\n1. **Domain-led:** Find every domain that has been registered to an organisation that is deemed to be ""public sector"" and determine the organisation that owns it\n2. **Organisation-led:** Find every organisation that is deemed to be ""public sector"" and every service that each such organisation runs online, and find the corresponding website.\n### 1. Domain-led strategy\nThe limitation of (1) is that (with certain exceptions, e.g. .ac.uk and .gov.uk) there is no regulation over what top-level domain should be used by the many flavours of public sector organisations. They could be anywhere.\nAt the time of writing (July 2020) there are approximately 1.7 billion websites in the world, hosted on 409 million domain names. The websites we're looking for could be hosted on \*.uk domains or generic .com/.net/.org (or even .me, .info, .tv or any potentially any other ""generic TLD"").\nOf the 149m country-specific domains registered, 7% (~10.5M domains) are \*.uk, so we can at least exclude the other 93% (139m) non-UK country-specific domains (that is, we are very unlikely to find a UK public sector website hosted on a .tk or .ru domain, for instance\*).\nThat leaves a mere 270m domains that Public Sector websites could potentially be hosted on. Scouring those domains for websites that fall into the definition of ""uk public sector"" would be a lengthy and resource-intensive task.\nNevertheless, a list of the domains that we can be sure are public sector, and that we can retrieve data about, would give the accessibility monitoring team a good range to select from.\nThe domains that fall into this category are:\n* gov.uk\n* nhs.uk\n* nhs.net\n* ac.uk\n* sch.uk\n* police.uk\n* parliament.uk\n* mod.uk\n* gov.wales\n* llyw.cymru\n* gov.scot\n### 2. Organisation-led strategy\nIt should be feasible to compile a list of organisation _types_ that are in the public sector (e.g. ""schools"", ""central government"", ""local authorities"", Universities"", ""NHS"").\nWe can then move onto making lists of the actual _organisations_ in each of those categories by referring to the bodies that regulate them.\nA lot of this information is online in one form or other, but certainly not all of it.\nSome initial work has been done by the accessibility monitoring team already, with the result being a ""list of lists"" that is currently in a Google Sheets spreadsheet. Each of these lists - in various formats - would need to be somehow imported into the database. Where a website for the organisation in the list is specified, this can be added to the domains list.\n\n## Decision\n","Our intention is to use both strategies.\n* We will create a **database of public sector domains**.\n* We will populate it with lists of domains and organisations from official sources, together with, as far as possible, contact details and other useful data such as page ranking, http(s) status.\n* We will also use the data gleaned from domain registers etc to seed a list of **public sector organisations**.\n* We will compile a list of public sector categories which will form a list-of-lists; these lists will then be used to further populate the database of organisations.\n* Organisations will be categorised by location (including ""national"") and by sector (probably using [the definitions of ""organisations"" categorised for the Local Governments Association by ESD](https://standards.esd.org.uk/?uri=list%2ForganisationTypes))\n* Wherever possible, links will be created between the domains and organisations. At some point in the future, organisations that don't have associated domains will need to have their associated websites found and entered into the database so that automated testing can be carried out.\n","The organization-led strategy is the better approach for creating a list of public sector websites. This strategy is more feasible because it involves compiling a list of organization types that are in the public sector and then making lists of the actual organizations in each category by referring to the bodies that regulate them. This information is more likely to be available online and in a more consistent format than the domain-led strategy, which would involve scouring a large number of domains for websites that fall into the definition of ""uk public sector."""
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe accessibility monitoring team require a means of:\n* Picking websites to test (from a random list weighted by a variety of criteria, from complaints passed to us, from previously-tested sites (to test changes) and sites specifically chosen by guiding bodies)\n* Collecting and maintaining a list of public sector websites from which to pick\n* Triaging each site (check it is live, check for accessibility statement(s), measure its size in pages etc)\n* Prioritising sites for testing\n* Tracking the progress of testing on each website\n* Creating a report for the completed test\n* Sending the report to the site's owner\n* Managing and recording interaction with the site's owner\nWhilst there are a number of accessibility testing solutions available, they are all geared towards testing of one's own site, and require (or at least, assume) access to the source code and deployment system. Testing of multiple sites by a 3rd party from outside the sites' domains is not a common requirement for vendors!\nSimilarly, whilst the reports generated by some of the testing tools are very good, they are not up to GDS standards for content and, ironically, accessibility. Furthermore, GDS have no control over the content of the generated reports.\nWe therefore need a combination of database administration, semi-automatic accessibility testing and a Customer Relationship / Support Ticket Management system, all able to move data between them in a usable fashion.\n\n## Decision\n","We will use 3rd-party tools where they provide part of the functionality that is required and can be made to communicate easily with each other.\nWe will write whatever code is necessary to form the ""glue"" that binds them together and to automate whatever can be (and is worth) automating.\n","**Develop a custom application that integrates the following features:**\n\n**Database:**\n\n* Manage and maintain a list of public sector websites\n* Store accessibility testing data and progress\n\n**Semi-Automatic Accessibility Testing:**\n\n* Randomly select websites from the list based on weighted criteria\n* Check websites for accessibility statement(s)\n* Measure website size in pages\n\n**Customer Relationship Management (CRM) System:**\n\n* Track interactions with site owners\n* Send accessibility testing reports\n* Prioritize sites for testing\n\n**Integration:**\n\n* Allow data to flow seamlessly between the database, testing system, and CRM\n* Provide a user-friendly interface for managing website selection, testing, and reporting\n\n**Additional Features:**\n\n* Reports that meet GDS content and accessibility standards\n* Ability to track changes to websites over time\n* Automated reminder system for site owners to address accessibility issues"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrent version of clang-format is aging an becoming unavailable.\n\n## Decision\n,We will move to clang-format 5 and not the bleeding edge version. Visual Studio 2017 will ship with clang-format 5 so it makes sense to standardize\non this version across the project.\n,Migrate to the latest version of clang-format to ensure availability and security updates
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVisual Studio 2017 has been released for roughly a year and includes improvements to C++ standards conformance. It is C++14\nfeature complete and has many C++17 features.\n\n## Decision\n,As part of the maintenance cycle after release 3.13 we will move the Windows compilers forward to\nVisual Studio 2017 in a step towards requiring C++14 as a minimum standard.\n,Upgrade to Visual Studio 2017 to utilize the improved C++ standards conformance and features.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRed Hat 7's default compiler is gcc 4.8. This is not C++14 compliant and prevents access to more modern C++ features.\n\n## Decision\n,As part of the maintenance cycle after release 3.13 we will move Red Hat 7 compilers over to use\ngcc 7 as part of the [devtoolset-7](https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/) tools provided by Red Hat.\n,"**Decision:** Migrate the default compiler to GCC 11 or a newer version that supports C++14.\n\n**Rationale:**\n\n* GCC 11 or newer versions provide full support for C++14 and offer numerous performance and security enhancements over legacy compilers like GCC 4.8.\n* Upgrading the compiler will unlock access to modern C++ features, improving the productivity and efficiency of development efforts.\n* Migrating to a newer compiler will align with industry best practices and enable the efficient adoption of emerging C++ standards.\n\n**Consequences:**\n\n* **Positive:**\n    * Access to modern C++ features and improved code quality\n    * Enhanced performance and security\n    * Reduced development time and improved code maintainability\n* **Negative:**\n    * Potential temporary disruption during the migration process\n    * May require code modifications to support the new compiler\n* **Mitigations:**\n    * Plan the migration carefully and conduct thorough testing to minimize disruptions\n    * Provide support and documentation to assist developers in adapting to the new compiler"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the future architectural decisions made.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Maintain an Architectural Decision Record (ADR)**\n\nEstablish a central repository to document all architectural decisions made during the development process. This record will include:\n\n* The decision itself\n* The context and rationale behind the decision\n* The alternatives considered and why they were rejected\n* The implications and consequences of the decision\n* The impact on other architectural components\n* The date and author of the decision\n* Any relevant supporting documentation
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a ""Hello World"" state with little to no modifications.\n# Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\n# Consequences\nThe lifcorp-init will be able to block users from skipping required parameters.\n# Status\nAccepted\n\n## Decision\n","Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\n# Consequences\nThe lifcorp-init will be able to block users from skipping required parameters.\n# Status\nAccepted\n","Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To use an Architectural Decision Record (ADR) format to document architectural decisions made on this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhat colors shall be used for buttons, statistics and inactive elements considering usability. This concerns first and\nforemost the tool applications.\n\n## Decision\n","Buttons or any usable elements should use the colors #ff9900, #9999ff, #cc6666 and #cc6699 as background and black text.\nDisplayed text should preferably be colored #ff9900 on black background. Other text colors may be used after consideration.\nGraphs, diagrams and other status and statistical displays should use the colors #ffcc99, #9999cc, #9999ff and for warning purposes #ff9966, #ff9900 and #cc6666.\nDesign elements should use the colors #cc99cc, #9999cc and #ff9966. These colors are less ""strong"" and don't take as much attention.\n[1]: http://mrbsdomain.com/gallery/var/albums/repository/lcars_colors.gif\n","**Decision:**\n\nFor buttons, **primary actions** should use the **primary color**, while **secondary actions** should use a **secondary color**.\n\nFor statistics, **positive or significant values** should use a **green color**, while **negative or insignificant values** should use a **red color**.\n\nFor inactive elements, a **grayed-out or disabled color** should be used."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\n\n## Decision\n","Chosen option: ""Xlib"", because it was used already for the additional tooling and there's lots of documentations and examples.\n### Positive Consequences <!-- optional -->\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\n### Negative Consequences <!-- optional -->\n* Refactoring code\n* Wayland might be more modern\n* XCB is faster at times due to asynchronous\n",Use XCB.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhat language shall be used for the development of the window manager?\n## Decision Drivers <!-- optional -->\n* I want to learn Kotlin\n* I want to learn Python\n* I don't want to take care of the need to update an API layer between the window manager code and the\n* Libraries for display management and tooling in that area is usually written in C\n\n## Decision\n,"* I want to learn Kotlin\n* I want to learn Python\n* I don't want to take care of the need to update an API layer between the window manager code and the\n* Libraries for display management and tooling in that area is usually written in C\nChosen option: ""Kotlin/Native"", because it compiles to fast native code and C-APIs can be used directly.\n### Positive Consequences <!-- optional -->\n* Can use C-APIs directly (converted automatically by tooling)\n* Compiles to fast native application\n* No extra API layer\n### Negative Consequences <!-- optional -->\n* Needs separate compiling on every machine type\n","Use Kotlin.\n\nKotlin is a modern language that is easy to learn and use. It is also interoperable with C, which is the language that is typically used for display management and tooling in that area. This means that you will not need to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe application menu needs to know the currently active windows, shall display the name and enable the user to select or close them. Therefor it is necessary to exchange the corresponding information with the window manager.\n## Decision Drivers <!-- optional -->\n* Fast, the application menu entry information should be updated ""instantly"" when windows open, close or get renamed\n* No message should be lost\n\n## Decision\n","* Fast, the application menu entry information should be updated ""instantly"" when windows open, close or get renamed\n* No message should be lost\nChosen option: ""Message queues"", because this approach is a faster than shared storage approaches and doesn't need to handle locking. Message queues are asynchronous (the producer doesn't need to wait for the consumer) and messages are queued and not lost.\n### Positive Consequences <!-- optional -->\n* Easy to use and flexible\n* Library support in Python\n### Negative Consequences <!-- optional -->\n* Manual implementation in Kotlin necessary (but straight forward)\n","Implement two-way communication on IPC channels, one for each direction"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\n\n## Decision\n,"Chosen option: ""Client Properties / Atoms"", because it is the most reliable and side-effect free way to identify the tool windows.\n",The window manager should use a special property or flag to mark these tool windows. This property or flag can be set by the application when creating the window. The window manager can then use this property or flag to identify the tool windows and treat them accordingly.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\n# Decision\n[Swiftlint configuration](./../../.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\n\n## Decision\n",[Swiftlint configuration](./../../.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\n,Use Swiftlint configuration to enforce adherence to code style conventions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nFor legibility and discoverability, it is helpful to have a clear ordering of members within each type. Criteria which factor into this include:\n1. Member kind (property, method, subtype)\n2. Access (public/internal/private)\n3. Nature of member (stored or computed property, override or unique method)\nThere are different approaches to how these should be prioritized in C++/Objective-C, whether you're focussing on the needs of the type's consumer or implementer and which slices of behavior you most want to separate.\n# Decision\nWhere possible, members should be organized as follows:\n```\nclass MyClass: BaseClass {\n// MARK: - Constants\npublic static let valueA = 1\nprivate static let valueB = 2\n// MARK: - Types\npublic struct SubTypeA {}\nprivate struct SubTypeB {}\n// MARK: - Stored Properties\npublic var propertyA = 1\nprivate var propertyB = 2\n// MARK: - Computed Properties\npublic var propertyC: Int { return propertyA * 3 }\nprivate var propertyD: Int { return propertyB * 4 }\n// MARK: - Constructors\npublic init() {}\nprivate init(param: Int) {}\n// MARK: - Methods\npublic static func k() {}\npublic func f() {}\nprivate func g() {}\nprivate static func h() {}\n// MARK: - BaseClass overrides\npublic override var propertyL: Int { return propertyA * 3 }\npublic override func base() {}\n}\nextension MyClass: SomeComformance {\npublic var i: Int { return 0 }\npublic func j() {}\n}\n```\nImportant points to note:\n1. public before private\n2. static lifetimes before properties before methods\n3. stored properties before computed properties\n4. constructors before other methods\n5. overrides grouped based on the class they override\n6. protocol conformances in separate extensions (unless auto-synthesis is involved)\nIn most cases, these sections will not all be present... don't use a heading for a section not included\n# Consequences\nThere are a couple points that aren't totally decided.\nThey do not *need* to have ""mark"" headings and when they do, provided the contents themselves are organized, a simple ""Properties"" or ""Methods"" is sufficient to cover all methods or properties (e.g. doesn't need to be broken into ""Stored"" and ""Computed"").\nHowever, overrides sections should have a heading indicating which class' methods they override, otherwise its purpose is difficult to understand.\nStatic methods are all in one section with the other methods, with public static first and private static last (after all non-static methods). However:\n1. Most public static functions are constructors and should go in the constructor section (probably ahead of init functions)\n2. Many private static functions are called from just one location, lifted out for purely syntactic reasons. Sometimes these might appear alongside the function they're lifted out-of, sometimes they might appear at the end of the file since they're mostly an implementation detail that can be ignored.\nThere's a little flexibility here and when reviewing PR's suggestions and requests for improvement may be made prior to approving a PR.\n\n## Decision\n","Where possible, members should be organized as follows:\n```\nclass MyClass: BaseClass {\n// MARK: - Constants\npublic static let valueA = 1\nprivate static let valueB = 2\n// MARK: - Types\npublic struct SubTypeA {}\nprivate struct SubTypeB {}\n// MARK: - Stored Properties\npublic var propertyA = 1\nprivate var propertyB = 2\n// MARK: - Computed Properties\npublic var propertyC: Int { return propertyA * 3 }\nprivate var propertyD: Int { return propertyB * 4 }\n// MARK: - Constructors\npublic init() {}\nprivate init(param: Int) {}\n// MARK: - Methods\npublic static func k() {}\npublic func f() {}\nprivate func g() {}\nprivate static func h() {}\n// MARK: - BaseClass overrides\npublic override var propertyL: Int { return propertyA * 3 }\npublic override func base() {}\n}\nextension MyClass: SomeComformance {\npublic var i: Int { return 0 }\npublic func j() {}\n}\n```\nImportant points to note:\n1. public before private\n2. static lifetimes before properties before methods\n3. stored properties before computed properties\n4. constructors before other methods\n5. overrides grouped based on the class they override\n6. protocol conformances in separate extensions (unless auto-synthesis is involved)\nIn most cases, these sections will not all be present... don't use a heading for a section not included\n# Consequences\nThere are a couple points that aren't totally decided.\nThey do not *need* to have ""mark"" headings and when they do, provided the contents themselves are organized, a simple ""Properties"" or ""Methods"" is sufficient to cover all methods or properties (e.g. doesn't need to be broken into ""Stored"" and ""Computed"").\nHowever, overrides sections should have a heading indicating which class' methods they override, otherwise its purpose is difficult to understand.\nStatic methods are all in one section with the other methods, with public static first and private static last (after all non-static methods). However:\n1. Most public static functions are constructors and should go in the constructor section (probably ahead of init functions)\n2. Many private static functions are called from just one location, lifted out for purely syntactic reasons. Sometimes these might appear alongside the function they're lifted out-of, sometimes they might appear at the end of the file since they're mostly an implementation detail that can be ignored.\nThere's a little flexibility here and when reviewing PR's suggestions and requests for improvement may be made prior to approving a PR.\n","Members should be ordered as follows:\n\n1. Constants\n2. Types\n3. Stored properties\n4. Computed properties\n5. Constructors\n6. Methods\n7. Base class overrides\n8. Protocol conformances in separate extensions\n\nImportant points to note:\n\n- Public members should come before private members.\n- Static lifetimes should come before properties and methods.\n- Stored properties should come before computed properties.\n- Constructors should come before other methods.\n- Overrides should be grouped based on the class they override.\n- Protocol conformances should be in separate extensions.\n\nIn most cases, not all of these sections will be present. Do not use a heading for a section that is not included.\n\nOverrides sections should have a heading indicating which class' methods they override. Otherwise, their purpose is difficult to understand.\n\nStatic methods are all in one section with the other methods, with public static first and private static last (after all non-static methods). However:\n\n- Most public static functions are constructors and should go in the constructor section (probably ahead of init functions).\n- Many private static functions are called from just one location, lifted out for purely syntactic reasons. Sometimes these might appear alongside the function they're lifted out of, sometimes they might appear at the end of the file since they're mostly an implementation detail that can be ignored."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\niOS applications can be written in Objective-C or Swift. Objective-C offers greater interaction with C++ code but is considered a legacy language choice in the iOS developer community. The `pact-consumer-swift` framework was built to support Objective-C as well, but it's proven to become a bigger challenge supporting both with newer Xcode and Swift versions.\n# Decision\nThe framework is written in Swift.\n# Consequences\n\n## Decision\n",The framework is written in Swift.\n# Consequences\n,The framework will be written in Swift.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nAlmost all software we write depends on some other code, library or development tool which allows us to build what we want faster. Although this project attempts to avoid bringing in 3rd party dependencies, there are is functionality already written that is critical to this projects success.\n# Decision\nThe main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation/pact-reference/rust](https://github.com/pact-foundation/pact-reference/tree/main/rust/pact_mock_server_ffi).\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and/or interact with different services (interacting with Pact Mock Server, etc.).\nMatt's [CwlPreconditionTesting](https://github.com/mattgallagher/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `./Carthage/Checkouts/CwlPreconditionTesting/*` added into the project itself). For SPM it is defined as a dependency in `./PactSwiftServices/Package.swift`.\n# Consequences\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\n# Follow-up (September 30, 2020)\nAll 3rd party dependencies have been successfully removed from this project/framework.\n\n## Decision\n","The main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation/pact-reference/rust](https://github.com/pact-foundation/pact-reference/tree/main/rust/pact_mock_server_ffi).\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and/or interact with different services (interacting with Pact Mock Server, etc.).\nMatt's [CwlPreconditionTesting](https://github.com/mattgallagher/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `./Carthage/Checkouts/CwlPreconditionTesting/*` added into the project itself). For SPM it is defined as a dependency in `./PactSwiftServices/Package.swift`.\n# Consequences\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\n# Follow-up (September 30, 2020)\nAll 3rd party dependencies have been successfully removed from this project/framework.\n",The main dependency of this project is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation/pact-reference/rust](https://github.com/pact-foundation/pact-reference/tree/main/rust/pact_mock_server_ffi). The binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`PactSwift` takes advantage of Mock Server FFI binaries built from shared Rust code. These are generally large binary files when it comes to iOS and macOS platforms and we are limited with hosting them in the GitHub repo. The FFI also follows it's own source and changes are available independently to changes to `PactSwift`'s functionality. Separating the responsibilities would be welcomed.\nFurthermore, the pain of managing multiple binaries with the same name but each with its specific architecture slice could be reduced by generating an `XCFramework` using an automated script and kept from the framework user. These can blow up to more than 100Mb each (the fat binary with all slices for iOS platform blew up to more than 300MB). Using `XCFramework` we can shed off a lot of the statically linked code. Mock Server FFI (`MockServer.swift`) is the only part of `PactSwift` package that depends on binaries being built for specific architectures and run platforms. With removal of binaries from the main `PactSwift` project, we should be able to avoid managing them, mixing them up (as they are all named the same), discarding them at `git add` and `commit` steps and rebuilding them at next `PactSwift` build/test cycle.\n\n## Decision\n",- Mock Server FFI interface and implementation to be split into it's own Swift Package called `PactSwiftMockServer` and distributed as a binary (`XCFramework`) when on Apple platforms and as a source package when used on Linux platforms.\n- Utilities used by both the main `PactSwift` and `PactSwiftMockServer` packages are split into one package called `PactSwiftToolbox`.\n- Where it makes sense the dependencies' versions should be exact. If exact version is not set for a valid reason then `.upToMinor()` must be used to avoid breaking changes when releasing packages in isolation.\n- Scripts to automate the release processes will be provided within the projects' scripts folders.\n,**Separate the Mock Server FFI into a separate repository and build XCFrameworks for each architecture.**\n\nThis will address the following concerns:\n\n* Reduce the size of the PactSwift repository by moving the large Mock Server FFI binaries to a separate repository.\n* Improve the management of the FFI binaries by using an automated script to generate XCFrameworks for each architecture.\n* Eliminate the need for framework users to manage multiple binaries with the same name.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\nnyc is one of the suggested replacements. Additional information is available\non [npm](https://www.npmjs.com/package/istanbul). This leaves the package open\nto security flaws that will not be patched. Features available in the latest\nversions of node will not be supported.\n\n## Decision\n,The decision is to migrate from istanbul to nyc.\n,Migrate to nyc or another supported package.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/).\nGitHub Actions will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repo, which has a basic use case will provide some\nexposure to the service.\n\n## Decision\n",The decision is to replace Travis CI with GitHub Actions.\n,Use GitHub Actions as the primary CI mechanism for this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLearning Machine handles a Blockcerts verifier in multiple repository and with different ways of deploying. This is costly and hard to maintain.\nDecision has been made to unify the verifier into one sole component and repository, with scalibility and maintainability at heart.\n\n## Decision\n","#### JS\nWe decided to use Polymer 3.0 as previous versions of the verifier were already written with Polymer. Also because Web Components seem like a promising technology that could open a interesting future for the usage of the component.\nState of the application will be handled by Redux, as demonstrated in the [example project of Polymer](https://github.com/Polymer/pwa-starter-kit).\n#### CSS\nWe decided to use ITCSS for its interesting way to handle CSS scalability and maintainability.\nMore information about this methodology can be found here:\nhttps://github.com/sky-uk/css\nhttps://www.xfive.co/blog/itcss-scalable-maintainable-css-architecture/\n#### Testing\nWe are using the tools provided by the Polymer Project, hence [WCT](https://github.com/Polymer/tools/tree/master/packages/web-component-tester). We also test for accessibility.\n#### Accessibility\nThe Web Component needs to be WCAG2.0 AA compliant.\n","Migrate the multiple Blockcerts verifiers into a single component and repository, using a scalable and maintainable architecture. The new component should provide a consistent interface for verifying Blockcerts, and should be easy to deploy and update."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[pwa-starter-kit](https://github.com/Polymer/pwa-starter-kit) is an example of a Redux application working with Polymer 3. While it serves its purpose, the separation of concerns is not entirely clear between the state and the views, which means that in the example views have too much knowledge of what provides and modfies the state.\nThis is potentially dangerous for scalability, and bloats the view code with decisions that shouldn't be of its concerns.\nReact provides an architectural pattern to handle this abstraction, in the shape of `Containers`. Containers are wrapper around the view component, which connect (via the `react-redux` helper function of the same name) the view and the state (to read it, and to action it).\nThis pattern is interesting because from an architectural standpoint every responsibility is properly distributed. The view receives props, may they be data or callback functions, and utilizes them, the container selects where to retrieve the data or callback functions, and the state solely stores the data and provide mechanisms to modify it.\n\n## Decision\n","We decided to keep this approach for a Polymer project too. Because the out-of-the-box tools do not provide that abstraction, we implemented our own architectural approach to fit this need.\nA visual representation of the intent is as follows:\n![Chain of connection: State > Connector > Container > Component](https://user-images.githubusercontent.com/12797962/41294972-a254d432-6e59-11e8-8e08-214c43772173.png)\n- The state handles the live data of the application.\n- The connector has an instantiation of the store, but is also a metaview, which enables communication with the methods of the store, without exposing it elsewhere.\n- The container is an instance of the connector, which allows defining which part of the state and which actions we would like to bind to its wrapped component.\n- And the wrapped component is just a regular component which expects some props, but does not know nor care where they come from.\n","We should implement a `Container` pattern for Redux applications using Polymer 3. This pattern will help to separate the concerns between the state and the views, making the applications more scalable and easier to maintain."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRedux provides a good mechanism to handle the state and its mutations, but not so much to handle the business logic.\nHowever, like always, to keep a good segregation of concerns, we want to handle all business related logic, as well as all external communication of the application isolated.\n\n## Decision\n","To do so, we introduce the concept of domain.\nThe domain is the part responsible for everything that's not the view concern, nor the state concern.\nThe view concern is to render data.\nThe state concern is to store and modify data.\nThe rest belongs to the domain.\nA visual representation is as follows:\n![Domain connection: outer world > domain > state > view](https://user-images.githubusercontent.com/12797962/41419432-4c70fd4e-6ff2-11e8-8acd-a18367087193.png)\nEach method of the next inner layer only knows about the layer above it.\nThis domain is only inspired from Domain Driven Design and aims at being a simpler implementation of the approach.\n","**Decision:** Create a separate layer for business logic.\n\n**Rationale:**\n\n* Redux is a good mechanism for managing state and mutations, but it is not well-suited for handling business logic.\n* Separating business logic from the state management layer will help to improve the application's architecture and maintainability.\n* An isolated business logic layer will be easier to test and debug.\n* This approach will allow for better reuse of business logic across different parts of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSometimes labels need to handle pluralization. While it could be just as easy as adding a `s` at the end of the word in English (it is not), French or Spanish, other languages have a variation of their plural form that require a better handling.\n\n## Decision\n",We chose to follow the second approach which adds less overhead to the bundle.\nIt comes at the cost of having the contributor to add the plural version of the string they want translated. But this explicit approach also reduces the risk of error and inconsistency.\n,Use the framework provided by the ICU4J library to generate the correct plural form (as a string) for the specified number of resource entries.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\n\n## Decision\n","Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\nShared styles will be hosted under `src/shared-styles` folder and organized following the ITCSS nomenclature.\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\n","We use a combination of CSS custom properties, CSS variables, and CSS mixins to achieve our goal of maintaining component-level styles while enabling shared styles.\n\n**CSS custom properties** allow us to define custom properties that can be used by any CSS rule. This allows us to create a central location for shared styles, and then use those shared styles in our component-level styles.\n\n**CSS variables** are similar to CSS custom properties, but they are scoped to a specific element or component. This allows us to create shared styles that are only used within a specific component or group of components.\n\n**CSS mixins** are a way to group CSS rules together and then reuse them in multiple places. This allows us to create reusable code snippets that can be used to create consistent styles across our components.\n\nBy combining these three techniques, we can create a scalable and maintainable style system that meets our needs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\nAnimating proves convenient on `max-height`\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\n\n## Decision\n","We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\nWe thus decided to trick the component:\n- we only calculate the `totalHeight` once, since content will not change once rendered.\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https://github.com/Polymer/lit-html/blob/master/src/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\n","**Decision:** Use `max-height: 100vh` for the substeps list to ensure that it can accommodate indeterminate heights while still allowing for animation.\n\n**Rationale:**\n\n* Using `max-height: 100vh` ensures that the substeps list can accommodate indeterminate heights, such as those caused by multi-line error messages.\n* Setting the maximum height to the viewport height allows the list to expand to the full height of the viewport, providing ample space for all substeps and any associated error messages.\n* This approach maintains the functionality of the progress bar, which is dependent on the height of the substeps list. By setting the maximum height to be consistent with the progress bar's height, we ensure that the progress bar continues to work as expected.\n* This solution provides flexibility and adaptability, allowing for the display of substeps with varying heights while still enabling animation and maintaining the functionality of the progress bar."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\ndon't choose and use both, but i want to make it consistent.\n## Decision Drivers\n* New empty project\n* Kotlin now fully supported for Android by Google\n\n## Decision\n","* New empty project\n* Kotlin now fully supported for Android by Google\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\nGoogle now, which eliminates the risk of being dropped any time soon.\nPositive Consequences:\n* Less code, more fun!\n* Having to learn a new language\n* Great interoperability with Java if needed\nNegative consequences:\n* Having to learn a new language\n","Use Kotlin as the primary language for new Android projects.\n\n**Rationale:**\n\n* Kotlin is fully supported by Google for Android development.\n* Kotlin offers several advantages over Java, including:\n    * Concise and expressive syntax\n    * Improved type safety\n    * Null safety\n* Adopting Kotlin as the primary language will ensure consistency across new projects."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nI need a way to document design decisions.\n\n## Decision\n,"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\nPositive Consequences:\n* Choices and reasoning are persisted and versioned\nNegative consequences:\n* Having to document\n",Use Architectural Decision Records (ADRs) to document the rationale behind architectural decisions and to ensure that these decisions remain aligned with business goals and objectives.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThis app shows how i would build an Android app. Therefore the app architecture is the goal itself,\nthe features this app has, are just a way to show an app architecture.\n## Decision Drivers\n* Availability\n* Realism\n* Content creation\n\n## Decision\n","* Availability\n* Realism\n* Content creation\nChosen option: ""The Movie Db"", because it is more realistic when the app consumes a real service.\nPositive Consequences:\n* It is realistic\nNegative consequences:\n* When it's down the app cannot consume it\n",**Decision:** Choose an Android app architecture that meets the following requirements:\n\n* **Availability:** The architecture should use well-established libraries and frameworks to ensure wide industry support.\n* **Realism:** The architecture should reflect best practices and design patterns commonly used in real-world Android app development.\n* **Content creation:** The architecture should facilitate the creation of high-quality content that showcases the app's features and functionality.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide how to deploy our Vue app in Heroku. We can achieve this either\nby using a Node.js web server, or by deploying the app as a static site with\nthe [Heroku static buildpack](https://github.com/heroku/heroku-buildpack-static).\n\n## Decision\n","We will use the static buildpack to deploy, as our app will essentially be a\nstatic frontend backed by TIMDEX. This is consistent with [Vue's deployment\nguidelines](https://cli.vuejs.org/guide/deployment.html#general-guidelines)\non deployments.\n","The decision is to deploy the Vue app as a static site using the Heroku static buildpack.\n\n**Rationale:**\n\n* The Heroku static buildpack is specifically designed for deploying static sites, which makes it a more suitable option for a Vue app that primarily serves static content.\n* Using the Heroku static buildpack eliminates the need for a Node.js web server, which simplifies the deployment process and reduces the runtime overhead.\n* The Heroku static buildpack supports various caching mechanisms, which can improve the app's performance and reduce load times.\n* As a static site, the Vue app will be served directly from Heroku's CDN, providing better scalability and reliability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNPM is causing confusion as to why lock files are changing in local\nenvironments when no changes have been made. We have found explanations and\nworkarounds, but it feels like the type of unexpected default behavior that will\nlead to frustration as new developers join the project.\nYarn is an alternative package manager that seems to have a more expected set\nof default behaviors while maintaining compatibility in case we need to revert.\n\n## Decision\n",We will use Yarn instead of NPM for this project.\n,"**Decision:** Migrate to Yarn as the package manager for this project.\n\n**Rationale:**\n\n* NPM's unexpected default behavior of modifying local lock files without clear reasons can lead to confusion and frustration among developers.\n* Yarn exhibits more expected default behaviors, reducing the risk of unexpected changes and simplifying package management.\n* Maintaining compatibility with NPM allows for a smooth transition and the option to revert if necessary."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe reviewed various options for building our front end and decided Vue.js was\nthe best fit. However, Vue is preparing for a major version upgrade. We could\nchoose to use the current stable version (2) and migrate to the new version (3)\nwhen it is released or start with the release candidates for the new version.\n\n## Decision\n",We will use Vue.js 3 releases candidates.\n,"**Decision:** Use the release candidates for Vue.js version 3.\n\n**Rationale:**\n\nWhile Vue.js version 2 is stable and battle-tested, version 3 offers significant performance and architectural improvements. By using the release candidates, we can:\n\n* Take advantage of the new features and optimizations in version 3 without the risk of major breaking changes.\n* Gain early experience with the new version and identify any potential migration issues.\n* Position ourselves to seamlessly upgrade to the stable version of Vue.js 3 when it is released.\n\n**Consequences:**\n\n* There is a slight risk that release candidates may contain bugs or unexpected behavior.\n* The team will need to be prepared to invest time in migrating to the stable version once it is available."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement Architectural Decision Records (ADRs) as a structured way to document and track architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe GOV.UK PaaS billing system receives a series of events from Cloud Foundry notifying, for each tenant, whether services or resources have been created, renamed, or deleted.\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\nIf there is a problem with a tenant's bill it was very difficult to find the source of the problem.\n\n## Decision\n","We need to have a persistent record of the services or resources each tenant has provisioned with a dates indicating when the service or resource was/is being used. This persistent record is in the new `resources` table.\nThe reason for this is that there is no need to regenerate historical records of services or resources provisioned for tenants each time billing is run each month since this information does not change. Furthermore, recording this information for each month makes it difficult for us to calculate bills between any two dates and times.\nThe `resources` table also acts as an audit point within GOV.UK PaaS billing. It makes investigation of discrepancies in tenant bills easier to investigate. Anyone supporting GOV.UK PaaS billing can first look at the contents of `resources` and see whether the discrepancy arose in the population of `resources` or afterwards in the actual calculation of the bill.\n","The new GOV.UK PaaS billing system will store a permanent record of the results of each stage of billing consolidation, including what services or resources tenants have provisioned and on which specific day."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe needed to decide where to terminate TLS connections for public and tenant\nfacing endpoints and how to manage the corresponding private keys.\nWe had previously decided to only support HTTPS to both deployed applications\nand Cloud Foundry endpoints.\nAt the time of writing there were 4 endpoints to consider:\n- Deployed applications (gorouter). Accessed by the public.\n- CF API. Accessed by tenants.\n- UAA. Accessed by tenants.\n- Loggregator. Accessed by tenants.\n- SSH proxy. In theory accessed by tenants, but not working in our environment.\nWe had an existing credentials store suitable for storing the private keys at\nrest. Only a small number of engineers within the team can access; the same\nones that can make IAM changes using our account-wide terraform config.\nPlacing ELBs in front of public-facing services is an architectural pattern\nadvised by Amazon [in order to reduce attack\nsurface](https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf).\nSpecifically they advise that it helps withstand volumetric Denial of Service\nattacks; the ELB handles TCP connections and therefore the responsibility for\nhandling DDOS at Layer 4 and below resides with the ELB team.\nWe did a spike, where we attempted to place everything public-facing or\ntenant-facing behind ELBs. We found that:\n- In HTTP mode the ELBs do not support web sockets. This is known to break\nloggregator, which relies on them for log streaming. It would also prevent\ntenants from using web sockets within their applications.\n- When the ELB is in TCP mode, we have no way of communicating the client IP\naddress to the downstream service. Practical consequences of this would be\nthat tenants would be unable to see in their logs who is using their service or\ndo any access control based on client IP address.\nIn attempting to solve the second problem, we explored some options:\n- ELB has support for the [Proxy\nProtocol](http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt), but\nunfortunately none of the downstream services, such as gorouter, support it. It\nseemed simple to add support to gorouter.\n- We could introduce another intermediary proxy such as HAProxy, which\nunderstands the proxy protocol and adds or appends to an `X-Forwarded-For`\nheader with the client IP address as provided via the proxy protocol.\nDecision\n========\nWe decided to:\n- use the ELB to terminate TLS\n- use the ELB in TCP mode\n- submit proxy protocol support to gorouter\n- use S3 logging to ensure we have the IP addresses of clients using the CF\nendpoint\nStatus\n======\nAccepted\nConsequences\n============\n- We played a [spike to investigate setting X-Forwarded-For\ncorrectly](https://www.pivotaltracker.com/projects/1275640/stories/116619465)\nwhich produced an [upstream PR to add proxy protocol support to\ngorouter](https://github.com/cloudfoundry/gorouter/pull/126) and [another to introduce\nX-Forwarded-Proto headers](https://github.com/cloudfoundry/gorouter/pull/127)\n- As an interim measure until gorouter gained support, [we added an\nintermediate HAProxy to introduce `X-Forwarded-For` and `X-Forwarded-Proto`\nheaders](https://www.pivotaltracker.com/story/show/116309951).\n\n## Decision\n","========\nWe decided to:\n- use the ELB to terminate TLS\n- use the ELB in TCP mode\n- submit proxy protocol support to gorouter\n- use S3 logging to ensure we have the IP addresses of clients using the CF\nendpoint\nStatus\n======\nAccepted\nConsequences\n============\n- We played a [spike to investigate setting X-Forwarded-For\ncorrectly](https://www.pivotaltracker.com/projects/1275640/stories/116619465)\nwhich produced an [upstream PR to add proxy protocol support to\ngorouter](https://github.com/cloudfoundry/gorouter/pull/126) and [another to introduce\nX-Forwarded-Proto headers](https://github.com/cloudfoundry/gorouter/pull/127)\n- As an interim measure until gorouter gained support, [we added an\nintermediate HAProxy to introduce `X-Forwarded-For` and `X-Forwarded-Proto`\nheaders](https://www.pivotaltracker.com/story/show/116309951).\n",- Terminate TLS using ELB.\n- Use ELB in TCP mode.\n- Submit proxy protocol support to gorouter.\n- Use S3 logging to capture client IP addresses for the CF endpoint.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo comply with [Payment Card Industry (PCI) Requirements](https://www.pcisecuritystandards.org/pci_security/maintaining_payment_security) we will remove the use of shared credentials anywhere in the platform.\nWe will use Google Single Sign On (SSO) to avoid credential sharing.\nTo enable SSO we will expose some of the APIs on the BOSH instance without using a SOCKS5 proxy. This is due to the SOCKS5 proxy not being compatible with our VPN, which in turn creates a support risk.\nWe discussed proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https://docs.google.com/document/d/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps/edit#heading=h.xscqoqxlc072)\n\n## Decision\n",We will remove the reliance on SOCKS5 or SSH tunnels for the User Account and Authentication Service (UAA) API endpoint so that we can remove the use of shared credentials.\nWith these in place we are unable to complete an SSO journey from one of our existing IDPs as the BOSH instance is not browser accessible.\nThe current method will be replaced with SSO in order to obtain a UAA token. This token is then used to interact with the rest of the APIs on the BOSH instance.\n,"To enable Google Single Sign On (SSO) and comply with PCI Requirements, the following decision was made:\n\n- Expose some of the APIs on the BOSH instance without using a SOCKS5 proxy."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe need to provide tenants with the ability to provision databases for use in\ntheir applications. Our first iteration of this will be using RDS.\nWe investigated some implementations of a service broker which supported RDS\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\n- [18F](https://github.com/18F/rds-service-broker)\nDecision\n========\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\nStatus\n======\nAccepted\nConsequences\n============\nWe will be maintaining a new service broker, but have a head start on creating\nit by basing it on an existing service broker.\n\n## Decision\n","========\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\nStatus\n======\nAccepted\nConsequences\n============\nWe will be maintaining a new service broker, but have a head start on creating\nit by basing it on an existing service broker.\n","We will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker) RDS broker. As this is not a supported product, we will fork this and maintain this and implement new features ourselves."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to enable, or provide the option to enable,\n[storage encryption (AKA encryption at rest) for the RDS instances](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html)\nof the tenants databases created by our [RDS broker](https://github.com/alphagov/paas-rds-broker). The broker has logic to enable this option on creation.\nBut there are [some limitations](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations):\n* Storage Encryption can only be enabled on creation of the DB. There is no way to update an instance to enable or disable encryption. The only way is by creating a encrypted (or unencrypted) copy of a snapshot, to then restore it to a DB instance.\n* <s>Storage Encryption is only supported [for some instance types](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#d0e47573). Specifically it is not supported for `db.t2.small` and `db.t2.micro.`, used in our Small and Free plans</s>\n* Update 2018-01-24: Amazon have now [enabled support for encryption of `t2` instances](https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-rds-enables-encryption-at-rest-for-additional-t2-instance-types/).\nIn consequence:\n* Users cannot migrate from a plan with encryption (Medium or Large) to a plan without encryption (Small or Free). But this use case is less common.\n* Users can migrate from Free/Small to a plan with encryption (Medium or Large), but the instance will not have encryption enabled.\n* If we enable encryption in the existing plans, the existing databases will remain without encryption.\n* Due the API broker limitations, it is not possible to query the attributes of an existing instance.\nWe have 3 options to proceed:\n1. Enable Encryption in the Medium and Large plans, and document the restrictions.\n* Less effort to implement.\n* We might end having unencrypted database in a plan that is meant to be encrypted, which is confusing for the users and operators.\n* Existing Instances will remain unencrypted.\n2. Change the instance type for the Small plan to `db.m3.medium`.\n* Would allow migrate from Small to Medium or Large.\n* We will still have the problem for the Free plan.\n* Increases the costs for the Small plan (double).\n3. Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption.\n* It would be more explicit and clear, and the plan would match the state of the existing database.\n* Existing instances would still match with the plan description.\n* We will add more plans, which makes it more confusing for the tenants.\n\n## Decision\n","We decided to provide additional explicit plans with Encryption enabled, and keep the old ones.\nWe will add logic in the broker to prevent updates between plans with or without encryption.\nWe have decided only add the option of encryption to the HA plans to minimise the number of new plans added. In most of the cases the tenants would choose HA together with encryption and, although adding more plans is easy, removing them is painful once they are being used.\n","Option 3. Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA service that's onboarding at the moment depends on MySQL, memcached and\nelasticsearch that we don't yet offer, but are likely to in future, and has\nchosen to host them on AWS using RDS MySQL, Elasticache memcached and hosted\nElasticache.\nWe wanted to ensure that only the app instances belonging to that service could\nconnect to those services in order to avoid other services being able to access or modify their data.\n- MySQL requires authentication and allows TLS\n- Elasticache memcached does not provide authentication or TLS\n- AWS Hosted elasticsearch [appears to provide the ability to sign\nelasticsearch requests using an AWS access\nkey](https://aws.amazon.com/blogs/security/how-to-control-access-to-your-amazon-elasticsearch-service-domain/).\nRequiring a custom elasticsearch client.\nThe options for authentication are:\nVia Internet, using network access controls\n-------------------------------------------\nThis is described for\n[elasticache](http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Access.Outside.html)\nHow we would set it up:\n- Tenant binds services to public IP addresses, restricts access to PaaS egress IPs.\n- PaaS team changes default [application security\ngroup](https://docs.cloudfoundry.org/adminguide/app-sec-groups.html) set to\nforbid access to those IP addresses.\n- PaaS team creates new application security group allowing access to those IP addresses.\n- PaaS team binds application security group to space(s) belonging to tenants\nRisks/costs:\n- Access via Internet IP is via NAT instance, introducing a likely single point\nof failure and additional running costs to tenants.\n- Traffic is in plaintext over the Internet for some services.\n- PaaS team need to do work every time a tenant adds/changes service IP addresses.\n- If they forget to request this, other PaaS tenants have access to their\nservice IPs.\n- PaaS team need to do work (assigning application security groups) in every\noccasion a space needs to gain or lose\naccess to the list of service IPs.\nUsing private address space (VPC peering)\n-----------------------------------------\nHow we would set it up:\n- Tenant raises support request to begin setup process\n- PaaS team responds with a unique IP allocation eg. 172.16.0.0/24 for tenant to use\n- Tenant creates VPC using that address space\n- Tenant creates AWS security group(s) restricting access from PaaS VPC to expected services\n- Tenant provides PaaS team with their AWS account id and the VPC id.\n- PaaS team sends VPC peering request\n- PaaS team creates new application security group allowing access to the VPC IP allocation\n- PaaS team binds application security group to space(s) belonging to tenants\n- Tenant accepts VPC peering request\nRisks/costs:\n- Introduces a new network security boundary between VPCs; a risk of\naccidentally introducing security group rules that allow more access from the\npeered VPC than intended.\n- PaaS team need to do work (assigning application security groups) on every\noccasion a space needs to gain or lose\naccess to the peered VPC.\nAlthough this specific example uses VPC peering because the tenant in question\nuses AWS, we could use the same principle (us assigning IP address space and\nchanging application security groups) to a VPN or some other network overlay\ntechnology to allow us to connect to things other than VPCs.\n\n## Decision\n",We will offer VPC peering to tenants in specific cases where it is appropriate.\n,"**Decision:** Use VPC peering to provide private access to the MySQL, memcached, and Elasticsearch services.\n\n**Rationale:**\n\n* VPC peering provides a secure and isolated private network connection between the PaaS platform and the tenant's AWS VPC.\n* This approach eliminates the need for public IP addresses and Internet-based access, reducing the risk of data exposure and data access by unauthorized entities.\n* VPC peering also avoids the need for custom elasticsearch clients, as the tenant can use existing Elasticsearch clients to connect via the peered VPC.\n* While VPC peering requires additional setup and configuration compared to using network access controls via the Internet, it offers a more secure and controlled solution for accessing the required services."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\nThere are 3 main network sections between the user and the application:\n* User to ELB\n* ELB to router\n* Router to cells\n\n## Decision\n,"* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\n","**Decision:** Use TLS/SSL encryption for all three network sections.\n\n**Rationale:** TLS/SSL encryption is a widely adopted and well-understood technology that provides strong encryption for data in transit. It is supported by all major browsers and web servers, and it is relatively easy to implement and manage.\n\n**Details:**\n\n* For the User to ELB section, use TLS/SSL termination at the ELB. This will encrypt all traffic between the user and the ELB.\n* For the ELB to router section, use TLS/SSL encryption between the ELB and the router. This will encrypt all traffic between the ELB and the router.\n* For the Router to cells section, use TLS/SSL encryption between the router and the cells. This will encrypt all traffic between the router and the cells."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR032](/architecture_decision_records/ADR032-ssl-only-for-applications-and-cf-endpoints) we decided that\nwe would only support https for applications on the PaaS, and that we would\ndrop plain http connections (port 80).\nSince then, we've observed this causing confusion for users on numerous\noccasions where they think their app isn't working after being pushed.\nThe situation is improved with the inclusion of the `cloudapps.digital` domain\nin the [HSTS preload](https://hstspreload.org/?domain=cloudapps.digital) list,\nbut this only helps users with recent versions of modern browsers.\nAs a result of the continued confusion for users we should revisit the decision\nfrom ADR443.\nThere are a number of things that could be done to address this:\n### Update the CF CLI to include the scheme\nCurrently, the CF CLI outputs the fully-qualified hostname of the app after\npushing, but doesn't include the scheme. This has caused confusion for users\nwhen this is copy/pasted into browsers, and then times out.\nGetting the CLI to include the scheme here will help with the specific case of\nusers getting confused immediately after pushing an app.\nIt's unclear how much work this involves, as currently information about\nwhether a route is http or https doesn't appear to be modeled in CloudFoundry\nanywhere.\nIf this involves changes to the CLI, there's no guarantee that users will\nupgrade their CLI.\nAdditionally, there is some debate about how effective this change would be. It\nwill probably fix some cases, but won't cover everything.\n### Redirect http to https\nAdd an endpoint that listens to all http requests on cloudapps.digital and\nredirects them to the corresponding https URL.\nThere's a risk with this that a service could link to the http version of a\npage by mistake and not notice due to the redirect. We can mitigate this be\nhaving the redirect strip the path and query when redirecting so that it always\nredirects to the base URL.\nThere's another risk that a misconfiguration could allow non-encrypted traffic\nthrough to applications. This would need to be mitigated by having acceptance\ntests to cover this.\n\n## Decision\n",We will redirect http traffic to the corresponding root https endpoint.\nWe will continue to maintain HSTS preload lists for our production domains.\n,We will redirect all http requests to https.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS would like to be able to isolate specific tenant apps and tasks to\ndifferent pools of virtual machines (VMs).\nGOV.UK PaaS would like to be able to prevent specific tenant apps and tasks\nfrom egressing to the internet.\nApps running inside the separate pools of VMs should be able to discover and\naccess other apps running within the platform, providing that the correct Cloud\nFoundry Network Policies have been created.\nApps running in the shared pools of VMs should be able to discover and access\napps running inside an isolation segment, providing that the correct Cloud\nFoundry Network Policies have been created.\n\n## Decision\n","GOV.UK PaaS will implement egress-restricted isolation segments.\nIsolation segments will be configured by a GOV.UK PaaS developer, in a similar\nmanner to VPC peering connections.\nIsolation segments will have the following variable properties:\n- Number of instances (e.g. 1, 2, 3, 6)\n- Instance type (e.g. small/large - maps to an AWS instance type + disk sizing)\n- Whether egress to the internet is restricted\nWe will use IPTables rules to achieve egress restriction.\n### Isolation segments\nCloud Foundry supports separating apps and tasks for specific Organizations\nand Spaces via a feature called\n[Isolation Segments](https://docs.cloudfoundry.org/adminguide/isolation-segments.html).\nAn Isolation Segment is a group of Diego cells with separate placement tags,\nwhich map to the isolation segment name.\nIsolation segments will be implemented as new instance groups defined in the\nBOSH deployment manifest, with additional placement tags. A placement tag\ncorresponds to an isolation group name.\nFor example, an instance group with the placement tags:\n- `fast-cpu`\n- `fast-network`\nenables us to run the following commands successfully:\n- `cf create-isolation-segment fast-cpu`\n- `cf create-isolation-segment fast-network`\nwhich creates two isolation segments.\nThese isolation segments can be shared such that a segment can be:\n- used by only a single organization or space\n- shared by multiple organizations and spaces\n### Egress restrictions\nContainer-to-container networking within Cloud Foundry is implemented via a\nVirtual Extensible Local Area Network\n([VXLAN](https://tools.ietf.org/html/rfc7348)).\nEach container is assigned a virtual IP address inside the subnet 10.255/16\n[Silk](https://github.com/cloudfoundry/silk)\nand VXLAN create/update/delete\n[IPTables](https://linux.die.net/man/8/iptables) rules\nvia the Container Network Interface\n([CNI](https://github.com/containernetworking/cni)),\nto ensure containers can talk to each other.\nIPTables is an interface to control networking within the Linux kernel.\nExisting network traffic restrictions are defined by Silk and VXLAN as\ndescribed above. We can configure extra IPTables rules with higher precedence\nto create tighter restrictions than currently exist.\nIPTables can be used to prevent unauthorised egress via REJECT rules,\ndepending on the destination IP address. This can be done, either:\n- In the global INPUT or FORWARD chains, with source IP qualifier to ensure only container traffic is affected\n- In each container’s “netout” chain\nImplementing such IPTables rules allows us to block traffic from an IP address\nwithin 10.255.0.0/16 (apps) to an address outside 10.0.0.0/8 (outside the VPC).\nThis would have the effect of preventing app traffic egressing from the\nplatform.\nWe would apply these IPTables rules to running apps and tasks, but not staging\napps.  This will allow staging apps to communicate with the outside world (e.g.\nfor downloading dependencies).\n",Implement Network Security Groups (NSGs) across all GOV.UK PaaS cloud regions. Configure NSGs to restrict egress and ingress traffic to/from isolated segments of the platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nThe alpha initially took the approach of starting with a vanilla set of cloud\nfoundry manifests, and merging new values into it using spiff. This became\ndifficult to reason about, and cf-release was forked because it was easier than\noverriding necessary values using spiff. However, the confusing spiff hierarchy\nremained.\nDecision\n========\nWe will create our own set of manifests based on those in cf-release.\nWe will modify these as required.\nWe will use spruce to merge a series of files into the yml required by cloud\nfoundry\nWe will name the files with a numeric prefix and rely on shell globbing to\ndetermine the merge order rather than listing the merge order in the\nbuild-manifest script.\nStatus\n======\nAccepted\nConsequences\n============\nWe will need to define an upgrade path for our cloud foundry manifests - making\nsure we pull in the required values from the new release manifests into ours.\n\n## Decision\n",========\nWe will create our own set of manifests based on those in cf-release.\nWe will modify these as required.\nWe will use spruce to merge a series of files into the yml required by cloud\nfoundry\nWe will name the files with a numeric prefix and rely on shell globbing to\ndetermine the merge order rather than listing the merge order in the\nbuild-manifest script.\nStatus\n======\nAccepted\nConsequences\n============\nWe will need to define an upgrade path for our cloud foundry manifests - making\nsure we pull in the required values from the new release manifests into ours.\n,"Create custom manifests based on cf-release, modify them as needed, and use spruce to merge them in the required order."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Audit events\nThe GOV.UK PaaS has components which expose events which can be used for\nauditing. For example:\n- BOSH director kernel audit logs\n- BOSH director Credhub security events\n- BOSH director UAA events\n- Cloud Foundry UAA events\n- Cloud Foundry Cloud Controller security events\nThe BOSH director and BOSH managed instances store these logs in\n`/var/vcap/sys/log/`.\n### Logging service\nThe Cyber Security team run a centralised log ingestion system called the\nCentral Security Logging Service (CSLS).\nThis service runs in AWS and uses [CloudWatch log group subscription\nfilters](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html)\nto ingest logs, which are then sent to Splunk for indexing.\n\n## Decision\n","We will use Terraform to create log groups in our existing pipelines:\n- `create-bosh-concourse`\n- `create-cloudfoundry`\nWe will store audit logs in CloudWatch for 18 months (545 days).\nWe will use Terraform to create log group subscription filters which will send\nlogs to CSLS.\nTerraform failing to create log group subscription filters should not block the\nexecution of the pipeline, so our pipelines are not coupled to CSLS.\nWe will run a fork of the\n[awslogs-boshrelease](https://github.com/alphagov/paas-awslogs-boshrelease),\non all instances that have relevant audit and security event logs, to send logs\nfrom the instances to CloudWatch.\nThe CloudWatch log groups will have subscription filters that will send logs\nto CSLS so that logs are indexed in Splunk.\n","The GOV.UK PaaS will ingest audit events from BOSH director, BOSH managed instances, and Cloud Foundry into the Central Security Logging Service (CSLS).\n\n**Technical Decision:**\n- BOSH director and BOSH managed instances store logs in `/var/vcap/sys/log/`.\n- Use CloudWatch log group subscription filters to ingest logs into CSLS.\n- Route logs from Cloud Foundry components to CSLS using their existing logging mechanisms.\n- Comply with GDS Operations Security Guidance.\n- Enhance security compliance reporting.\n- Improve audit event analysis by consolidating them in a centralized log repository."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[PaaS product pages] have been reviewed and a number of accessibility issues\nhave been identified. To resolve those we, would need to make an upgrade and\nand review if any additional changes are needed to align with the GOV.UK Design System.\nAs those pages are built in Ruby and in [ADR024] we've made the decision\nto develop our user-facing applications on Node, it's a good opportunity to\nlook at rebuilding the product pages.\nWe've discussed user needs and it emerged that:\n* anyone in the team (developer and non-developer) should be able to update pages\nwith less effort\n* pages should be performant for end users\n* pages should be rendered by the server\n* keeping pages up to date with GOV.UK Design System releases should be quicker and easier\n* alignment of technologies for our user-facing web products should provide better\ndeveloper experience and give us the option to have shared component libraries\nWith the above in mind we researched options. Our admin interface is built in React,\nso we narrowed the scope to React-based static site generators.\nWe ended up comparing two: [NextJS] with static page export and [GatsbyJS]\nwhich exports static pages by default.\nFor page content we agreed that writing pages in [Markdown] is a good option,\nso we tested both with [MDX] which can also embed React components inside content pages.\n[NextJS] and [GatsbyJS] have different approaches to development and there are minor\nperformance differences between them.\nOur use case for now is narrow enough, and with the primary need of anyone in the team\nbeing able to update pages, [NextJS] marginally gets more votes as Gatsby cannot be installed and run on\nnon-developer machines.\n\n## Decision\n",We will use [NextJS] together with [MDX] to author PaaS product pages content in\n[Markdown] and deliver them to users as static pages.\n,To build the product pages using NextJS with static page export and MDX for page content.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](../ADR032-ssl-only-for-applications-and-cf-endpoints).\nTo add our domains to [HSTS preload lists](https://hstspreload.appspot.com/), there are these requirements:\n1. Serve a valid certificate.\n2. Redirect from HTTP to HTTPS on the same host.\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\n4. Serve an HSTS header on the base domain for HTTPS requests:\nWe need an endpoint to provide these requirements.\nOur Cloud Foundry app endpoint already [serves the\nright HSTS Security header with HAProxy](../ADR008-haproxy-for-request-rewriting)\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\nWe can implement a second ELB to listening on HTTP and HTTPS and use\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\nBut this increases our dependency on the HAProxy service.\nWe must serve from the root domain (or apex domain), but it is not allowed to\nserve [CNAME records in the root/apex domain](http://serverfault.com/questions/613829/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\nan issue when serving the service using ELB or CloudFront.\n\n## Decision\n","* We will implement a basic [AWS API Gateway](https://aws.amazon.com/api-gateway/)\nwith a default [MOCK response](https://aws.amazon.com/about-aws/whats-new/2015/09/introducing-mock-integration-generate-api-responses-from-api-gateway-directly/)\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\ncontent of the response is irrelevant, it can be a 302.\nA [Custom Domain Name](http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nwhich creates a [AWS Cloud Front distribution](http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html),\nwill provide public access to this API.\n* We will use [AWS Route 53 `ALIAS` resource record](http://docs.aws.amazon.com/Route53/latest/APIReference/CreateAliasRRSAPI.html)\nto [serve the IPs of the AWS Cloud Front distribution as A records](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html).\n","Implement a custom endpoint, using Nginx, and configure it to serve the HSTS header on the base domain for HTTPS requests. This endpoint should be served from the root domain (or apex domain) and should listen on both HTTP and HTTPS. Nginx should be configured to redirect from HTTP to HTTPS on the same host and serve all subdomains over HTTPS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nIn https://www.pivotaltracker.com/story/show/115252309 we described a pattern\nfor defining security groups between services and the VMs that connect to\nthem. This involved creating a client security group for each service that\nthe service allowed connections from. This client security group was then\napplied to the VMs that needed to be able to connect.\nThis has the problem that in some cases we would need to add several client\nsecurity groups to a given VM, and we are at risk of running into the AWS\nlimits (by default it's a max of 5 groups per interface).\n# Decision\nWe will use an alternative approach where a security group is defined for each\ncomponent (or component group) in the system. Services then define rules\nallowing connections from the components that need access. There are some\ninstances where a component will be both a client of other services and a\nprovider of a service. We will use the same security group for both of these\nroles.\nThis approach has a number of advantages:\n* The number of security groups applied to a VM is driven by the number of\ncomponents on that VM, not the number of things that they need to connect to.\nThe number of components is likely to be small, and we're better able to\ncontrol this that we can control the number of things a  component talks to.\n* It's easier to see what components connect to a given service by looking at\nthe rules for that service's security group.\n* When moving a component between VMs it's much clearer which security groups\nneed to move with it.\n# Status\nAccepted\n# Consequences\n* This will enable us to avoid hitting the AWS limits for the number of\nsecurity groups applied to a given interface.\n* It will make it clearer which components are allowed to connect to a given\nservice.\n* There will be some work to update our existing groups to use this pattern.\nThere are some potential issues:\n* There's the possibility for cycles in the Terraform dependency graph when we\nhave 2 components that both initiate connections to each other. This can be\navoided by using the `aws_security_group_rule` Terraform resource instead of\ndefining the rules inline in the `aws_security_group`.\n* When looking at a given VM, it's harder to see what services it connects to.\nThis is not seen as a major problem because it should be possible to create\ntooling to visualise this without much effort.\n\n## Decision\n","We will use an alternative approach where a security group is defined for each\ncomponent (or component group) in the system. Services then define rules\nallowing connections from the components that need access. There are some\ninstances where a component will be both a client of other services and a\nprovider of a service. We will use the same security group for both of these\nroles.\nThis approach has a number of advantages:\n* The number of security groups applied to a VM is driven by the number of\ncomponents on that VM, not the number of things that they need to connect to.\nThe number of components is likely to be small, and we're better able to\ncontrol this that we can control the number of things a  component talks to.\n* It's easier to see what components connect to a given service by looking at\nthe rules for that service's security group.\n* When moving a component between VMs it's much clearer which security groups\nneed to move with it.\n# Status\nAccepted\n# Consequences\n* This will enable us to avoid hitting the AWS limits for the number of\nsecurity groups applied to a given interface.\n* It will make it clearer which components are allowed to connect to a given\nservice.\n* There will be some work to update our existing groups to use this pattern.\nThere are some potential issues:\n* There's the possibility for cycles in the Terraform dependency graph when we\nhave 2 components that both initiate connections to each other. This can be\navoided by using the `aws_security_group_rule` Terraform resource instead of\ndefining the rules inline in the `aws_security_group`.\n* When looking at a given VM, it's harder to see what services it connects to.\nThis is not seen as a major problem because it should be possible to create\ntooling to visualise this without much effort.\n",Use an alternative approach where a security group is defined for each component (or component group) in the system. Services then define rules allowing connections from the components that need access.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\nIn Q2 2019 (August/September) we moved all of our platform secrets from AWS S3 to [Credhub](https://docs.cloudfoundry.org/credhub/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https://github.com/pivotal-cf/credhub-release/blob/master/docs/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\n\n## Decision\n","Credhub has the notion of a transitional certificate. As written in [their documentation](https://github.com/pivotal-cf/credhub-release/blob/master/docs/ca-rotation.md), a transitional certificate is\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\nThe new `rotate-certs` job has three tasks:\n- `remove-transitional-flag-for-ca`\n- `move-transitional-flag-for-ca`\n- `set-transitional-flag-for-ca`\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\n",Automate certificate rotation using Credhub.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe've been running with the cell provisioning policy in ADR017 since February.\nWe haven't ever run out of cell capacity, but we've observed that there's\nexcess capacity that we're paying for.\nAt the time that we wrote ADR017 we had fewer tenants and an individual\ntenant's quota was a much greater proportion of total memory size. In other\nwords a single tenant could conceivably use a greater proportion of our excess\ncapacity.\nCells are still deployed across 3 AZs.\nWe still don't have a way to autoscale the number of cells to meet demand, so\nwe need to ensure that we have surplus capacity for when we're not around.\nCells are almost completely uncontended; we're not experiencing CPU or disk I/O\ncontention and not all the cell memory is being used.\nOver the 3 month period from 1st August - 1st November\n- Usable memory (free + cached + buffered) is running between 77% and 92% of total cell memory\n- The maximum increase in memory usage over an exponentially smoothed average from a week previously was 36%\n- We're running at about 10% of our total container capacity\n- container usage has peaked at about 20% above the previous weeks\n- Average CPU usage is about 10%. We see daily peaks of 80%\n- reps think that about 50% of the capacity of cells is used\n- the largest amount that rep's allocated memory increased week on week was 55%\n\n## Decision\n","Our objectives are:\nState | Expected behaviour\n------|-------------------\n# All cells operational | Enough capacity to allow some but not all tenants to scale up to their full quota. The amount of excess capacity required should be enough to accommodate the fluctuations we can expect over a 3 day period (weekend + reaction time)\n# While CF being deployed | As above: enough capacity to allow some tenants to scale up to their full quota\n# One availability zone failed/degraded | Enough capacity to maintain steady state app usage. Not guaranteed to be able to scale apps up.\n# More than one AZ failed | The system is not expected to have sufficient capacity to host all running apps.\nTo achieve this we need to start basing our capacity planning on current memory\noccupied by processes on cells, rather than the sum of all quotas given to\nusers. We will define alerts for capacity planning purposes, the in-hours\nsupport person is expected to respond by adjusting the number of cells.\nWe want to ensure that cells have some headroom above a smoothed\naverage:\n- to allow some headroom for increases in the memory consumed by apps.\n- to allow buffering and caching to occur and not adversely impact application\nperformance.\nFrom our data analysis (see context) the amount of memory consumed by apps\ncan reach about 36% over a week-ago's smoothed average. We round up to 40% to\ninclude buffering/caching.\nIf an AZ fails, we need enough capacity remaining to host all our apps. The\nfailed AZ's apps are evenly divided amongst the surviving AZs. Because we have\ntwo remaining AZs, each surviving AZ will have 1.5x as many apps running.\nBecause we want 40% headroom, we'll want 1.4 (headroom) x 1.5 (evacuated apps)\ncurrent usage. This is about 2x actual memory consumed by processes on cells.\nTherefore we need to start alerting when the memory occupied by processes on\ncells is above 50%, when suitably smoothed to avoid noise / small spikes\ncausing frequent alarms.\nCPU usage is assumed to be a linear relation of memory usage and we will have a\nsimilar alert defined when it exceeds 50% on cells.\nIn addition to wanting the cells to not run short on memory, we also want\ntenants to be able to scale apps up and down when all AZs are functional. In\norder to ensure this, we need to allow for a ~50% increase in requested memory,\nwhich means alerting when all the reps have a cumulative remaining capacity of\n~33%, when smoothed to avoid false alarms.\nWe also need enough container capacity to allow tenants to scale apps up and\ndown and deploy new apps. We should alert when we're using > 80% of the sum of\nour reps' container capacity. Again, this should be smoothed to ensure that\nshort lived fluctuations in usage don't cause unnecessary alerts.\nIt is likely that patterns such as the fluctuation in memory use over a week\nmay change over time. We should review this decision after 6 months.\n",Reduce the number of cells and reduce the memory allocation for each cell to bring the memory used into line with actual usage patterns of tenant nodes.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe wanted to open up access to tenant applications in our production environment.\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\n* [#116104189 - set up Pingdom](https://www.pivotaltracker.com/story/show/116104189)\n* [#115347323 - allow public access to tenant applications](https://www.pivotaltracker.com/story/show/115347323)\nDecision\n========\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\nStatus\n======\nProposed\nConsequences\n============\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\n### Positive\n* We are now able to test accessibility using the staging environment\n* We are now able to use Pingdom to assert not just application health, but routing as well.\n* We have maintained consistency between staging and production\n### Negative\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\n* We would no longer have healthchecks via Pingdom for our CI environment.\n\n## Decision\n","========\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\nStatus\n======\nProposed\nConsequences\n============\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\n### Positive\n* We are now able to test accessibility using the staging environment\n* We are now able to use Pingdom to assert not just application health, but routing as well.\n* We have maintained consistency between staging and production\n### Negative\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\n* We would no longer have healthchecks via Pingdom for our CI environment.\n","Make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn https://www.pivotaltracker.com/story/show/115252309 we described a pattern\nfor defining security groups between services and the VMs that connect to\nthem. This involved creating a client security group for each service that\nthe service allowed connections from. This client security group was then\napplied to the VMs that needed to be able to connect.\nThis has the problem that in some cases we would need to add several client\nsecurity groups to a given VM, and we are at risk of running into the AWS\nlimits (by default it's a max of 5 groups per interface).\n\n## Decision\n","We will use an alternative approach where a security group is defined for each\ncomponent (or component group) in the system. Services then define rules\nallowing connections from the components that need access. There are some\ninstances where a component will be both a client of other services and a\nprovider of a service. We will use the same security group for both of these\nroles.\nThis approach has a number of advantages:\n* The number of security groups applied to a VM is driven by the number of\ncomponents on that VM, not the number of things that they need to connect to.\nThe number of components is likely to be small, and we're better able to\ncontrol this that we can control the number of things a  component talks to.\n* It's easier to see what components connect to a given service by looking at\nthe rules for that service's security group.\n* When moving a component between VMs it's much clearer which security groups\nneed to move with it.\n","**Decision:**\n**Client security groups for service mesh will be defined using AWS Network Firewall. AWS Network Firewall consumes one security group per interface, rather than one per rule.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are starting to develop a number of user-facing applications with web\ninterfaces that need to be GOV.UK branded. In order to keep things consistent\nwe want to pick a single programming language to write these in.\nWe've previously chosen Ruby on Rails as our desired framework in the previous\n[ADR022]. And whether or not it was a right choice, we decided to revisit and\nreconsider that choice.\nThe reason for that, is simply the direction GDS is heading at. It would appear\nthat old components will be soon deprecated and the Frontend Community has no\ndesire to support gems in the future. Saying that, we could have remain with\nRails and delegate the component management to NPM. This would however increase\nthe amount of possible maintenance work we would need to undertake, due to the\nuse of [nunjucks] by the Design System team.\nWe questioned the need of running Rails application for something that\nessentially is a templating system for existing data and API.\nAfter some more research, discussion with the head of the Frontend Community,\nmembers of the team and some others in GDS, the better choice would be Node for\nthe following reasons:\n* It's the way Frontend Community is heading at\n* It will be easier to rotate/onboard Frontend Developers\n* The initial applications are to be simple (thin layer between API calls and\nHTML parsing)\n* It's light and essentially is JavaScript\n* It supports [nunjucks] which will help us in maintenance\n\n## Decision\n",We will use Node to create new user-facing applications that render a web\ninterface for our service but will not be used to implement any significant\n‘application logic’.\n,Use Node.js for developing user-facing applications with web interfaces that need to be GOV.UK branded.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe want to serve [HSTS\nheaders](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security) for all\nHTTPS requests to the apps domains, but it will safeguard existing users from\nbeing MITMed over insecure connections and it will improve the user experience\nwhen they click on a hostname that doesn't have a protocol.\n(Note that without pre-loading in browsers this won't help first time users,\nbut that is out of context)\nWe want to leave open the option of able to override these headers from\nthe tenant application if they wish.\nThis feature requires conditionally process and modify the request headers.\nThere are several possible implementations:\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\nand add the header if required, by:\n* Supporting the specific HSTS headers, and allowing configure some\nsort of behaviour and default value.\n* Allow inject any additional header if they are missing.\nBut [current `gorouter` implementation](https://github.com/cloudfoundry/gorouter/commit/0d475e57b1742c42ba6d98d1ed853edc9f709893)\ndoes not support any of these features, which require being added.\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\nthe go-routers and after the ELB.\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\nAll the commercial CDN have the capacity to add additionally headers\nconditionally.\n4. AWS ELB: They do not support this logic and will not in the short term.\nIn consequence they cannot be used to solve this problem.\nDecision\n========\nWe do not want to add any additional logic in the CDN, as they will\nbe an optional part of the platform and we will try to keep as simple\nas possible.\nWe consider that the optional solution would be implement this logic in\nthe `gorouter`, but that requires some development effort and a PR being merged\nupstream.\nBecause that we will implement, in the short term, the second option: a proxy\nin front of the `gorouter`.\n* We will implement [HAproxy](http://www.haproxy.org/) in front of the go router.\n* Ha-proxy is the default LB solution for the official CF distribution.\n* It is really powerful and has good support.\n* Enough features to cover our needs.\n* It will be setup colocated with the `gorouter`, proxying directly to\nlocalhost.\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\nis OK as the two services are colocated in the same VM.\n* We will reuse the code from [official haproxy job from cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/haproxy),\nalthough we will have to fork it to add additional settings in the\nhaproxy configuration.\nFuture work:\n* We will implement and propose a PR to add logic in go-router to allow\ndefine additional headers.\nStatus\n======\nAccepted\nConsequences\n============\n### Positive\n* We will be able to easily add more logic to rewrite the HTTP communication\nto the applications using HAProxy.\n* HAProxy SSL termination has better performance than `gorouter`, although\nthis has a low impact because ELB is terminating the end user connections\nand using keep alive connections to the gorouter/haproxy.\n* HAProxy supports web-sockets and does HTTP multiplexing.\n* We can implement HTTP => HTTPS redirect in HAProxy.\n### Negative\n* Adds some additional latency to every request.\n* We have to maintain our custom haproxy release.\n* Another moving part to monitor and take into account.\n### See Also\n[ADR012](ADR012-haproxy-healthcheck/)\n\n## Decision\n","========\nWe do not want to add any additional logic in the CDN, as they will\nbe an optional part of the platform and we will try to keep as simple\nas possible.\nWe consider that the optional solution would be implement this logic in\nthe `gorouter`, but that requires some development effort and a PR being merged\nupstream.\nBecause that we will implement, in the short term, the second option: a proxy\nin front of the `gorouter`.\n* We will implement [HAproxy](http://www.haproxy.org/) in front of the go router.\n* Ha-proxy is the default LB solution for the official CF distribution.\n* It is really powerful and has good support.\n* Enough features to cover our needs.\n* It will be setup colocated with the `gorouter`, proxying directly to\nlocalhost.\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\nis OK as the two services are colocated in the same VM.\n* We will reuse the code from [official haproxy job from cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/haproxy),\nalthough we will have to fork it to add additional settings in the\nhaproxy configuration.\nFuture work:\n* We will implement and propose a PR to add logic in go-router to allow\ndefine additional headers.\nStatus\n======\nAccepted\nConsequences\n============\n### Positive\n* We will be able to easily add more logic to rewrite the HTTP communication\nto the applications using HAProxy.\n* HAProxy SSL termination has better performance than `gorouter`, although\nthis has a low impact because ELB is terminating the end user connections\nand using keep alive connections to the gorouter/haproxy.\n* HAProxy supports web-sockets and does HTTP multiplexing.\n* We can implement HTTP => HTTPS redirect in HAProxy.\n### Negative\n* Adds some additional latency to every request.\n* We have to maintain our custom haproxy release.\n* Another moving part to monitor and take into account.\n### See Also\n[ADR012](ADR012-haproxy-healthcheck/)\n","Implement HAProxy as a proxy in front of the gorouter to conditionally process and modify request headers, including HSTS headers."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to provide tenants with the ability to provision databases for use in\ntheir applications. Our first iteration of this will be using RDS.\nWe investigated some implementations of a service broker which supported RDS\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\n- [18F](https://github.com/18F/rds-service-broker)\n\n## Decision\n,"We will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\n",Use the RDS service broker from 18F
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\n\n## Decision\n","We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\n","**Decision:** Simplify the deployment pipeline by removing the Staging environment.\n\n**Rationale:**\n\n* The Staging environment is not providing significant value compared to the time and cost overheads it incurs.\n* Removing the Staging environment will reduce the time it takes for changes to reach Production, improving the overall efficiency of the deployment process.\n* Simplifying the pipeline will also reduce the operational complexity, making it easier to manage and maintain.\n\n**Implementation:**\n\n* Remove the Staging environment from the deployment pipeline configuration.\n* Update the CI/CD process to promote changes directly from CI to Production, with appropriate safeguards in place (e.g., automated testing, manual approvals).\n* Monitor and evaluate the performance of the simplified pipeline to ensure it meets the desired goals of faster deployments and reduced overhead."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS uses [AWS Shield Advanced](https://aws.amazon.com/shield/features/#AWS_Shield_Advanced) as well as AWS WAF to protect from DDoS attacks.\nHowever the mitigations are not automatic and we have access to the AWS DDoS Response Team\n(DRT) who are experts in mitigating these types of attack.\n```\nShield Advanced detects web application layer vectors, like web request floods and\nlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web\napplication layer vectors, you must employ AWS WAF rules or the DRT must employ the\nrules on your behalf.\n```\nIn order to be functional they require access to our AWS WAF logs in order to identify what\nthe attack is and where is is coming from, and API access to the WAF in order to apply the\nmitigating rules.\nTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger\nthe [emergency engagement Lambda](https://s3.amazonaws.com/aws-shield-lambda/ShieldEngagementLambda.pdf)\n\n## Decision\n",We will grant access to the AWS DRT to read from restricted S3 buckets\n,**Decision:** To grant the AWS DDoS Response Team (DRT) read-only access to our AWS WAF logs and API access to AWS WAF to enable them to mitigate web application layer DDoS attacks.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn ADR008 and ADR014 we decided to use HAProxy, for three reasons:\n- Writing HSTS header if they are not present in the upstream request\n- Implementing HTTP -> HTTPS redirect\n- Custom health check for Gorouter\nThese problems have since been fixed:\n- Header rewriting was implemented in [v0.183.0](https://github.com/cloudfoundry/routing-release/releases/tag/0.183.0)\n- HTTP healthchecking was implemented in [v0.139.0](https://github.com/cloudfoundry/routing-release/releases/tag/0.139.0)\n- HTTP -> HTTPS redirect can be done using AWS ALBs\nWe currently use multiple ELBs (classic) which we want to replace with ALBs.\nWe want to use ALBs because:\n- ELBs are deprecated in terraform and cause crashes\n- ALBs can give us more metrics in CloudWatch\n- ALBs have better support for X-Forwarded-For\n- ALBs support fixed-response which can be used for HTTP -> HTTPS rewriting\nHAProxy adds significant complexity to our routing deployment and maintenance:\n- Proxy Protocol is non-standard and hard to understand\n- HTTP -> HTTPS rewriting is hard to understand\n- HAProxy config is rarely touched\n- We have to maintain our own HAProxy BOSH release\n- HAProxy duplicates the number of logs we receive because every platform request is written twice\n- HAProxy adds an extra network hop for every request\n\n## Decision\n",- Replace ELBs with ALBs\n- Use ALB fixed-response to redirect HTTP -> HTTPS\n- Use Gorouter directly for:\n- TLS termination\n- HSTS header rewriting\n- Healthchecking the router instance\n- Remove HAProxy\n,Deprecate HAProxy and use ALBs for routing traffic.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\n![pipeline image](images/pipeline.jpg)\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\n### Naming considerations\nA number of aspects were considered as part of the naming process.\n* Clear sense of purpose\n* Clear distinction between Production and other Environments\n* No overly technical names (for example, hosting/paas/scalable-elastic-public-government-hosting)\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\nDecision\n========\nFor _non_ production environments we will be using the following domains:\n* [environment name].cloudpipeline.digital\n* [app name].[environment name].cloudpipelineapps.digital\nFor our production environment we will be using the following domains:\n* cloud.service.gov.uk\n* [app name].cloudapps.digital\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\nDomain Overview\n===============\n### Development Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.foo.dev.cloudpipeline.digital\nCloud Foundry API | api.foo.dev.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.foo.dev.cloudpipeline.digital\nApplications | bar.foo.dev.cloudpipelineapps.digital\n### Continuous Integration (CI) Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.master.ci.cloudpipeline.digital\nCloud Foundry API | api.master.ci.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.master.ci.cloudpipeline.digital\nApplications | bar.master.ci.cloudpipelineapps.digital\n### Staging Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.staging.cloudpipeline.digital\nCloud Foundry API | api.staging.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.staging.cloudpipeline.digital\nApplications | bar.staging.cloudpipelineapps.digital\n### Production Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.cloud.service.gov.uk\nCloud Foundry API | api.cloud.service.gov.uk\nCloud Foundry User Account and Authentication | uaa.cloud.service.gov.uk\nApplications | bar.cloudapps.digital\nStatus\n======\nAccepted\nConsequences\n============\nCertificates etc. had to be purchased, domains registered, and our automated deployments configured to allow us to specify the domains for each stage of the pipeline.\n\n## Decision\n","========\nFor _non_ production environments we will be using the following domains:\n* [environment name].cloudpipeline.digital\n* [app name].[environment name].cloudpipelineapps.digital\nFor our production environment we will be using the following domains:\n* cloud.service.gov.uk\n* [app name].cloudapps.digital\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\nDomain Overview\n===============\n### Development Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.foo.dev.cloudpipeline.digital\nCloud Foundry API | api.foo.dev.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.foo.dev.cloudpipeline.digital\nApplications | bar.foo.dev.cloudpipelineapps.digital\n### Continuous Integration (CI) Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.master.ci.cloudpipeline.digital\nCloud Foundry API | api.master.ci.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.master.ci.cloudpipeline.digital\nApplications | bar.master.ci.cloudpipelineapps.digital\n### Staging Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.staging.cloudpipeline.digital\nCloud Foundry API | api.staging.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.staging.cloudpipeline.digital\nApplications | bar.staging.cloudpipelineapps.digital\n### Production Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.cloud.service.gov.uk\nCloud Foundry API | api.cloud.service.gov.uk\nCloud Foundry User Account and Authentication | uaa.cloud.service.gov.uk\nApplications | bar.cloudapps.digital\nStatus\n======\nAccepted\nConsequences\n============\nCertificates etc. had to be purchased, domains registered, and our automated deployments configured to allow us to specify the domains for each stage of the pipeline.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\nThe PaaS has several environments which will need to use Aiven. These\nenvironments should be isolated from each other so that changes made in testing\nand development environments do do not affect production users.\nAiven provide a ""Project"" abstraction where a user can be a member of several\nprojects. API tokens are user specific. By creating one user per project it's\npossible to scope API tokens to a project.\n\n## Decision\n","We'll use separate projects for separate environments, initially using the\nfollowing Aiven projects:\n* ci-testing (for the CI environment for the elasticsearch broker itself)\n* paas-cf-dev\n* paas-cf-staging\n* paas-cf-prod\nFor staging and prod we will use separate API tokens within the same project to\nseparate credentials between the London and Ireland regions.\nWe will have the following per-project users to hold API tokens:\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\nThe credentials for the ci and dev users will be stored in the\n`paas-credentials` passwordstore. staging and prod will be stored in\n`paas-credentials-high`.\nMembers of the PaaS team will each have their own user which will have access\nall of the projects for management purposes.\n","Use Aiven's ""Project"" abstraction to create one user per PaaS environment. Use\nAPI tokens to authenticate with the Elasticsearch backing service, with each API\ntoken scoped to a single project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNote: This has been superceeded. See [Status](#status) below.\nIt is expected for the government websites to be secure and keep the user\ninteractions private. Because that we want to enforce all communications to\nany application and to the platform endpoints to use only and always HTTPS,\nas [it is described in the Gov Service Manual](https://www.gov.uk/service-manual/technology/using-https).\nWhen a user inputs a website name without specifying the\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\nEven if the server always redirect HTTP to HTTPS, an initial\nunprotected request including user information will be transferred\nin clear: full URL with domain, parameter, [cookies without secure flag](https://en.wikipedia.org/wiki/HTTP_cookie#Secure_and_HttpOnly)\nor browser meta-information.\n[HTTP Strict Transport Security](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security)\nmitigates this issue by instructing modern browsers that support it to\nalways connect using HTTPS.\nThis is also a [requirement in the service manual](https://www.gov.uk/service-manual/technology/using-https).\nThere is still a potential initial unprotected HTTP request that might happen\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\nTo solve this issue, the root domain can be added to\n[HSTS preload list](https://hstspreload.appspot.com/) which will be used by most\ncommon browsers.\nCurrently the only way to avoid any clear text HTTP interaction is closing or\ndropping any attempt to connect to the port 80 at TCP level.\nAlthough not all application deployed on the PaaS will be ""services""\nas in the service manual meaning, we must not allow HTTP to make\nit easier to service owners to comply with this requirements.\n\n## Decision\n",We will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\nWe will implement and maintain HSTS preload lists for our production domains.\n,"To make it easier for service owners to comply with the Government Service Manual requirement to encrypt all communication, any TCP connection over port 80 should be closed or dropped at TCP level."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWhen building pipelines using concourse, we investigated using the [pool\nresource](https://github.com/concourse/pool-resource) in order to control flow\nthrough jobs. This was an alternative to the use of the\n[semver resource](https://github.com/concourse/semver-resource).\nThese 2 resources are both workarounds to solve the problem of triggering jobs\nwhen we haven't made changes to a resource.\nThe problem is that the pool resource relies on write access to a github repo,\nwhich means we must pass public keys that allow this access into the pipeline\nand deployed concourse instance - we want to minimise the number of credentials\nwe pass, and the semver resource relies on AWS credentials that are already\npassed.\nDecision\n========\nWe will not use the pool resource for flow between jobs - instead we will use\nthe semver resource\nStatus\n======\nAccepted\nConsequences\n============\nThis was an investigation into a different approach, so no consequences\n\n## Decision\n","========\nWe will not use the pool resource for flow between jobs - instead we will use\nthe semver resource\nStatus\n======\nAccepted\nConsequences\n============\nThis was an investigation into a different approach, so no consequences\n","Use the semver resource for flow between jobs in pipelines, instead of the pool resource."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\nThis is a form of end-to-end encryption.\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\nThis is a form of end-to-end encryption.\nThe [cf-dev mailing list](https://lists.cloudfoundry.org/g/cf-dev/message/9143) alleges\nthat the IPSec release is no longer maintained.\n\n## Decision\n,We will not run IPSec in our BOSH deployments.\n,To remove IPSec from GOV.UK PaaS.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to serve [HSTS\nheaders](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security) for all\nHTTPS requests to the apps domains, but it will safeguard existing users from\nbeing MITMed over insecure connections and it will improve the user experience\nwhen they select a hostname that doesn't have a protocol.\n(Note that without pre-loading in browsers this won't help first time users,\nbut that is out of context)\nWe want to leave open the option of able to override these headers from\nthe tenant application if they wish.\nThis feature requires conditionally process and modify the request headers.\nThere are several possible implementations:\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\nand add the header if required, by:\n* Supporting the specific HSTS headers, and allowing configure some\nsort of behaviour and default value.\n* Allow inject any additional header if they are missing.\nBut [current `gorouter` implementation](https://github.com/cloudfoundry/gorouter/commit/0d475e57b1742c42ba6d98d1ed853edc9f709893)\ndoes not support any of these features, which require being added.\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\nthe go-routers and after the ELB.\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\nAll the commercial CDN have the capacity to add additionally headers\nconditionally.\n4. AWS ELB: They do not support this logic and will not in the short term.\nIn consequence they cannot be used to solve this problem.\n\n## Decision\n","We do not want to add any additional logic in the CDN, as they will\nbe an optional part of the platform and we will try to keep as simple\nas possible.\nWe consider that the optional solution would be implement this logic in\nthe `gorouter`, but that requires some development effort and a PR being merged\nupstream.\nBecause that we will implement, in the short term, the second option: a proxy\nin front of the `gorouter`.\n* We will implement [HAproxy](http://www.haproxy.org/) in front of the go router.\n* Ha-proxy is the default LB solution for the official CF distribution.\n* It is really powerful and has good support.\n* Enough features to cover our needs.\n* It will be setup colocated with the `gorouter`, proxying directly to\nlocalhost.\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\nis OK as the two services are colocated in the same VM.\n* We will reuse the code from [official haproxy job from cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/haproxy),\nalthough we will have to fork it to add additional settings in the\nhaproxy configuration.\nFuture work:\n* We will implement and propose a PR to add logic in go-router to allow\ndefine additional headers.\n",1. Implement the logic in the `gorouter` itself.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS runs a fork of the [RDS Service Broker](https://github.com/alphagov/paas-rds-broker) and uses it to provide Postgres database services\nto its tenants.\nAs of writing, our service plan 'menu' looks like this:\n```\n$ cf marketplace -e postgres\nGetting service plan information for service offering postgres in org gds-tech-ops / space sandbox as 117196824971928474330...\nbroker: rds-broker\nplan                            description                                                                                                                                                                          free or paid   costs   available\ntiny-unencrypted-10             5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.micro. Free for trial orgs. Costs for billable orgs.    free                   yes\nsmall-10                        20GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.                                            paid                   yes\nsmall-ha-10                     20GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.                          paid                   yes\nmedium-10                       100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.                                           paid                   yes\nmedium-ha-10                    100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.                         paid                   yes\nlarge-10                        512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.                                        paid                   yes\nlarge-ha-10                     512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.                      paid                   yes\nxlarge-10                       2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.                                          paid                   yes\nxlarge-ha-10                    2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.                        paid                   yes\ntiny-unencrypted-11             5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.    free                   yes\nsmall-11                        100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-11                     100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-11                       100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-11                    100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-11                        512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.                                        paid                   yes\nlarge-ha-11                     512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.                      paid                   yes\nxlarge-11                       2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                                          paid                   yes\nxlarge-ha-11                    2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                        paid                   yes\ntiny-unencrypted-10-high-iops   25GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.   free                   yes\nsmall-10-high-iops              100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-10-high-iops           100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-ha-10-high-iops          500GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-10-high-iops              2.5TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.2xlarge.                                        paid                   yes\nlarge-ha-10-high-iops           2.5TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.2xlarge.                      paid                   yes\nxlarge-10-high-iops             10TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.4xlarge.                                         paid                   yes\nxlarge-ha-10-high-iops          10TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.4xlarge.                       paid                   yes\ntiny-unencrypted-11-high-iops   25GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.   free                   yes\nsmall-11-high-iops              100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-11-high-iops           100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-11-high-iops             500GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-11-high-iops          500GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-11-high-iops              2.5TB Storage Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.                                paid                   yes\nlarge-ha-11-high-iops           2.5TB Storage Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.              paid                   yes\nxlarge-11-high-iops             10TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                                         paid                   yes\nxlarge-ha-11-high-iops          10TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                       paid                   yes\nmedium-10-high-iops             500GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.large.                                           paid                   yes\ntiny-unencrypted-12             5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.    free                   yes\nsmall-12                        100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-12                     100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-12                       100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-12                    100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-12                        512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.                                        paid                   yes\nlarge-ha-12                     512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.                      paid                   yes\nxlarge-12                       2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                                          paid                   yes\nxlarge-ha-12                    2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                        paid                   yes\ntiny-unencrypted-12-high-iops   25GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.   free                   yes\nsmall-12-high-iops              100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-12-high-iops           100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-12-high-iops             500GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-12-high-iops          500GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-12-high-iops              2.5TB Storage Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.                                paid                   yes\nlarge-ha-12-high-iops           2.5TB Storage Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.              paid                   yes\nxlarge-12-high-iops             10TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                                         paid                   yes\nxlarge-ha-12-high-iops          10TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                       paid                   yes\n```\nThere's some background information needed to understand some decisions made when setting up our plans:\n* [AWS's gp2 storage](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2) scales at 3 IOPS per GB of volume size, between a base of 100 IOPS at 33.33GB\ndisk size up to 16kIOPS at 5,334GB disk size.\n* AWS does not provide gp3 storage for RDS.\n* [RDS storage](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html) does not allow you to decrease storage of a database instance, or increase storage of\na database instance without adding at least 10%.\nWe have identified several inconsistencies and issues with the current plans when studying them in\norder of volume size.\n## Problems and potential options\nReplacing plans in this section refers to:\n* renaming the existing plan to have a `-deprecated` suffix\n* marking it private\n* scoping it down to only the existing spaces using it\n* creating a new one under its old name, with new configuration\nThis way, we avoid problems with billing and existing service instances using these plans.\n### Problem 1\nThere is no upgrade path available from `medium` `high-iops` plans to `large` plans, because of the\nRDS 10% storage increase rule - 500GB to 512GB would violate it.\n1. We could replace the `medium` `high-iops` plans with ones of smaller disk space, say 465GB (`⌊512/1.1⌋`).\n* This would leave existing users of these plans (6 instances) unable to upgrade (both in terms\nof plan size and database version) without going up to `large`.\n2. We could replace the `large` plans with ones of larger disk space, say 564GB (`⌈512*1.1⌉`).\n### Problem 2\nThe `small` plans are inconsistent - `small-10` (and `small-ha-10`) provide 20GB disk space, but\nthe Postgres 11 and 12 variants provide 100GB disk space.\n1. We could replace the `small-11`, `small-ha-11`, `small-12`, and `small-ha-12` plans with 20GB\neditions.\n* This would leave existing users of these plans (170 instances) unable to upgrade (both in\nterms of plan size and database version) without going up to `small` `high-iops` or `medium`.\n2. We could replace the `small-10` and `small-ha-10` plans with 100GB editions.\n* This would make approximately zero sense as they'd be identical to the `small-10-high-iops`\nand `small-ha-10-high-iops plans`.\n3. We could do nothing and leave it be. It's not technically breaking anything irght now.\n* This might create a dilemma when we come to introduce `small-13` and `small-ha-13`. Should it\nbe 20GB or 100GB? `small-13-high-iops` and `small-ha-13-high-iops` will be 100GB so it would\nbe illogical, however if we went for 20GB, there would be no `small-12` -> `small-13` upgrade path.\n### Problem 3\nThe Postgres engine version is part of the plans themselves. It is not relevant to billing or\nparticularly how the underlying resource is structed on AWS.\n1. We could make it a config option.\n* This might make the broker more complex as we'd have to handle existing instances.\n2. We could do nothing and leave it be.\n* This means a lot of duplication and potential for inconsistency (e.g., as we've seen between\nsmall-10 and small-12).\n### Problem 4\nThe disk space is part of the plans but this leads to massive jumps in plan whenever you just want\na bit more disk space.\n1. We could make it a config option.\n* We'd have to factor this into billing.\n* This might make the broker more complex as we'd have to handle existing instances.\n2. We could do nothing and leave it be.\n* Problem to solve in a future PaaS? Would future PaaS even want to do service plans? Probably,\nbut we shouldn't pre-empt it.\n### Problem 5\nIt's not clear that users know there could be legitimate reasons to want to upgrade from\n`medium` `high-iops` to `large` (non-`high-iops`). If a user doesn't understand the disk space\n<-> IOPS link, they might have `medium` `high-iops`, need more RAM/a bigger CPU/more connections,\nand decide straight to `large` `high-iops` instead of plain `large`.\n1. We could add IOPS information to each plan's description.\n* These descriptions are already quite wordy and this would make things worse.\n2. We could do nothing and leave it be.\n### Problem 6\nThe `xlarge` `high-iops` plans provide a disk larger than that needed to achieve maximum\nAWS gp2 IOPS.\n1. We could make future xlarge `high-iops` plans 5,334GB in disk size.\n* However there would then be no upgrade path for the current user.\n2. We could do nothing and leave it be.\n\n## Decision\n",We will take these actions:\n* Problem 1: Action 2 - replace the `large` plans with 564GB versions\n* Problem 2: Action 3 - do nothing for now\n* Problem 3: Action 2 - do nothing for now\n* Problem 4: Action 2 - do nothing for now\n* Problem 5: Action 2 - do nothing for now\n* Problem 6: Action 2 - do nothing for now\n,"### Decision\nHere are the decisions we have made and are planning to implement:\n- **Problem 1:** Replace the `medium-10-high-iops` and `medium-11-high-iops` plans with\n100GB disk space equivalents (465GB is not a valid disk size in AWS).[1]\n- **Problem 2:** Replace the `small-10` and `small-ha-10` plans with 100GB disk space\nequivalents, and remove the other `small` plans which provide 20GB disk space.[1]\n- **Problem 3:** Make the Postgres engine version a config option.[4]\n- **Problem 4:** Do nothing and leave it be.[2]\n- **Problem 5:** Do nothing and leave it be.[2]\n- **Problem 6:** Do nothing and leave it be.[2]\n\n[1]: This is potentially backwards-incompatible, as a user could be using these plans on the\nbasis of disk size, rather than IOPS. However, as disk space on AWS RDS cannot be reduced\nwithout data loss, the chances of this are low.\n[2]: The current plan structure has been built up over a period of years, and it's not\nclear that any alternative would be better. It would have been better to design them in a\nmore structured way from the beginning, but that is difficult to do in practice (for example,\nthe type and number of RDS instances which can be provisioned using a particular RDS provisioned\nIOPS value changes over time).\n[4]: This will have backwards-compatibility implications as existing service instances which are\nconfigured with a particular Postgres version will need to be migrated to using a config option.\nThis is sub-optimal, but until we can finalise a plan for a future PaaS setup (and whether\nservice plans will exist within it), it is the least disruptive option.\n### Alternatives\nAs mentioned in the options section, here are some alternative decisions which could be made:\n- **Problem 1:** Replace the `large` plans with 564GB equivalents, which would allow the\n`medium` `high-iops` plans to be upgraded directly to `large`.\n- **Problem 2:** Replace the `large` plans with 100GB equivalents, and then create small\nplans with 20GB equivalents.\n- **Problem 4:** Make the disk space a config option.\n- **Problem 6:** Make future large `high-iops` plans 5,334GB in disk size.\n### Reason\nThese decisions have been made based on the following reasoning:\n- **Problem 1:** Replacing the `medium` `high-iops` plans with 100GB disk space equivalents\navoids introducing a new 564GB disk size which would not have an upgrade path from `small`\nplans. It is the lesser inconvenience to existing users.\n- **Problem 2:** Replacing the `small-10` and `small-ha-10` plans with 100GB disk space\nequivalents and removing the other `small` plans will address the inconsistency in plan size\nfor Postgres 10, 11, and 12.\n- **Problem 3:** Making the Postgres engine version a config option will make it more flexible\nto change the Postgres version without requiring a plan replacement. It is the most backward-\ncompatible and future-proof option, and will allow us to remove the current plan\nduplication.\n- **Problem 4:** Doing nothing and leaving it be is the least disruptive option for existing users.\n- **Problem 5:** Doing nothing and leaving it be is the easiest option, and the consequences are\nmild.\n- **Problem 6:** Doing nothing and leaving it be is the least disruptive option for existing users.\n\n### Conclusion\nThese decisions will be implemented over the coming weeks.\n\n### non-functional decisions\n### Performance\nThe performance of the service will be unaffected.\n### Scalability\nThe scalability of the service will be unaffected.\n### Availability\nThe availability of the service will be unaffected.\n### Security\nThe security of the service will be unaffected.\n### Reliability\nThe reliability of the service will be unaffected.\n### Testability\nThe testability of the service will be unaffected.\n### Maintainability\nThe maintainability of the service will be unaffected.\n### Cost\nThe cost of the service will be unaffected."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\n```\nMemory reserved by orgs: 368640 MB (360 GB)\nMemory reserved by apps: 107108 MB (104 GB)\nMemory actually used by apps: 32868 (32 GB)\n```\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\n\n## Decision\n","We will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\n",We will provision 2x memory capacity per AZ.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\n![pipeline image](../images/pipeline.jpg)\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\n### Naming considerations\nA number of aspects were considered as part of the naming process.\n* Clear sense of purpose\n* Clear distinction between Production and other Environments\n* No overly technical names (for example, hosting/paas/scalable-elastic-public-government-hosting)\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\n\n## Decision\n","For _non_ production environments we will be using the following domains:\n* [environment name].cloudpipeline.digital\n* [app name].[environment name].cloudpipelineapps.digital\nFor our production environment we will be using the following domains:\n* cloud.service.gov.uk\n* [app name].cloudapps.digital\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\n","**Use a subdomain for the system domains and a separate subdomain for app domains.**\n\n**Rationale:**\n\n* This approach provides a clear sense of purpose for each subdomain, with system domains handling management and monitoring tasks and app domains hosting application traffic.\n* The distinction between Production and other environments is maintained by using different subdomains for each environment.\n* The names chosen avoid overly technical terms and are easy to understand and remember.\n* The use of a separate subdomain for app domains prevents confusion between system and app domains and eliminates the possibility of domains suggesting live services."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe want to enable, or provide the option to enable,\n[storage encryption (AKA encryption at rest) for the RDS instances](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html)\nof the tenants databases created by our [RDS broker](https://github.com/alphagov/paas-rds-broker). The broker has logic to enable this option on creation.\nBut there are [some limitations](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations):\n* Storage Encryption can only be enabled on creation of the DB. There is no way to update an instance to enable or disable encryption. The only way is by creating a encrypted (or unencrypted) copy of a snapshot, to then restore it to a DB instance.\n* Storage Encryption is only supported [for some instance types](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#d0e47573). Specifically it is not supported for `db.t2.small` and `db.t2.micro.`, used in our Small and Free plans.\nIn consequence:\n* Users **can not** migrate from a plan with encryption (Medium or Large) to a plan without encryption (Small or Free). But this use case is less common.\n* Users **can** migrate from Free/Small to a plan with encryption (Medium or Large), but the instance will not have encryption enabled.\n* If we enable encryption in the existing plans, the existing databases will remain without encryption.\n* Due the API broker limitations, it is not possible to query the attributes of an existing instance.\nWe have 3 options to proceed:\n1. Enable Encryption in the Medium and Large plans, and document the restrictions.\n* Less effort to implement.\n* We might end having unencrypted database in a plan that is meant to be encrypted, which is confusing for the users and operators.\n* Existing Instances will remain unencrypted.\n2. Change the instance type for the Small plan to `db.m3.medium`.\n* Would allow migrate from Small to Medium or Large.\n* We will still have the problem for the Free plan.\n* Increases the costs for the Small plan (double).\n3. Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption.\n* It would be more explicit and clear, and the plan would match the state of the existing database.\n* Existing instances would still match with the plan description.\n* We will add more plans, which makes it more confusing for the tenants.\nDecision\n========\nWe decided to provide additional explicit plans with Encryption enabled, and keep the old ones.\nWe will add logic in the broker to prevent updates between plans with or without encryption.\nWe have decided only add the option of encryption to the HA plans to minimise the number of new plans added. In most of the cases the tenants would choose HA together with encryption and, although adding more plans is easy, removing them is painful once they are being used.\nStatus\n======\nAccepted.\nConsequences\n============\n* More plans will be added to the offer, which is more confusing the tenants. Better documentation would be required to help the tenants to pick the right plan.\n* It is not possible to migrate from a Non-Encrypted plan to an Encrypted plan, and vice versa.\n\n## Decision\n","========\nWe decided to provide additional explicit plans with Encryption enabled, and keep the old ones.\nWe will add logic in the broker to prevent updates between plans with or without encryption.\nWe have decided only add the option of encryption to the HA plans to minimise the number of new plans added. In most of the cases the tenants would choose HA together with encryption and, although adding more plans is easy, removing them is painful once they are being used.\nStatus\n======\nAccepted.\nConsequences\n============\n* More plans will be added to the offer, which is more confusing the tenants. Better documentation would be required to help the tenants to pick the right plan.\n* It is not possible to migrate from a Non-Encrypted plan to an Encrypted plan, and vice versa.\n","Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur service plans have evolved incrementally over the last few years and are in\nneed of some attention. Names are inconsistent, potentially confusing and\nin many cases contain irrelevant redundant information that is of no practical\nuse to the platform operators or to tenants consuming the service.\nAdding additional versions of services has the potential to compound the\nproblem by multiplying plans of different characteristics with different\nversions.\n\n## Decision\n","We have decided to use the following naming convention for naming plans going forward:\n```\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\n```\nWhere:\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\n* `HA` is the string `ha` to indicate highly available if relevent.\n* `LABEL` is a string describing some specific variant of the service if relvent.\n* `VERSION` is the version number of the service plan.\n### For example:\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\n### Example migrations of some existing plans:\n```\nL-HA-enc-dedicated-9.5 => large-ha-9.5\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\ntiny-clustered => tiny-clustered-3.2\ntiny-unclustered => tiny-unclustered-3.2\n```\n### Additionally:\n* We will avoid use of the word ""free"" in names.\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\n* We will avoid use of uppercase characters in names.\n* We will avoid abbriviations where possible (""medium"" instead of ""M"", ""large"" instead of ""L"").\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\n","**Standardize Service Plan Naming**\n\n**Goal:**\nCreate a consistent and meaningful naming convention for service plans to improve clarity and reduce confusion.\n\n**Rationale:**\n* Inconsistent and redundant plan names make it difficult for platform operators and tenants to identify and manage services.\n* Different versions of services exacerbate the problem, leading to a proliferation of plan options with varying characteristics.\n* A standardized naming convention will provide a clear and concise way to differentiate between plans, simplify navigation, and streamline service management.\n\n**Implementation Details:**\n\n* Establish a set of guidelines for plan naming that defines:\n    * Clear and descriptive prefixes and suffixes\n    * Removal of unnecessary or redundant information\n    * Consistent formatting and capitalization\n* Review existing plans and rename them according to the new naming convention.\n* Implement automated validation rules to ensure new plans adhere to the naming standards.\n* Update documentation and communication materials to reflect the new plan names.\n\n**Benefits:**\n\n* Improved clarity and understanding of service plans\n* Reduced confusion and ambiguity for platform operators and tenants\n* Simplified management and navigation of service offerings\n* Consistency and alignment between plans and their corresponding services\n* Reduced risk of misconfigurations due to plan confusion"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTenants provision a wide variety of services and AWS/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry.\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\nIn the GOV.UK PaaS billing rewrite, this has been changed. We want to calculate bills for variable time periods and also to forecast bills for the future (for the web-based billing calculator). However, the method to calculate the actual bill always needs to be the same.\n\n## Decision\n","The code to calculate the bill has been decoupled from the code used to calculate the bill. This is so we can use exactly the same code for calculation of all bills, whether these bills are for tenants or for prospective tenants (using the billing calculator).\nThe approach taken is:\n1. Populate database temporary table with what is being billed (which resources over which time interval, including the future). This can be in a stored function or embedded SQL. This is the code entry point into billing.\n2. Call a stored function to calculate the bill ([`calculate_bill`](https://github.com/alphagov/paas-billing/blob/main/billing-db/sprocs/calculate_bill.sql)) only using the contents of the temporary table populated in step 1 above. No parameters are passed into ([`calculate_bill`](https://github.com/alphagov/paas-billing/blob/main/billing-db/sprocs/calculate_bill.sql)).\n","**Decision:** Introduce and use database tables to store only the latest state of billing data, to avoid garbage collection and provide the most up-to-date data for billing periods."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are starting to develop a number of user-facing applications with web\ninterfaces that need to be styled to look like GOV.UK etc. In order to keep\nthings consistent we want to pick a single programming language and framework\nto write these in.\nWe've previously used [Sinatra][] for this, but ran into\nissues with its default configuration which isn't secure, leading to an XSS\nvulnerability. We therefore want to choose something that comes with secure\ndefaults, and makes it easier to avoid this sort of issue.\nRequirements:\n* Must be well supported by [govuk_template][] and [govuk_frontend_toolkit][] (as\nwell as the future [govuk-frontend][] project)\n* Must be understood broadly by members of the team.\n* Must be understood broadly by members of the frontend developer community within GDS.\nAfter dicsussion with the head of the frontend community and members of the\nteam, the choice seems to be Rails for the following reasons:\n* Most of www.gov.uk is written in Rails, as is the Verify frontend, and\ntherefore is well known within the frontend developer community.\n* It's well supported by the frontend toolkits (both projects are available as\ngems that provide a Rails engine). Given the wide use of Rails with GDS, the\nfuture [govuk-frontend][] project is likely to support it.\n* It's the framework that's most familiar to our team.\n* It is opinionated, and comes with secure defaults making it much easier to\ncreate a secure web app.\n\n## Decision\n",We will use Ruby and Rails to create new user-facing applications with web frontends.\n,Rails is the chosen framework for developing user-facing applications with web interfaces that need to be styled like GOV.UK.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp to the release of postgres 13 plans to tenants, the policy for choosing\nthe set of postgres extensions we allow for a certain major version was ad-hoc.\nA mixture of ""allow everything"", simply copying the list from the previous\npostgres version and not actually trying any of these extensions had lead\nto an extensions list (published at https://docs.cloud.service.gov.uk/deploying_services/postgresql/#add-or-remove-extensions-for-a-postgresql-service-instance)\nwith:\n- misnamed entries\n- entries removed from earlier postgres versions\n- missing newly-offered extensions\n- extensions that could never be used without superuser access\n- extensions that could never be used with the VPC restrictions we have\nin place for our RDS instances.\nSome of these extensions had been omitted from the list in the documentation,\nbut it was unclear how these decisions had been made and where we stood on\neach extension.\nThis is confusing for tenants and could lead them down a path of trying to\nuse an extension which will never work, or even start designing a system\nthat relies on functionality in a listed extension only to find it unusable.\n\n## Decision\n","For postgres 13 onwards, offer a selection of extensions limited to those\nwe know can be successfully enabled and think are feasible to use given\nthe limitations of our platform.\nMaintain a document (initially a spreadsheet https://docs.google.com/spreadsheets/d/100qBo3Q2mfY70ek9fNWbpEsS4HzjOarF3q1a_hPR1uU/edit?usp=sharing)\ntracking our conclusions on each extension by postgres major version.\nWhen preparing to offer a new postgres major version to tenants, a new\nsheet should be copied from the previous release's sheet and adjusted\naccording to the new list of extensions in the RDS documentation. New\nentries should be researched and decided upon whether they are feasible\nto use without superuser privileges and from within our VPC.\nUsing a dev environment with the new postgres available and a script\nsuch as the one found in\nhttps://github.com/alphagov/paas-rds-broker/tree/main/scripts (or preferably\nan improved one), each extension remaining in the ""final list"" should be\nchecked in turn to ensure it can be enabled and then disabled. This may\ninvolve discovering (through trial and error) which extensions have\ndependencies on others. It's likely to be the same as the previous release,\nso that's a good starting point. This is good information to maintain as it\nwould come in useful if we ever decided to make the rdsbroker smarter and able\nto auto-load dependencies.\nThis should also reveal whether any new extensions require\n`shared_preload_libraries`. Any that do should have appropriate additions\nmade to https://github.com/alphagov/paas-rds-broker/blob/main/rdsbroker/supported_extensions.go .\nBeyond making sure an extension loads, actually trying out the functionality\nof each extension is beyond the time committment we're willing to give\nthis for now. We may update the documentation to note the semi-supported\nnature of postgres extensions on our platform.\n","**Decision:**\n\nCreate a curated list of allowed extensions for each major Postgres version and publish it in the documentation.\n\n**Rationale:**\n\n* Standardizes the process for selecting allowed extensions, ensuring consistency and transparency.\n* Reduces confusion for tenants by providing a clear and comprehensive list of supported extensions.\n* Eliminates the risk of tenants attempting to use extensions that are not supported or do not work within the service's constraints.\n* Facilitates system design by providing tenants with reliable information about available functionality.\n\n**Implementation:**\n\n* Establish a process to review and evaluate extensions for each new major Postgres version.\n* Create a curated list of approved extensions, excluding those that:\n    * Are misnamed or defunct.\n    * Require superuser access or conflict with VPC restrictions.\n* Regularly update the documentation to reflect the latest list of approved extensions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR040 BOSH access without SOCKS](../ADR040-bosh-access-without-socks/) we removed the requirement for using a SOCKS5 proxy or SSH tunnel to access the User Account and Authentication Service (UAA).\nWe are moving towards a [zero trust network model](https://www.ncsc.gov.uk/blog-post/zero-trust-architecture-design-principles) and as part of this, we are removing the IP allow lists that have been in place.\nWe discussed the proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https://docs.google.com/document/d/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps/edit#heading=h.xscqoqxlc072)\n\n## Decision\n","We will remove the reliance on IP allow lists for all services on the BOSH instance.\nMutual TLS will replace the allow lists.\nCyber prefer this method, as it give a much stronger authentication to the platform. This is due to authenticating both the individual and the machine that are accessing critical services.\n",We will remove all the IP allow lists and add an OAuth-based authentication method.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL/TCP mode ELB is not able to set any `X-Forwarded` header.\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http://serverfault.com/questions/775010/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\nThere are two options to workaround this:\n* Use a more complex configuration of HAProxy with two frontends/listeners chained\n* Disable SSL between ELB and HAProxy\nDecision\n========\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\nWe don't think this has any significant increase in risk because:\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\n* the inner interface of the ELB is on an internal network in our VPC\nStatus\n======\nAccepted\nConsequences\n============\nThe http traffic between ELB and HAProxy will not be encrypted.\n\n## Decision\n",========\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\nWe don't think this has any significant increase in risk because:\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\n* the inner interface of the ELB is on an internal network in our VPC\nStatus\n======\nAccepted\nConsequences\n============\nThe http traffic between ELB and HAProxy will not be encrypted.\n,Disable SSL encryption between the internal IP of the Elastic Load Balancer (ELB) and HAProxy to allow the use of ProxyProtocol.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use [Bosh](https://bosh.io/) to create and manage our cloudfoundry deployment on AWS.\nTo deploy software, Bosh needs certain binary dependencies available.\nThese are known as bosh [releases](https://bosh.io/docs/release.html).\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https://concourse-ci.org/) pipeline.\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\n### Building Bosh Releases\nWe investigated different approaches to creating bosh releases, in particular\n* Multiple pipelines created dynamically using [branch manager](https://github.com/alphagov/paas-concourse-branch-manager)\n* A single pipeline using [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource)\nThe work on these spikes was recorded in\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/115142265\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/128937731\n\n## Decision\n",We will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\n,We should continue building bosh releases manually and store them on GitHub for use in our Bosh deployment.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](../ADR006-rds-broker).\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\nThe broker must:\n1. Start the restore from snapshot, which can take minutes.\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\n3. Once that is finish, reset the master password of the RDS instance.\n4. Finally reset the passwords of the users previously bind in the original DB.\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\nThe rds-broker must respond accordingly.\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html). This requires a lot of additional work and integration effort.\n* Store the state in some database or k/v store.\n\n## Decision\n","We decided:\n* Implement a state machine using the [AWS tags](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html) of the instance.\nWe will add a list of tags for each pending operations to execute.\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\nWe assume that:\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\n* that update the tags is atomic and synchronous.\n",Store the state in a database or k/v store.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe offer our users the following backing services through [Aiven](https://aiven.io):\n- Elasticsearch\n- InfluxDB (private beta)\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\n- debug and respond to usage and service performance changes\n- understand the operational characteristics of their applications and services\n- make better capacity planning and budgeting decisions\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\n- shipping logs to an Elasticsearch/Rsyslog\n- sending metrics to Datadog\n- sending metrics to Aiven Postgres/InfluxDB\n- exposing metrics in Prometheus exposition format\nWe currently run Prometheus for monitoring the platform, using the [Prometheus BOSH release](https://github.com/bosh-prometheus/prometheus-boshrelease) and have confidence and experience using it.\nWe will need to think about Prometheus failover. If we load balance Prometheus without sticky sessions, the metrics Prometheus reports will be erratic, as different instances report different metrics.\n\n## Decision\n",We will use Prometheus to scrape Aiven-provided services.\nWe will deploy new Prometheus in the Cloud Foundry BOSH deployment using the Prometheus BOSH release. This will reduce blast radius - tenant usage of metrics will not affect our ability to operate and monitor the platform using Prometheus.\nWe will need to automate the following tasks:\n1. Service discovery: make sure Prometheus has an updated list of Aiven services to scrape. We must colocate this automation with the Prometheus instance.\n2. Service integration: make sure every eligible Aiven-provided service uses the Aiven service integration for Prometheus.\n,Use Prometheus with Aiven service integration to send metrics to InfluxDB in order to provide users with visibility into the metrics for their Aiven backing services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn our Concourse pipelines a locking mechanism was required to prevent concurrent deploys. Concurrent deploys cause\nproblems because:\n* one deployment may make changes which break another\n* we have availability tests which tell us if a specific change causes app or API downtime\n* Bosh has its own locking mechanisms, which mean concurrent deploys will fail.\n* concurrent testing puts extra load on the platform and Concourse\n\n## Decision\n","We decided to implement locking using the Concourse pool resource, AWS CodeCommit, and Terraform.\nThe Concourse pool resource was chosen because it is the Concourse-native solution for locking. This created a\ndependency on having a Git repository for the lock, as this is how the pool resource is implemented. AWS CodeCommit was\nchosen over Github for several reasons:\n* client libraries for interacting with the necessary APIs were much more mature for AWS and this particular use case.\nIt saved a lot of work.\n* Github would have required managing users and SSH keys, or tokens. AWS could use the existing instance profile of the\nConcourse VM.\n* Github would have meant the repository containing the locks would have been public.\nTerraform was required to allow our pipelines (the Concourse instance profile) to manage AWS CodeCommit. We used a\npattern whereby we allow the creation of IAM users under a specific name prefix, and allow adding these users to a\npredefined IAM group. The permissions of said group are defined in a private repository in Terraform configuration\nwhich is ran manually. This limits the permissions the Concourse instance profile can grant to users.\n",Implement a semaphore-based locking mechanism in Concourse to prevent concurrent deploys.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen building pipelines using concourse, we investigated using the [pool\nresource](https://github.com/concourse/pool-resource) in order to control flow\nthrough jobs. This was an alternative to the use of the\n[semver resource](https://github.com/concourse/semver-resource).\nThese 2 resources are both workarounds to solve the problem of triggering jobs\nwhen we haven't made changes to a resource.\nThe problem is that the pool resource relies on write access to a github repo,\nwhich means we must pass public keys that allow this access into the pipeline\nand deployed concourse instance - we want to minimise the number of credentials\nwe pass, and the semver resource relies on AWS credentials that are already\npassed.\n\n## Decision\n",We will not use the pool resource for flow between jobs - instead we will use\nthe semver resource\n,Use the semver resource over the pool resource due to the reduced credential exposure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\netc.) use the APIs to manage or access IaaS resources.\nThe most common mechanism for authenticating the API calls is to create an\nIdentify and Access Management (IAM) user with the appropriate permissions,\ngenerate an Access Key ID and Secret Access Key for that user, and export\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\nmost utilities and libraries.\nThe problem with this approach is that it's very easy to accidentally leak\nthe plain text keys. They can appear in output from your shell, which you\nmight copy+paste into a gist or email when debugging a problem. You might\nadd them to your shell configuration or include them in a script, which can\nbe pushed to a public code repository.\nOur team have leaked keys like this on more than one occasion. It's worth\nnoting that even if you realise that you've done this, delete the commit and\nrevoke the keys, they may have already been used maliciously because\nautomated bots monitor sites like GitHub using the [events firehose][] to\ndetect any credentials.\n[events firehose]: https://developer.github.com/v3/activity/events/\nAs an alternative to using pre-generated keys, AWS recommends that you use\n[IAM roles and instance profiles][] when accessing the API from EC2\ninstances. You delegate permissions to the EC2 instance and temporary\ncredentials are made available from the instance metadata service. Most\ntools and libraries automatically support this. The credentials are\nregularly rotated and never need to be stored in configuration files.\n[IAM roles and instance profiles]: http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-roles-with-ec2\n\n## Decision\n","To reduce the likelihood of us leaking AWS keys we will use IAM roles and\ninstance profiles for all operations that run from EC2 instances. This\nincludes everything that happens within Concourse and Cloud Foundry.\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\nan [`aws:SourceIp` condition][condition] to\nenforce that IAM accounts for team members are only used from the office IP\naddresses.\n[condition]: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#iam-policy-example-deny-source-ip-address\nThe IAM roles, profiles, and policies will be managed by our\n[aws-account-wide-terraform][] repo.\n[aws-account-wide-terraform]: https://github.digital.cabinet-office.gov.uk/government-paas/aws-account-wide-terraform\n",Migrate the deployment environment to use IAM roles and instance profiles for authentication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wanted to open up access to tenant applications in our production environment.\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\n* [#116104189 - set up Pingdom](https://www.pivotaltracker.com/story/show/116104189)\n* [#115347323 - allow public access to tenant applications](https://www.pivotaltracker.com/story/show/115347323)\n\n## Decision\n","It was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\n","To open up access to tenant applications in production, configure security groups on an application-by-application basis instead of opening up access to all."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to decide where to terminate TLS connections for public and tenant\nfacing endpoints and how to manage the corresponding private keys.\nWe had previously decided to only support HTTPS to both deployed applications\nand Cloud Foundry endpoints.\nAt the time of writing there were 4 endpoints to consider:\n- Deployed applications (gorouter). Accessed by the public.\n- CF API. Accessed by tenants.\n- UAA. Accessed by tenants.\n- Loggregator. Accessed by tenants.\n- SSH proxy. In theory accessed by tenants, but not working in our environment.\nWe had an existing credentials store suitable for storing the private keys at\nrest. Only a small number of engineers within the team can access; the same\nones that can make IAM changes using our account-wide terraform config.\nPlacing ELBs in front of public-facing services is an architectural pattern\nadvised by Amazon [in order to reduce attack\nsurface](https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf).\nSpecifically they advise that it helps withstand volumetric Denial of Service\nattacks; the ELB handles TCP connections and therefore the responsibility for\nhandling DDOS at Layer 4 and below resides with the ELB team.\nWe did a spike, where we attempted to place everything public-facing or\ntenant-facing behind ELBs. We found that:\n- In HTTP mode the ELBs do not support web sockets. This is known to break\nloggregator, which relies on them for log streaming. It would also prevent\ntenants from using web sockets within their applications.\n- When the ELB is in TCP mode, we have no way of communicating the client IP\naddress to the downstream service. Practical consequences of this would be\nthat tenants would be unable to see in their logs who is using their service or\ndo any access control based on client IP address.\nIn attempting to solve the second problem, we explored some options:\n- ELB has support for the [Proxy\nProtocol](http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt), but\nunfortunately none of the downstream services, such as gorouter, support it. It\nseemed simple to add support to gorouter.\n- We could introduce another intermediary proxy such as HAProxy, which\nunderstands the proxy protocol and adds or appends to an `X-Forwarded-For`\nheader with the client IP address as provided via the proxy protocol.\n\n## Decision\n",We decided to:\n- use the ELB to terminate TLS\n- use the ELB in TCP mode\n- submit proxy protocol support to gorouter\n- use S3 logging to ensure we have the IP addresses of clients using the CF\nendpoint\n,"Do not terminate TLS at the ELBs. Instead, terminate TLS at the individual backend applications and Cloud Foundry services. All traffic to the applications and services should be in plain text."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nStories: [#123490171](https://www.pivotaltracker.com/story/show/123490171) & [#121933113](https://www.pivotaltracker.com/story/show/121933113)\nWe were investigating how to avoid downtime to deployed applications when we\nmade changes to the platform. We had discovered that short outages occurred\nwhen the gorouter was taken out of service.\ngorouter has drain functionality to allow upstream load balancers to gracefully\ntake instances of the gorouter out of service before any requests start to\nfail.\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor/1.1`\nstart failing with HTTP 503. User requests are allowed to continue.\nTwo things prevented us from using drain mode:\n- we had the ELB configured to use TCP healthchecks, not HTTP\n- the ELB sends HTTP healthcheck requests with `User-Agent:\nELB-HealthChecker/1.0`, which means they were not recognised as healthcheck\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\ndraining gorouter out of service.\nThe result was a very small amount of downtime for deployed apps that received\nrequests during a short window of < 1 second.\nDecision\n========\nWe decided to:\n- submit a change upstream to [allow the gorouter to recognise ELB\nhealthchecks](https://github.com/cloudfoundry/gorouter/pull/138)\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\nwhich appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects\n- enable HTTP healthchecks on the ELB\nStatus\n======\nAccepted\nConsequences\n============\n- There was less downtime for deployed applications during a deploy.\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\n\n## Decision\n","========\nWe decided to:\n- submit a change upstream to [allow the gorouter to recognise ELB\nhealthchecks](https://github.com/cloudfoundry/gorouter/pull/138)\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\nwhich appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects\n- enable HTTP healthchecks on the ELB\nStatus\n======\nAccepted\nConsequences\n============\n- There was less downtime for deployed applications during a deploy.\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\n","- Submit a change upstream to allow the gorouter to recognise ELB healthchecks.\n- Implement a healthchecking port 82, in the HAProxy, which appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects.\n- Enable HTTP healthchecks on the ELB."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith the implementation of ADR021 we have reduced the number of cells in\nproduction in order to make more efficent use of our budget. This in turn means\nthat we have increased the load on the individual cells. Originally the idle CPU\nmonitor was set in line with the free memory on cells monitor (for alerting on\na need to scale the cell pool), however CPU usage does not appear to grow\nlinearly with allocated memory for tenant applications.\n\n## Decision\n","In order to avoid false positives from triggering due to CPU load spiking rather\nthan being a constant level we will increase the monitoring window to 24 hours.\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\nthresholds to warn at 37% and become critical at 33%.\n",The minimum CPU utilization threshold should be reduced to 40% to ensure that cells are scaled up before they become overloaded.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\nThere are 3 main network sections between the user and the application:\n* User to ELB\n* ELB to router\n* Router to cells\nDecision\n========\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\nStatus\n======\nAccepted\nConsequences\n============\nThe traffic is encrypted end-to-end between the user and the applications.\n\n## Decision\n","========\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\nStatus\n======\nAccepted\nConsequences\n============\nThe traffic is encrypted end-to-end between the user and the applications.\n","**Decision:**\nTo encrypt traffic between the user and application instances via TLS for the User to ELB and ELB to router sections, and via IPSec for the Router to cells section.\n\n**Rationale:**\n- TLS is used between the user and the ELB to protect data in transit, as it is a widely accepted and secure protocol.\n- The ELB uses a certificate issued by a trusted certificate authority (CA), and access to the private key is strictly controlled.\n- The ELB connects to the router VM via TLS, ensuring that data in transit is protected.\n- The Cloud Foundry platform does not support TLS between the gorouter and application instances, and some application instances may not be able to speak TLS. Therefore, IPSec is used to encrypt traffic between the router and application instances transparently.\n\n**Consequences:**\n- Secure and encrypted traffic between the user and application instances, protecting data from network eavesdropping."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe alpha initially took the approach of starting with a vanilla set of cloud\nfoundry manifests, and merging new values into it using spiff. This became\ndifficult to reason about, and cf-release was forked because it was easier than\noverriding necessary values using spiff. However, the confusing spiff hierarchy\nremained.\n\n## Decision\n",We will create our own set of manifests based on those in cf-release.\nWe will modify these as required.\nWe will use spruce to merge a series of files into the yml required by cloud\nfoundry\nWe will name the files with a numeric prefix and rely on shell globbing to\ndetermine the merge order rather than listing the merge order in the\nbuild-manifest script.\n,Refactor to declarative manifests that are generated from a common set of data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nStories: [#123490171](https://www.pivotaltracker.com/story/show/123490171) & [#121933113](https://www.pivotaltracker.com/story/show/121933113)\nWe were investigating how to avoid downtime to deployed applications when we\nmade changes to the platform. We had discovered that short outages occurred\nwhen the gorouter was taken out of service.\ngorouter has drain functionality to allow upstream load balancers to gracefully\ntake instances of the gorouter out of service before any requests start to\nfail.\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor/1.1`\nstart failing with HTTP 503. User requests are allowed to continue.\nTwo things prevented us from using drain mode:\n- we had the ELB configured to use TCP healthchecks, not HTTP\n- the ELB sends HTTP healthcheck requests with `User-Agent:\nELB-HealthChecker/1.0`, which means they were not recognised as healthcheck\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\ndraining gorouter out of service.\nThe result was a very small amount of downtime for deployed apps that received\nrequests during a short window of < 1 second.\n\n## Decision\n","We decided to:\n- submit a change upstream to [allow the gorouter to recognise ELB\nhealthchecks](https://github.com/cloudfoundry/gorouter/pull/138)\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\nwhich appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects\n- enable HTTP healthchecks on the ELB\n","**Decision:** Re-configure the ELB to use HTTP healthchecks, and update gorouter to handle HTTP healthchecks with `User-Agent: ELB-HealthChecker/1.0`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe use [Bosh](https://bosh.io/) to create and manage our cloudfoundry deployment on AWS.\nTo deploy software, Bosh needs certain binary dependencies available.\nThese are known as bosh [releases](https://bosh.io/docs/release.html).\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https://concourse.ci/) pipeline.\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\n### Building Bosh Releases\nWe investigated different approaches to creating bosh releases, in particular\n* Multiple pipelines created dynamically using [branch manager](https://github.com/alphagov/paas-concourse-branch-manager)\n* A single pipeline using [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource)\nThe work on these spikes was recorded in\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/115142265\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/128937731\nDecision\n========\nWe will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\nStatus\n======\nAccepted\nConsequences\n============\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\nWe may have to add additional config in projects we fork to allow us to create final builds.\n\n## Decision\n",========\nWe will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\nStatus\n======\nAccepted\nConsequences\n============\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\nWe may have to add additional config in projects we fork to allow us to create final builds.\n,We will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nWe have a single pipeline `create-cloudfoundry` which creates a Cloud Foundry\ndeployment, and also deploys additional services to the platform.\nThese services include:\n- PaaS Accounts\n- PaaS Admin\n- PaaS Billing\n- PaaS Metrics\nWhich are core components to our platform, but not to Cloud Foundry.\nCurrently these services are unnecessarily coupled in a couple of places:\n- The `post-deploy` job\n- The `custom-acceptance-tests` job\nUnnecessarily coupling has resulted in flakey app tests blocking CVE\nremediation from reaching production.\n# Decision\nMove, where possible, PaaS services into their own jobs (within the same\n`create-cloudfoundry` pipeline) such that they do not impede progress of\ndeployment to the core platform.\n# Status\nAccepted\n# Consequences\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\nThe individual jobs in the pipeline will be less mysterious.\n\n## Decision\n","Move, where possible, PaaS services into their own jobs (within the same\n`create-cloudfoundry` pipeline) such that they do not impede progress of\ndeployment to the core platform.\n# Status\nAccepted\n# Consequences\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\nThe individual jobs in the pipeline will be less mysterious.\n",Move PaaS services into their own jobs (within the `create-cloudfoundry` pipeline) such that they do not impede progress of deployment to the core platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](ADR443-ssl-only-for-applications-and-cf-endpoints.md).\nTo add our domains to [HSTS preload lists](https://hstspreload.appspot.com/), there are these requirements:\n1. Serve a valid certificate.\n2. Redirect from HTTP to HTTPS on the same host.\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\n4. Serve an HSTS header on the base domain for HTTPS requests:\nWe need an endpoint to provide these requirements.\nOur Cloud Foundry app endpoint already [serves the\nright HSTS Security header with HAProxy](ADR008-haproxy-for-request-rewriting.md)\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\nWe can implement a second ELB to listening on HTTP and HTTPS and use\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\nBut this increases our dependency on the HAProxy service.\nWe must serve from the root domain (or apex domain), but it is not allowed to\nserve [CNAME records in the root/apex domain](http://serverfault.com/questions/613829/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\nan issue when serving the service using ELB or CloudFront.\nDecision\n========\n* We will implement a basic [AWS API Gateway](https://aws.amazon.com/api-gateway/)\nwith a default [MOCK response](https://aws.amazon.com/about-aws/whats-new/2015/09/introducing-mock-integration-generate-api-responses-from-api-gateway-directly/)\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\ncontent of the response is irrelevant, it can be a 302.\nA [Custom Domain Name](http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nwhich creates a [AWS Cloud Front distribution](http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html),\nwill provide public access to this API.\n* We will use [AWS Route 53 `ALIAS` resource record](http://docs.aws.amazon.com/Route53/latest/APIReference/CreateAliasRRSAPI.html)\nto [serve the IPs of the AWS Cloud Front distribution as A records](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html).\nStatus\n======\nAccepted\nConsequences\n============\nTo setup AWS API Gateway Domain Names, it is required access to the SSL certificates. There is the option of uploading the certificates in a different step and create the AWS Cloud Front distribution manually.\n\n## Decision\n","========\n* We will implement a basic [AWS API Gateway](https://aws.amazon.com/api-gateway/)\nwith a default [MOCK response](https://aws.amazon.com/about-aws/whats-new/2015/09/introducing-mock-integration-generate-api-responses-from-api-gateway-directly/)\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\ncontent of the response is irrelevant, it can be a 302.\nA [Custom Domain Name](http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nwhich creates a [AWS Cloud Front distribution](http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html),\nwill provide public access to this API.\n* We will use [AWS Route 53 `ALIAS` resource record](http://docs.aws.amazon.com/Route53/latest/APIReference/CreateAliasRRSAPI.html)\nto [serve the IPs of the AWS Cloud Front distribution as A records](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html).\nStatus\n======\nAccepted\nConsequences\n============\nTo setup AWS API Gateway Domain Names, it is required access to the SSL certificates. There is the option of uploading the certificates in a different step and create the AWS Cloud Front distribution manually.\n",* Implement a basic AWS API Gateway with a default MOCK response that returns the right HTTP header Strict-Transport-Security.\n* Use AWS Route 53 ALIAS resource record to serve the IPs of the AWS Cloud Front distribution as A records.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.\nIt was paused because some IA issues with Logit were not resolved. At one point\nRE recommended that PaaS should host our own logstash as this part of the\nservice was not widely available by market Elastic SaaS providers. The PaaS\nteam was also considering to use Elasticsearch on AWS with our accounts.\n\n## Decision\n,"An updated conversation with the RE tool team has confirmed that the IA issues\nhad been resolved, and that GDS can continue to use Logit for now.\nIt is a GDS strategy to use a consistent logging solution. Hence, we should\ncontinue our migration of platform logs to logit, including our logstash\nfilters.\nThere is considerably less maintenance work for us if we use Logit's logstash\nfilter rather than hosting the bosh release one. In the future if GDS choose to\nuse another vendor that do not have a hosted logstash solution, they would need\nto provide a migration strategy for all the current logstash users.\n",**Decision:** Continue working on shipping cloud foundry platform logs to Logit.\n\n**Rationale:**\n\n* Logit is a market-leading provider of log management services.\n* Logit has resolved the IA issues that were previously preventing the integration with Cloud Foundry.\n* PaaS is not currently equipped to host its own logstash or Elasticsearch on AWS.\n* Continuing to work on the Logit integration will provide PaaS with a reliable and scalable solution for managing Cloud Foundry platform logs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nIt is expected for the government websites to be secure and keep the user\ninteractions private. Because that we want to enforce all communications to\nany application and to the platform endpoints to use only and always HTTPS,\nas [it is described in the Gov Service Manual](https://www.gov.uk/service-manual/technology/using-https).\nWhen a user inputs a website name without specifying the\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\nEven if the server always redirect HTTP to HTTPS, an initial\nunprotected request including user information will be transferred\nin clear: full URL with domain, parameter, [cookies without secure flag](https://en.wikipedia.org/wiki/HTTP_cookie#Secure_and_HttpOnly)\nor browser meta-information.\n[HTTP Strict Transport Security](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security)\nmitigates this issue by instructing modern browsers that support it to\nalways connect using HTTPS.\nThis is also a [requirement in the service manual](https://www.gov.uk/service-manual/technology/using-https).\nThere is still a potential initial unprotected HTTP request that might happen\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\nTo solve this issue, the root domain can be added to\n[HSTS preload list](https://hstspreload.appspot.com/) which will be used by most\ncommon browsers.\nCurrently the only way to avoid any clear text HTTP interaction is closing or\ndropping any attempt to connect to the port 80 at TCP level.\nAlthough not all application deployed on the PaaS will be ""services""\nas in the service manual meaning, we must not allow HTTP to make\nit easier to service owners to comply with this requirements.\nDecision\n========\nWe will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\nWe will implement and maintain HSTS preload lists for our production domains.\nStatus\n======\nAccepted\nConsequences\n============\nWe must configure and maintain our domain in the HSTS preload lists.\nUsers of browsers which do not support HSTS, or HSTS preload lists, will not\nbe able to connect to the sites without specify the protocol `https://` in\nthe URL. This only happens when the user manually inputs the URL in the\nbrowser.\n\n## Decision\n","========\nWe will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\nWe will implement and maintain HSTS preload lists for our production domains.\nStatus\n======\nAccepted\nConsequences\n============\nWe must configure and maintain our domain in the HSTS preload lists.\nUsers of browsers which do not support HSTS, or HSTS preload lists, will not\nbe able to connect to the sites without specify the protocol `https://` in\nthe URL. This only happens when the user manually inputs the URL in the\nbrowser.\n",We will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\n\nWe will implement and maintain HSTS preload lists for our production domains.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a ""parent"" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\n\n## Decision\n","We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\n",Remove the ability for users other than the one that created the database to modify the database schema.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL/TCP mode ELB is not able to set any `X-Forwarded` header.\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http://serverfault.com/questions/775010/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\nThere are two options to workaround this:\n* Use a more complex configuration of HAProxy with two frontends/listeners chained\n* Disable SSL between ELB and HAProxy\n\n## Decision\n",We have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\nWe don't think this has any significant increase in risk because:\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\n* the inner interface of the ELB is on an internal network in our VPC\n,Disable SSL between ELB and HAProxy
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\n### Current plans\nAs of 2019-09-03 [GOV.UK PaaS](https://cloud.service.gov.uk) offers the following postgres and mysql\nplans to everyone:\n#### MySql\n| service plan           | description                                                                                                                    | free or paid |\n|------------------------|--------------------------------------------------------------------------------------------------------------------------------|--------------|\n|#  tiny-unencrypted-5.7   | 5GB Storage, NOT BACKED UP, Dedicated Instance. MySQL Version 5.7. DB Instance Class: db.t2.micro.                             | free |\n|#  medium-ha-5.7          | 100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.large.     | paid |\n|#  large-ha-5.7           | 512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.2xlarge.   | paid |\n|#  xlarge-ha-5.7          | 2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.4xlarge.     | paid |\n|#  small-ha-5.7           | 20GB Storage, Dedicated Instance, Highly Available. Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.t2.small.      | paid |\n|#  small-5.7              | 20GB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.t2.small.                        | paid |\n|#  medium-5.7             | 100GB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.large.                       | paid |\n|#  large-5.7              | 512GB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.2xlarge.                     | paid |\n|#  xlarge-5.7             | 2TB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.4xlarge.                       | paid |\n#### Postgres\n| service plan           | description                                                                                                                                                        | free or paid |\n|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n|#  tiny-unencrypted-9.5   | 5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.t2.micro.                               | free |\n|#  medium-ha-9.5          | 100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.large.      | paid |\n|#  large-ha-9.5           | 512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.2xlarge.   | paid |\n|#  xlarge-ha-9.5          | 2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.4xlarge.     | paid |\n|#  small-ha-9.5           | 20GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.t2.small.       | paid |\n|#  small-9.5              | 20GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.t2.small.                         | paid |\n|#  medium-9.5             | 100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.large.                        | paid |\n|#  large-9.5              | 512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.2xlarge.                     | paid |\n|#  xlarge-9.5             | 2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.4xlarge.                       | paid |\n|#  tiny-unencrypted-10    | 5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.micro.                                | free |\n|#  small-10               | 20GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.                          | paid |\n|#  small-ha-10            | 20GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.        | paid |\n|#  medium-10              | 100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.                         | paid |\n|#  medium-ha-10           | 100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.       | paid |\n|#  large-10               | 512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.                      | paid |\n|#  large-ha-10            | 512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.    | paid |\n|#  xlarge-10              | 2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.                        | paid |\n|#  xlarge-ha-10           | 2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.      | paid |\n### Current analysis\nIn [#167839682](https://www.pivotaltracker.com/n/projects/1275640/stories/167839682) we had a look\nat the usage on our existing databases and concluded that, for the most part, the existing plans are\nappropriate for current usage. We didn't identify any databases that could easily have been on a\nsmaller instance size if only it had higher IOPS (which was one hypothesis).\nThere was some evidence in the form of support tickets that our current small plans don't have\nenough IOPS, causing people to prematurely upgrade to mediums.  AWS warn you against creating DBs\nwith less than 100G disk due to IOPS restrictions, so it seems sensible that we shouldn't provided\nplans smaller than that.\nAt the moment, prices for postgres 10.5 Tiny, Small and Medium instances are:\n| plan      | price / month |\n|-----------|---------------|\n|# tiny      | £12.03        |\n|# small     | £24.50        |\n|# small-ha  | £48.98        |\n|# medium    | £125.94       |\n|# medium-ha | £251.79       |\nIf we were to increase the disk size on small instances from the current 2G to 100G, this would\ncause the small plans to increase roughly as follows (based on Ireland prices):\n| plan      | price / month       |\n|-----------|---------------------|\n|# tiny      | £12.03              |\n|# small     | £34.40 (was £24.50) |\n|# small-ha  | £67.88 (was £48.98) |\n|# medium    | £125.94             |\n|# medium-ha | £251.79             |\nWe have about 100 ""small"" databases, of which 28 are HA and 70 are not. This means if we changed the\ndisk on the existing plans our tenants would have to pay an extra £1,200/month (but on the flip\nside, they might be able to stick with small plans for longer, because of the better IOPS).\nAlternatively we could add a new pair of plans like ""small-high-iops"" and ""small-ha-high-iops"".\nThis would allow an upgrade path of `small -> small-high-iops -> medium`, instead of the current\n`small -> medium`, which would avoid people paying for CPU and memory they don't need.\nSeparately, we should add support for postgres 11 and mysql 8, which are both supported by RDS now.\nFor these new plans we should use the newest available instance types (so t3 / m5 instead of t2 /\nm4).\nDecision\n--------\n### Create new plans for postgres 11\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322288):\n| service plan          | summary                                                     |\n|-----------------------|-------------------------------------------------------------|\n|# tiny-unencrypted-11   | 5GB Storage, NOT BACKED UP. DB Instance Class: db.t3.micro. |\n|# small-11              | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# small-ha-11           | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# medium-11             | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# medium-ha-11          | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# large-11              | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# large-ha-11           | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# xlarge-11             | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n|# xlarge-ha-11          | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new plans for mysql 8\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322475):\n| service plan       | summary                                          |\n|--------------------|--------------------------------------------------|\n|# tiny-unencrypted-8 | 5GB Storage. DB Instance Class: db.t3.micro.     |\n|# small-8            | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# small-ha-8         | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# medium-8           | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# medium-ha-8        | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# large-8            | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# large-ha-8         | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# xlarge-8           | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n|# xlarge-ha-8        | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new ""small-high-iops"" and ""small-ha-high-iops"" plans, for mysql 5.7, postgres 9.5 and 10\nWe should play [a story to add ""High IOPS"" plans for the existing postgres / mysql versions](https://www.pivotaltracker.com/story/show/168322617).\nThese should have 100G of storage and be on the `t3.small` instance type (since there's no reason to\nbe adding new plans on old instance types).\nThis story may turn out not to be a priority.\n### Do not change the instance types for the existing plans\nWe considered whether to upgrade existing databases to new m5 / t3 instances, or to make it so that\nnew databases on old versions (i.e. postgres 9.5 / 10, mysql 5.7) get latest-generation instance\ntypes.\nWe decided not to do this because it would be tricky to administer the change, and it's nice to have\nthe new instance types as an incentive for people to upgrade.\nStatus\n------\nAccepted\nConsequences\n------------\n* Small databases for the newer engine versions (postgres 11 / mysql 8) will be more expensive and\nhave more disk.\n* Some tenants may be able to stick with small databases for longer before they have to upgrade to\nthe medium plan, saving them money.\n* Tenants will be able to get higher performing databases by using the newest engine versions\n(postgres 11 / mysql 8).\n* It will be possible for us to implement ""downgrading"" from `medium` to `small` databases for the\nnew plans in the broker (since they have the same sized disks) - this could save tenants money if\nthey optimise their database usage and discover they could move from a `medium` to a `small`.\n* We will have 9 to 11 more plans for postgres and mysql.\n\n## Decision\n","--------\n### Create new plans for postgres 11\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322288):\n| service plan          | summary                                                     |\n|-----------------------|-------------------------------------------------------------|\n|# tiny-unencrypted-11   | 5GB Storage, NOT BACKED UP. DB Instance Class: db.t3.micro. |\n|# small-11              | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# small-ha-11           | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# medium-11             | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# medium-ha-11          | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# large-11              | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# large-ha-11           | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# xlarge-11             | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n|# xlarge-ha-11          | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new plans for mysql 8\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322475):\n| service plan       | summary                                          |\n|--------------------|--------------------------------------------------|\n|# tiny-unencrypted-8 | 5GB Storage. DB Instance Class: db.t3.micro.     |\n|# small-8            | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# small-ha-8         | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# medium-8           | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# medium-ha-8        | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# large-8            | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# large-ha-8         | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# xlarge-8           | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n|# xlarge-ha-8        | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new ""small-high-iops"" and ""small-ha-high-iops"" plans, for mysql 5.7, postgres 9.5 and 10\nWe should play [a story to add ""High IOPS"" plans for the existing postgres / mysql versions](https://www.pivotaltracker.com/story/show/168322617).\nThese should have 100G of storage and be on the `t3.small` instance type (since there's no reason to\nbe adding new plans on old instance types).\nThis story may turn out not to be a priority.\n### Do not change the instance types for the existing plans\nWe considered whether to upgrade existing databases to new m5 / t3 instances, or to make it so that\nnew databases on old versions (i.e. postgres 9.5 / 10, mysql 5.7) get latest-generation instance\ntypes.\nWe decided not to do this because it would be tricky to administer the change, and it's nice to have\nthe new instance types as an incentive for people to upgrade.\nStatus\n------\nAccepted\nConsequences\n------------\n* Small databases for the newer engine versions (postgres 11 / mysql 8) will be more expensive and\nhave more disk.\n* Some tenants may be able to stick with small databases for longer before they have to upgrade to\nthe medium plan, saving them money.\n* Tenants will be able to get higher performing databases by using the newest engine versions\n(postgres 11 / mysql 8).\n* It will be possible for us to implement ""downgrading"" from `medium` to `small` databases for the\nnew plans in the broker (since they have the same sized disks) - this could save tenants money if\nthey optimise their database usage and discover they could move from a `medium` to a `small`.\n* We will have 9 to 11 more plans for postgres and mysql.\n","We should play a story to add the following new plans:\n| service plan          | summary                                                     |\n|-----------------------|-------------------------------------------------------------|\n|# tiny-unencrypted-11   | 5GB Storage, NOT BACKED UP. DB Instance Class: db.t3.micro. |\n|# small-11              | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# small-ha-11           | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# medium-11             | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# medium-ha-11          | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# large-11              | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# large-ha-11           | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# xlarge-11             | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n|# xlarge-ha-11          | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n<br>\nWe should play a story to add the following new plans:\n| service plan       | summary                                          |\n|--------------------|--------------------------------------------------|\n|# tiny-unencrypted-8 | 5GB Storage. DB Instance Class: db.t3.micro.     |\n|# small-8            | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# small-ha-8         | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# medium-8           | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# medium-ha-8        | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# large-8            | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# large-ha-8         | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# xlarge-8           | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n|# xlarge-ha-8        | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n<br>\nWe should play a story to add ""High IOPS"" plans for the existing postgres / mysql versions.\nThese should have 100G of storage and be on the `t3.small` instance type (since there's no reason to\nbe adding new plans on old instance types).\n<br>\nWe considered whether to upgrade existing databases to new m5 / t3 instances, or to make it so that\nnew databases on old versions (i.e. postgres 9.5 / 10, mysql 5.7) get latest-generation instance\ntypes.\n<br>\nWe decided not to do this because it would be tricky to administer the change, and it's nice to have\nthe new instance types as an incentive for people to upgrade."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\netc.) use the APIs to manage or access IaaS resources.\nThe most common mechanism for authenticating the API calls is to create an\nIdentify and Access Management (IAM) user with the appropriate permissions,\ngenerate an Access Key ID and Secret Access Key for that user, and export\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\nmost utilities and libraries.\nThe problem with this approach is that it's very easy to accidentally leak\nthe plain text keys. They can appear in output from your shell, which you\nmight copy+paste into a gist or email when debugging a problem. You might\nadd them to your shell configuration or include them in a script, which can\nbe pushed to a public code repository.\nOur team have leaked keys like this on more than one occasion. It's worth\nnoting that even if you realise that you've done this, delete the commit and\nrevoke the keys, they may have already been used maliciously because\nautomated bots monitor sites like GitHub using the [events firehose][] to\ndetect any credentials.\n[events firehose]: https://developer.github.com/v3/activity/events/\nAs an alternative to using pre-generated keys, AWS recommends that you use\n[IAM roles and instance profiles][] when accessing the API from EC2\ninstances. You delegate permissions to the EC2 instance and temporary\ncredentials are made available from the instance metadata service. Most\ntools and libraries automatically support this. The credentials are\nregularly rotated and never need to be stored in configuration files.\n[IAM roles and instance profiles]: http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-roles-with-ec2\nDecision\n========\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\ninstance profiles for all operations that run from EC2 instances. This\nincludes everything that happens within Concourse and Cloud Foundry.\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\nan [`aws:SourceIp` condition][condition] to\nenforce that IAM accounts for team members are only used from the office IP\naddresses.\n[condition]: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#iam-policy-example-deny-source-ip-address\nThe IAM roles, profiles, and policies will be managed by our\n[aws-account-wide-terraform][] repo.\n[aws-account-wide-terraform]: https://github.gds/government-paas/aws-account-wide-terraform\nStatus\n======\nAccepted\nConsequences\n============\nWe'll still need to use AWS keys for operations that run outside of EC2.\nCare must be taken when storing and handling these credentials. These\noperations include:\n- Creation of Bootstrap Concourse instance\n- Running of `aws-account-wide-terraform`\nUsing IAM profiles has the drawback that any process running on the VM can\nget the same credentials. This model does not play well when it is required\nto assign the credentials to specific processes running in different containers\n(for example concourse, CF apps), as all the containers will have access to\nthe AWS IAM profile.\nWe'll need to maintain our own forks of some standard Concourse resources to\nadd support for IAM roles and instance profiles because the maintainers\ndon't wish to support this feature ([concourse/s3-resource/pull/22][]).\nThese resources include:\n[concourse/s3-resource/pull/22]: https://github.com/concourse/s3-resource/pull/22\n- [alphagov/paas-s3-resource](https://github.com/alphagov/paas-s3-resource)\n- [alphagov/paas-semver-resource](https://github.com/alphagov/paas-semver-resource)\nWe'll need to use the office VPN to administer AWS when outside of the\noffice. This matches what we have to do to administer Concourse or Cloud\nFoundry from outside the office. There are disaster recovery provisions for\nthe VPN if the office has connectivity problems.\n\n## Decision\n","========\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\ninstance profiles for all operations that run from EC2 instances. This\nincludes everything that happens within Concourse and Cloud Foundry.\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\nan [`aws:SourceIp` condition][condition] to\nenforce that IAM accounts for team members are only used from the office IP\naddresses.\n[condition]: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#iam-policy-example-deny-source-ip-address\nThe IAM roles, profiles, and policies will be managed by our\n[aws-account-wide-terraform][] repo.\n[aws-account-wide-terraform]: https://github.gds/government-paas/aws-account-wide-terraform\nStatus\n======\nAccepted\nConsequences\n============\nWe'll still need to use AWS keys for operations that run outside of EC2.\nCare must be taken when storing and handling these credentials. These\noperations include:\n- Creation of Bootstrap Concourse instance\n- Running of `aws-account-wide-terraform`\nUsing IAM profiles has the drawback that any process running on the VM can\nget the same credentials. This model does not play well when it is required\nto assign the credentials to specific processes running in different containers\n(for example concourse, CF apps), as all the containers will have access to\nthe AWS IAM profile.\nWe'll need to maintain our own forks of some standard Concourse resources to\nadd support for IAM roles and instance profiles because the maintainers\ndon't wish to support this feature ([concourse/s3-resource/pull/22][]).\nThese resources include:\n[concourse/s3-resource/pull/22]: https://github.com/concourse/s3-resource/pull/22\n- [alphagov/paas-s3-resource](https://github.com/alphagov/paas-s3-resource)\n- [alphagov/paas-semver-resource](https://github.com/alphagov/paas-semver-resource)\nWe'll need to use the office VPN to administer AWS when outside of the\noffice. This matches what we have to do to administer Concourse or Cloud\nFoundry from outside the office. There are disaster recovery provisions for\nthe VPN if the office has connectivity problems.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of GOV.UK PaaS we have a number of applications that we consider to be\nan essential part of the platform deployed as cloud foundry apps. These include:\n* paas-admin\n* paas-billing\n* paas-accounts\nPrior to July 2019 the versions of these applications were pinned in paas-cf,\nand they were only deployed along with the whole platform. This had a couple of\nnegative consequences:\n* Following a change to an app's source and a passing build an additional\nmanual change to paas-cf was required to deploy it\n* Deploying apps required waiting for unrelated slow bits of the pipeline to\ncomplete (e.g. terraform apply, bosh deploy, cf acceptance tests), despite\nthe fact that a change to an app could not possibly affect or be affected by\nthese steps\nThere were also some advantages of specifying the version of applications\nexplicitly:\n* There's an record of which versions of the applications work with which\nversions of the platform.\n* Merges to paas-cf require a signed merge commit to be deployed, so we can be\nexplicit about the set of people who are allowed to deploy changes\n\n## Decision\n",We will update the pipeline to:\n* Require GPG signatures on git commits before deploying applications in staging and production\n* Automatically deploy every build of the master branch of the applications to staging\n* In staging: tag the application repo when a deployment passes its acceptance tests\n* In production: automatically deploy any build which has been tagged by staging\n,"The decision is to change the deployment process of the essential PaaS applications to allow for individual app deployments. This will remove the need to update the pinned versions in the platform repository, and will allow for faster deployments of app changes. The change will also require additional work to ensure that the versions of the applications and the platform are compatible, and to manage the merge commit process for these applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are moving AWS hosting from Ireland to London. This ADR contains the decisions of the DNS names we will use for apps and system components that will be hosted in London.\n\n## Decision\n,"We will use the following domain patterns for the London hosting:\n* `(system_component).london.(system_domain)`\n* `(app_name).london.(app_domain)`\nWhere:\n* (system_component) -- api, uaa, doppler, ssh, etc.\n* (system_domain) -- _cloud.service.gov.uk_, _staging.cloudpipeline.digital_\n* (app_domain) -- _cloudapps.digital_, _staging.cloudpipelineapps.digital_\nThe reasons are:\n* We should re-use the (system_component) first part to minimise the changes to the Cloud Foundry manifests.\n* We should re-use the (system_domain) and (app_domain) last part, because these domains are assigned to GOV.UK PaaS as the public interface.\n* The domain part `london` is preferrable to `uk`, because AWS may provide multiple-region hosting within the UK in the future.\nThe domain structure for the dev and CI environments won't change. For the dev environments we will create a flag to choose where to create the deployment.\n### Examples\n#### Production\n|Ireland|London|\n|----|------|\n|# _api.cloud.service.gov.uk_|_api.london.cloud.service.gov.uk_|\n|# _sample-app.cloudapps.digital_|_sample-app.london.cloudapps.digital_|\n#### Staging\n|Ireland|London|\n|----|------|\n|# _api.staging.cloudpipeline.digital_|_api.london.staging.cloudpipeline.digital_|\n|# _sample-app.staging.cloudpipelineapps.digital_|_sample-app.london.staging.cloudpipelineapps.digital_|\n",The following DNS names will be used for apps and system components that will be hosted in London:\n\n**Apps:**\n- app1.london.example.com\n- app2.london.example.com\n\n**System Components:**\n- db1.london.example.com\n- cache1.london.example.com
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nWe run a Cloud Foundry platform in London (AWS eu-west-2 region) to allow GOV.UK PaaS tenants to host their applications and services in the UK. This is driven by the hosting needs of some of the existing GOV.UK PaaS tenants.\nIn addition we need to manage the use of Amazon Web Service resources to reduce our running costs. Having staging environments in both Ireland and the UK increases the total infrastructural costs, which is not justified by the benefit of the additional tested cases.\nThe London region is newer than the Ireland and currently offers a subset of services compared to the Ireland region. Hence, having the staging environment in London should allow us to capture the cases that may cause failure due to the unavailability of services.\nTherefore, the risks of running a single staging environment are offset by the cost savings and simplification of deployment pipelines.\n# Decision\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\n# Status\nAccepted\n# Consequences\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\n\n## Decision\n",We will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\n# Status\nAccepted\n# Consequences\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\n,Remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\n```\nMemory reserved by orgs: 368640 MB (360 GB)\nMemory reserved by apps: 107108 MB (104 GB)\nMemory actually used by apps: 32868 (32 GB)\n```\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\nDecision\n========\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\nStatus\n======\nAccepted\nConsequences\n============\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\n\n## Decision\n","========\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\nStatus\n======\nAccepted\nConsequences\n============\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\n","We will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a ""parent"" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\nDecision\n========\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\nStatus\n======\nAccepted\nConsequences\n============\nWe return the same credentials to all apps bound to the same PostgreSQL database (RDS) instance.\n\n## Decision\n","========\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\nStatus\n======\nAccepted\nConsequences\n============\nWe return the same credentials to all apps bound to the same PostgreSQL database (RDS) instance.\n",Issue the same credentials to all applications bound to the database. The credentials will be stored in the backing database and encrypted using a symmetric key held by the broker.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDrains are an important aspect of any model, they are an essential boundary\ncondition for cases such as biomass drains. Curently rate laws are only specified using\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\nIntroducing drains into Maud requires implementing this into the ODE system which\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\nTo specify drains we create a new class independent of reactions, despite the fact that\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\non varying enzyme concentrations, which we've established that drains do not have.\nDrains are considered as priors to ensure that prior predictive checks are informative\nof the experimental condition.\n\n## Decision\n",Drains will not be considered as a reaction class.\n,"The decision is to introduce a new class of drains that are independent of reactions, despite the fact that they occur in the system of ODEs. This benefits post-processing as some techniques rely on varying enzyme concentrations, which we've established that drains do not have. Drains are considered as priors to ensure that prior predictive checks are informative of the experimental condition."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe try to structure our package in logical sub-units but we want to maintain a\nconsistent public interface.\n\n## Decision\n,We allow for arbitrarily nested sub-packages but export important classes and\nfunctions to the top level thus exposing a public interface. Our unit tests\nshould reflect this package structure.\n,**Decision:**\n\nCreate a module system to encapsulate the internal implementation details of the package while exposing a well-defined public interface.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make a decision on the testing framework for our project.\n\n## Decision\n,We will make use of pytest. It is a de facto standard in the Python community\nand has unrivaled power.\n,**Decision:**\n\nThe testing framework selected for this project is [Insert Testing Framework Name].
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter sampling from the posterior distribution users may want the option\nof validating their results against experimental data or to predict\ncellular behaviour using a trained kinetic model.\n\n## Decision\n,"Posterior predictive samples will be implemented using the posterior\ndraws for kinetic and thermodynamic parameters. The boundary conditions\nfrom the predicted experiments will be sampled from their marginal distributions.\nThe number of samples will be limited to the number of samples from the posterior\ndistribution.\nIn order to define what is part of the training set and prediction set a new\nfile will be introduced called experiment_metadata. A toml file where the training\nprediction split will be defined under the headers [""training""], and [""prediction""]\nrespectively.\nA new stan file will be made where there is no model block (minimal if required).\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\nfor their respective generative distributions.\nlog_probabilities should be calculated if measurements are also included for the\nprediction experiments.\n",**Option 1: Provide a web-based interface for users to submit experimental data and receive predictions from trained kinetic models.**\n\n**Benefits:**\n\n* Allows users to easily validate their results against experimental data.\n* Provides a platform for users to explore cellular behaviour using trained models.\n* User workload is minimised.\n\n**Considerations:**\n\n* Requires development and maintenance of a web-based interface.\n* May require additional security measures to protect user data.\n\n**Option 2: Provide users with a set of tools and instructions for downloading and running kinetic models locally.**\n\n**Benefits:**\n\n* Gives users more control over the modelling process.\n* Allows users to customise models to their specific needs.\n* Less work for developers.\n\n**Considerations:**\n\n* Requires users to have programming skills or access to technical support.\n* Users are responsible for model installation and maintenance.\n\n**Option 3: Provide a combination of both options 1 and 2.**\n\n**Benefits:**\n\n* Provides flexibility for users with different needs and skill levels.\n* Increases the likelihood that users will be able to find a solution that meets their requirements.\n* More work for developers.\n\n**Considerations:**\n\n* Requires development and maintenance of both a web-based interface and a set of tools for local model execution.\n* May require additional security measures to protect user data.\n\n## Decision\n\nOption 3: Provide a combination of both Options 1 and 2.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSetting up different testing environments and configurations can be a painful\nand error prone process.\n\n## Decision\n,"We use tox to define, configure, and run different test scenarios.\n","**Decision:** Use a testing framework such as Selenium or Cypress to automate the testing process.\n\n**Rationale:**\n\n* Selenium and Cypress are popular open-source testing frameworks that simplify the creation and maintenance of automated tests.\n* They provide a variety of features to support different testing scenarios, including cross-browser compatibility, parallel testing, and reporting.\n* Automation eliminates the need for manual testing, reducing the risk of errors and improving the efficiency of the testing process."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\nto natively support f-strings which are sweet.\n\n## Decision\n,We make an early decision to only support Python 3.6 and above.\n,**Decision**: Migrate the codebase to Python 3.6 or later to take advantage of f-strings and other modern Python features.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo determine the steady state conditions of the model, we require an ODE solver.\nWe switched to the adjoint ODE solver for faster gradient evaluations,\nhowever, recent use suggests that the solver is unstable and fails silently.\n\n## Decision\n","Reverting to the ode_bdf_tol() solver implemented in Stan still solves\nstiff ODE problems but slightly slower. This is overlooked as the current\nadjoint solver is too unstable to use and fails on larger systems. The current\nexample files in the `tests/data/` folder work appropriately, however, larger\nsystems fail with the step size approaching 0.\nAn attempt was made to make the flux calculations in the ode more stable by\nusing built in functions and converting products to sums of logs etc.\nThis did not help the adjoint solver and hence this revertion was made.\nFuture versions of Stan can easily be tested by reverting to the previous\nadjoint solver specification and the input files will still accept the tolerances\nfor the solver.\n",Switch back to the original standard ODE solver.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPhosphorylation is the process which deactivates metabolic enzymes.\nThe process is often conducted using kinases, which phosphorylate enzymes,\nand phosphotases, which dephosphorylate the enzyme. This is a major metabolic\nregulator and is essential for the accurate simulation of metabolic networks.\n\n## Decision\n","To mininimise the number of parameters and reducing sampling time,\nthe linear rate law was selected (see [1] for a review). The linear mechanism\nis approximately correct when the following assumptions are satisfied:\n* [Kinase] and [Phosphotase] << [Metabolic Enzyme],\n* Rapid equilibrium binding between phosphorylation enzymes and metabolic enzyme,\n* [Kinase] << dissociation constant for Kinase,\n* [Phosphotase] << dissociation constant for Phosphotse,\n* Competitive binding of phosphotases and kinases is negligable,\n* The ATP/ADP ratio remains approximately constant,\n* Phosphorylation and Dephosphorylation is an irreversible process.\nUsing these assumptions the steady state phosphorylated concentration is defined as:\n`fraction_phosphorylated = alpha / (alpha + beta)`.\nWhere, alpha and beta correspond to the phosphorylation and dephosphorylation\nrates, which are linear with respect to the kinase and phosphotase concentrations.\n```\nalpha = kcat * [Kinase]\nbeta = kcat * [Phosphatase]\n```\nThe activity of the metabolic enzyme is proportional to the dephosphorylated\namount. To avoid situations where the kinase and phosphatase have the opposite\nimpact on the target enzymem, Maud will only refer to them as activating\nand inhibiting enzymes.\n","Use Kinases and Phosphatases to phosphorylate and dephosphorylate enzymes, respectively, as the mechanism for regulating metabolic pathways."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a simple way to manage our package version.\n\n## Decision\n,We use versioneer to do this for us.\n,Use [semantic versioning](https://semver.org/).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDefining measurements for independent fluxes isn't always clear.\nThis can occur when you measure more fluxes than there are degrees\nof freedom in a network.\nAn example would be this simplified network:\nA -> B -> C\nwhere reaction 1 and reaction 2 are dependent, implying that\nno additional information is achieved by including both.\nAnother issue is knowing when you do not have enough fluxes\nmeasured, resulting in an underdetermined system. Due to the\nBayesian implementation of Maud, these systems are still theoretically\nresolvable. However, supplementing as much information\nas possible will likely be beneficial.\n\n## Decision\n","Identifying underdetermined systems is acomplished by first calculating\nthe null space of the matrix. This gives the number of degrees of freedom\nof the system as well. Then we calculate the reduced row echelon form of\nthe transpose of the null space. The resulting matrix represents the\nindependent flux pathways through the network as rows. If you take the\nmeasured subset of reactions and there is a row containing no non-zero\nentries then the system is not fully described using the current measurements.\nDetermining if the system is overspecified is achieved by comparing the\nnumber of measurements to the degrees of freedom. If the number of measurements\nis larger than the degrees of freedom then the system is overdetermined.\nIt is possible to both have an underdetermined system which is overspecified\nby having multiple measurements on dependent paths. It is also possible to\nrecieve the warning that the system is overspecified by independent measurements.\nFor instance, a linear pathway where the influx and efflux are both measured.\nThis is still valid as they are independent measurements.\n","Define only the fluxes that are independent, and thus provide unique information about the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\nmodel. Experimental conditions are defined with respect to these values and drains,\nwhich are already defined as priors. Therefore, our prior knowledge about the\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\nthe likelihood evaluations to the prior information.\nThe benefit of treating priors in this way is that we define a prior on the phenotype\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\nare still considered using weakly informative priors (read: within biologically relevant\nboundaries).\n\n## Decision\n",Unbalanced metabolites and enzyme concentrations can also be considered as prior distributions\nrather than likelihood evaluations.\n,**Decision:** Shift the measurements of enzymes and unbalanced metabolites from likelihood evaluations to prior information.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose an appropriate license before making Maud public. The main\ncontenders are the Apache license, which would allow others to use Maud in\nproprietary software, and the GPL v3 license, which would not.\n\n## Decision\n","We will use the GPL license but may change to a more permissive license in a\nfuture release, depending on the circumstances at the time.\n","We should choose the Apache license.\n\n**Rationale:**\n\nThe Apache license is a permissive license that allows others to use Maud in proprietary software. This would allow a wider range of users to benefit from Maud, including those who develop proprietary software. The GPL v3 license is a copyleft license that requires any derived works to be released under the same license. This could be a barrier to some users, as it may not be compatible with their business models."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to version control our code in order to avoid disasters and maintain\nsanity. We also want to collaborate online with a wider community.\n\n## Decision\n,We use git for version control and GitHub for collaboration.\n,Use Git as a version control system and GitHub as a collaboration platform.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to run simulation studies in order to verify that our models work\nand to answer questions about how informative particular measurements and\npriors are.\n\n## Decision\n,"We will initially make a python script that implements a single simulation\nstudy, targeting the model at `ecoli_small_experiments.toml`. Based on how\nthat goes, we can then think about how to make a more general system for\nrunning arbitrary simulation studies.\nIt's better to creep towards a simulation study system like this rather than\njump straight to the general tool because it's quite a big job, and we don't\nreally know exactly what the general tool will look like yet.\n",Use python's `scipy.stats` package
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe `code_generation.py` module is probably the ugliest part of Maud,\nparticularly the function `create_fluxes_function`. It could be cleaned up a\nbit by removing non-modular enzyme mechanisms.\nNon-modular mechanisms aren't used in any of the models we are currently\ntesting, but for historical reasons the logic in `code_generation.py` basically\nassumes that they are the default. The modular rate law is kind of an\nadd-on. This change would let us move towards switching this around, having the\nmodular rate law as the default and working in other mechanisms as and when\nnecessary.\n\n## Decision\n",We remove non-modular rate laws from the `code_generation.py` module.\n,Remove all the non-modular enzyme mechanisms from `code_generation.py`.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWriting code that adheres to style guides and other best practices can be\nannoying. We want to standardize on some best-in-class tools.\n\n## Decision\n,"We will use isort, black, and flake8.\n",We will use the following tools to ensure code quality:\n\n* [ESLint](https://eslint.org/) for linting\n* [Prettier](https://prettier.io/) for formatting\n* [Husky](https://github.com/typicode/husky) for pre-commit hooks
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Use an Architectural Decision Record (ADR) template to document architectural decisions.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhich latex compiler to use?\n* pdflatex\n* xelatex\n* lualatex\n\n## Decision\n,"Chosen option: ""pdflatex"", because compiles faster and correct ligatures are required at the final typesetting step only.\n### Positive Consequences\n* Faster compile time\n### Negative Consequences\n* Using lualatex just before publication might lead to a different layout and additional effort\n* lualatex's power might not be used at all\n","Use xelatex.\n\n## Evidence and Rationale\nxelatex is the most modern and feature-rich of the three LaTeX compilers. It supports Unicode, OpenType fonts, and advanced typographic features like micro-typography and ligatures. It is also the fastest of the three compilers.\n\npdflatex is the oldest and most widely used LaTeX compiler. It is reliable and produces high-quality output. However, it does not support Unicode or OpenType fonts, and its typographic features are more limited than xelatex.\n\nlualatex is a newer LaTeX compiler that is based on the Lua programming language. It is similar to xelatex in terms of features and performance, but it is still under development and may not be as reliable as xelatex.\n\n## Consequence\nUsing xelatex will allow us to take advantage of its advanced features and produce high-quality documents that are visually appealing and typographically correct."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor high-quality documents, the [ligatures](https://en.wikipedia.org/wiki/Typographic_ligature) have to be right.\nSee [english.stackexchange.com](https://english.stackexchange.com/q/50660/66058) for a long discusisson.\nSee a German rant on wrong ligatures: <https://web.archive.org/web/20150425155310/http://www.mengensatz.de/blog/?p=79>.\nFor instance, in the English word ""offline"", the letters ""f"" and ""l"" must not be joined.\nIn the German word ""Auflage"", the letters ""f"" and ""l"" must not be joined.\nSee also the last lines in <https://tex.stackexchange.com/a/64457/9075>.\n### More readings\n* <https://www.typolexikon.de/ligatur/>\n* Questions on ligatures: <https://tex.stackexchange.com/questions/tagged/ligatures>\n* Disable ligatures in headings: <https://tex.stackexchange.com/q/198743/9075>\n\n## Decision\n","Chosen option: ""lualatex"", because comes out best (see below).\n","**Decision:** Use the the `microtype` package with `Ligatures=TeX` for `\XeLaTeX` or `\LuaLaTeX` (or an equivalent package for other engines).\n\n**Rationale:**\n\n* By default, LaTeX uses the ligatures of the underlying font engine, which may not be appropriate for all languages or typographic styles.\n* The `microtype` package provides control over ligatures, allowing us to specify which ligatures should be enabled or disabled.\n* The `Ligatures=TeX` setting in `microtype` approximates the ligature behavior of TeX, which is considered typographically correct for many languages.\n* This setting ensures that the letters ""f"" and ""l"" are not joined in the English word ""offline"" and the German word ""Auflage"", as required by the problem statement.\n\n**Consequences:**\n\n* This decision will alter the appearance of ligatures in the document by enabling or disabling certain ligatures based on the specified settings.\n* It is important to ensure that the chosen settings are appropriate for the target language and typographic style.\n* Using a package like `microtype` adds an additional dependency to the document."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSource code needs to be highlighted\n\n## Decision\n,"Chosen option: ""minted"", because\n1. Listings is inferior because it doesn't utilize a full lexer\n2. Minted offers integration with [pygments](http://pygments.org/) and is actively maintained.\n3. The person who took over minted development, [evaluated all alternatives](https://tex.stackexchange.com/a/103471/9075) and came to the conclusion that minted is the most promising solution.\nPositive Consequences:\n* Source is highlighted properly\nNegative consequences:\n* Users need to install python and pygments\n","Use a syntax highlighter library, such as Highlight.js or Prism.js, to highlight the source code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAllocation of Prison Offender Managers is a new process being introduced by\nOMiC, and it's different from Offender Supervisor allocation (data for which\nis mostly stored in per-prison spreadsheets). The data which will be created\nby this process does not live anywhere yet.\nWe've heard that some prisons are starting to use the new terminology and have\nstarted to store staff names as POMs against prisoners in NOMIS (although they\nare not actually using the OMiC model yet). There is no support in NOMIS for\nthe tiering data or anything else needed to support the allocation process.\nWe have learned from our allocation discovery that staff around the prison\nneed to know who is responsible for a prisoner, but there's no indication that\nstoring that data in NOMIS is a good way of meeting that need.\nWe don't know much yet about the user needs around handover of responsibility\nfrom prison to probation. It's possible that there will be a stronger need to\nstore some allocation data in Delius (for example) so we may need to revisit\nthis decision in future.\nHMPPS needs to reduce its dependence on NOMIS over time, and building an\necosystem of APIs around new datastores will enable data to be used in other\nservices without it all needing to be in one central database. The keyworker\nAPI is an existing example of this pattern.\n\n## Decision\n",The allocation API will store all data created by the allocation process in its\nown database.\nWe will not also write allocation data back to NOMIS or any other system.\n,"The data required for Allocation of Prison Offender Managers will be stored in a new datastore. This datastore will be separate from NOMIS, and will be accessible via an API. This will allow other services to use the data without having to store it in their own databases."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to run the smallest number of pre-production environments which give us\nuseful feedback on changes to our applications before we deploy them to\nproduction.\nWe think that as we integrate with more systems (NOMIS, Delius, OASys etc) we\nare likely to need more pre-production environments of our own in order to test\nagainst pre-production environments of those systems. It's unlikely that data\nwill match up well across systems in those environments so we may need to\ncreate matching data in them ourselves in order to test our applications.\nWe don't know yet how many pre-production environments would be useful for us\nto have in this context.\nWe suspect that challenges around data quality and how quickly records are\nmatched will only be clear in production data, so we're keen to start working\nwith production systems as soon as we can. However only NOMIS has available\nAPIs in production, and working with only one system is less likely to reveal\nthe scale of those challenges.\nSo far we've set up [one environment for our new applications](https://github.com/ministryofjustice/cloud-platform-environments/tree/1afcd91536201415b868ccebcaf1aeb8ecc2d339/namespaces/cloud-platform-live-0.k8s.integration.dsd.io/offender-management-staging)\non the new cloud platform, called Staging.\nIt's straightforward and quick to set up further environments as we need them,\nbut there's no way at the moment to share config between cloud platform\nenvironments so there is a code maintenance cost to having many environments.\nWe still need to finish setting up authentication on our applications as a\nminimum before we start using production APIs to other systems. There's little\nbenefit in us having a Production environment ourselves until we can do that -\nwe're a long way from having real users.\nSetting up authentication means that we need to pick a NOMIS pre-production\nenvironment to use from our Staging environment, because we're going to use the\nNOMIS OAuth2 server (see [ADR 0011](0011-use-nomis-oauth-server2-for-allocation-api-authentication.md)).\nOur team already have comprehensive access to the T3 NOMIS environment from our\nwork on Visit someone in prison. That environment contains anonymised data,\nwhich is sufficient for our needs at this stage. All the NOMIS-based services\nwe need are running there. It's commonly used as a development environment\n(rather than staging) by other services.\n\n## Decision\n",We will start out with one shared Staging environment for our new applications.\nWe will use the T3 NOMIS environment from that environment to start with.\n,Set up an Authentication environment and use it to make our first attempts to use production APIs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur main source of data on prisoners and prison staff is NOMIS. The\nallocation tool currently uses the Custody API, as decided in [ADR 0006](https://github.com/ministryofjustice/offender-management-architecture-decisions/blob/master/decisions/0006-use-the-custody-api-to-access-nomis-data.md), to retrieve\ndata on both offenders and staff.\nThere are still four APIs into NOMIS providing general data access, with varying\napproaches to presenting the data and authentication. We still do not want to add\nto this duplication.\nAlthough it has been agreed by the HMPPS technical community that we would\nlike to move all clients to use the Custody API in preference to the other APIs,\ninitial use of the Custody API has raised some issues. Problems exist\nwith the locality of data, N+1 API requests, and data that is unavailable\nthrough the currently published endpoints. These issues are intrinsic to\nthe current design of the Custody API, and it is unlikely that they will\nbe resolved or mitigated in the short to medium term. The scale of the work\nrequired makes it unrealistic that we would be able to deliver this in a\nrealistic timescale.\nThe Elite2 API, is under more active development, is entirely owned by the\nSheffield team and provides endpoints more focused on the need of clients.\nMost of the endpoints available work on the caseload of the connecting\nclient token, but also support the provision of more specific parameters\nto handle alternate needs. Although we will not be authenticating with\nthe token of the eventual end user, enough flexibility exists for us to\nobtain the data we require from Elite2.\nElite2 provides functionality closer to what we currently require\nand its design encompasses the need for extra API endpoints for specific\nservices. Moving to Elite2 is a pragmatic, and tactical approach to\nresolving the issues around the Custody API to allow us to deliver the\nallocation component of MOiC. This decision doesn't invalidate the\noverall agreed strategy of moving HMPPS services to the Custody API\nover time, but it highlights that more coordinated work is needed to\nachieve that than we are able to take on ourselves.\n\n## Decision\n","We will use the Elite2 API for access to the data we require from NOMIS.\nWe will work with the team in Sheffield on development of the Elite2 API to\nadd support for accessing the data we need, in the structure that we need it.\n",The tactical decision is to use the Elite2 API to access NOMIS data for the allocation component of MOiC.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Users\nWe need to authenticate users in order for them to access the service. We will\nhave users from prisons and probation services, both public and private in both\ncases.\nWe expect our earliest users to be prison-based Senior Probation Officers and\nOffender Management Unit administrative staff, followed by Prison Offender\nManagers and community-based Senior Probation Officers. It's possible that we\nwill also need to enable Community Offender Managers to log in in future, but\nwe don't yet know what we might build for them so that's not certain.\n### Existing options\nWe do not want to build user authentication from scratch ourselves. There are\ntwo shared approaches for authenticating users which are currently in use with\nHMPPS digital services, both of which are based on OAuth2 and support\nrole-based access control:\n- MOJ Signon\n- NOMIS OAuth server, including new SSO functionality\nBoth of these originally emerged from the needs around one or two services, and\nwere subsequently adopted by a few other services. A discovery aiming to\ndevelop a clearer strategic direction in this space has recently concluded, and\nfound that neither of the two existing approaches meet the needs of users and\nteams well across the range of HMPPS services and their user groups. For now,\nthough, these are the two available options to consider.\nIf we use MOJ Signon, we would use the authorization code grant type. The team\nare familiar with MOJ Signon from their work on Visit someone in prison.\nIf we use NOMIS Auth, we would need to decide whether to use the new SSO\nfunctionality with the authorization code grant type (which has been developed\nrecently and is not yet available in production) from the beginning, or whether\nto have our own login page and use the password grant type initially, only\nswitching over to use the SSO approach when it's available in production.\nThe SSO approach means that we would have less to build ourselves, and wouldn't\nneed to handle users' passwords, and those advantages mean more generally that\nit's intended to replace existing use of the password grant type by other\nservices.\n### Service\nThe services we're working on for Manage Offenders in Custody are seen as being\nfairly closely related to a group of services which already use NOMIS Auth.\nThe users of our service who are working in prisons will need to have NOMIS\naccounts anyway. Probation staff have so far not had NOMIS accounts, but\nanother service team (Home Detention Curfew) is using NOMIS Auth and intending\nto create accounts for probation users for that service. The new web-based\nservices being built around NOMIS mean that users won't have to be on the\nQuantum network to access the services, and in future it will be possible to\nmanage a NOMIS account without access to that network as well (for password\nresets, updating profile info, managing roles etc).\nSome of our users (OMU admins in some prisons) will already have MOJ Signon\naccounts in order to use Visit someone in prison, but many won't be doing that\nalready (and they would all have NOMIS accounts anyway).\n### Route to production\nWe expect that matching up data between NOMIS and Delius will be the main\nchallenge for us, and that we'll only be able to see how well that works in\nproduction because there aren't any pre-production environments with related\ndata across the two systems. We're keen to get into production as soon as we\ncan for that reason.\nHowever we don't currently expect to have any way of getting access to the\nDelius API for some months, which reduces the urgency of this consideration.\nOur priority now is to make decisions which enable us to make progress with\ndevelopment.\nThe NOMIS Auth team expect the new SSO to be in production within the next\ncouple of months, which is very likely to be before we get Delius API access.\n\n## Decision\n","We will use NOMIS accounts for authenticating users of the allocation manager.\nWe will use NOMIS SSO from the start, rather than building our own login page.\n","Use the NOMIS Auth server with the new SSO functionality, using the authorization code grant type."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to protect the allocation API with authentication, but we'd rather not\nhave to come up with an approach to do that ourselves from scratch.\nThe new [NOMIS OAuth2 server](https://github.com/ministryofjustice/nomis-oauth2-server)\nis already being used in production for authentication on almost all of the\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\nauthenticate with the Custody API, and the other services which may need to use\nthe allocation API are very likely to already be using this authentication\nmethod for the other APIs they use.\nClients can use one token (of a particular grant type) to authenticate with all\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\nthose services - they don't have to work with multiple different authentication\napproaches.\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\nservices can verify the integrity and authenticity of tokens presented by\nclients using the corresponding public key.\nWe've decided that the allocation manager will be entirely responsible for user\naccess control and will call other APIs directly, and the allocation API will\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\nThat means that the allocation API doesn't need to know which user it's\nreturning data for, and we can use a system-to-system approach to\nauthentication.\nWe don't know of any other shared approaches to API authentication which are\nused in the prison space.\n\n## Decision\n",We will use the NOMIS OAuth2 server for authentication on the allocation API.\nWe will use the client credentials OAuth2 grant type for authentication on the\nallocation API.\nWe will verify signatures on presented tokens in the allocation API.\nWe will respect expiration times on presented tokens in the allocation API.\n,"The allocation API will use NOMIS OAuth2 for authentication.\n\n**Consequences**\n\n* Reduces risk by using a shared approach to authentication which is already in production and used for other APIs.\n* Simplifies development by not having to design and implement our own authentication approach.\n* Reduces time by not having to design, implement and test our own authentication approach.\n* Increases maintainability by using a well-supported and maintained authentication server."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have reached a point where we are going to start reading and writing data to our Allocation API application database and will need to manage database migrations when adding tables, columns and so forth.  This ADR decision outlines the agreed upon approach that the team will take for managing these migrations.\n\n## Decision\n","We will use blue-green deployments split into the following steps (using adding column as an example):\n* Add a database migration that inserts the new column\n* Update the application so that all new data gets written to new column\n* Run a task to copy all the data from the old column to the new column\n* Update the application so that it reads from the new column\n* Add a database migration that removes the old column\nWe will also ensure that any migrations include 'up' and 'down' methods, rather than just 'change' to avoid any situations where Rails doesn't know how to handle the inverse of the up or down.\n",Use Flyway to manage database migrations.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe recommended way of managing application config needed at runtime on the\ncloud platform is to use Kubernetes secrets. These can be created manually\nusing kubectl on the command line to apply config files, or as part of a\ndeployment process.\nWe want to manage as much of our application config as possible in\nversion-controlled code, so that we can see when config was changed and so\nthat we can easily reproduce changes and rebuild environments.\nThe team are used to using [git-crypt](https://www.agwa.name/projects/git-crypt/)\nto encrypt files containing secrets from their work on Visit someone in prison,\nbut that service is hosted on a different platform which means that the secrets\nare in a private repo, separate from the application. It's easy to forget to\nupdate config at the right time when it's managed separately from the\napplication, and that has resulted in production outages in the past on that\nservice.\nIf we use git-crypt to encrypt secrets files in public repos rather than\nprivate ones, the risk associated with accidentally pushing secrets in\nunencrypted files is increased. If we take this approach, we would want to make\nit easier to avoid doing that rather than relying only on people being careful.\nWe are using kubectl for deploying. Some other teams are using helm, which has\nsupport for templating so that values for config files can be kept separately\nfrom the structure of the configuration. Unless we start using templating of\nsome kind in our config files, we will need to keep the whole files containing\nKubernetes secrets definitions private instead of just the secret values.\n### Other options\nAlternatives to using git-crypt in public repos include:\n- separating out secrets into a private repo, using git-crypt there instead.\nThis would increase the likelihood of us not updating the config when it's\nneeded, leading to production outages.\n- creating Kubernetes secrets manually from the command line, using config\nfiles committed in the application repo without secret values or encryption -\nso the secret values would be generated as needed and only stored in the\nrelevant namespace. This would mean that our Kubernetes secrets could not be\ndeployed as part of our automated pipeline but would need to be manually\nupdated, and the manual workflow for updating a single value in a secret\nwith multiple data values would be more complex. We would need to be careful\nto not accidentally commit the values in the secrets files after deploying\nthem.\n- storing secret values in CircleCI envvars, to be used when creating\nKubernetes secrets during deployment, using templating in our secrets files.\nCircleCI has no concept of environments (all envvars for a project are\navailable to all jobs) so all of our test and build steps would have access\nto production secrets, and we would need to define an approach to namespacing\nCircleCI envvars for different environments. Even so, envvars needed for\nrunning tests, building and pushing images, running deployments and running\napplications in different environments would all live together in one list,\nwhich would make that list more confusing than it already is.\n- using another secrets store to manage our secrets, probably instead of using\nKubernetes secrets. This would add another external dependency for our\napplications and would have the same challenges around either managing\nsecrets values manually (thereby losing the ability to automate processes) or\nfinding somewhere to manage version-controlled secrets from, and having\nanother place to coordinate changes to required config with application\ndeploys. It could also introduce another system for the team to authenticate\nwith in order to manage secrets.\n\n## Decision\n","We will keep the Kubernetes secrets definitions which are related to our\napplications in the public application repos, encrypted with git-crypt.\nWe will only add team members who need access to production secrets to the\ngit-crypt setup. The only exception to this is our CircleCI deployment jobs\nwhich need to decrypt the secrets to deploy them.\nWe will define a standard for creating and managing GPG keys and only use keys\nwith the git-crypt setup which meet that standard.\nWe will set up pre-commit hooks on our repos which contain encrypted secrets\nto help us use git-crypt well.\nWe will agree processes for revoking credentials with other relevant teams in\ncase they are accidentally exposed.\nIf another standard way of managing secrets for applications running on the\ncloud platform is developed, we will consider using that instead.\n",Use git-crypt to encrypt secrets files in public repos and implement a pre-commit hook to prevent pushing unencrypted secrets.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we decided to start off with separate API and user-facing applications\n(see [ADR 0004](0004-separate-api-and-user-facing-applications.md)) we thought\nthat we would need to build more than one service as part of this work, to\nmeet different needs of different users. That may still be true in general, but\nfrom what we've learned in the Handover discovery it's looking more likely at\nthe moment that the Handover phase of our work will involve expanding on\nAllocations rather than being an entirely separate journey and service.\nWe were also intending that the allocation manager would be a fairly minimal\nuser-facing application, which would fetch all the data it needed from the\nallocation API and then render pages based directly on it.\nWe've been looking at how we should set up authentication on our two\napplications and have realised that that split of responsibilities between the\nallocation manager and API could lead to some confusions:\n- Calling the Custody API from the allocation API means that the allocation API\ncan be seen as sitting in two different layers depending on the context:\neither above the Custody API from the point of view of the allocation manager,\nbut on the same level as the Custody API from the point of view of other\nservices which need to use allocation data. This lack of clarity raises\nquestions around which OAuth2 grant type to use for authentication on the\nallocation API.\n- Responsibility for access control may be split between the allocation\nmanager and API. The user-facing application would need to know what the user\nshould have access to in order to show the right options, but the API may\nalso need to know what the user should have access to in order to fetch the\nright data from other APIs.\nWe still think there's value in having a separate allocation API, but it would\nbe easier to understand if it were a more RESTful interface to the data it\nstores.\n\n## Decision\n","The allocation API will be a simpler interface to the data held in its database.\nThe allocation API will not call other APIs to return data from them to the\nallocation manager.\nThe allocation manager will call the allocation API and other APIs (Custody,\nDelius etc) directly.\nThe allocation manager will be entirely responsible for access control for its\nusers.\n","Move the minimal user-facing application into the allocation API (either as a separate component, or using a microframework that runs in the same process as the allocation API), and use the API as a very thin wrapper around the data that Allocations handles."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are moving into beta on allocations, which is the first area of our work on\nManage Offenders in Custody. The team is now also in discovery on handover, and\nthere are more areas which we know very little about at this stage which we\nintend to look at in future.\nWe anticipate that we will build more than one product as part of this work, to\nmeet different needs of different users. That means that we're very likely to\nbuild more than one application as part of this larger service area in future.\nWe need to decide now whether we should start out by building one application\nfor allocations, or follow the common pattern of separating an API layer from\npresentation.\n### Microservices\nThere are many advantages to a microservices architecture, but also some costs.\nIf we start off with more than one application, the decisions we make early on\nabout their responsibilities may take more work to change later on, as we learn\nmore.\nOn the other hand, if we start off with one application and decide later that\nwe want to split it into several, that work is likely to be substantial. Given\nthe timescales involved in OMiC, it may be hard to prioritise paying off that\ntechnical debt when it becomes a burden.\nWe anticipate that other services will need to use allocation data, and so we\nare likely to need to make it accessible via APIs at some stage anyway. Those\nservices may need to access allocation data in different ways from our\nfrontend, so more work may be needed then to support them even if we do have\nan API from the start.\n### Operating microservices\nMicroservices can introduce more operational complexity by moving function\ncalls to network calls and by introducing more moving parts which need to be\ndeployed. However we expect the number of microservices to remain relatively\nsmall so do not anticipate this complexity to be significant for us.\nOur choice of the Kubernetes-based Cloud Platform for hosting (see [ADR 0002](0002-use-cloud-platform-for-hosting.md))\nshould make it easier to scale small services efficiently and independently.\nIt also gives us easy access to tooling designed to support many small\nservices.\nWe expect our decision on how far we should follow a microservices approach to\nhave minimal impact on our hosting costs.\n\n## Decision\n","We will start off with two applications for allocation:\n- an allocation API which will call other APIs (to read data from NOMIS, and\nin future Delius and OASys)\n- an allocation frontend which will call the allocation API and serve\nprogressively enhanced HTML to users (see [ADR 0003](0003-use-progressive-enhancement.md))\n","Start off with one application for allocations, but build it in a way that makes it easy to split into many applications later on.\n\nThis decision is based on the following factors:\n\n* The benefits of a microservices architecture outweigh the costs for our use case.\n* We anticipate that we will need to make allocation data accessible via APIs at some stage anyway.\n* The operational complexity of microservices is not a major concern for us.\n\nBy starting off with one application, we can avoid the costs of splitting it into multiple applications later on. However, we will need to make sure that the application is designed in a way that makes it easy to split later on. This will require us to use a modular design and to avoid creating tight coupling between different parts of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs documented in [ADR 0014](0014-access-the-delius-api-via-ndh.md) we are\ngoing to access the Delius API via the NOMIS data hub using TLS mutual auth,\nmanaged on our side by a sidecar container.\nWe need to find a way of testing our auth setup - in particular that it only\nallows access to the Delius API from our allocation manager. The NDH only\nexists in production, so we need to decide how to test this setup and\ndeployment of the sidecar in our other environments (at the moment we only\nhave staging and production). We want staging to be as similar as possible to\nproduction so that we're testing changes in a realistic environment before\npushing them to production.\n\n## Decision\n","We will set up a separate namespace on the cloud platform for a Delius staging\nenvironment.\nWe will deploy the NDH-side mutual auth container into that environment, along\nwith the Delius API with a dataset constructed to match T3 NOMIS data.\nInitially our main concern is testing the mutual auth setup so instead of the\nDelius API we could deploy a simple HTTP server if that's easier in the short\nterm.\nWe will deploy the sidecar in our staging environment and connect to our Delius\nstaging environment from it.\n","Create a ""mock NDH"" in staging for testing purposes only. This mock NDH should:\n- Be as similar as possible to the real NDH in terms of its API and behaviour, so that we can test our auth setup and sidecar deployment in a realistic environment.\n- Be isolated from the real NDH so that it cannot be used to access the real Delius API.\n- Be easy to set up and tear down, so that it can be used for testing purposes only."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe service manual clearly states that teams should use progressive enhancement\nwhen building services: https://www.gov.uk/service-manual/technology/using-progressive-enhancement\nThis supports [point 12 of the service standard](https://www.gov.uk/service-manual/service-standard/create-a-service-thats-simple),\nwhich is about ensuring that all users can use the service successfully the\nfirst time they try - including users with accessibility needs.\nThe service manual is also clear that [internal services should be held to the\nsame standard as public-facing services](https://www.gov.uk/service-manual/design/services-for-government-users).\nSome of the services for prison and probation staff which have been built over\nthe last couple of years are not progressively enhanced. Without JavaScript\nthey display no content.\nSince these services are in a similar space to our work and have overlapping\nuser bases with ours (although they are not the only existing services in this\nspace), we have considered whether we should take a similar approach to them.\n\n## Decision\n",We will use progressive enhancement for all our user-facing applications.\n,Adhere to the service manual's guidance on progressive enhancement and ensure that all internal services are progressively enhanced.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Data and policy\nWe've known since very early on in our work on Manage Offenders in Custody that\nwe have a critical dependency on being able to use current or very recent data\nfrom Delius to inform the allocation process, in line with the new OMiC policy.\nThe policy defines two pieces of data which determine the type of Prison\nOffender Manager who can be allocated, and which staff in prisons have no other\naccess to:\n- whether the case will be handled by the National Probation Service or a\nCommunity Rehabilitation Company\n- the tier (A/B/C/D), which is based on the scores for three risk assessments\nOur user research has indicated that other information held in Delius is also\nlikely to be useful in informing the allocation decision, and is important for\nPOMs and Senior Probation Officers in carrying out their responsibilities under\nOMiC.\nOne of the main aims of the policy is to enable prison and probation services\nto work together more effectively, for which better sharing of information is\ncrucial.\n### Existing Delius API\nAn API for Delius already exists and is deployed alongside Delius, but is only\naccessible from inside that environment. Providing secure access to the API for\nother services has long been thought to depend on the Delius migration to AWS\nbeing completed.\nHowever that migration has taken longer than originally hoped, and we need to\nfind a viable approach to get access to the data we need so that we can provide\na service which has some value for our users.\nWe know that some work on the API itself will also be needed to make it return\nthe data we need, but getting access to the API at all is the more significant\nchallenge.\nThe API itself does not have authentication built in at the moment, so all\naccess to it needs to be secured in other ways. It will use the NOMIS OAuth\nserver in future.\n### Overnight extract\nIn the absence of API access, the Home Detention Curfew team have set up an\novernight batch process to write the names of responsible officers into NOMIS\nfrom a Delius extract.\nWe could use this approach for the data we need for MOiC, but it's far from\nideal for us:\n- We wouldn't have live data available, which could lead to allocation\ndecisions being made on the basis of outdated risk information\n- It would introduce more dependencies on an outdated architectural pattern\n- Duplicating data between systems means that the ownership of the data is\nunclear\n- We would need to support this approach until the API is accessible, and then\nswitch to using the API - this would take more work and redesigning interfaces\nthan using the API from the start\nWe want to avoid this approach if we possibly can, but will consider it as a\nlast resort.\n### Other services which might provide risk assessment data\nWe have also heard that another team's work around risk assessments might make\nthe data we need available from their new service, but we have just learned\nthat the scope of their work doesn't extend that far in the timescales we're\nworking to.\n### Delius API access via NDH\nAnother option was proposed recently: to give us access to the API via the\nNOMIS Data Hub, which already has secured access to the API. We could do this\nwith mutual TLS auth between the NDH and our allocation manager, using the\nsidecar pattern with our application. This approach is estimated to need less\nthan a week of work and isn't dependent on the Delius migration being\ncompleted.\nWe would need help from people working on the Delius migration to take this\napproach, particularly in setting up mutual auth on the NDH end, but given the\nextent to which this approach unblocks our work on MOiC that is a reasonable\ntradeoff. We would need help from the same people for any of the approaches\nwe've considered.\nThe NDH only exists in production, so if we take this approach we will need to\ndecide what to do in other environments. We know that Delius and NOMIS don't\nhave any shared pre-production environments anyway, so we don't expect this to\ncause any extra problems for us.\n\n## Decision\n","We will set up access to the Delius API via the NDH route.\nWhen the API is accessible through a more standard route, with authentication,\nwe will switch to use that.\nWe will keep our deployment pipeline consistent by using the\nmutual-auth-via-sidecar pattern in our pre-production environments as well as\nin production, if possible.\n",Access the Delius API via the NOMIS Data Hub using mutual TLS authentication between the NDH and the allocation manager.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have implemented the VCR gem as part of our testing suite; this gem allows you to ""record your test suite's HTTP interactions and replay them during future test runs for fast, deterministic, accurate tests."". The interactions are saved into yaml files (cassettes) as part of the spec/fixtures folder and committed to the repository.\nVCR allows you to filter out sensitive information, but this does not cover any data returned in the body.  We get a short-lived access token returned in the body as part of the OAuth flow and need to find a way of ensuring that these are not exposed and used to get access to sensitive data.  Whilst the tests are run against T3, which is an anonymised dataset, we don't want to risk the possibility of someone being able to engineer a way of undoing that and seeing real data.\nThe main options:\n- making the github repository private\n- git-crypting the VCR cassettes folder\n- adding the VCR cassettes folder to .gitignore\n- not using VCR and instead using other ways of mocking external calls in tests\nMaking the github repository private contradicts our commitment to coding in the open, and eventually opening a previously closed repository is often risky and hard to prioritise.\nEncrypting the VCR cassettes folder would effectively mean making a portion of the repository private.  There are three criteria for keeping code closed and they are: not publishing keys and credentials; algorithms related to detecting fraud; and anything around unreleased policy.  This situation does not meet any of these criteria.  Full documentation related to open source code guidance can be found [here](https://www.gov.uk/government/publications/open-source-guidance/when-code-should-be-open-or-closed).\nAdding the VCR cassettes folder to gitignore would mean that we could continue to make use of the gem in local development and during the CircleCi workflow.  The APIs that we are using are being actively developed against and it's important that we catch any changes as early as possible. Therefore a developer can re-record all cassettes at the start of a feature to check for changes, and then just reuse them to speed up the tests.  The cassettes then don't get committed to Github and access tokens are not exposed. CircleCi can continue to run the tests as normal and whilst anyone can access the project, cannot see the contents of the cassettes.\nLastly, there is an option to not use the VCR gem at all and use something like WebMock to mock every API call in the test suite. However, if there were any issues with API documentation being out of date we wouldn't necessarily know what to expect in the response.  Also, if we mock the calls there might have been changes we weren't aware of and find out late in to the development process, whereas actually making the calls will cause the tests to fail and let us investigate.\n\n## Decision\n","We will add the VCR cassettes folder to gitignore.\nWe will change the VCR config to allow a recording mode to be added i.e. ""all"" -> re-record all cassettes, ""new_episodes"" -> append the cassette(s) with any new calls made, ""once"" -> just record them once and reuse.\n",Add the VCR cassettes folder to .gitignore
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have already decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\nThe team are already very familiar with Rails and it is widely used within MOJ.\n\n## Decision\n,We will use Rails as our web framework for our new applications.\n,**Decision:** Use Rails as the web framework for our new Ruby applications.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur main source of data on prisoners and prison staff which we need for\nallocations is NOMIS.\nThere are now four APIs into NOMIS providing general data access, with varying\napproaches to presenting the data and authentication. We do not want to add to\nthis duplication.\nThe APIs which have been developed more recently are more under the control of\nHMPPS than the earlier ones (which were developed by a supplier). That gives us\nmore flexibility around how we work with them and makes it possible to get\nchanges into production more quickly. Using one of the newer APIs should mean\nthat we are less blocked by delays around API changes than we have been on our\nwork on Visit someone in prison.\nIt has been agreed by the HMPPS technical community that we would like to move\nall clients to use the Custody API in preference to the other APIs over time.\nAlthough that work has not yet been prioritised, using the Custody API for new\napplications will reduce the work needed in future to align our API usage.\nThe Custody API has been designed to give a more direct view of the data in\nNOMIS than the previous APIs have been - earlier approaches have favoured\nimplementing specific endpoints to meet the needs of service teams rather than\ngiving a more comprehensive view of all the data.\n\n## Decision\n",We will use the Custody API to access the NOMIS data we need.\nWe will work with the team in Sheffield on development of the Custody API to\nadd support for accessing the data we need.\n,We will use the Custody API for all new applications that require data from NOMIS.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Language use across HMPPS\nHMPPS has active development in four languages, including services with\nsignificant prison-staff-facing components in all four: Ruby, Python,\nJavaScript and Java.\nHMPPS has live services (which have passed service assessments) built in Ruby\nand Python. Ensuring that we support our existing users should be our top\npriority, so it is essential to maintain our skills in the languages used in\nour live services.\nOne of the advantages of a microservices approach is that teams can work on\nseparate services in different languages, using HTTP APIs to share data and\nfunctionality. There is no need for all services to be built in the same\nlanguage. We are already using this approach across HMPPS.\nThere is no clear vision or strategy at the moment for changing the number of\nlanguages in use across HMPPS. We are not in a position to decide that for all\nof HMPPS.\n### Team skills\nAll four languages in active use across HMPPS are represented to varying\ndegrees in the skill sets of the current members of the team, but only Ruby is\ncommon to all of them. The team have worked together on a live service built in\nRuby for all of their time at MOJ/HMPPS. We still own that service and\ncontinuously improve it alongside our work on Manage Offenders in Custody,\nalthough we are spending the majority of our time on the latter.\nThe primary language skills of HMPPS's civil servant developers and technical\narchitect (a significant proportion of whom are on this team) are in Ruby and\nPython. It is unrealistic to expect people to be equally proficient in many\nlanguages at the same time.\nThe team have already committed to learning about Kubernetes for the new Cloud\nPlatform (see [ADR 0002](0002-use-cloud-platform-for-hosting.md)) and to\nlearning Java so that we can collaborate on the APIs which are being built in\nSheffield (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)).\nThis is already a significant proportion of unfamiliar technologies for the\nteam to learn and use.\nDeveloping applications involves much more than using the standard library of\na language. The ecosystem of libraries tools around that language often takes\nmuch more work and time to become familiar with than the basics of the language\nitself. Although all of the team know some JavaScript, we do not all have\nexperience of using it for building server-side applications. We would therefore\nhave a lot more to learn if we were to choose to use JavaScript, as some\nrelated but separate services do.\n### Time constraints\nThe Offender Management in Custody programme has fixed timelines for its\nnational rollout in the next year. Although we are not committing to delivering\nparticular services at set dates months in advance, we will reduce our\nopportunity for learning from a smaller set of real users before the national\nrollout if we are not ready to take advantage of the Wales pilot which begins\nin January.\nWe know that allocations is only the first of several areas of the programme\nwhich are likely to need support from us, so timescales are tight for us.\nWe anticipate that the complexity of building this service lies in managing the\nquality of the data available across NOMIS, Delius and OASys, rather than in\nrepresenting that data to users.\nChoosing to use a less familar language for developing our applications, on top\nof what we already need to learn, would put us at significant risk of not\ndelivering working software until several months after our first users need it.\n### Code reuse\nUsing the same language for groups of similar services can make it easier to\nprovide a coherent experience for users by allowing presentation code to be\nshared more easily between services. However, the same HTML structure of pages\ncan be produced by services written in different languages. Since we are\ncommitted to progressive enhancement (see [ADR 0003](0003-use-progressive-enhancement.md)),\nwe will use client-side JavaScript solely to enhance the functionality of those\npages, and that JavaScript can be reused across services regardless of the\nlanguage used on the server.\nAs an example of this approach, there is a strong and active cross-government\ncommunity which develops, researches and supports design patterns, styles and\ncomponents which are used on services built in many different languages:\nhttps://design-system.service.gov.uk/\nWe will base our user-facing applications on this established design system in\nany case. There is already a variety of design approaches in use across the\nprison-staff-facing services we have, and our best chance of standardising that\nwell is to align ourselves with the cross-government approach.\nThat approach is supported by extensive user research over several years and\nacross many services and departments. Using it as our starting point reduces\nthe need for us to undertake duplicate research ourselves to understand the\neffectiveness of alternatives to those existing patterns. We expect that we\nwill need to extend those patterns and develop others inspired by them to meet\nour users' needs, and we will contribute what we learn back to the HMPPS and\ncross-government communities.\nSince it has been agreed that all services which need to use a NOMIS API should\nmigrate to the Custody API (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)),\nany API client library which we build in Ruby can be reused by other Ruby\nservices to ease their migration.\n### Operational considerations\nThe team has considerable experience of operating live services built in Ruby\nat scale.\nWe do not anticipate scaling to be a significant concern for allocation - we\nexpect to have a couple of hundred users a day at most for it.\nThe new Cloud Platform makes it easy, quick and cheap for us to scale up if we\nneed to.\n\n## Decision\n",We will use Ruby for the new applications we build in London as part of Manage\nOffenders in Custody.\n,"Based on the context provided, the decision is to continue using Ruby as the primary language for developing new services in the team. This decision is made based on the following considerations:\n\n* **Existing skills:** Ruby is common to all team members, and the team has significant experience in developing and operating live services in Ruby.\n* **Time constraints:** The team has tight timelines for delivery, and learning a new language would introduce additional risk and delay.\n* **Code reuse:** While it is possible to share presentation code between services written in different languages, the team believes that using Ruby will facilitate code reuse within the team's own services.\n* **Operational considerations:** The team has experience operating Ruby services at scale, and the new Cloud Platform makes it easy to scale up if needed.\n\nThe team acknowledges that other languages may have advantages, but believes that the benefits of using Ruby outweigh the potential benefits of switching languages at this time. The team will continue to monitor language trends and may consider using other languages in the future if it becomes necessary or advantageous."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose tooling for various continuous integration and deployment\ntasks:\n- running automated tests on branches/pull requests\n- running security and code quality checks on branches/pull requests\n- building Docker images\n- deploying applications to our environments\nJenkins and CircleCI are commonly used at MOJ for these tasks, and the team\nhave experience with both of them, as well as other options such as Travis.\nThe Cloud Platform team operate a Jenkins server but are keen to move services\naway from it, especially anything which doesn't use Template Deploy.\nThe LAA fee calculator is built and deployed using CircleCI with the new cloud\nplatform, so we'd have an example to follow if we also chose CircleCI. That\nteam didn't evaluate a lot of options when they made that decision, but it's\nworking well so far for them. Our applications are a little more complex than\ntheirs since we have a database, but we don't expect to be doing anything\nunusual in our build and deploy pipelines.\nThere's also some [documentation](https://ministryofjustice.github.io/cloud-platform-user-docs/02-deploying-an-app/004-use-circleci-to-upgrade-app/)\non using CircleCI with Helm for continuous deployment to the new cloud\nplatform - we may not want to follow it exactly, but it covers some useful\ntopics.\nOur team haven't been practising continuous deployment (that is, deploying\nevery change automatically to production) on Visit someone in prison - deploys\nto production for that service need to be triggered by a human. We'd like to\nkeep that option open for this service, though.\nIt's important to us that our build and deployment configuration is managed in\ncode. We've decided to start out with two applications (see [ADR 0004](0004-separate-api-and-user-facing-applications.md))\nand want to be able to easily make and reproduce changes to builds and\ndeployments for both.\nWe're keen to get started quickly - we'd rather not spend time assessing lots\nof tooling options at this stage. Our needs will evolve anyway, so it's fine\nto pick something that works for us now and revisit that decision later on if\nwe need to.\n\n## Decision\n",We will use CircleCI for continuous integration.\nWe will use CircleCI for deploying.\n,"We will use CircleCI for our continuous integration and deployment tasks.\n\nCircleCI is a well-supported and widely used tool that meets our needs for:\n\n- Running automated tests on branches/pull requests\n- Running security and code quality checks on branches/pull requests\n- Building Docker images\n- Deploying applications to our environments\n\nAdditionally, CircleCI is already in use by other teams at MOJ, which gives us access to shared knowledge and support."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPreviously, it was decided in [ADR 0004](https://github.com/ministryofjustice/offender-management-architecture-decisions/blob/master/decisions/0004-separate-api-and-user-facing-applications) that we would separate data and presentation concerns.  This however was reversed by [ADR 0010](https://github.com/ministryofjustice/offender-management-architecture-decisions/blob/master/decisions/0010-allocation-api-has-less-responsibility) which meant there was an overlap of data concerns with some data access from the presentation layer, and some via the Allocation API.\nIt was envisioned that starting off with more than one application would mean\nthat we would be able to structure the responsibilities early in the process and\nreduce later efforts, but in practice this has not happened. With more exposure\nto some of the APIs we are dependent on which service should access them has\nbecome less clear over time.  For instance, it was discovered that it was better\nfor the Allocation API to retrieve staff data from Elite2, rather than the\npresentation layer.\nThere was concern that later migration from a monolith to separate services would\nbe technical debt that we would be unable to pay off in future, due to other\ncompeting pressures. The cost of managing two different services, sharing contexts\nand overlapping boundaries has however increased the development complexity and\ncognitive load.\nThere was a requirement that other services are likely to require access to the\nallocation information that we have stored. This made sense when there was a\nclean separation of concerns (with all data access via the Allocation API) but\ncurrently provides little benefit. Whether the API is a separate service or\na modular component of a monolith is currently a deployment strategy as\narchitecturally it provides few benefits over a modular application. It is\nentirely possible to provide an API via a modular monolith.\nAs we have progressed with development, we have encountered issues with\nour approach of enriching API sourced data with locally acquired data. Processes\nwhere we retrieve data, enrich it with data from external APIs and then enrich it\nwith data from local APIs result in the movement of lots of data which has\nperformance costs.  Direct access to the database for 'local' data would\nremove issues with both performance and moving data across boundaries containing\nisolated (but related) logic.\n\n## Decision\n","We will integrate the existing Allocation API into the Allocation Manager, and make\nthe public api available at /api.\nWe will work in a single unified codebase in a well-designed modules to\nreduce some future effort in separating concerns.\n",We should migrate to a monolith architecture and deprecate the current Allocation API.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe team are familiar with the [Template Deploy](https://github.com/ministryofjustice/template-deploy)\nstack from their work on Visit someone in prison. This approach was developed\nseveral years ago as a temporary solution, but it doesn't meet our needs for\nmany reasons, including:\n- It defines the initial state of the stack, but does not reliably manage the\nstate of resources after that, so manual changes can be made to those\nresources which are not easily visible or managed in code.\n- It's expensive to run: each application has multiple dedicated EC2 instances,\nwhich typically run at very low load.\n- Deploys for some applications take 10-15 minutes, and if they're cancelled\nthe stack is left in an inconsistent state.\n- The monitoring configuration isn't specific to each application, and the\nstandard alert limits aren't appropriate for everything.\n- Few people understand how it works.\nThe Cloud Platform team are working on a new\n[Kubernetes-based hosting platform](https://ministryofjustice.github.io/cloud-platform-user-docs/#cloud-platform-user-guide),\nto replace Template Deploy. The LAA fee calculator is already live on that\nplatform, and other teams are using it for development and pre-live services.\nThis platform is MOJ D&T's strategic hosting choice.\nThe Sheffield ops team also run services built in the Digital Studio on AWS and\nAzure, including some in related areas to our work such as New NOMIS, Keyworker\nand Licences. There is an intent to move those applications to the new Cloud\nPlatform. Our team aren't familiar with those stacks.\n\n## Decision\n",We will use the Cloud Platform for hosting our applications.\n,The team will use the new Kubernetes-based hosting platform for all new services and will work with the Cloud Platform team and the Sheffield ops team to migrate existing services to the new platform as soon as possible.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDuring our work on Manage Offenders in Custody, we will have to make\narchitectural decisions around both design and scope of the work.\nWhen making decisions, we should record them somewhere for future reference,\nto help us remember why we made them, and to help teams working in related areas\nunderstand why we made them.\nWe should make our decisions public so that other teams can find them more\neasily, and because [making things open makes things better](https://www.gov.uk/guidance/government-design-principles#make-things-open-it-makes-things-better).\nWork done as a result of decisions we need to record is likely to be split\nacross multiple repositories.\n\n## Decision\n","We will use Architecture Decision Records, as described by Michael Nygard in\nthis article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\nAn architecture decision record is a short text file describing a single\ndecision.\nWe will keep ADRs in this public repository under decisions/[number]-[title].md\nWe should use a lightweight text formatting language like Markdown.\nADRs will be numbered sequentially and monotonically. Numbers will not be\nreused.\nIf a decision is reversed, we will keep the old one around, but mark it as\nsuperseded. (It's still relevant to know that it was the decision, but is no\nlonger the decision.)\nWe will use a format with just a few parts, so each document is easy to digest:\n**Title** These documents have names that are short noun phrases. For example,\n""ADR 1: Record architectural decisions"" or ""ADR 9: Use Docker for deployment""\n**Status** A decision may be ""proposed"" if it's still under discussion, or\n""accepted"" once it is agreed. If a later ADR changes or reverses a decision, it\nmay be marked as ""deprecated"" or ""superseded"" with a reference to its\nreplacement.\n**Context** This section describes the forces at play, including technological,\npolitical, social, and local to the service. These forces are probably in\ntension, and should be called out as such. The language in this section is\nvalue-neutral. It is simply describing facts.\n**Decision** This section describes our response to these forces. It is stated\nin full sentences, with active voice. ""We will ...""\n**Consequences** This section describes the resulting context, after applying\nthe decision. All consequences should be listed here, not just the ""positive""\nones. A particular decision may have positive, negative, and neutral\nconsequences, but all of them affect the team and service in the future.\nThe whole document should be one or two pages long. We will write each ADR as\nif it is a conversation with a future person joining the team. This requires\ngood writing style, with full sentences organised into paragraphs. Bullets are\nacceptable only for visual style, not as an excuse for writing sentence\nfragments.\n[adr-tools](https://github.com/npryce/adr-tools) can help us work with our ADRs\nconsistently.\nWe will link to these ADRs from other documentation where relevant.\n",Not provided in the given context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow to convert image files properly to pdf?\nSince there are thousands of different image formats, there will never\nbe support for all. The most common containers should be supported,\nthough:\n- jpeg (jfif, exif)\n- png\n- tiff (baseline, single page)\nThe focus is on document images, maybe from digital cameras or\nscanners.\n# Considered Options\n* [pdfbox](https://pdfbox.apache.org/) library\n* [imagemagick](https://www.imagemagick.org/) external command\n* [img2pdf](https://github.com/josch/img2pdf) external command\n* [tesseract](https://github.com/tesseract-ocr/tesseract) external command\nThere are no screenshots here, because it doesn't make sense since\nthey all look the same on the screen. Instead we look at the files\nproperties.\n**Input File**\nThe input files are:\n```\n$ identify input/*\ninput/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\ninput/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\ninput/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\ninput/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\n```\nSize:\n- jfif.jpg 240k\n- letter-en.jpg 467k\n- letter-en.png 191k\n- letter-en.tiff 4.0M\n\n## Decision\n","Tesseract.\nTo not use more external tools, imagemagick and img2pdf are not\nchosen, even though img2pdf shows the best results and is fastest.\nPdfbox library would be the favorite, because results are good and\nwith the [twelvemonkeys](https://github.com/haraldk/TwelveMonkeys)\nlibrary there is support for many images. The priority is to avoid\nmore external commands if possible.\nBut since there already is a dependency to tesseract and it can create\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\nwith images can be converted to searchable PDFs with images. And text\nextraction is required anyways.\n","Based on the information available, the best option for the use case is the `pdfbox` library. This library is specifically designed for working with PDF documents and has support for converting common image formats such as JPEG, PNG, and TIFF to PDF."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nThere are multiple web applications with their rest servers and there\nare multiple document processors. These processes must communicate:\n- once a new job is added to the queue the rest server must somehow\nnotify processors to wake up\n- once a processor takes a job, it must propagate the progress and\noutcome to all rest servers only that the rest server can notify the\nuser that is currently logged in. Since it's not known which\nrest-server the user is using right now, all must be notified.\n# Considered Options\n1. JMS (ActiveMQ or similiar): Message Broker as another active\ncomponent\n2. Akka: using a cluster\n3. DB: Register with ""call back urls""\n# Decision Outcome\nChoosing option 3: DB as central synchronisation point.\nThe reason is that this is the simplest solution and doesn't require\nexternal libraries or more processes. The other options seem too big\nof a weapon for the task at hand. They are both large components\nitself and require more knowledge to use them efficiently.\nIt works roughly like this:\n- rest servers and processors register at the database on startup each\nwith a unique call-back url\n- and deregister on shutdown\n- each component has db access\n- rest servers can list all processors and vice versa\n\n## Decision\n",Choosing option 3: DB as central synchronisation point.\nThe reason is that this is the simplest solution and doesn't require\nexternal libraries or more processes. The other options seem too big\nof a weapon for the task at hand. They are both large components\nitself and require more knowledge to use them efficiently.\nIt works roughly like this:\n- rest servers and processors register at the database on startup each\nwith a unique call-back url\n- and deregister on shutdown\n- each component has db access\n- rest servers can list all processors and vice versa\n,Use a database as a central synchronization point for communication between web applications and document processors.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nTo allow searching the documents contents efficiently, a separate\nindex is necessary. The ""defacto standard"" for fulltext search on the\nJVM is something backed by [Lucene](https://lucene.apache.org).\nAnother option is to use a RDBMS that supports fulltext search.\nThis adds another component to the mix, which increases the complexity\nof the setup and the software. Since docspell works great without this\nfeature, it shouldn't have a huge impact on the application, i.e. if\nthe fulltext search component is down or broken, docspell should still\nwork (just the fulltext search is then not working).\n# Considered Options\n* [Apache SOLR](https://solr.apache.org)\n* [ElasticSearch](https://www.elastic.co/elasticsearch/)\n* [PostgreSQL](https://www.postgresql.org/docs/12/textsearch.html)\n* All of them or a subset\n# Decision Outcome\nIf docspell is running on PostgreSQL, it would be nice to also use it\nfor fulltext search to save the cost of running another component. But\nI don't want to lock the database to PostgreSQL *only* because of the\nfulltext search feature.\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\npart of Lucene and therefore lives in the Apache ecosystem. I would\nchoose SOLR over ElasticSearch, because I used it before.\nThe last option (supporting all) is interesting, since it would enable\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\nthe database for docspell.\nIn a first step, identify what docspell needs from a fulltext search\ncomponent and create this interface and an implementation for Apache\nSOLR. This enables all users to use the fulltext search feature. As a\nlater step, an implementation based on PostgreSQL and/or ElasticSearch\ncould be provided, too.\n\n## Decision\n","If docspell is running on PostgreSQL, it would be nice to also use it\nfor fulltext search to save the cost of running another component. But\nI don't want to lock the database to PostgreSQL *only* because of the\nfulltext search feature.\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\npart of Lucene and therefore lives in the Apache ecosystem. I would\nchoose SOLR over ElasticSearch, because I used it before.\nThe last option (supporting all) is interesting, since it would enable\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\nthe database for docspell.\nIn a first step, identify what docspell needs from a fulltext search\ncomponent and create this interface and an implementation for Apache\nSOLR. This enables all users to use the fulltext search feature. As a\nlater step, an implementation based on PostgreSQL and/or ElasticSearch\ncould be provided, too.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow should the application be structured into its main components? The\ngoal is to be able to have multiple rest servers/webapps and multiple\ndocument processor components working togehter.\n# Decision Outcome\nThe following are the ""main"" modules. There may be more helper modules\nand libraries that support implementing a feature.\n\n## Decision\n","The following are the ""main"" modules. There may be more helper modules\nand libraries that support implementing a feature.\n","The application will be structured into the following main components:\n\n- **REST servers/webapps**: These components will be responsible for handling incoming HTTP requests and returning responses.\n- **Document processor components**: These components will be responsible for processing documents and performing other business logic.\n\nThese components will be deployed to separate servers or containers, and they will communicate with each other over a network using a message bus or other communication mechanism."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nSome PDFs contain only images (when coming from a scanner) and\ntherefore one is not able to click into the pdf and select text for\ncopy&paste. Also it is not searchable in a PDF viewer. These are\nreally shortcomings that can be fixed, especially when there is\nalready OCR build in.\nFor images, this works already as tesseract is used to create the PDF\nfiles. Tesseract creates the files with an additional text layer\ncontaining the OCRed text.\n# Considered Options\n* [ocrmypdf](https://github.com/jbarlow83/OCRmyPDF) OCRmyPDF adds an\nOCR text layer to scanned PDF files, allowing them to be searched\n\n## Decision\n",Add ocrmypdf as an optional conversion from PDF to PDF. Ocrmypdf is\ndistributed under the GPL-3 license.\n,"Integrate OCRmyPDF to automatically add an OCR text layer to scanned PDF files, enabling searching and text selection."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nSince docspell may store important documents, it should be possible to\nencrypt them on the server. It should be (almost) transparent to the\nuser, for example, a user must be able to login and download a file in\nclear form. That is, the server must also decrypt them.\nThen all users of a collective should have access to the files. This\nrequires to share the key among users of a collective.\nBut, even when files are encrypted, the associated meta data is not!\nSo especially access to the database would allow to see tags,\nassociated persons and correspondents of documents.\nSo in short, encryption means:\n- file contents (the blobs and extracted text) is encrypted\n- metadata is not\n- secret keys are stored at the server (protected by a passphrase),\nsuch that files can be downloaded in clear form\n# Decision Drivers\n* major driver is to provide most possible privacy for users\n* even at the expense of less features; currently I think that the\nassociated meta data is enough for finding documents (i.e. full text\nsearch is not needed)\n# Considered Options\nIt is clear, that only blobs (file contents) can be encrypted, but not\nthe associated metadata. And the extracted text must be encrypted,\ntoo, obviously.\n\n## Decision\n","* major driver is to provide most possible privacy for users\n* even at the expense of less features; currently I think that the\nassociated meta data is enough for finding documents (i.e. full text\nsearch is not needed)\n# Considered Options\nIt is clear, that only blobs (file contents) can be encrypted, but not\nthe associated metadata. And the extracted text must be encrypted,\ntoo, obviously.\nNo encryption, because of its complexity.\nFor now, this tool is only meant for ""self deployment"" and personal\nuse. If this changes or there is enough time, this decision should be\nreconsidered.\n",Decision: Encrypt only the contents of private documents.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nCurrently there is a `Scheduler` that consumes tasks off a queue in\nthe database. This allows multiple job executors running in parallel\nracing for the next job to execute. This is for executing tasks\nimmediately – as long as there are enough resource.\nWhat is missing, is a component that maintains periodic tasks. The\nreason for this is to have house keeping tasks that run regularily and\nclean up stale or unused data. Later, users should be able to create\nperiodic tasks, for example to read e-mails from an inbox or to be\nnotified of due items.\nThe problem is again, that it must work with multiple job executor\ninstances running at the same time. This is the same pattern as with\nthe `Scheduler`: it must be ensured that only one task is used at a\ntime. Multiple job exectuors must not schedule a perdiodic task more\nthan once. If a periodic tasks takes longer than the time between\nruns, it must wait for the next interval.\n# Considered Options\n1. Adding a `timer` and `nextrun` field to the current `job` table\n2. Creating a separate table for periodic tasks\n\n## Decision\n","The 2. option.\nFor internal housekeeping tasks, it may suffice to reuse the existing\n`job` queue by adding more fields such that a job may be considered\nperiodic. But this conflates with what the `Scheduler` is doing now\n(executing tasks as soon as possible while being bound to some\nresource limits) with a completely different subject.\nThere will be a new `PeriodicScheduler` that works on a new table in\nthe database that is representing periodic tasks. This table will\nshare fields with the `job` table to be able to create `RJob` records.\nThis new component is only taking care of periodically submitting jobs\nto the job queue such that the `Scheduler` will eventually pick it up\nand run it. If the tasks cannot run (for example due to resource\nlimitation), the periodic scheduler can't do nothing but wait and try\nnext time.\n```sql\nCREATE TABLE ""periodic_task"" (\n""id"" varchar(254) not null primary key,\n""enabled"" boolean not null,\n""task"" varchar(254) not null,\n""group_"" varchar(254) not null,\n""args"" text not null,\n""subject"" varchar(254) not null,\n""submitter"" varchar(254) not null,\n""priority"" int not null,\n""worker"" varchar(254),\n""marked"" timestamp,\n""timer"" varchar(254) not null,\n""nextrun"" timestamp not null,\n""created"" timestamp not null\n);\n```\nPreparing for other features, at some point periodic tasks will be\ncreated by users. It should be possible to disable/enable them. The\nnext 6 properties are needed to insert jobs into the `job` table. The\n`worker` field (and `marked`) are used to mark a periodic job as\n""being worked on by a job executor"".\nThe `timer` is the schedule, which is a\n[systemd-like](https://man7.org/linux/man-pages/man7/systemd.time.7.html#CALENDAR_EVENTS)\ncalendar event string. This is parsed by [this\nlibrary](https://github.com/eikek/calev). The `nextrun` field will\nstore the timestamp of the next time the task would need to be\nexecuted. This is needed to query this table for the newest task.\nThe `PeriodicScheduler` works roughly like this:\nOn startup:\n- Remove stale worker values. If the process has been killed, there\nmay be marked tasks which must be cleared now.\nMain-Loop:\n0. Cancel current scheduled notify (see 4. below)\n1. get next (= earliest & enabled) periodic job\n2. if none: stop\n3. if triggered (= `nextrun <= 'now'`):\n- Mark periodic task. On fail: goto 1.\n- Submit new job into the jobqueue:\n- Update `nextrun` field\n- Check for non-final jobs of that name. This is required to not\nrun the same periodic task multiple times concurrently.\n- if exist: goto 4.\n- if not exist: submit job\n- Unmark periodic task\n4. if future\n- schedule notify: notify self to run again next time the task\nschedule triggers\n","Option 2: Create a separate table for periodic tasks.\n\nThis option is preferred because it provides better isolation and scalability for periodic tasks. Having a separate table allows for more efficient querying and indexing of periodic tasks, and it also makes it easier to add new features to the periodic task system in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nThe question is whether the REST Api should return an ISO8601\nformatted string in UTC timezone, or the unix time (number of\nmilliseconds since 1970-01-01).\nThere is quite some controversy about it.\n- <https://stackoverflow.com/questions/47426786/epoch-or-iso8601-date-format>\n- <https://nbsoftsolutions.com/blog/designing-a-rest-api-unix-time-vs-iso-8601>\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\nis the better readability. But elm folks are on the other side:\n- <https://package.elm-lang.org/packages/elm/time/1.0.0#iso-8601>\n- <https://package.elm-lang.org/packages/rtfeldman/elm-iso8601-date-strings/latest/>\nOne can convert from an ISO8601 date-time string in UTC time into the\nepoch millis and vice versa. So it is the same to me. There is no less\ninformation in a ISO8601 string than in the epoch millis.\nTo avoid confusion, all date/time values should use the same encoding.\n# Decision Outcome\nI go with the epoch time. Every timestamp/date-time values is\ntransfered as Unix timestamp.\nReasons:\n- the Elm application needs to frequently calculate with these values\nto render the current waiting time etc. This is better if there are\nnumbers without requiring to parse dates first\n- Since the UI is written with Elm, it's probably good to adopt their\nstyle\n\n## Decision\n","I go with the epoch time. Every timestamp/date-time values is\ntransfered as Unix timestamp.\nReasons:\n- the Elm application needs to frequently calculate with these values\nto render the current waiting time etc. This is better if there are\nnumbers without requiring to parse dates first\n- Since the UI is written with Elm, it's probably good to adopt their\nstyle\n",Use epoch time for timestamp/date-time values.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nDocspell currently only supports PDF files. This has simplified early\ndevelopment and design a lot and so helped with starting the project.\nHandling pdf files is usually easy (to view, to extract text, print\netc).\nThe pdf format has been chosen, because PDFs files are very common and\ncan be viewed with many tools on many systems (i.e. non-proprietary\ntools). Docspell also is a document archive and from this perspective,\nit is important that documents can be viewed in 10 years and more. The\nhope is, that the PDF format is best suited for this. Therefore all\ndocuments in Docspell must be accessible as PDF. The trivial solution\nto this requirement is to only allow PDF files.\nSupport for more document types, must then take care of the following:\n- extracting text\n- converting into pdf\n- access original file\nText should be extracted from the source file, in case conversion is\nnot lossless. Since Docspell can already extract text from PDF files\nusing OCR, text can also be extracted from the converted file as a\nfallback.\nThe original file must always be accessible. The main reason is that\nall uploaded data should be accessible without any modification. And\nsince the conversion may not always create best results, the original\nfile should be kept.\n# Decision Drivers\nPeople expect that software like Docspell support the most common\ndocument types, like all the “office documents” (`docx`, `rtf`, `odt`,\n`xlsx`, …) and images. For many people it is more common to create\nthose files instead of PDF. Some (older) scanners may not be able to\nscan into PDF files but only to image files.\n# Considered Options\nThis ADR does not evaluate different options. It rather documents why\nthis feature is realized and the thoughts that lead to how it is\nimplemented.\n# Realization\n\n## Decision\n","People expect that software like Docspell support the most common\ndocument types, like all the “office documents” (`docx`, `rtf`, `odt`,\n`xlsx`, …) and images. For many people it is more common to create\nthose files instead of PDF. Some (older) scanners may not be able to\nscan into PDF files but only to image files.\n# Considered Options\nThis ADR does not evaluate different options. It rather documents why\nthis feature is realized and the thoughts that lead to how it is\nimplemented.\n# Realization\n","Docspell will support the most common document types (like `docx`, `rtf`, `odt`, `xlsx`, ...) and images in addition to PDF files."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nWith support for more file types there must be a way to extract text\nfrom all of them. It is better to extract text from the source files,\nin contrast to extracting the text from the converted pdf file.\nThere are multiple options and multiple file types. Again, most\npriority is to use a java/scala library to reduce external\ndependencies.\n# Considered Options\n\n## Decision\n","- MS Office files: POI library\n- Open Document files: Tika, but integrating the few source files that\nmake up the open document parser. Due to its huge dependency tree,\nthe library is not added.\n- PDF: Apache PDFBox. I know this library better than itext.\n","**Option 1: Tika**\n\n**Advantages:**\n\n* Open source\n* Supports a wide range of file types, including PDF, DOCX, and XLSX\n* Java library, reducing external dependencies\n\n**Option 2: Apache PDFBox**\n\n**Advantages:**\n\n* Open source\n* Specifically designed for handling PDF files\n* Provides fine-grained control over PDF processing\n\n**Option 3: JODConverter**\n\n**Advantages:**\n\n* Open source\n* Java library\n* Can convert a wide range of file types to PDF, including DOCX, XLSX, and PPTX\n\n**Decision:**\n\n**Option 1: Tika**\n\nTika is the recommended option due to its wide file type support, Java library, and open source nature. It provides a comprehensive solution for extracting text from a variety of file formats."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nDocspell should have support for files that contain the actual files\nthat matter, like zip files and other such things. It should extract\nits contents automatcially.\nSince docspell should never drop or modify user data, the archive file\nmust be present in the database. And it must be possible to download\nthe file unmodified.\nOn the other hand, files in there need to be text analysed and\nconverted to pdf files.\n# Decision Outcome\nThere is currently a table `attachment_source` which holds references\nto ""original"" files. These are the files as uploaded by the user,\nbefore converted to pdf. Archive files add a subtlety to this: in case\nof an archive, an `attachment_source` is the original (non-archive)\nfile inside an archive.\nThe archive file itself will be stored in a separate table `attachment_archive`.\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\n- `attachment_source`: report.jpg\n- `attachment`: report.pdf\n- `attachment_archive`: files.zip\nArchive may contain other archives. Then the inner archives will not\nbe saved. The archive file is extracted recursively, until there is no\nknown archive file found.\n# Initial Support\nInitial support is implemented for ZIP and EML (e-mail files) files.\n\n## Decision\n","There is currently a table `attachment_source` which holds references\nto ""original"" files. These are the files as uploaded by the user,\nbefore converted to pdf. Archive files add a subtlety to this: in case\nof an archive, an `attachment_source` is the original (non-archive)\nfile inside an archive.\nThe archive file itself will be stored in a separate table `attachment_archive`.\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\n- `attachment_source`: report.jpg\n- `attachment`: report.pdf\n- `attachment_archive`: files.zip\nArchive may contain other archives. Then the inner archives will not\nbe saved. The archive file is extracted recursively, until there is no\nknown archive file found.\n# Initial Support\nInitial support is implemented for ZIP and EML (e-mail files) files.\n",Store archive files in a separate `attachment_archive` table. Each archive file can contain multiple `attachment_source` files. The `attachment_source` table will contain references to the original (non-archive) files inside an archive.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow can office documents, like `docx` or `odt` be converted into a PDF\nfile that looks as much as possible like the original?\nIt would be nice to have a java-only solution. But if an external tool\nhas a better outcome, then an external tool is fine, too.\nSince Docspell is free software, the tools must also be free.\n# Considered Options\n* [Apache POI](https://poi.apache.org) together with\n[this](https://central.sonatype.com/artifact/fr.opensagres.xdocreport/org.apache.poi.xwpf.converter.pdf/1.0.6)\nlibrary\n* [pandoc](https://pandoc.org/) external command\n* [abiword](https://www.abisource.com/) external command\n* [Unoconv](https://github.com/unoconv/unoconv) external command\nTo choose an option, some documents are converted to pdf and compared.\nOnly the formats `docx` and `odt` are considered here. These are the\nmost used formats. They have to look well, if a `xlsx` or `pptx`\ndoesn't look so great, that is ok.\nHere is the native view to compare with:\nODT:\n{{ figure(file=""example-odt-native.jpg"") }}\n\n## Decision\n","Unoconv.\nThe results from `unoconv` are really good.\nAbiword also is not that bad, it didn't convert the chart, but all\nfont markup is there. It would be great to not depend on something as\nbig as libreoffice, but the results are so much better.\nAlso pandoc deals very well with DOCX files (using the `context`\nengine). The only thing that was not rendered was the embedded chart\n(like abiword). But all images and font styling was present.\nIt will be a configurable external command anyways, so users can\nexchange it at any time with a different one.\n",Use pandoc
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nUsers want to add custom metadata to items. For example, for invoices\nfields like `invoice-number` or `total`/`amount` make sense. When\nusing a pagination stamp, every item gets a pagination number.\nThis is currently not possible to realize in docspell. But it is an\nessential part when organizing and archiving documents. It should be\nsupported.\n# Considered Options\n\n## Decision\n","- values are strings at the database\n- values are strings when transported from/to server\n- client must provide the correct formatted strings per type\n- numeric: some decimal number\n- money: decimal number\n- text: no restrictions\n- date: a local date as iso string, e.g. `2011-10-09`\n- bool: either `""true""` or `""false""`, case insensitive\n","**Decision:**\n\nImplement an extension mechanism for custom metadata.\n\n**Rationale:**\n\nThis decision allows for a flexible and extensible solution to the problem of adding custom metadata to items. It avoids having to hardcode specific fields into the system, and instead allows users to define their own custom fields as needed. This flexibility is essential for supporting a wide range of use cases, including the organization and archiving of documents."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow can plain text and markdown documents be converted into a PDF\nfiles?\nRendering images is not important here, since the files must be self\ncontained when uploaded to Docspell.\nThe test file is the current documentation page of Docspell, found in\n`microsite/docs/doc.md`.\n```\n---\nlayout: docs\nposition: 4\ntitle: Documentation\n---\n# {page .title}\nDocspell assists in organizing large amounts of PDF files that are\n...\n\n## Decision\n","Java library ""flexmark"".\nI think all results are great. It depends on the type of document and\nwhat one expects to see. I guess that most people expect something\nlike pandoc-html produces for the kind of files docspell is for (it is\nnot for newspaper articles, where pandoc-latex would be best fit).\nBut choosing pandoc means yet another external command to depend on.\nAnd the results from flexmark are really good, too. One can fiddle\nwith options and css to make it look better.\nTo not introduce another external command, decision is to use flexmark\nand then the already existing html->pdf conversion.\n","**Decision**: Integrate headless Puppeteer as a lambda function.\n\n**Rationale**: Headless Puppeteer is a library that allows you to control a headless Chrome instance and generate PDFs from web pages. It is well-suited for this task because it:\n\n* **Is self-contained**: Headless Puppeteer does not require any additional dependencies or software to run, making it easy to deploy as a lambda function.\n* **Supports markdown**: Headless Puppeteer can convert HTML and markdown documents into PDFs.\n* **Is fast and efficient**: Headless Puppeteer is known for its speed and efficiency, making it suitable for processing large numbers of documents.\n\n**Alternatives Considered**:\n\n* **Third-party PDF converter APIs**\n    * **Pros:** Easy to integrate, no need to manage infrastructure.\n    * **Cons:** Can be expensive, lack of control over the conversion process, potential privacy concerns.\n* **Server-side rendering**\n    * **Pros:** Can render dynamic content, more control over the conversion process.\n    * **Cons:** Requires setting up and maintaining a server, can be more complex to implement."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow can HTML documents be converted into a PDF file that looks as much\nas possible like the original?\nIt would be nice to have a java-only solution. But if an external tool\nhas a better outcome, then an external tool is fine, too.\nSince Docspell is free software, the tools must also be free.\n# Considered Options\n* [pandoc](https://pandoc.org/) external command\n* [wkhtmltopdf](https://wkhtmltopdf.org/) external command\n* [Unoconv](https://github.com/unoconv/unoconv) external command\nNative (firefox) view:\n{{ figure(file=""example-html-native.jpg"") }}\nI downloaded the HTML file to disk together with its resources (using\n*Save as...* in the browser).\n\n## Decision\n",wkhtmltopdf.\nIt shows the best results.\n,Use the `wkhtmltopdf` external command.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nProject Thoth itself consists of many components all having their own release cycles and delivery artifacts such as\ncontainer image or Python libraries.\n\n## Decision\n,"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\nproject/product level while maintianing freedom of others to update.\n### Positive Consequences <!-- optional -->\n* users have a clear base line of versions, these versions have been tested with each other and have\nundergone integration testing.\n* a release can be referenced from documents, so that operational procedures have a clear relationship with component\nversions being used\n* we can maintain sets of different versions for different deployment environments\n* we can provide a version string with each API provided by the project\n### Negative Consequences <!-- optional -->\n* A release might not contain the latest versions of components\n<!-- markdownlint-disable-file MD013 -->\n",**Decision:** Establish a standard release cadence for all Project Thoth components.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n[Qeb-Hwt](https://github.com/marketplace/qeb-hwt) is a GitHub Application which adds `thamos advise` based output to\nPull Requests as a check. This functionality could be integrated into https://github.com/marketplace/khebhut and\ncomplexity and maintain costs.\n## Decision Drivers <!-- optional -->\n* cost of maintaining Qeb-Hwt code and app\n* redundancy of infrastructure\n\n## Decision\n,"* cost of maintaining Qeb-Hwt code and app\n* redundancy of infrastructure\nChosen option: ""merge function into Khebhut"", because we can reduce the cost of maintaining our software infrastructure\nby reducing redundancy.\n","**Decision:** Integrate the functionality of Qeb-Hwt into Khebhut.\n\n**Rationale:**\n\n* **Reduced maintenance costs:** Eliminating the need to maintain a separate codebase and application for Qeb-Hwt will significantly reduce maintenance costs.\n* **Simplified infrastructure:** Integrating the functionality into Khebhut will eliminate the need for separate infrastructure and setup for Qeb-Hwt, reducing complexity and potential points of failure.\n* **Increased efficiency:** By consolidating the functionality into a single application, the development and deployment process will be streamlined, resulting in increased efficiency and reduced lead times."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nYetibot currently resides across two primary repos:\n- github.com/yetibot/yetibot\n- github.com/yetibot/yetibot-core\nThese code bases continue to grow in size, and consist of a diverse range of\nfeatures, many of which many users won't care to use.\n\n## Decision\n","Switching to a plugin system allows us to split up the code base into much more\nfine grained, logical units. For example, we may split the `github` command into\nits own plugin.\nThe first plugin is [yetibot-kroki](https://github.com/yetibot/yetibot-kroki).\nWe will continue to extract plugins from both of the above code bases.\n","Migrate Yetibot codebase into a single repo, with submodules for each component."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nImage names are important for branding and let others identify easily a specific image they need. For example ""I want to work on computer vision project with Tensorflow, what stack and image should I use?"" Having a trusted well maintained source of images with clean naming convention can help on that.\n\n## Decision\n","Selected option: `ps-{application}` as it shows what our intention is: we want to provide a curated/predictable software stack, it might be used by ODH or RHODS or others, it might use S2I or other technology. Moreover helps from pipeline creation point of view, because the length of repo name on quay can crate issues.\n### Positive Consequences <!-- optional -->\n* users can immediately select an image based on the application they want.\n* using overlays we can have a variety of combination, not just for ml_framework\n### Negative Consequences <!-- optional -->\n* N/A\n<!-- markdownlint-disable-file MD013 -->\n","Establish a naming convention for images that include information about the image's content, purpose, and context. This convention should be documented and enforced to ensure consistency and ease of identification. The naming convention can may include:\n\n- Stack (TensorFlow, PyTorch, etc.)\n- Model (EfficientNet, ResNet, etc.)\n- Dataset (ImageNet, CIFAR-10, etc.)\n- Task (Classification, Object Detection, etc.)\n- Size (Small, Medium, Large)\n- Version (1.0, 2.0, etc.)"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Use an Architectural Decision Record (ADR) template to document architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## **Context**:\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\n\n## Decision\n","In order to do this passively, we changed the dependency class to default to primary unless otherwise specified.\nWe did this because we don't know if our consumers rely on the previous behavior of meta/health checking dependencies if they were of unspecified importance.\n","**Decision:** Replace the primary health checks with health checks on the Kubernetes Service created for the primary pods in a deployment.\n\n**Rationale:**\n\n* Ensures that the health check endpoint only checks the primary dependencies of a project, as intended.\n* Leverages the built-in health check capabilities of Kubernetes, ensuring consistency and reliability.\n* Simplifies the primary health check implementation and reduces maintenance overhead.\n* Facilitates easier configuration and management of health checks for primary dependencies."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** To implement an ADR (Architectural Decision Record) process.\n\n**Rationale:**\n\n* Ensures that architectural decisions are documented, reviewed, and approved.\n* Provides a single source of truth for architectural knowledge.\n* Facilitates communication and collaboration among stakeholders.\n* Helps to maintain consistency and architectural integrity throughout the project.\n\n**Implementation Details:**\n\n* Establish a template for ADRs.\n* Define a process for creating, reviewing, and approving ADRs.\n* Identify stakeholders responsible for participating in the ADR process.\n* Store ADRs in a version-controlled repository.\n* Review ADRs regularly to ensure they remain relevant and up-to-date."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\n\n## Decision\n","We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\nTable names                                       |\n--------------------------------------------------|\nrefunds-sessions-front-<opg_stackname>            |\nrefunds-sessions-caseworker-front-<opg_stackname> |\n","**Implement a centralized key management service to manage encryption keys for frontend user sessions.**\n\n**Rationale:**\n\n* **Improved Security:** A centralized key management service provides a secure and centralized location to manage and control encryption keys, reducing the risk of unauthorized access.\n* **Simplified Key Management:** By centralizing key management, operations such as key rotation, generation, and disposal become more efficient and auditable.\n* **Prevention of Synchronization Issues:** A centralized service ensures that all services have the most up-to-date encryption keys, eliminating synchronization issues and preventing errors during scaling operations.\n* **Control and Governance:** A centralized service allows for better control and governance over encryption key usage and access, ensuring compliance with security regulations and best practices.\n* **Supports Scaling:** A centralized key management service can efficiently handle key management requirements for scaling systems, reducing administrative overhead and ensuring secure key management even as the system grows."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe app primarily displays information for medical staff working in hospitals.  The information needs to come from somewhere and there are a few options:\n### Download content from a CMS/S3 bucket/Website\nWe could pull the information in the app from somewhere on the web.  Some kind of CMS, S3 bucket or website where the files are hosted would be the most obvious choices.\nThis does make it easier to quickly the information in the app, but would require the build out of more infrastructure and architecture to support all this.\nIt also could mean that if the internet connection goes down to a hospital in  a crisis any information that isn't already cached in the app would be unavailable to users.\n### Embed the content in the app\nThe alternative method is to embed all of the information in the app so that all of the information is included, after downloading the app.\nThis is architecturally much simpler and means all the content will be available offline if the internet should go down.\nSubsequently, a new version of the app needs to be released to update the information.\n\n## Decision\n","For the initial versions of the application, we will embed the content in the app.\nOnce the situation stabilises, and we can provide support to more hospitals, we can re-visit the decision and decide if we want to download content.\n","**Embed the content in the app.**\n\n**Rationale:**\n\n* Offline availability: Internet outages could prevent access to information from external sources, which is critical for medical staff. Embedding the content ensures information remains accessible even offline.\n* Architectural simplicity: Embedding the content reduces the complexity and infrastructure requirements compared to downloading from external sources.\n* Maintainability: App updates are the primary means of updating content, which is simpler compared to managing external infrastructure."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFlutter is a framework for building apps provided by Google.  Google describes it as a:\n> ... UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\nIt emphasizes fast development, expressive and flexible UI, and native performance.\nGregg Miller, a Senior Anesthesiologist at Western Health hospitals, reached out to us with a mobile app he had been developing to provide COVID-19 information to frontline medical staff.  The app that Gregg had developed in was with Flutter.  Gregg sought assistance and guidance from the GDG community to help him complete the app.  Considering time constraints, we needed to put together and ship an app to the hospital before the crisis swamped the staff, which at that time as predicted to occur in about two weeks.\nLuke immediately put out the call and assembled a team of Flutter developers from the GDG community.  We performed an initial evaluation on the codebase and decided that it would be best to deliver a re-write rather than building on the existing codebase.\nAt this stage it would have been possible to drop Flutter and build out native applications, or built the app using some other cross platform toolkit such as react native.\n\n## Decision\n",Time was of the essence - as we already had a team of Flutter developers ready to go we decided to continue on with using Flutter to build the app.  It was also likely that Flutters emphasis on fast development and an expressive UI would be helpful in quickly building out the app.\n,"**Decision:** Use Flutter to build the COVID-19 information application for frontline medical staff.\n\n**Rationale:**\n\n* Flutter is a framework that emphasizes fast development, expressive and flexible UI, and native performance.\n* The app needs to be shipped quickly to the hospital before the crisis swamped the staff.\n* A re-write of the existing codebase was necessary.\n* Dropping Flutter and building native applications or using a different cross-platform toolkit would have taken more time."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are three different versions of flutter that you can build out: Stable, Beta and Dev. Each version will come with various trade-offs.\nCurrently, there has been a missed stable release, and Beta is required to build iOS correctly.\n\n## Decision\n","We are using the beta channel.  Once some of the fixes land in stable, we may decide to revisit this decision.\n","Use the Beta version of Flutter to build iOS correctly, given the missed stable release."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDo we need a MarketPlace component?\n\n## Decision\n,It doesn't look like there is a requirement for a need of a MarketPlace component.  It's decided we will not have one for now.\n,"**Decision:** Yes, a Marketplace component is required.\n\n**Justification:**\n\nThe Marketplace component will provide a centralized platform for users to discover, acquire, and manage software and services. This will improve the overall user experience by simplifying the process of finding and using software and services. It will also enable us to better monetize our offerings by providing a platform for developers to sell their products."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and/or SMS.\n\n## Decision\n",A Notifier component is needed to send email and/SMS to customers.\n,"Implement a notification system using a microservices architecture with a message broker (such as Kafka or RabbitMQ) to handle communication between services. The system should include a service for sending emails, a service for sending SMS messages, and a central service for managing notifications and routing them to the appropriate service."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDo we need a Loyalty Program component?\n\n## Decision\n,It doesn't look like there is a requirement for a need of a Loyalty Program component.  It's decided we will not have one for now.\n,"**Decision:** Yes, we need a Loyalty Program component.\n\n**Rationale:**\n\n* The Loyalty Program component will provide customers with personalized rewards and incentives to increase customer loyalty and retention.\n* The program will track customer purchases, preferences, and interactions, enabling targeted campaigns and promotions.\n* By providing value and recognition to customers, the program can drive repeat business and positive word-of-mouth.\n* The data collected from the Loyalty Program will also provide valuable insights into customer behavior and preferences, which can be used to improve product offerings and marketing strategies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo ensure scalability, reliability and performance, microservice architecture should be used\n\n## Decision\n",All agreed with this decision\n,"**Decision:**\n\nAdopt a microservice architecture for the system to achieve scalability, reliability, and high performance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe state and data are required for the following components: Inventory, Customer, Order Management, Rating Manager and Recommendation Manager.  We had a database for each of the component initially.  However, it can save on the number of distrubted transactions by storing state and data in one database.\n\n## Decision\n",It was agreed that one database would be sufficient.\n,Store the state and data in a single database to reduce the number of distributed transactions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe purpose of Segmentation engine are:\n- grouping of customers' goals\n- targeted groups\n- history\n- purchasing behaviour\n- eating habit, location\n- managing coupons and promotions\nWhat is the difference between Recommendation and Segmentation engines?\n\n## Decision\n",It was decided to not have it.\n,"**Recommendation engines:**\n\n* Focus on personalizing content and product recommendations for individual users\n* Use machine learning and artificial intelligence to analyze user behavior and preferences\n* Provide tailored recommendations based on past purchases, browsing history, and similar user profiles\n\n**Segmentation engines:**\n\n* Divide customers into distinct groups based on shared attributes\n* Create targeted campaigns and marketing strategies for each segment\n* Use segmentation criteria such as demographics, psychographics, and purchasing behavior\n* Help businesses understand their customer base better and optimize marketing efforts"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nShould we have an Inventory Manager and a Fridge Manager where the Inventory Manager would hold the food items and Fridge would manage the physical fridges?\n\n## Decision\n,Inventory Manager can manage the following:\n- manage fridges\n- manage food items including their pricing\n- manage the number of items in each fridge\n,"**Decision:** Yes, we will have an Inventory Manager and a Fridge Manager.\n\n**Rationale:**\n\n* **Separation of concerns:** Separating the management of food items from the management of physical fridges allows for better organization and testability.\n* **Scalability:** The Inventory Manager can manage a large number of food items, while the Fridge Manager can handle numerous physical fridges.\n* **Flexibility:** The Inventory Manager can be updated independently of the Fridge Manager, allowing for changes to the food inventory without affecting fridge operations.\n* **Extensibility:** Additional features can be added to either the Inventory Manager or Fridge Manager without compromising the integrity of the system.\n* **Improved data management:** The Inventory Manager maintains a centralized record of all food items, while the Fridge Manager tracks the location and availability of food within each fridge."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\n\n## Decision\n,The decision is to introduce another queue for Order Management\n,**Prioritize messages from Order Management over messages from Rating Manager and Recommendation Manager in the queuing system.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\n\n## Decision\n",Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\n,"**Decision:** Yes, the messages sent from Order Management, Rating Manager, and Recommendation Manager can be asynchronous.\n\n**Rationale:**\n\n* Asynchronous messaging decouples the senders and receivers of messages, allowing them to operate independently and reducing the risk of system failures.\n* It enables scalability by allowing multiple instances of the message handlers to be deployed, distributing the load and improving throughput.\n* It provides flexibility in message delivery, allowing for different message delivery semantics (e.g., at least once, best effort) to be used based on the business requirements.\n* It supports eventual consistency between the sending and receiving systems, reducing the impact of temporary message failures on the overall system functionality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe shouldn't overwhelm the Notifier.  This would happen then we have many orders, surveys and recommendations to send to customers.\n\n## Decision\n",The decision is to introduce a queue for all these messages.\n,Aggregate and send notifications in batches to avoid overwhelming the Notifier.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\n\n## Decision\n,The decision is to have the polling mechanism to regularly obtain Smartfridges' state/inventory.\n,"Implement a cache of the Smartfridges' inventory. The cache will be updated regularly by the polling mechanism. This way, we can still have the last known inventory even if a Smartfridge goes down."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\n\n## Decision\n","We will have separate components to manage kiosks (along with point of sale systems), fridges, customers and vendors\n","Yes, we need a Kiosks-Fridge-customer-vendor-management system to manage kiosks, fridges, customers, and vendors. This system will help us to track the status of our kiosks and fridges, manage customer and vendor relationships, and ensure that our operations are running smoothly. This system will allow us to track key performance indicators (KPIs), such as sales, inventory, and customer satisfaction. It will also help us to identify trends and opportunities for improvement. By providing a centralized view of our operations, this system will help us to make better decisions and improve our overall performance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPeople are very used to traditional layered architectures and request/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\n\n## Decision\n",We will use a docker-compose.yml (v.2) as the primary entry point of the project\n,Use Docker to create a simple multi-container application
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want automated ETE tests on build to keep things consistent. We also want this to be free - because free is good. CircleCI provides a free tier and solid docker support.\n\n## Decision\n,Use CircleCI for automated ETE tests\n,Use CircleCI for automated ETE tests on build.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDue to WDQS having a lag on catching up with edits to Wikidata, and that lag being affected\nby the size of the entities being edited not just the rate of those edits, high volume edits can\ncreate strain on WDQS due to update processing of the dispatched update jobs,\nmore than WDQS update capacity can handle in a timely manner, resulting in sluggish WDQS.\nExample incident can be found here:\nhttps://www.wikidata.org/w/index.php?title=Wikidata:Administrators%27_noticeboard&oldid=963260218#Edoderoobot_edit_rate\n\n## Decision\n","As part of [T221774](https://phabricator.wikimedia.org/T221774), we want to\nretrieve WDQS lag, from relevant clusters, and add it to maxlag logic provided\nby mediawiki, in order to block edits when WDQS is lagging too much.\nWe also decided that querying the relevant WDQS endpoints lag should happen\nasynchronously, with lag info being cached with a reasonable ttl. That cached\nlag information then can be accessed, in a hook handler for\n[ApiMaxLagInfo](https://www.mediawiki.org/wiki/Manual:Hooks/ApiMaxLagInfo)\nthat will update lag info when they are available.\nThe following visual diagram shows the elements of the design\non conceptual level:\n![](./assets/QueryServiceLagHook_ConceptualDesign.png)\n","Set up a maintenance job that limits the rate of updates sent to the WDQS dispatch queue, based on the size of the entity being updated."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\n\n## Decision\n","The structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer/context approach in favour of `redux` complexity.\n","Use [Redux Modules](https://redux.js.org/tutorials/advanced/beyond-the-basics#using-multiple-reducers) to organize the application's state into separate, self-contained modules. Each module should contain its own reducer, actions, selectors, and sagas. This will make the application more modular, easier to understand, and less error-prone."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp to the point of writing this ADR, the SIA application aims to service only the municipality of Amsterdam. However, because of the nature of the application, more municipalities have shown an interest in the functionality the application has to offer.\nThis poses a challenge, because there is a lot in the application that is specific to the municipality of Amsterdam. To sum up, amongst others (in random order):\n- Docker registry URLs\n- URLs of API endpoints, machine learning service, Authz service and map server\n- Nginx configuration\n- HTML `<title />`\n- [`Sentry` Package](https://www.npmjs.com/package/@sentry/browser) dependency and configuration\n- [`Matomo` Package](https://www.npmjs.com/package/@datapunt/matomo-tracker-js) dependency and configuration\n- PWA manifest\n- Logo\n- Favicon\n- PWA icons\n- Hardcoded strings\n- Main menu items\n- Theme configuration\n- Maps settings\n- [`amsterdam-stijl` Package](https://www.npmjs.com/package/amsterdam-stijl) Package dependency\n- Package URL and description\nAll of the above need to be configurable or should be taken out of the equation to be able to publish a white-label version of `signals-frontend`.\n\n## Decision\n","Taking the pros and cons, the application's architecture, the Datapunt infrastructure and upcoming client wishes into account, the decision is made to go for the [Server-side / container](#Server-side / container) option.\nThe repository will contain default configuration options so that it can be run locally and still be deployed to (acc.)meldingen.amsterdam.nl without the application breaking.\nConfiguration is injected in the `index.html` file so it can be read at run-time.\n",**Decision:** Introduce a municipality-agnostic architecture.\n\n**Rationale:**\n\n* Allows the application to be easily customized for different municipalities.\n* Eliminates the need for hardcoding municipality-specific information.\n* Provides a framework for adding new municipalities and managing their configurations.\n\n**Consequences:**\n\n* Requires modifications to the existing codebase.\n* May introduce additional complexity to the application.\n* Requires careful coordination with stakeholders to ensure configuration settings are accurate.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDatabase schema migrations are often considered as part of an application change -\n""in order to do X we will need a new index Y""\nHowever, this does not work well with a zero-downtime approach, when we have\na single replicated database - we can't deploy version 0.2 of the application\nto some servers simultaneously with version 0.2 of the schema to their databases;\nwe need to deploy changes to the database either before or after the\ncorresponding application changes\n\n## Decision\n","Database schema changes should be made independently of application changes.\nWhere an application needs a change to the database, this may entail extra releases\nto make sure the application and database changes can be safely applied and\nsafely rolled back without compatibility problems.\nFor example, if there's a need to change a column name from ""foo"" to ""bar""\nyou may need:\n1. An application release that detects and will work with either a ""foo"" or a ""bar""\ncolumn\n2. A database schema release that renames the column from ""foo"" to ""bar""\n3. An application release that removes the logic in release 1, and just works with ""bar""\nAlternatively, the database change could be made first, using a view or triggers or other\nmechanisms so writes and reads to both ""foo"" and ""bar"" change the same data. This\nis highly dependant on the change needed and the database features available.\nThe database schema migration change should be treated as a first class release:\n1. The change should be run and tested in conjunction with the application\n2. The change should be reviewed and approved by an appropriate approver\n3. The change should be deployed to the Staging environment, and normal acceptance/smoke tests run\n4. The change should be deployed to Production/DR\n### A note on backups\nDepending on the complexity of the change, you may wish to coordinate a backup snapshot\nof the database before running a migration.  This will never be perfect, as with\na zero-downtime system there will still be data being written in the interval between\nthe backup being started (on a replica) and the migration running, so this data\nwould be at risk of being lost.\nMany database migrations however are totally safe and should not need a backup -\nfor instance, adding an index or adding a column is a very low risk change.  Nightly\nbackups should be enough to mitigate against any risk with this sort of change.\n### Alternative approach for Event Store using queues to avoid downtime\n*Only if really necessary* we could perform coordinated releases using queues\nto avoid downtime, for the special case of the Event Store which can be temporarily\nsuspended while messages get queued.\nThe approach would be similar to:\n1. Modify the event recorder Lambda timing so the event recorder does not run, or point it to a test queue instead\n2. Make a new database backup snapshot - it's assumed that for a change this complex, you need a backup\n3. Deploy the database change\n4. Test the change\n5. Re-enable the Lambda\nNote that the SQS queue has a limit of 100,000 queued messages - at peak we have\nhistorically received around 75,000 messages an hour.  So this technique is quite time constrained;\nif anything goes wrong you only have a small amount of time to fix it.\n","Implement database migrations as a separate deployable artifact, parallel to the application. For MySQL, use Flyway as a library embedded in the database migration artifact; For PostgreSQL, use Liquibase, a standalone command-line utility."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSee also ADR 0002 ""Database migrations are standalone releases""\nAs our system is designed for zero downtime, we have to be careful that\nwe don't change the database in a way that causes production issues\n\n## Decision\n","Where possible, we should avoid database migrations that will lock the database\nfor any significant amount of time.  This is hard to enforce, but we will\nmake sure there is documentation in the project README (and here!) on ways\nto achieve this.\nThis mostly affects index creation and changes - we have several years of data\nin our database, and adding or changing indexes can be slow.  In general,\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\ncreated in a non-blocking way.  See https://www.postgresql.org/docs/current/static/sql-createindex.html\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\nto queue up while the index change is made.  *BEWARE* however that SQS queues\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\nIf you have a very complex change, you should consider:\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\naltering indexes - generally our reports run intermittently, so it is safe to have\nno indexes for a period of time, data will still be appended with no problems\n- Performance testing the change - we have a large fake dataset available that\ncan be used to simulate a production database in a test environment\n- Duplicating the database - you could apply the change to a new database containing\na copy of production data, then switch databases, and migrate any missed changes\nfrom the old database to the new.\n### Transactional DDL changes\nMost Postgresql schema changes can be made transactionally - this is\na great feature, as it allows for making multiple changes and having them\nall roll back if something goes wrong.  For example:\n```\nBEGIN;\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\nUPDATE TABLE fizzbuzz set foo = 'splat';\nCOMMIT;\n```\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\nit depends on being able to change the table incrementally, which doesn't fit\nthe transaction model.  If the index creation fails, you are recommended to\ndrop the index and re-create it, as it won't be rolled back and may be\npartially created.\n### Avoiding blocking changes\nThere is a useful table in [this article](https://www.citusdata.com/blog/2018/02/15/when-postgresql-blocks/) which I've reproduced below (as the article may disappear):\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\nOur reports should only use SELECT so most operations won't block them.\nThe Event Recorder however needs to insert data, so you can't run\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\nNote however that some changes may be fast enough despite blocking - adding a column\nfor example.  However you should performance test these changes against a\nproduction-sized database to be sure!\n","To avoid database changes causing production issues, we will make all database migrations standalone releases. This will ensure that any changes to the database are thoroughly tested before being deployed to production."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way of running database migrations to the verify event store database.\n\n## Decision\n,We have chosen to use [Flyway](https://flywaydb.org) since the team has some experince with it and it seems like a simple and lightweight option.\n,"We decided to use Flyway to manage database migrations for the verify event store database.\n\n**Rationale**\n\nFlyway is a popular database migration tool that is well-suited for managing migrations in a continuous integration and deployment (CI/CD) pipeline. It is easy to use and configure, and it supports a wide range of database platforms.\n\n**Consequences**\n\n* **Benefits:**\n    * Flyway will help us to ensure that the verify event store database is always up-to-date with the latest migrations.\n    * Flyway will make it easier to manage database migrations in a CI/CD pipeline.\n* **Drawbacks:**\n    * Flyway will require some additional setup and configuration.\n    * Flyway may not be compatible with all database platforms."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe aim of this project is to allow teams to quickly create their own Jenkins\nplatform, running on AWS.\nJenkins has a master/agent architecture, where a single master node can trigger\njobs running on multiple agents. This decision concerns how we deploy, configure\nand manage both the master and the agents.\n\n## Decision\n","We will make both the master and the agents Docker containers, running on\nseparate managed EC2 instances. We will provision and manage these directly for\nnow, without using an orchestration service like ECS or Kubernetes.\n### Use of separate instances\nHaving separate instances for the master and the workers increases security by\nmaking it impossible for code running in worker jobs to affect the master.\nIn addition, this allows for teams to increase capacity by adding extra worker\nboxes as required.\n### Use of Docker for master\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\nteams' abilities to quickly respond when new Jenkins versions are released in\nresponse to security vulnerabilities, for example.\nIn addition, using Docker means that the configuration can be kept in the git\nrepository along with the rest of the code, rather than managed via the Jenkins\nUI.\n### Use of Docker for agents\nRunning the workers as Docker containers allows isolation of each job, ensuring\nthat each job starts from a known state, making it possible to target a specific\nconfiguration via the Jenkinsfile, and increasing security by making it\nimpossible for the job to affect the underlying VM.\n### Not using orchestration frameworks\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\nsimple enough that it is not worth introducing the added complexity for this\nproject.\n",The master and agents will be deployed using a combination of Ansible and AWS CLI\ncommands.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to implement HTTPS and TLS termination as part of the template we offer in our MVP.\nWe identified 4 ways to get that done.\n### Technique #1 - using existing nginx inside the Jenkins container\nOur Jenkins master container has nginx installed in it. We can install the certificates and keys\nthere. That is probably the simplest solution without the need of introducing other\ninfrastructure components. Also, that allows us to benefit from having an EIP, which is\nstatic, so it could be used for IP whitelisting, if necessary.\nHowever, the installation and renewal of certificates has to be done manually, which is a major downside.\n### Technique #2 - using ELB\nWe introduce an ELB in front of the existing infrastructure, like so:\nDNS resolution -> ELB -> Jenkins master EC2\nCompared to #1, the major benefit is the ability to manage the provisioning of the certificate fully within AWS using [AWS Certificate Manager](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-server-cert.html).\nThe downsides is not having an EIP (for whitelisting) and that automatic renewal of the certificate meaning higher operational overhead.\n### Technique #3 - using ELB with ASG\nThis is like Technique #2 but we introduce an ASG (auto-scaling group) for the EC2 master instance.\nBy using ACM, ELB and ASG together, it allows AWS to manage the certificate provisioning and renewal, thus removing any operational overhead.\n### Technique #4 - using a NLB\nLike #2 but with a Network Load Balancer (NLB), like so:\nDNS resolution -> EIP -> NLB -> ELB -> Jenkins master EC2\nThe advantage is that we can have an EIP. However, we probably need Cloudwatch\nto detect changes in the EIP which would trigger a Lambda to register the new EIP with the NLB, this is a suggested solution from AWS, but the downsides is that it is complex, expensive and difficult to manage.\n\n## Decision\n","After exploring and understanding all of the techniques and discussion within the team, option 3 was chosen.\nWe draw upon experience of the solution from others within GDS who had successfully implemented it in the past.\nWe also liked that it reduces the operational overhead of managing certificates completely.\nWe were concerned that since does not offer static IPs (via EIP) but we decided that they were not necessary for the MVP as they had not been identified as important during our user research sessions.\n",Technique #3 - using ELB with ASG
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\nWe have identified a number of ways to do this:\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\n2. Automatically create the jobs by scanning a Github account\n3. [Jenkins Job Builder]\n4. [Job DSL plugin]\n### Option 1\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\n### Option 2\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\n### Option 3\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\n### Option 4\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\n\n## Decision\n","We decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\n",Option 1: Define jobs with Groovy and inject the script
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo guide our work, we need to decide whether we use AWS’ Elastic File System\n(EFS) or Elastic Block Store (EBS) as the cloud storage service for our Jenkins\nbuild.\nThe decision is needed as it will impact on the team’s work and may be affected\nby wider GDS or Government strategy.\n\n## Decision\n","Based on the below pros and cons, we have decided to remain with EFS as the\ncloud storage service, until such a time that there is a strategic need to\nchange.\nBecause of the way in which the team has compartmentalised the architecture\nof and approach to this Jenkins build, it is eminently possible to move to EBS\nat any point. It also means that further development of the system will not make\nit more complicated to do so.\nIt is for this reason that the decision is not to carry out this work in\nanticipation of it being needed, but rather focus on existing required work and\nonly carry out the work of converting to EBS should the need arise, at which\npoint that work would need to be prioritised.\n","**Decision:** Use Elastic File System (EFS) for the cloud storage service for the Jenkins build.\n\n**Rationale:**\n\n* **Shared access:** EFS allows for multiple EC2 instances to access the same file system, enabling a team to work on the same build.\n* **High availability:** EFS is a distributed file system that provides redundancy and high availability, ensuring that builds can continue even if individual EC2 instances fail.\n* **Elasticity:** EFS automatically adjusts its capacity based on demand, ensuring that it can handle the varying storage needs of the build process.\n* **Cost:** EFS offers pay-as-you-go pricing without the need for upfront capacity purchases, making it more cost-effective for the team's current needs.\n* **Alignment with GDS strategy:** EFS is a recommended cloud storage service by GDS for use in production environments, indicating its alignment with wider GDS strategy."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo guide our work, we need to decide on our approach to supporting (or not)\na build system that we may provide. This document outlines the pros and cons of\npossible options and recommends an approach.\nOnce decided, this decision will be used to hold further workshops to design the\nprocesses and technology required to put that approach into practice.\n### The options\nWe discussed 3 options that affect our support approach, plus a fourth that\ninvolves a third party and does not necessarily affect the way we support our\nproduct. As such, the fourth does not have pros and cons articulated below.\n1. Release a working and stable infrastructure as code. Unpinned versions, so\neach fresh installation includes the latest versions of everything.\n2. Host it (essentially become a SaaS provider) and maintain a limited suite of\nplugins.\n3. Periodically release stable versions of Jenkins (as code), with a suite of\nplugins with their versions pinned. Provide guidance on upgrading with each\nrelease.\n4. Pay for an externally hosted (and therefore maintained) version of Jenkins.\nDon’t provide anything ourselves except recommendation and perhaps a little\nguidance\nHybrids of some of these are possible. For example, we could have Jenkins as\ncode (1 or 3) for those who need more configurability whilst also\nrecommending/paying for an externally hosted thing (4). Or we could both provide\ncode (1 / 3) and also host something (2).\n### Option 1: Infrastructure as code only\n**Pros**\n* Supposedly more secure (because latest versions include security updates)\n* Encourages good behaviour (not providing a black box solution forces teams to\nengage with the solution)\n**Cons**\n* Implicit trust up-stream means security vulnerabilities could be introduced\n* Everything could break each time a new version is released (we can’t ensure\ncompatibility)\n* The build becomes unreproducible\n* Build versions can’t be tracked\n* Latest versions would only be installed for fresh installs, so potentially\nincreasing divergence from secure versions\n### Option 2: Host Jenkins ourselves\n**Pros**\n* Teams transfer all of the ‘cons’ to us\n* Teams don’t need certain expertises\n* We can enforce upgrades (security)\n**Cons**\n* Expensive (people, infrastructure, etc.)\n* Cost recovery has to be dealt with\n* Cost might be a barrier to adoption\n* Less configurable for teams\n* Residual unused instances have to be managed\n* Have to manage OLAs/SLAs\n* Need MTM/SMT agreement\n* Service becomes a single point of failure for all service teams using it\n(up-time, etc.)\n### Option 3: Release stable pinned versions periodically\n**Pros**\n* Users are reassured that it will work\n* ^ therefore lower barrier to adoption\n* Central point for monitoring security fixes\n* Automated testing (integration/acceptance) of version compatibility, etc. is\npossible\n* Guidance and optimised workflow = easier for teams to do things properly\n**Cons**\n* Someone has to stay current (security and updates)\n* Someone has to maintain and update it\n* Limited plugins: the fewer we support the more teams have to support (but may\nnot)\n* Limited plugins: if teams use a lot around GDS then maybe puts pressure on us\nto provide more in our 'limited' suite\n\n## Decision\n","Based on the above pros and cons, we recommend that option 3 (perhaps with\noption 4 as well, depending on user needs that are uncovered) is the best\noption.\n","Option 3 should be implemented, which includes periodically releasing stable versions of Jenkins (as code), with a suite of plugins with their versions pinned. It will provide guidance on upgrading with each release."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a desire from users to be able to build docker images as an artifact from their jobs, this creates a security problem because of technical limitations within docker.  This ADR is the result of discovery work to discern whether there are alternative solutions to building docker images securely.  The discovery focussed on evaluating the software product called [Buildah].\n## Technical Problem\nAs the workers run within a docker container, they may need to build another docker image as an artifact.  To do so requires that the host servers docker socket be exposed to the worker container.  The docker daemon runs as root and has full control of the host server, meaning malicious code running within the workers docker container could trivially gain access to the host system by mounting (via -v) the host systems root.\nAs Docker does not support RBAC to control and restrict access to its socket there is no way provided by docker to prevent this privilege escalation method occuring.\nBuildah solves this problem by being able to build a docker container without the need to use docker and the vulnerable socket.\n\n## Decision\n","Buildah addresses the problems of docker container privilege escalation, is feature rich to support typical use cases (as it is feature compatible with docker), and provides us a means in which to provide docker image artifacts.\n",To use Buildah to build docker images.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in\n[documenting architecture decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",Create an Architectural Decision Record (ADR).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAny service activator pattern will have a message pump, which reads from a queue.\nThere are different strategies we could use, a common one for example is to use a BlockingCollection to hold messages read from the queue, and then use threads from the threadpool to process those messages.\nHowever, a multi-threaded pump has the issue that it will de-order an otherwise ordered queue, as the threads will pull items from the blocking collection in parallel, not sequentially.\nIn addition, where we have multiple threads it becomes difficult to create resources used by the pump without protecting them from race conditions.\nThe alternative is to use a single-threaded message pump that reads from the queue, processes the message, and only when it has processed that message, processes the next item. This prevents de-ordering of the queue, because items are read in sequence.\nIf a higher throughput is desired with a single threaded pump, then you can create multiple pumps. In essence, this is the competing consumers pattern, each performer is its own message pump.\nThe message pump performs the usual sequence of actions:\n- GetMessage. Read Message From Queue\n- Translate Message. Translate Message from Wire Format to Type\n- Dispatch Message. Dispatch Message based on Type\nPrior art for this is the Windows Event Loop which uses this approach, and is used by COM for integration via the Single-Threaded Apartment model.\n\n## Decision\n",Use a single-threaded message pump to preserve ordering and ensure sequential access to shared resources. Allow multiple pump instances for throughput.\n,Use a single-threaded message pump.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement a formal Architectural Decision Record (ADR) process.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https://www.terraform.io/docs/enterprise/guides/recommended-practices/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\n\n## Decision\n",Use workspaces to manage multiple environments.\n,**Decision:** Use Terraform workspaces to manage multiple environments.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\n\n## Decision\n,Use Terraform Cloud for teams\n,**Decision:** Use Terraform Cloud for state management.\n\n**Rationale:**\n\n* Terraform Cloud stores state encryption at rest.\n* Terraform Cloud provides collaboration features for workspaces.\n* Terraform Cloud reduces the burden of managing AWS/GC storage and DynamoDB locking.\n* Terraform Cloud is suitable for small teams.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement Architectural Decision Records (ADRs) using a lightweight template and a version-controlled repository.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA couple of client projects need a simple authentication system. One of the\nprojects already uses Flask and Postgres, while another is in the design phase.\nIn the short term we want to have a minimal and functional authentication system\nimplemented as soon as possible.\nIn the very long term we hope that this implementation would be reused many\ntimes and to have easily customized drop-in libraries for Flask projects.\n\n## Decision\n","Create a minimal reference authentication implementation that uses Flask and\nPostgres. Include unit tests (hopefully strive for very high code coverage), and\ndatabase migrations.\nOrganize the database logic into a simplified CQRS-inspired style code structure:\n* app/models.py contain all sql models.\n* app/services.py contain all db commands that modify database state.\n* app/queries.py contain all db queries to the database.\nDelay any features that aren't in our current project requirements. The kinds of\nfeatures that may be addressed in future versions, but not initially:\n* Implement this reference as a [Flask extension](https://flask.palletsprojects.com/en/1.1.x/extensiondev/), and possibly add to the PyPi public repositories.\n* Create a React reference implementation.\n* password resets\n* email account verification\n* permissions\n* multi factor authentication\n* OAuth\n* Expiring tokens\n* Revoking/blacklisting tokens\n* prevent re-use of old passwords\n","**Decision:** Utilize Flask-Login as the authentication system for both immediate and long-term needs.\n\n**Rationale:**\n\n* **Short-term suitability:** Flask-Login is a lightweight and straightforward authentication library for Flask applications, meeting the immediate requirement for a functional system.\n* **Long-term viability:** Flask-Login has a proven track record and a large community, supporting its potential for future reuse and customization.\n* **Flexibility:** Flask-Login allows for easy customization, enabling adaptation to specific project requirements.\n* **Ease of implementation:** Flask-Login's straightforward integration with Flask simplifies implementation, reducing development time.\n* **Community support:** The active community provides resources and support for troubleshooting and feature enhancements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe plan to use AWS as a development/staging environment and as a production environment, on ECS.\n\n## Decision\n","Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\nCircleCI will need to be configured with the following environmental variables\nin order to deploy docker images (using [CircleCI's aws-ecr\norb](https://circleci.com/developer/orbs/orb/circleci/aws-ecr)):\n- AWS_ACCESS_KEY_ID\n- AWS_SECRET_ACCESS_KEY\n- AWS_REGION\n- AWS_ECR_ACCOUNT_URL\nThis project's Dockerfile requires access to a privately hosted github project\n(openfido-utils). Use [Buildkit](https://docs.docker.com/develop/develop-images/build_enhancements/) on CircleCI to grant SSH access to the docker build processes.\n","**Decision:** Use the same account for staging/development and production environments on AWS ECS.\n\n**Rationale:**\n\n* **Cost-effectiveness:** Using a single account eliminates the need for cross-account data transfer charges.\n* **Simplified management:** It simplifies user management, resource allocation, and monitoring across environments.\n* **Improved security:** By using a single account, access control and security measures can be applied consistently to all environments.\n* **Streamlined deployment:** Automated deployment processes can be optimized for a single account, ensuring faster and more reliable deployments.\n* **Consistency:** Maintaining a single account ensures that configurations, tools, and practices are consistent across environments, reducing potential errors and facilitating cross-team collaboration."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** Implement an Architectural Decision Record (ADR) process.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent method for capturing and documenting architectural decisions.\n* They improve transparency and traceability by providing a single source of truth for architectural decisions.\n* ADRs enable the tracking of decisions over time, facilitating audits and future decision-making.\n* They promote collaboration by facilitating the sharing and discussion of architectural options among team members.\n\n**Details:**\n\n* Establish a template or framework for ADRs, including fields for decision identifier, date, decision context, options considered, decision rationale, and impact.\n* Define a process for creating, reviewing, and approving ADRs.\n* Establish a central repository or knowledge base for storing ADRs and ensuring accessibility to all stakeholders.\n* Encourage team members to contribute to and maintain the ADR repository.\n* Regularly review and update ADRs as the project evolves to ensure they remain current and accurate."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Migrate to React Query/Context API/Hooks from Redux\n## Context\nCurrently, HospitalRun Frontend uses [redux](https://react-redux.js.org/) to manage async data\nfetching through [redux thunk](https://github.com/reduxjs/redux-thunk).\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\nsince it always goes back to PouchDB/CouchDB for the most recent data. Redux code is often verbose and\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https://redux-toolkit.js.org/),\nthe application still has a lot of boilerplate. Due to the limited amount of global application\nstate, the desire to reduce boilerplate redux code, and new/other libraries available,\nRedux is no longer the correct tool.\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\ndifficult to isolate testing just to the specific component. For components that save data, it\nis difficult to mock the actions that are dispatched.\n[react-query](https://github.com/tannerlinsley/react-query) is a library for ""handling fetching,\ncaching, and updating asynchronous data in React"". This library has become a popular replacement\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\nstore.\nFor the few uses cases that require global application state (i.e. session information),\nthe [React Context API](https://reactjs.org/docs/context.html) is an alternative from the\nReact library. The context API is designed for the use case of sharing state across\ncomponents.\n\n## Decision\n","HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\ncontext api to manage shared application state such as user information, and hooks for sharing\ncode and business logic.\n",Migrate to React Query and the React Context API to replace Redux for managing async data fetching and global application state.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nWe want to give support to any new Froala major version. For this reason, we must decide whether to create a new plugin for the new version of Froala, or use an existing one.\nBetween major versions of Froala, there can be *differences*, or not, in the way to integrate external plugins. This will be the reason to create a new MathType plugin or not:\nWe Currently have one package for each major Froala version, even though that's not the way Froala works: they have a unique package that stores all the versions. Even if we decided to follow the conventions Froala does, we would have to rename or delete our current modules, and npm wouldn't allow it, meaning that once a name is used, it can't be used never again for a new package.\n\n## Decision\n","We will create and give the Froala MathType packages for any new major version, the following name: *@wiris/mathtype-froalaX*, where X equals the oldest Froala major version. Then, depending on whether the way to integrate in external plugin is the same or different:\n- If the integration is different &rightarrow; Create a new MathType package for the major Froala version.\n- If the integration is the same &rightarrow; Use the same package that already exists for the previous major version. A good example is the situation we are in with the versions 3 and 4, where the integration is the same, so the package for both versions is the mathtype-froala3.\n### Pros and Cons of the Options\n#### A same MathType package to support more than one Froala major versions\nGood, because you'll only maintain one package for multiple identical major versions.\nGood, because it'll reduce DRI.\nBad, because the package name is not self-explanatory.\nBad, because the package name can create confusion on our users\nBad, because our current package convention name includes the version number on it\n#### Create a new package for every new major Froala version using symlinks when the versions external plugin integrations are identical\nGood, because it'll reduce DRI.\nGood, because we know it works, for previous experience with TinyMCE v4 and v5.\nGood, because the name is self-explanatory, since includes the Froala major version.\nBad, because, we'll have to maintain more than one package, at least.\nBad, because the infrastructure is more complex.\n#### Create a common package to store the identical code between Froala major versions that have the same external plugins integration, and create a new package for each version that uses the common one\nGood, because it'll reduce DRI.\nGood, because the name is-self explanatory, since includes the froala major version.\nBad, because, we'll have to maintain more than one package, at least thee.\n","Decision: Create a new plugin for the new version of Froala.\n\n*Reasoning:*\n\n* Between major versions of Froala, there can be differences in the way to integrate external plugins. This could require significant changes to our existing MathType plugin, or even make it incompatible with the new version of Froala.\n\n* Creating a new plugin for the new version of Froala would allow us to take advantage of any new features or improvements in the Froala API, and ensure that our plugin is fully compatible with the new version.\n\n* While it would require some additional development effort, we believe that creating a new plugin is the best long-term solution for supporting multiple versions of Froala."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nWhen writing custom scripts in the package.json files, one must take into account that those commands\nshould be compatible with both sh and CMD.exe. So, it is common to use third-party npm packages that\nact as an API to mimic these tasks in both environments.\nOne of the most common tasks we do in these scripts is file manipulation (deleting, copying, moving).\n### Pros and Cons of the Options:\n#### Difference Between ShellJS and shx\n**ShellJS:**\nGood for writing long scripts, all in JS, running via NodeJS (e.g. node myScript.js).\n**shx:**\nGood for writing one-off commands in npm package scripts (e.g. ""clean"": ""shx rm -rf out/"").\n\n## Decision\n","It is proposed to use [shx][shx-url], a wrapper for sh commands that works on Windows too, and behaves as the corresponding sh commands.\nIt has more than 150K downloads per week, is well-maintaned by the community and it occupies less space than our current past alternatives: rimfaf and copyfiles.\n","Use shx for writing one-off commands in npm package scripts, since it's more convenient and easy to read."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nIn the context of the Telemetry project, we want to generate\nvalid and consisten UUIDs since they are needed to ensure the uniqueness, randomeness and its validity.\nUUID identifiers have an specification defined as the RFC-4122 standard  [A Universally Unique IDentifier (UUID) URN Namespace](https://tools.ietf.org/html/rfc4122) on ietf.org.\nWe don't want to reinvent the wheel and it seems unwise to write our own library to generate this UUIDs since there are third party solutions with good support, small and secure.\n\n## Decision\n","We'll use [uuid package] to generate RFC4122 version 4 UUIDs to use on the Telemetry implementation. The code of the [uuid project] is available at github.\nTherefore, **uuid** becomes the first functional dependency of the 'MathType Web Integration JavaScript SDK', known as npm package as '@wiris/mathtype-html-integration-devkit'.\n* [uuid project](https://github.com/uuidjs/uuid)\n* [uuid package](https://www.npmjs.com/package/uuid)\n### Pros and Cons of the Options\n#### Implement our own Javascript library for that\n- Bad, because Javascript Math.random function is not very good.\n- Bad, because we'll need to maintain it.\n- Bad, because more work to the backlog.\n- Bad, because we're reinventing the wheel.\n#### Using a third party library like github.com/uuidjs/uuid\n- Good, because supports RFC4122 version 1, 3, 4, and 5 UUIDs.\n- Good, because its well maintained, no issues and widely used; (26.085.977 downloads/week).\n- Good, because solves our problem immediately.\n- Good, because it's secure, small and cross-platform.\n- Bad, because we're adding a dependency to our core library, and therefore, to all our Javascript plugins.\n","Use the UUID library as it is small, secure and provides good support for generating valid and consistent UUIDs as per RFC-4122 standard."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nReading the Telemetry documentation proposed by the Data Science team, we have\nencountered that the sender and session ids are not concretely defined for each\nproduct.\nIn the case of MathType web, there could be various interpretations, e.g.\n- senderId: changes on page load.\n- sessionId: changes on opening MathType.\n\n## Decision\n","We asked the Data Science team and finally settled on the following interpretation:\n- senderId: ideally lasts for ever. In practice, should at least last in a same web session, across page loads.\n- sessionId: changes on page load.\n### Pros and Cons of the Options\n#### Change senderId on page load and sessionId on opening MathType\n- Bad, because it does not truly identify individual users.\n#### Keep senderId and change sessionId on page load\n- Good, because it better represents individual users.\n- Bad, because it's hard to keep permanent data on the client's browser.\n","The senderId should change on page load, and the sessionId should change on opening MathType."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA method to authorize against the youtube API to get the access token needs to be added to the application.\n\n## Decision\n,I decided to use the launchWebAuthFlow instead of the getAuthToken api to do the oauth2 authentication. This is\nbecause getAuthToken can only authorize the google signed in in chrome. This prevents me from using a different account\nfor youtube from my chrome account.\n,"The application will use OAuth 2.0 to authorize against the YouTube API. OAuth 2.0 is an industry-standard protocol for authorization. It is simple to implement and provides a secure way to authorize applications.\n\nThe application will use the following steps to authorize against the YouTube API:\n\n1. The user will be redirected to the YouTube authorization page.\n2. The user will be asked to grant the application access to their YouTube account.\n3. If the user grants access, the application will be redirected back to the application with an authorization code.\n4. The application will use the authorization code to request an access token from the YouTube API.\n5. The application will use the access token to make requests to the YouTube API."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","To record architectural decisions, we will use the Architectural Decision Record (ADR) template. ADRs will be stored in a central repository and will be reviewed and approved by the architecture team before being implemented. The ADR template will include the following fields:\n\n- Title: A brief summary of the decision\n- Decision: The decision that was made\n- Context: The background and rationale for the decision\n- Consequences: The potential consequences of the decision\n- Related Decisions: Links to other ADRs that are related to this decision\n\nThis process will ensure that architectural decisions are made in a thoughtful and deliberate manner, and that they are documented and communicated to the team."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to create a riglet for our new bookit project so that we practice what we preach.\n\n## Decision\n,"We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: Travis\n","**Implement a riglet for the bookit project.**\n\n**Rationale:**\n\n* **Alignment with best practices:** Practicing ""what we preach"" demonstrates our commitment to the principles we advocate for.\n* **Educational value:** Building a riglet for the bookit project provides a hands-on learning experience for team members, allowing them to apply design principles in a real-world context.\n* **Documentation:** The riglet can serve as a living example of our architectural decisions and best practices, enabling future teams to learn from our experiences.\n* **Collaboration:** The riglet development process fosters collaboration between architects, developers, and other stakeholders, promoting a shared understanding of the system architecture.\n* **Continuous improvement:** The riglet can be used as a testbed for new technologies or design patterns, allowing us to continuously refine our architectural approach.\n\n**Consequences:**\n\n* **Time and resources:** Implementing the riglet will require time and effort from the development team.\n* **Complexity:** The riglet should be designed carefully to avoid introducing unnecessary complexity into the bookit project.\n* **Maintenance:** The riglet will need to be maintained and updated as the bookit project evolves.\n\n**Alternatives considered and rejected:**\n\n* **Not implementing a riglet:** This would miss out on the educational and documentation benefits of building a riglet.\n* **Using an existing riglet:** This would limit our ability to tailor the riglet to the specific needs of the bookit project."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n\n## Decision\n,* Use AWS CodePipeline and CodeBuild instead of Travis\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n,Use Azure Pipelines as the CI/CD solution.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContext here...\n\n## Decision\n,Decision here...\n,Decision is missing in the provided context.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Log architectural decisions in an Architectural Decision Record (ADR).**\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to document architectural decisions.\n* They clearly state the decision, its context, and the rationale behind it.\n* ADRs facilitate transparency and collaboration by making architectural decisions accessible to all stakeholders.\n* They serve as a valuable reference for future maintenance and enhancements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR details the behavior for `actions/checkout@v2`.\nThe new action will be written in typescript. We are moving away from runner-plugin actions.\nWe want to take this opportunity to make behavioral changes, from v1. This document is scoped to those differences.\n\n## Decision\n","### Inputs\n```yaml\nrepository:\ndescription: 'Repository name with owner. For example, actions/checkout'\ndefault: ${{ github.repository }}\nref:\ndescription: >\nThe branch, tag or SHA to checkout. When checking out the repository that\ntriggered a workflow, this defaults to the reference or SHA for that\nevent.  Otherwise, uses the default branch.\ntoken:\ndescription: >\nPersonal access token (PAT) used to fetch the repository. The PAT is configured\nwith the local git config, which enables your scripts to run authenticated git\ncommands. The post-job step removes the PAT.\nWe recommend using a service account with the least permissions necessary.\nAlso when generating a new PAT, select the least scopes necessary.\n[Learn more about creating and using encrypted secrets](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets)\ndefault: ${{ github.token }}\nssh-key:\ndescription: >\nSSH key used to fetch the repository. The SSH key is configured with the local\ngit config, which enables your scripts to run authenticated git commands.\nThe post-job step removes the SSH key.\nWe recommend using a service account with the least permissions necessary.\n[Learn more about creating and using\nencrypted secrets](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets)\nssh-known-hosts:\ndescription: >\nKnown hosts in addition to the user and global host key database. The public\nSSH keys for a host may be obtained using the utility `ssh-keyscan`. For example,\n`ssh-keyscan github.com`. The public key for github.com is always implicitly added.\nssh-strict:\ndescription: >\nWhether to perform strict host key checking. When true, adds the options `StrictHostKeyChecking=yes`\nand `CheckHostIP=no` to the SSH command line. Use the input `ssh-known-hosts` to\nconfigure additional hosts.\ndefault: true\npersist-credentials:\ndescription: 'Whether to configure the token or SSH key with the local git config'\ndefault: true\npath:\ndescription: 'Relative path under $GITHUB_WORKSPACE to place the repository'\nclean:\ndescription: 'Whether to execute `git clean -ffdx && git reset --hard HEAD` before fetching'\ndefault: true\nfetch-depth:\ndescription: 'Number of commits to fetch. 0 indicates all history for all tags and branches.'\ndefault: 1\nlfs:\ndescription: 'Whether to download Git-LFS files'\ndefault: false\nsubmodules:\ndescription: >\nWhether to checkout submodules: `true` to checkout submodules or `recursive` to\nrecursively checkout submodules.\nWhen the `ssh-key` input is not provided, SSH URLs beginning with `git@github.com:` are\nconverted to HTTPS.\ndefault: false\n```\nNote:\n- SSH support is new\n- `persist-credentials` is new\n- `path` behavior is different (refer [below](#path) for details)\n### Fallback to GitHub API\nWhen a sufficient version of git is not in the PATH, fallback to the [web API](https://developer.github.com/v3/repos/contents/#get-archive-link) to download a tarball/zipball.\nNote:\n- LFS files are not included in the archive. Therefore fail if LFS is set to true.\n- Submodules are also not included in the archive.\n### Persist credentials\nThe credentials will be persisted on disk. This will allow users to script authenticated git commands, like `git fetch`.\nA post script will remove the credentials (cleanup for self-hosted).\nUsers may opt-out by specifying `persist-credentials: false`\nNote:\n- Users scripting `git commit` may need to set the username and email. The service does not provide any reasonable default value. Users can add `git config user.name <NAME>` and `git config user.email <EMAIL>`. We will document this guidance.\n#### PAT\nWhen using the `${{github.token}}` or a PAT, the token will be persisted in the local git config. The config key `http.https://github.com/.extraheader` enables an auth header to be specified on all authenticated commands `AUTHORIZATION: basic <BASE64_U:P>`.\nNote:\n- The auth header is scoped to all of github `http.https://github.com/.extraheader`\n- Additional public remotes also just work.\n- If users want to authenticate to an additional private remote, they should provide the `token` input.\n#### SSH key\nThe SSH key will be written to disk under the `$RUNNER_TEMP` directory. The SSH key will\nbe removed by the action's post-job hook. Additionally, RUNNER_TEMP is cleared by the\nrunner between jobs.\nThe SSH key must be written with strict file permissions. The SSH client requires the file\nto be read/write for the user, and not accessible by others.\nThe user host key database (`~/.ssh/known_hosts`) will be copied to a unique file under\n`$RUNNER_TEMP`. And values from the input `ssh-known-hosts` will be added to the file.\nThe SSH command will be overridden for the local git config:\n```sh\ngit config core.sshCommand 'ssh -i ""$RUNNER_TEMP/path-to-ssh-key"" -o StrictHostKeyChecking=yes -o CheckHostIP=no -o ""UserKnownHostsFile=$RUNNER_TEMP/path-to-known-hosts""'\n```\nWhen the input `ssh-strict` is set to `false`, the options `CheckHostIP` and `StrictHostKeyChecking` will not be overridden.\nNote:\n- When `ssh-strict` is set to `true` (default), the SSH option `CheckHostIP` can safely be disabled.\nStrict host checking verifies the server's public key. Therefore, IP verification is unnecessary\nand noisy. For example:\n> Warning: Permanently added the RSA host key for IP address '140.82.113.4' to the list of known hosts.\n- Since GIT_SSH_COMMAND overrides core.sshCommand, temporarily set the env var when fetching the repo. When creds\nare persisted, core.sshCommand is leveraged to avoid multiple checkout steps stomping over each other.\n- Modify actions/runner to mount RUNNER_TEMP to enable scripting authenticated git commands from a container action.\n- Refer [here](https://linux.die.net/man/5/ssh_config) for SSH config details.\n### Fetch behavior\nFetch only the SHA being built and set depth=1. This significantly reduces the fetch time for large repos.\nIf a SHA isn't available (e.g. multi repo), then fetch only the specified ref with depth=1.\nThe input `fetch-depth` can be used to control the depth.\nNote:\n- Fetching a single commit is supported by Git wire protocol version 2. The git client uses protocol version 0 by default. The desired protocol version can be overridden in the git config or on the fetch command line invocation (`-c protocol.version=2`). We will override on the fetch command line, for transparency.\n- Git client version 2.18+ (released June 2018) is required for wire protocol version 2.\n### Checkout behavior\nFor CI, checkout will create a local ref with the upstream set. This allows users to script git as they normally would.\nFor PR, continue to checkout detached head. The PR branch is special - the branch and merge commit are created by the server. It doesn't match a users' local workflow.\nNote:\n- Consider deleting all local refs during cleanup if that helps avoid collisions. More testing required.\n### Path\nFor the mainline scenario, the disk-layout behavior remains the same.\nRemember, given the repo `johndoe/foo`, the mainline disk layout looks like:\n```\nGITHUB_WORKSPACE=/home/runner/work/foo/foo\nRUNNER_WORKSPACE=/home/runner/work/foo\n```\nV2 introduces a new contraint on the checkout path. The location must now be under `github.workspace`. Whereas the checkout@v1 constraint was one level up, under `runner.workspace`.\nV2 no longer changes `github.workspace` to follow wherever the self repo is checked-out.\nThese behavioral changes align better with container actions. The [documented filesystem contract](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/virtual-environments-for-github-hosted-runners#docker-container-filesystem) is:\n- `/github/home`\n- `/github/workspace` - Note: GitHub Actions must be run by the default Docker user (root). Ensure your Dockerfile does not set the USER instruction, otherwise you will not be able to access `GITHUB_WORKSPACE`.\n- `/github/workflow`\nNote:\n- The tracking config will not be updated to reflect the path of the workflow repo.\n- Any existing workflow repo will not be moved when the checkout path changes. In fact some customers want to checkout the workflow repo twice, side by side against different branches.\n- Actions that need to operate only against the root of the self repo, should expose a `path` input.\n#### Default value for `path` input\nThe `path` input will default to `./` which is rooted against `github.workspace`.\nThis default fits the mainline scenario well: single checkout\nFor multi-checkout, users must specify the `path` input for at least one of the repositories.\nNote:\n- An alternative is for the self repo to default to `./` and other repos default to `<REPO_NAME>`. However nested layout is an atypical git layout and therefore is not a good default. Users should supply the path info.\n#### Example - Nested layout\nThe following example checks-out two repositories and creates a nested layout.\n```yaml\n# Self repo - Checkout to $GITHUB_WORKSPACE\n- uses: checkout@v2\n# Other repo - Checkout to $GITHUB_WORKSPACE/myscripts\n- uses: checkout@v2\nwith:\nrepository: myorg/myscripts\npath: myscripts\n```\n#### Example - Side by side layout\nThe following example checks-out two repositories and creates a side-by-side layout.\n```yaml\n# Self repo - Checkout to $GITHUB_WORKSPACE/foo\n- uses: checkout@v2\nwith:\npath: foo\n# Other repo - Checkout to $GITHUB_WORKSPACE/myscripts\n- uses: checkout@v2\nwith:\nrepository: myorg/myscripts\npath: myscripts\n```\n#### Path impact to problem matchers\nProblem matchers associate the source files with annotations.\nToday the runner verifies the source file is under the `github.workspace`. Otherwise the source file property is dropped.\nMulti-checkout complicates the matter. However even today submodules may cause this heuristic to be inaccurate.\nA better solution is:\nGiven a source file path, walk up the directories until the first `.git/config` is found. Check if it matches the self repo (`url = https://github.com/OWNER/REPO`). If not, drop the source file path.\n### Submodules\nWith both PAT and SSH key support, we should be able to provide frictionless support for\nsubmodules scenarios: recursive, non-recursive, relative submodule paths.\nWhen fetching submodules, follow the `fetch-depth` settings.\nAlso when fetching submodules, if the `ssh-key` input is not provided then convert SSH URLs to HTTPS: `-c url.""https://github.com/"".insteadOf ""git@github.com:""`\nCredentials will be persisted in the submodules local git config too.\n### Port to typescript\nThe checkout action should be a typescript action on the GitHub graph, for the following reasons:\n- Enables customers to fork the checkout repo and modify\n- Serves as an example for customers\n- Demystifies the checkout action manifest\n- Simplifies the runner\n- Reduce the amount of runner code to port (if we ever do)\nNote:\n- This means job-container images will need git in the PATH, for checkout.\n### Branching strategy and release tags\n- Create a servicing branch for V1: `releases/v1`\n- Merge the changes into the default branch\n- Release using a new tag `preview`\n- When stable, release using a new tag `v2`\n","The `actions/checkout@v2` action will use the new typescript runner, and will make the following behavioral changes from v1:\n\n- Improved performance by leveraging the new runner's optimized resource utilization.\n- Enhanced security by implementing best practices and following industry security standards.\n- Simplified configuration by providing a more user-friendly and intuitive interface.\n- Expanded functionality by offering additional features and integrations with other tools and services.\n- Improved documentation and support resources to ensure seamless adoption and troubleshooting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTo add/modify content on our patternpediacontent repo 'on behalf of the user', we need to use OAuth Authentification in our client-side application.\nBut requesting https://github.com/login/oauth/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https://github.com/isaacs/github/issues/330)).\nUnfortunately, Github prevents you from implementing the OAuth Web Application Flow on a client-side only application (Reason for it: security-related limitations).\n\n## Decision\n","Chosen option: ""Use our own server-side component"", because\n* we don't want to provide our their client id and secret to an (possibly untrusted) open reverse proxy (comment from the stackoverflow-article: If the owner of the proxy wants to log credentials from the requests they can)\n",**Implement the Github App OAuth flow**\n\n**Pros:**\n- Github Apps does support CORS for requesting an OAuth access token\n- You can implement on a client-side application\n\n**Cons:**\n- Requires creating a GitHub App and installing it on your repositories\n- You will need to ask for additional permissions from your repo owners\n- You will need to manage app installation and permissions\n- Might not be suitable if you need to use multiple GitHub accounts\n- The GitHub App OAuth flow might be more complex to implement than the web application flow
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen creating and updating pattern languages and patterns, the structure should be defined so that users can implement their own renderers for a specific pattern language.\n## Decision Drivers\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\n\n## Decision\n","* Pattern languages can be very different, so we should restrict their properties as little as possible.\nA pattern consists of\n* name (string), e.g. ""Elastic Infrastructure""\n* type (string), e.g. <https://purl.org/patternpedia/cloudcomputingpatterns#CloudComputingPattern>\n* uri (string, e.g. <https://purl.org/patternpedia/cloudcomputingpatterns/elasticinfrastructure#ElasticInfrastructure>)\n* a Map for the section properties of its pattern language (Map<section, string | string[]>),\ne.g. for the section  https://purl.org/patternpedia/cloudcomputingpatterns#hasLogo we can obtain the corresponding value  ""https://www.cloudcomputingpatterns.org/img/book.png"" or an array of strings\nTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like ""https://www.cloudcomputingpatterns.org/img/book.png"" as links/pictures and not only as text.\n",Define a JSON schema for the pattern language and pattern structures.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAWS Bare Metal rig gives you the choice between EC2 hosting or FARGATE for compute.\n\n## Decision\n,"For the Twig riglet, we will use FARGATE.  Primary driver for this decision is to have a reference the uses FARGATE instead of EC2, and we are in the process of updating the Twig riglet.\n",Use EC2 hosting for compute.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to create a riglet for our new twig project so that we practice what we preach.\n\n## Decision\n,"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: CodePipeline, with Jenkins as an eventual target\n",Create a simple twig riglet and document the process.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer/simpler riglet flavor\nand put newer approaches to the test.\n\n## Decision\n,"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n",Migrate Twig to a newer riglet flavor and experiment with newer approaches.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\n\n## Decision\n","Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\n",Leverage a container-based continuous integration/continuous delivery (CI/CD) solution to streamline deployment processes and reduce the need for full rebuilds for configuration changes.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\n\n## Decision\n,We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\n,"Implement a central logging solution, such as Elasticsearch, Logstash, and Kibana (ELK), or a managed service such as Cloud Logging."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Architectural Decision:** Implement an architectural decision record (ADR) template.\n\n**Rationale:** To ensure consistency and transparency in documenting architectural decisions, facilitating collaboration and knowledge sharing within the team."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHistorically, our automated deployment scripts have tended to include a large amount of common boilerplate. This is partly because they are self-contained—they can typically be run on their own to perform a deployment. And it is partly just because they originate from what the Visual Studio tooling creates.\nThe problem with this is that it makes the project-specific details hard to spot. When looking at a sea of code that's almost identical to every other project, it's hard to see what it's doing that is in any way different from everything else.\n\n## Decision\n","With `Marain.Instance`, deployment scripts in individual services do not communicate directly with either ARM or Azure AD. (They should not even be aware of what mechanisms are being used to perform this work—they should not need to know whether we are using the PowerShell Az module, the az CLI or even custom library code to talk to Azure, for example.)\nAnything that needs to be done either in Azure or AAD must be done through operations provided by the shared `Marain.Instance` code. It passes in an object that provides various methods that provide the necessary services.\n",Standardize deployment scripts.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMarain services need to be able to deliver diagnostic information somewhere.\n\n## Decision\n,`Marain.Instance` creates a single Application Insights instance and makes its key available to all services. All services use it.\n,"**Decision:** Marain services should deliver diagnostic information to an instance of Splunk.\n\n**Rationale:**\n\n* Splunk is a leading provider of machine data analytics software.\n* Splunk is used by many large organizations to collect and analyze data from a variety of sources, including servers, applications, and networks.\n* Splunk has a proven track record of reliability and scalability.\n* Splunk offers a wide range of features for data collection, analysis, and visualization.\n\n**Consequences:**\n\n* Marain services will be able to deliver diagnostic information to a central location.\n* This information will be available to Marain developers and administrators for troubleshooting and performance analysis.\n* The use of Splunk will provide Marain with a scalable and reliable solution for diagnostic information management."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeployment requirements need to be expressed somehow—either declaratively or in a programming language.\n\n## Decision\n,Individual Marain services express their deployment requirements in the form of a set of PowerShell scripts. These will be run in PowerShell core v6.\n,"Declarative deployment is a method of expressing deployment requirements in a declarative language. This allows for a more concise and expressive way to define deployments, as well as making it easier to reason about the deployment."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\n\n## Decision\n","The `Marain.Instance` repo (this repo) includes a master service list, `Solutions/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer—the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\n","Establish a Marain instance taxonomy, with the following categories:\n\n* Core services\n* Optional services\n* Plug-ins\n* Agents"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have defined (in [ADR 0005](0005-multitenancy-approach-for-marain.md)) the way in which we intend to implement Tenancy in Marain instances using the `Marain.Tenancy` service. As noted in that ADR, managing the desired model by hand would be excessively error prone and as such, we need to design tooling that will allow us to create and manage new tenants, and to allow them to use the Marain services they are licenced for.\nBefore we can build that tooling we need to design the underlying process by which tenant onboarding, enrollment and offboarding will work. This needs to allow new Client Tenants to be onboarded into Marain services without tightly coupling the services so some central thing that knows everything about them.\n\n## Decision\n","We are envisaging a central control-plane API (referred to for the remainder of this document as the ""Management API"") for Marain which primarily builds on top of the Tenancy service. This will provide the standard operations such as creating new tenants and enrolling them to use Marain services.\nIt will also need to allow us to manage concerns such as licensing, billing, metering and so on, but these are out of the scope of this ADR and will be covered by additional ADRs, work items and documentation when required.\n### Onboarding\nOnboarding is a relatively simple part of the process where we create a new Tenant for the client. We will need to determine how we intend licensing to work and what part, if any, the Management API plays.\n### Enrollment\nService enrollment is a more interesting aspect of the process. In order to avoid tightly coupling Marain services to the Management API, we need two things:\n- A means of discovering the available services.\n- A means of determining the configuration that's needed to enroll for a service, receiving and attaching that configuration to tenants being enrolled, and a defined way of creating the required sub-tenants for services to use when making calls to dependent services on behalf of clients.\nAs described in [ADR 0005](0005-multitenancy-approach-for-marain.md), we are envisaging that each service has a Tenant created for it, under a single parent for all Service Tenants. These tenants can then underpin the discovery mechanism that allows the management API to enumerate services that tenants can be enrolled into.\nOnce we have provided a discovery mechanism, we need to define a way in which we can gather the necessary information needed to enroll a tenant to use a service. We are intending to make this work by defining a common schema through which a service can communicate both the configuration it requires as well as the services upon which it depends. Services can then attach a manifest file containing this information to their Service Tenant via a well known property key, allowing the Management API to obtain the manifest as part of the discovery process.\nSince the process of enrollment and unenrollment is standard across tenants, the actual implementation of this can form part of the Management API, driven by the data in the manifests. If we ever encounter a situation where services need to perform non-standard actions as part of tenant enrollment, we can extend the process to support a way in which services can be notified of new enrollments - this could be a simple callback URL, or potentially a broadcast-type system using something like Azure Event Grid. Since we don't yet have any services that would need this, we will not attempt to define that mechanism at this time.\nEnrolling a tenant to use a service does two things:\n- Firstly, it will attach the relevant configuration for the service to the tenant that's being enrolled.\n- Secondly, if the service that's being enrolled in has dependencies on other services, it will create a new sub-tenant of the Service Tenant for the service being enrolled that the service will use when accessing dependencies on behalf of the client. This new subtenant will then be enrolled for each of the depended-upon services, with any further levels of dependency dealt with in the same way.\n### Example\nConsider a scenario when we have two clients and three services:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|\n+-> OPERATIONS\n|\n+-> FOOBAR\n```\nThe dependency tree for the services looks like this:\n```\n+------------+\n|            |\n+-------> WORKFLOW   +------+-----------------+\n+---------+       |       |            |      |                 |\n|         +-------+       +-^----------+      |                 |\n| Contoso |                 |                 |                 |\n|         |                 |                 |                 |\n+----+----+                 |           +-----v------+          |\n|                      |           |            |          |\n|                      |     +-----> OPERATIONS +----+     |\n|      +---------+     |     |     |            |    |     |\n|      |         +-----+     |     +------------+    |     |\n|      | Litware |           |                       |     |\n|      |         +-----------+                       |     |\n|      +---------+                               +---v-----v--+\n|                                                |            |\n+------------------------------------------------> FOOBAR     |\n|            |\n+------------+\n```\nAs can be seen from this diagram:\n- Contoso is licenced to use WORKFLOW and FOOBAR\n- Litware is licenced to use WORKFLOW and OPERATIONS\n- WORKFLOW has dependencies on OPERATIONS and FOOBAR\n- OPERATIONS has a dependency on FOOBAR\nLet's assume that each of the three services require storage configuration, and have a look at what happens when Litware is enrolled to use Workflow.\nFirstly, we will use the manifest attached to the Workflow Service Tenant to obtain the list of required configuration to enroll a tenant for use with Workflow. The Workflow manifest states that Workflow requires CosmosDB storage configuration, and also that it is dependent on Operations and FooBar. We can then use the manifests on the Operations and FooBar Service Tenants to determine what configuration is required for them. Operations is dependent on FooBar, so the process is repeated there. From this process, we can determine that the list of required configuration for Workflow is the sum of four things: Workflow storage config, Operations storage config, FooBar storage config when invoked from Workflow, and FooBar storage config when invoked from Operations.\nThen, we will assemble this information (most likely via a Marain ""management UI"") and begin the process of enrollment. First we enroll the Litware tenant to use workflow by attaching the workflow storage configuration to the Litware tenant. Then, because Workflow has dependencies, we create a sub-tenant of the Workflow Service Tenant that will be used to call these dependencies on behalf of Litware and we attach the ID of the new tenant to the Litware tenant using a well known property name specific to Workflow:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|\n+-> OPERATIONS\n|\n+-> FOOBAR\n```\nNext, we need to enroll the new WORKFLOW+Litware tenant with the Operations and FooBar services.\nWe start with Operations, which repeats the process we have just carried out for Litware and Workflow, resulting in a similar outcome: the WORKFLOW+Litware tenant has configuration attached to it for the Operations service, and the dependency of Operations on FooBar results in a sub tenant being created for Operations to use when calling FooBar on behalf of WORKFLOW+Litware, with the ID of the new tenant being attached to WORKFLOW+Litware using an Operations-specific well-known key:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|\n+-> FOOBAR\n```\nNext, the new OPERATIONS+WORKFLOW+Litware tenant is enrolled for the FooBar service. The FooBar service has no dependencies so we do not need to create any further tenants; we simply attach the storage configuration for FooBar to the tenant being enrolled and returns. This also completes the WORKFLOW+Litware tenant's enrollment for Operations.\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nNext, we continue the Workflow enrollment for the Litware tenant by enrolling the new WORKFLOW+Litware tenant for Workflow's other dependency, FooBar. As with the Operations service enrolling OPERATIONS+WORKFLOW+Litware with FooBar, this does not result in any further tenants being created, just the FooBar config being attached to WORKFLOW+Litware:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nThis completes Litware's enrollment for the Workflow service. As can be seen, this has resulted in multiple service-specific Litware tenants being created but Litware is never explicitly made aware of the existence of these tenants, nor is it able to use them directly. They are used by their parent services to make calls to their dependencies _on behalf_ of the Litware tenant.\nHowever, there is a further step: Litware also needs to be enrolled in the Operations service. At present, Workflow is able to use Operations on Litware's behalf using the WORKFLOW+Litware tenant. However, this is an implementation detail of Workflow and something that should be able to change without impacting Litware - as long as it does not result in a change to the public Workflow API. So, in order to allow Litware to use the Operations service directly, the process we went through for Workflow is repeated. The storage configuration for Operations is attached to the Litware tenant, and then a further sub-tenant of Operations will be created for it to use when accessing FooBar on behalf of Litware:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+Litware\n|\n+-> FOOBAR\n```\nThen, the new OPERATIONS+Litware tenant will be enrolled for the FooBar service, which results in FooBar storage configuration being attached to the OPERATIONS+Litware service:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nThis completes the enrollment of Litware to the Workflow and Operations services. As can be seen from the above, there are three different paths through which Litware makes indirect use of the FooBar service, and it's possible for the client to use separate storage for each. In fact, this will be the default; even if the client is using Marain storage, the data for their three different usage scenarios for FooBar will be stored in different containers.\nIt should be noted that the client does not get to configure these new sub-tenants directly. In fact, they will be unaware of them - they are essentially implementation details of our approach to multi-tenancy in Marain. They will not be able to retrieve the sub-tenants from the tenancy service or update them directly. That said, it's likely that the management API will allow the configuration to be changed - but without exposing the fact that these sub-tenants exist.\n### Default configuration\nWhilst we want to allow users to ""bring their own storage"" for the Marain services, this may not be the most likely scenario. There are effectively four main ways in which Marain can be used:\n- Fully hosted, using the default storage for each service (this storage is deployed alongside the service)\n- Fully hosted, using managed but non-standard storage (we deploy separate storage accounts per client)\n- Hosted, but using client-provided storage (the ""bring your own storage"" model)\n- Self-hosted (i.e. deployed into a client's own Azure subscription), in which case we would expect the storage deployed with the service to be used - essentially the same as the fully hosted option.\nIn order to make the first and last options simpler to use, we will make the configuration for the default storage available to the enrollment process so it can simply be copied to tenants as they are enrolled, rather than requiring it to be explicitly stated for every enrollment. As such, we need the manifest file schema to allow marking configuration as optional, indicating that defaults should be used if that configuration is not provided. The most sensible location to store this default configuration is on the Service Tenant itself.\n### Offboarding\nOffboarding needs to be considered further; there are many questions about what happens to client data if they stop using Marain, and these will likely depend on the licensing agreements we put in place. As a result this will be considered at a later date.\n","We will use a two-step process for provisioning new tenants:\n\n1. A Tenant is onboarded into the Marain platform, a tenancy record is created in the database, an Identity Provider is configured, SCIM2 tokens are configured and the relevant API keys are created.\n2. The new tenant is enrolled into the services they are licenced for. This process may create new projects, datasets and other resources depending on the services to be enabled and the licencing model.\n\nTenants will be offboarded by first removing them from the services they are enrolled into, and then deleting the Tenant record from the database, removing the Identity Provider configuration and deleting the SCIM2 tokens."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain ""world"".\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\n\n## Decision\n","To support this, we have made the following decisions\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as ""Client Tenants"".\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as ""Service Tenants"".\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called ""Client Tenants"" which parents all of the Client Tenants, and an equivalent one called ""Service Tenants"" that parents the Service Tenants (this is shown in the diagram below).\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|\n+-> OPERATIONS\n```\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|     |     +-> (Operations storage configuration)\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|\n+-> OPERATIONS\n```\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\n```\n+------------+\n|            |\n+-------> WORKFLOW   +------+-----------------+\n+---------+       |       |            |      |                 |\n|         +-------+       +-^----------+      |                 |\n| Contoso |                 |                 |                 |\n|         |                 |                 |                 |\n+----+----+                 |           +-----v------+          |\n|                      |           |            |          |\n|                      |     +-----> OPERATIONS +----+     |\n|      +---------+     |     |     |            |    |     |\n|      |         +-----+     |     +------------+    |     |\n|      | Litware |           |                       |     |\n|      |         +-----------+                       |     |\n|      +---------+                               +---v-----v--+\n|                                                |            |\n+------------------------------------------------> FOOBAR     |\n|            |\n+------------+\n```\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|\n+-> OPERATIONS\n|\n+-> FOOBAR\n```\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\nThis leaves the tenant hierarchy looking like this:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nWe now repeat the process of enrolling Litware for the Workflow service:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|     |     +-> (Operations storage configuration)\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|     |     +-> (Operations storage configuration)\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\n","**Decision:**\n\nWe will implement a tenancy model that supports the following scenarios:\n\n* **Managed service:** Marain is deployed and managed by us, and clients are licensed to use it as a PaaS offering.\n* **Private instance:** Clients deploy private instances of Marain into their own cloud subscriptions.\n* **Data isolation:** Clients can store their data in their own storage accounts or databases, while we run the compute aspects of the platform on their behalf.\n* **Multi-tenanted services:** Clients can use Marain to implement their own multi-tenanted services and isolate their clients' storage.\n* **Service dependencies:** Clients can use services indirectly as dependencies of other services they are using.\n\nThe tenancy model will be implemented using the `Marain.Tenancy` service. Tenants will be created and managed by Marain, and each tenant will have its own isolated storage and database. Clients will be assigned to tenants, and they will only be able to access the resources within their own tenant.\n\n**Rationale:**\n\nThis tenancy model meets the requirements of the context and provides a flexible and extensible solution for managing tenants within the Marain ""world"". It supports both managed and private deployments, data isolation, multi-tenanted services, and service dependencies. Additionally, it can be implemented using the existing `Marain.Tenancy` service, which reduces the development effort and allows us to leverage the existing functionality of the service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR defines an approach for a pipeline security management process.  Itt enables application deployment pipelines to perform all tasks associated with the deployment of typical Azure PaaS-based solutions, in a way that adheres to the principle of least privilege and supports oversight and audit requirements.\nImportantly, the process for making such security changes utilises a standard development workflow that allows teams to manage their own configuration with minimal friction, whilst still facilitating centralised oversight when needed (e.g. by the security team).\nBefore describing the process in detail, here are the sorts of tasks that an application deployment pipeline typically needs to be able to perform:\n1. Create resources in the target subscription\n1. Perform ARM role assignments in the target subscription\n1. Query Azure Active Directory (AAD) for existing applications & service principals\n1. Create new AAD applications & service principals\n1. Manage those AAD applications & service principals (e.g. application role assignment, deletion etc.)\nLet's start by outlining how these requirements are commonly achieved today.\n### Create resources in the target subscription\nBy default, the 'Service Connections' that Azure Pipelines use to authenticate with Azure already provide this, with the option of constraining its access to a given resource group.\n### Perform ARM role assignments in the target subscription\nIf enabled for automated approaches this is typically achieved by one of the following:\n* granting the identity behind the service connection the 'owner' role in the target subscription\n* a separate identity with the required permissions, whose credentials are available to pipelines\nFor manual scenarios:\n* raising a service desk ticket\n* asking someone who has permissions to do it for you\n### Query AAD for existing applications & service principals\nThe main pipeline is restructured to expect the required information to be provided as manually-maintained configuration.\n### Create new AAD applications & service principals\nIf enabled for automated scenarios, this is commonly handled in one of three ways:\n* an entirely separate pipeline running under a different identity which does have the necessary permissions\n* this can only be considered fully-automated if the dependent pipeline is able to trigger this pipeline on-demand\n* running a subset of the main pipeline, under a different identity which does have the necessary permissions, such that only the tasks that require the elevated privileges are run\nFor manual scenarios:\n* an out-of-band process whereby an individual with the appropriate permissions executes the necessary steps (either manually or using a script)\n### Manage AAD applications & service principals\nThis is largely equivalent to the preceding 'Create' scenario, with the addition of:\n* a dependency on the 'Query AAD' requirement\n* having permissions to update applications & principals owned by others (where they were created separately)\n>NOTE: All of this functionality may have to run multiple times in the case of an evolving application, particularly where scoping of permissions is being applied.\n\n## Decision\n","In the context of pipelines running in Azure DevOps, an executing pipeline uses an Azure Resource Manager type of [service connection](https://docs.microsoft.com/en-us/azure/devops/pipelines/library/service-endpoints) to authenticate to Azure for any tasks it needs to perform.\nBelow is a configuration model that defines the security requirements for each such service connection, which is maintained in a Git repository and has a separate pipeline that applies that configuration in Azure.\n>NOTE: These YAML fragments would exist as individual files in the Git repo, with a logical folder structure to help organise and reflect ownership as required (e.g. across development teams, products etc.).\n```\n---\n#\n# This fragment maintains the shared configuration that defines the permissions used by\n# Azure DevOps service connections managed by this system.\n#\nazurerm:\ncustom_role:\nname: acme-azure-deployer\nallowed_actions:\n- Microsoft.Authorization/roleAssignments/read\n- Microsoft.Authorization/roleAssignments/write\n- Microsoft.Authorization/roleAssignments/delete\nrequired_roles:\n- contributor\n- acme-azure-deployer\nazuread:\nrequired_graph_permissions:\n- 5778995a-e1bf-45b8-affa-663a9f3f4d04      # readDirDataPermissionId\n- 824c81eb-e3f8-4ee6-8f6d-de7f50d565b7      # manageOwnAppsPermissionId\n---\n#\n# Defines the logical environments used as part of application release promotion and their\n# associated Azure subscription.\n#\nenvironments:\n- name: dev\nsubscription: <subscription-id>\n- name: test\nsubscription: <subscription-id>\n- name: prod\nsubscription: <subscription-id>\n---\n#\n# Defines an application/system that deploys resources across 2 resource groups and requires\n# service connections for 3 environments\n#\nname: product_a\nenvironments:\n- dev\n- test\n- prod\nresource_groups:\n- producta-services                       # NOTE: this option relies on a naming convention-based expansion: e.g. 'acme-<environment>-producta-services-rg'\n- acme-${ENVIRONMENT}-producta-data-rg    # NOTE: this option uses a simpler token replacement approach\n---\n#\n# Defines an application/system that deploys resources to a single resource groups and requires\n# service connections for 2 environments\n#\nname: product_b\nenvironments:\n- test\n- prod\nresource_groups:\n- acme-${ENVIRONMENT}-productb-rg\n```\nA security management pipeline, running in Azure DevOps, is able to read this configuration model and performs the following tasks to ensure that a least privilege Azure DevOps service connection is maintained per application, per environment:\n* Ensures the required resource groups exist\n* Maintains the custom role that grants the elevated permissions and maintains its set of assignable scopes (i.e. the resource groups)\n* Maintains the required AAD service principals\n* Assigns the specified roles to the AAD service principals with the required resource group scoping constraints\n* Assigns the required AAD permissions to the service principals\nThis pipeline needs to run with high-level permissions, therefore it must be tightly controlled to ensure that its elevated rights are not exploited:\n* Use of an Azure DevOps project with no user access outside the responsible team\n* The privileged service connection is only available within this restricted project and requires explicit authorisation before it can be used by a new pipeline\n* The pipeline only triggers for changes on the `master` branch\n* The `master` branch is protected and cannot be directly committed to\n* Anyone wishing to make changes to the configuration must raise a Pull Request that is subject to a review/approval policy by at least the responsible team\n","**Decision**\n\nThe pipeline security management process will be implemented using a combination of automated and manual tasks, with the following key principles:\n\n* **Principle of least privilege:** The pipeline will only be granted the minimum permissions necessary to perform its tasks.\n* **Centralized oversight:** The security team will have oversight of all pipeline security changes.\n* **Standard development workflow:** The pipeline security management process will use a standard development workflow that allows teams to manage their own configuration with minimal friction.\n\n**Automated tasks**\n\nThe following tasks will be automated:\n\n* Creating resources in the target subscription\n* Performing ARM role assignments in the target subscription\n* Querying Azure Active Directory (AAD) for existing applications & service principals\n\n**Manual tasks**\n\nThe following tasks will be manual:\n\n* Creating new AAD applications & service principals\n* Managing those AAD applications & service principals (e.g. application role assignment, deletion etc.)\n\n**Rationale**\n\nThe decision to automate some tasks and manual others was made based on the following factors:\n\n* **Complexity:** The automated tasks are relatively simple and can be easily automated. The manual tasks are more complex and require human judgment.\n* **Security:** The automated tasks can be performed without compromising security. The manual tasks require elevated privileges, so they must be performed with care.\n* **Efficiency:** The automated tasks can be performed more efficiently than the manual tasks.\n\n**Implementation**\n\nThe pipeline security management process will be implemented using the following tools:\n\n* **Azure Pipelines:** Azure Pipelines will be used to automate the tasks that can be automated.\n* **Azure AD Service Principal:** The pipeline will use an Azure AD service principal to authenticate with Azure and AAD. The service principal will be granted the minimum permissions necessary to perform the tasks that the pipeline needs to perform.\n* **Terraform:** Terraform will be used to provision the resources in the target subscription.\n* **ARM Template:** ARM Templates will be used to define the role assignments in the target subscription.\n* **AAD PowerShell Module:** The AAD PowerShell module will be used to query AAD for existing applications & service principals, and to create and manage AAD applications & service principals.\n\n**Monitoring**\n\nThe pipeline security management process will be monitored by the security team. The security team will review the pipeline logs and audit the pipeline configuration on a regular basis."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n[Handlebars](https://www.npmjs.com/package/mustache), an extension of [Mustache](https://www.npmjs.com/package/mustache) ([see the differences](https://github.com/handlebars-lang/handlebars.js#differences-between-handlebarsjs-and-mustache)) used in `fxa-auth-server` email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system [began here](https://github.com/mozilla/fxa/issues/4627). While converting our emails to a more modern templating solution, there is an opportunity to evaluate what stack would be the most ideal for FxA emails beyond a proposed templating solution. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.\n\n## Decision\n","### Templating and Styling\n- Option A - Continue to use Mustache and ad-hoc inline styles\n- Option B - Use React server-side to generate static HTML templates with TailwindCSS\n- Option C - Use EJS and MJML, using CSS options offered by MJML\n### Email previewing\n- Option A - Continue to use the `write-emails` command\n- Option B - Use Storybook\n- Option C - Use Mailtrap\n---\nFurthermore, there are a few **other decisions** worth noting that won’t necessarily have pros/cons lists:\n- How to handle generating plaintext versions\n- Transitioning to Fluent over GetText for localization\n- Plans around integration involving feature flagging and the QA process\n### Templating and Styling\nChosen option: ""Option C - Use EJS and MJML, using CSS options offered by MJML"", because:\n- HTML email has a lot of quirks - MJML shifts the burden of maintaining solutions for these off of FxA engineers now and in the future\n- While we use React and Tailwind in other parts of FxA, React is heavy for an email solution since no state is involved, component reuse across FxA would likely be very minimal, and it involves a more complex build setup than EJS\n- MJML helps significantly with responsiveness in emails, reducing time spent developing templates and making future email redesigns easier\n### Email Previewing\nChosen option: ""Option B - Use Storybook"", because:\n- It provides us the flexibility to preview all of the different email states together\n- We have a Storybook deployment already in place so hooking it up for `fxa-auth-server` will ensure consistency\n- Easy to test and perform QA without needing to touch the codebase\n- Support for CSS (whether it's plain old CSS, TailwindCSS or something else)\nNote: Along with using storybook to view the various states of the templates, we are also planning on continued support of [Maildev](https://www.npmjs.com/package/maildev) since it has applications beyond just previewing emails.\n### Other\n**Plaintext files**: We'll use [html-to-text](https://github.com/html-to-text/node-html-to-text) to automatically generate plaintext versions of templates rendered to HTML. There may be a scenario in which the automatically-generated plaintext version does not look exactly how we'd like, in which case we can look into exporting both an MJML _and_ plaintext version of the email from the template file.\n**Localization**: We will upgrade the localization tool from GetText to [Fluent](https://github.com/projectfluent/fluent.js) since it's preferred by the l10n team and other FxA front-ends are using it. With our chosen templating option we can make use of Fluent's [`@fluent/dom`](https://github.com/projectfluent/fluent.js/tree/master/fluent-dom) package.\n**Integration & QA**: During development templates will be marked as being part of a release group. This could be in the form of a mapped list of template names or some variable associated with the template file. Each group will have a corresponding environment variable flag. When this release group’s flag is enabled any templates that fall under it will be used when generating an email template in that environment; until a template's corresponding release group is enabled it is not used and the old/current template will continue to be served. Release groups will be initially enabled in staging until QA has had an opportunity to thoroughly test and evaluate each template in the group, after which they can be incrementally enabled in production. To give us the most flexibiity we can add a new auth-server configuration value that can control which users email are supported and which email templates are supported for mjml. The auth-server can then expose a feature flag method to check this value and then render the correct template.\n### Positive Consequences\n- Using MJML abstracts HTML email peculiarities away and handles responsiveness for us\n- Allows us to move away from inline CSS during development, and improves style reusability and consistency in template files\n- Using EJS allows us to write templates using JavaScript, removing the complexities of custom syntaxes like JSX and Mustache\n### Negative Consequences\n- The Storybook setup will be much more complex than with a React and Tailwind solution\n- Introduces new dependencies, and MJML introduces a small learning curve\n- Our email templates have been battle-tested over the years, and this change could introduce potential new bugs across various clients\n","**Migrate to a more modern templating solution that provides greater flexibility and functionality.**\n\n**Considerations:**\n\n* Evaluate different templating solutions, such as Handlebars, Mustache, and Nunjucks.\n* Consider the trade-offs between flexibility, performance, and ease of use.\n* Explore options for integrating with existing infrastructure and tools.\n\n**Alternatives:**\n\n* Continue using Handlebars, despite its limitations.\n* Use multiple templating solutions for different purposes.\n* Develop a custom templating solution.\n\n**Consequences:**\n\n* Improved flexibility and functionality in email templates.\n* Reduced technical debt and maintenance costs.\n* Potential for increased developer productivity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\n## Decision Drivers\n- Reduce complexity\n- Improve performance\n- Share code\n\n## Decision\n","- Reduce complexity\n- Improve performance\n- Share code\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\n",Move the database service to a separate physical layer with its own IP address and more restricted permissions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nExternal OAuth Relying Parties (RPs) that Firefox Account users authenticate\nto need to be kept up to date about whether the user still has an active\nsubscription as well as knowing when to delete the user data. For internal\nMozilla use, Mozilla provides [Firefox Service Notifications][] over SQS\nqueues. These events are intended for internal trusted use and not suitable\nfor external RPs.\nA solution for these concerns is a new system referred to as the FxA Event\nBroker which will store FxA related events and distribute them via webhooks\nto relevant RPs that the user has accessed. The initial version described here applies primarily to external RPs which will only receive subscription status\nchanges and account deletion events. This solution is being built with future\nexpansion in mind which will require event stream storage per user and\nnotification of internal Mozilla RPs.\nDifficulties inherent in this solution lie in where to store the source of\ntruth for what RPs a user has authenticated to, where the delivery functionality\nshould reside, where the event streams for a user should reside, what set of\ndata should be included in [Firefox Service Notifications][], and how this\ndata should be communicated to the delivery system to avoid additional API\nquery load on existing FxA services.\n## Decision Drivers\n- Subscription services deadlines\n- Effort required and experience available for FxA changes\n- Separation of concerns from existing FxA microservices\n- Difficulty of schema migrations in existing FxA microservices\n- Suitability of existing FxA databases for large-scale event storage\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\nin-progress)\n\n## Decision\n","- Subscription services deadlines\n- Effort required and experience available for FxA changes\n- Separation of concerns from existing FxA microservices\n- Difficulty of schema migrations in existing FxA microservices\n- Suitability of existing FxA databases for large-scale event storage\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\nin-progress)\nChosen Option: C. Implementing RP notification as a new FxA Event Broker service\nwith webhook and login activity stored in FxA Event Broker, because\n- Less subscription platform timeline risk to store new data in new database vs.\nmodify existing FxA OAuth database.\n- Storing events at scale has database requirements that don't fit in well with\nthe limitations with MySQL encountered in FxA Auth/OAuth.\n- Having FxA Auth be the only store of which RP to notify would require each\nnotification to also include what RPs to notify, increasing the load on the\nFxA Auth database.\n","The decision is to decouple the FxA Event Broker from the FxA backend by creating a new microservice that will store the source of truth for what RPs a user has authenticated to, and will be responsible for delivering events to those RPs via webhooks.\n\nThis decision was made in order to address the following concerns:\n\n- **Subscription services deadlines:** The FxA Event Broker needs to be able to deliver events to RPs in a timely manner in order to meet subscription service deadlines. Decoupling the FxA Event Broker from the FxA backend will allow us to scale the FxA Event Broker independently of the FxA backend, which will help us to meet these deadlines.\n- **Effort required and experience available for FxA changes:** The FxA backend is a complex system, and making changes to it can be time-consuming and error-prone. Decoupling the FxA Event Broker from the FxA backend will allow us to make changes to the FxA Event Broker without affecting the FxA backend, which will save us time and effort.\n- **Separation of concerns from existing FxA microservices:** The FxA Event Broker has a different purpose than the existing FxA microservices, and it should be decoupled from them in order to avoid unnecessary complexity.\n- **Difficulty of schema migrations in existing FxA microservices:** The FxA backend's schema is complex, and it can be difficult to migrate the schema without causing downtime or data loss. Decoupling the FxA Event Broker from the FxA backend will allow us to make changes to the FxA Event Broker's schema without affecting the FxA backend's schema, which will reduce the risk of downtime or data loss.\n- **Suitability of existing FxA databases for large-scale event storage:** The FxA backend's databases are not designed for large-scale event storage. Decoupling the FxA Event Broker from the FxA backend will allow us to use a different database that is better suited for large-scale event storage.\n- **Architectural desire to treat FxA Auth and OAuth as one (Merging in-progress):** The FxA Event Broker will be responsible for delivering events to both FxA Auth and OAuth RPs. Decoupling the FxA Event Broker from the FxA backend will allow us to treat FxA Auth and OAuth as one system, which will make it easier to manage and maintain the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince becoming a monorepo we haven't found a way to share code across packages that works well in local development, CI, and production without fragile, ad-hoc workarounds.\n## Decision Drivers\n- Simpler code sharing\n- Minimal overhead in setup and deployment\n- Works well for js and ts packages\n- Low maintenance\n- Long term viability\n\n## Decision\n","- Simpler code sharing\n- Minimal overhead in setup and deployment\n- Works well for js and ts packages\n- Low maintenance\n- Long term viability\nThe [workspace:\*](https://yarnpkg.com/features/workspaces) dependency type along with typescript [project references](https://www.typescriptlang.org/docs/handbook/project-references.html) provide a good internal dependency solution that addresses the shortcomings of our current situation.\nWorkspaces are not yet supported by npm but are on the [v7 roadmap](https://blog.npmjs.org/post/617484925547986944/npm-v7-series-introduction) so it's pretty safe to assume that they will be widely supported for the foreseeable future. In order to start using workspaces today we'll need to use yarn. This is a slight risk because although it is well supported it doesn't have the level of backing that npm does, however once workspaces are supported by npm there is nothing locking us in to yarn.\n### Advantages\n- No manual linking of dependencies\n- Simpler builds\n- Single lock file\n- Hoisted dependencies\n### Disadvantages\n- In the short term this requires us to switch from npm to yarn as our package manager. This will change dev workflows and is another tool to understand.\n- Some code changes are required to be compatible with yarn, however they're also backward compatible with npm.\n- Yarn doesn't currently support auditing dependencies like npm does.\n","**Use git subtree instead of yarn workspaces for code sharing across monorepo packages**\n\n**Justification:**\n\n* Git subtree meets all the decision drivers:\n    * It provides simple and straightforward code sharing.\n    * It has minimal setup and deployment overhead.\n    * It works well for both JS and TS packages.\n    * It is relatively low-maintenance.\n    * It offers long-term viability as it is a well-established and supported version control mechanism.\n* Compared to yarn workspaces, git subtree is more flexible and less prone to breakage.\n* Git subtree is more suitable for our current development and deployment setup, where packages are managed individually and not all dependencies are managed through yarn workspaces."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have a variety in tools used for unit & functional tests. This variety means developers need to learn & know many different tools. It would be nice to reduce this mental workload, while still using appropriate tools for testing.\n## Decision Drivers\n- Ensuring we're using the right tool for the job.\n- Reducing developer mental workload for working with tests.\n\n## Decision\n",- Ensuring we're using the right tool for the job.\n- Reducing developer mental workload for working with tests.\nIdentify a consistent stack of testing tools per aspect of the project\n,**Use a testing framework that supports multiple testing tools.** This allows developers to choose the right tool for the job without having to learn a new tool. It also reduces the mental workload of working with tests by providing a consistent interface to all of the tools.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFirefox Account originally implemented multi-factor authentication (MFA) support in Q1 of 2018.\nThis feature used TOTP based codes and was based on RFC 6238.\nAdditionally, if users lost their MFA device, they could use one time backup authentication codes to regain access to the account.\nHaving MFA support has helped secure our users' accounts and given them more security flexibility.\nHowever, over time it has become more obvious that users that lose their MFA device (usually phone) are at risk of getting locked out of their account because they don't save their backup authentication codes.\nThere are a non-trivial amount of users that don’t save or download their backup authentication codes, which is currently the only way they can regain access.\nIn 2019 Q4, FxA started requiring that users confirm backup authentication codes before enabling MFA.\nWhile this did help reduce lockouts, we still want to reduce it further.\nWe believe that adding a new MFA method to Firefox Accounts that has the similar security properties as TOTP would allow users to have another method to recover their account.\n## Decision Drivers\n- Improve user's account security\n- Reduce the risk of account lockout because of lost device/backup authentication code\n- Could be completed in roughly a quarter\n\n## Decision\n","- Improve user's account security\n- Reduce the risk of account lockout because of lost device/backup authentication code\n- Could be completed in roughly a quarter\nChosen option: Option A, because this is more inline with Mozilla's security and privacy principles than option B.\nThere is less security risk to users if this feature is added.\n","**Decision:** Implement a second MFA method for Firefox Accounts.\n\n**Rationale:**\n\n* **Improved user security:** Adding a second MFA method increases the security of user accounts by providing an additional layer of authentication.\n* **Reduced risk of lockouts:** A second MFA method reduces the risk of users getting locked out of their accounts due to lost devices or backup authentication codes.\n* **Feasibility:** The implementation of a second MFA method is estimated to take about a quarter, which is a reasonable timeframe."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Subscription Platform uses [Stripe Metadata][stripe-metadata] for numerous\npurposes, including product checkout content, legal document download URLs,\nand product support form options. The metadata can be updated by product\nmanagers through the Stripe dashboard, enabling product managers to manage\ncertain aspects of their subscription products without involving Subscription\nPlatform engineers. However, Stripe metadata has a limit on the number of\nvalues that we will reach in the near future. In order to continue to use\nmetadata in the same fashion in the Subscription Platform, we need to overcome\nthe limit placed by Stripe.\n[stripe-metadata]: https://stripe.com/docs/api/metadata\n## Decision Drivers\n- A higher storage limit than Stripe Metadata\n- A UI for the managers of products on the Subscription Platform to edit the\nmetadata\n- Access management for the UI\n- Updated data accessible in their respective environments without deployment\n\n## Decision\n","- A higher storage limit than Stripe Metadata\n- A UI for the managers of products on the Subscription Platform to edit the\nmetadata\n- Access management for the UI\n- Updated data accessible in their respective environments without deployment\nChosen option: Google Cloud Firestore, because\n- Data management UI and user access control provided through the Google Cloud\nPlatform (GCP) console.\n- It does not require deployments for data updates.\n- It is not novel tech; FxA already uses it elsewhere.\n","Integrate [Celesta][celesta], a metadata store, with Subscription Platform to overcome the limitation of Stripe Metadata. Celesta will provide a higher storage limit and a UI for managing metadata. Celesta also supports access management for the UI and provides updated data in respective environments without deployment.\n\n[celesta]: https://celesta.dev"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [Settings Redesign project](https://github.com/mozilla/fxa/issues/3740) offers an ideal opportunity to review CSS conventions and architecture for FxA to not only use while building new components for this project, but also for FxA moving forward.\nThis [part 1](https://github.com/mozilla/fxa/issues/4808) of [2](https://github.com/mozilla/fxa/issues/5087) ADR concerns the use of CSS variables, how they come into play with SCSS (vs using SCSS variables), CSS-in-JS considerations, and CSS modules.\nNote that 1) ""CSS variables"" are technically called ""CSS custom properties"" but will be referred to as the common ""variable"" nomenclature throughout this ADR and that 2) SASS is a CSS preprocessor while SCSS is our preferred syntax of SASS (explained [here](https://stackoverflow.com/questions/5654447/whats-the-difference-between-scss-and-sass/5654471#5654471)).\n## Decision Drivers\n- Simplicity and clarity\n- DRY and reusable styles\n- Easy integration and use\n- Current engineering experience and preferences\n\n## Decision\n","- Simplicity and clarity\n- DRY and reusable styles\n- Easy integration and use\n- Current engineering experience and preferences\nChosen options: ""CSS variables"" and ""SCSS"", because:\n- While using CSS variables with SASS will prevent using mixins (or SASS functions like `darken`) that rely on variable values, they can be used without impeding core functionality FxA uses SASS for like nested selectors, extends, and functions and mixins that don't rely on variable values like the generation and use of our media queries.\n- CSS variables are native to the browser and decision to use them now is a prudent one. They have good browser support and can be used without a preprocessor and changed after preprocessor compilation. They are targetable by JavaScript, easily themed, and can be scoped globally or on an element, providing options for usage.\n- Scoped component solutions (CSS-in-JS, CSS modules) encourage self-contained components rather than building the UI as a whole. While one goal of FxA is to reuse components across the ecosystem where possible (e.g. the `fxa-react` package), FxA will likely reap more benefits from a class-based approach. This allows for globally shared styles and a loosely coupled stylesheet rather than a tightly coupled CSS-in-JS solution with conditional styles based on props and how the component is used. Classes promote a DRYer and more consistent approach.\n- CSS-in-JS would add one additional layer of tech to learn (like `styled-components` syntax and best practices) while the Settings Redesign project is already introducing other novel tech, and members of the FxA engineering team also as a whole personally prefer not to use CSS-in-JS.\n- This decision doesn't preclude the option of implementing CSS modules at a later time if it's determined that we would prefer component scoped styles with SCSS, and CSS variables could still be used if we later switch to CSS-in-JS.\n",**Option 1: Use CSS variables exclusively**\n* **Benefits:**\n    * Simple and clear to use\n    * DRY and reusable styles\n    * Easy integration and use\n* **Drawbacks:**\n    * Not as well supported as SCSS variables\n    * Can be less efficient than SCSS variables\n* **Recommendation:** Use CSS variables for simple and reusable styles. Use SCSS variables for more complex styles or when efficiency is a concern.\n\n**Option 2: Use SCSS variables exclusively**\n* **Benefits:**\n    * Well supported\n    * Efficient\n    * Can be used for more complex styles\n* **Drawbacks:**\n    * Can be more difficult to use than CSS variables\n    * Not as DRY as CSS variables\n* **Recommendation:** Use SCSS variables for complex styles or when efficiency is a concern. Use CSS variables for simple and reusable styles.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nPayment data used in Stripe, PayPal, Apple, and Google IAP are handled as individual documents. We typically need to access different portions of these at different times. In addition, their schema occasionally changes (API updates). Stripe and PayPal have API limits, which means we typically need to duplicate/cache data we retrieve from them. At the moment we cache this data in Redis. This mostly works, but restricts our ability to run queries across payment data sets without pulling it all into BigQuery, or making substantial amounts of Stripe/PayPal requests.\nTo handle IAP, we need to store Apple receipts and Google subscription objects. These should be organized with the FxA user they belong to. They're arbitrary JSON documents that we also need copied to BigQuery for ETL jobs. To future-proof our payment data storage needs we should have the flexibility needed to store Stripe/Paypal payment data in the same data store with the same ETL capability.\n## Decision Drivers\n- Engineering resources\n- Arbitrary JSON document querying ability\n- Ease of loading data for ETL jobs\n- Capable of storing arbitrary JSON data\n- Integration with Stripe and other payment systems\n\n## Decision\n","- Engineering resources\n- Arbitrary JSON document querying ability\n- Ease of loading data for ETL jobs\n- Capable of storing arbitrary JSON data\n- Integration with Stripe and other payment systems\nGoogle Firestore. We've used Firestore in the event-broker, it has a rich API, and extensions that overlap with our requirements that reduces engineering and operations resources.\n","**Store all payment data in a document store with flexible querying capabilities.**\n\nThis decision is driven by the following considerations:\n\n* **Engineering resources:** A document store will require less engineering effort to implement and maintain than a relational database.\n* **Arbitrary JSON document querying ability:** A document store will allow us to query arbitrary JSON documents, which is essential for our use case.\n* **Ease of loading data for ETL jobs:** A document store will make it easy to load data for ETL jobs, as it can handle large volumes of data and does not require us to define a schema upfront.\n* **Capable of storing arbitrary JSON data:** A document store can store arbitrary JSON data, which is necessary for our use case.\n* **Integration with Stripe and other payment systems:** There are existing integrations available for document stores that make it easy to integrate with Stripe and other payment systems."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the implementation of payment features for subscription services, we've\ndecided to use third-party JavaScript for payment widgets.\nBest practices established by our security team indicate that third-party JS\nshould not be included on the highly-sensitive pages - i.e. such as those used\nfor general account management on Firefox Accounts.\nSo, we need a way to isolate the pages responsible for subscription sign-up and\nmanagement from the rest of Firefox Accounts.\n## Decision Drivers\n* Security when dealing with financial transactions.\n* Security when including with third-party JS code for payment widgets.\n* Simplicity in user experience flows.\n* Delivering against the subscription services deadline.\n\n## Decision\n","* Security when dealing with financial transactions.\n* Security when including with third-party JS code for payment widgets.\n* Simplicity in user experience flows.\n* Delivering against the subscription services deadline.\nChosen option: ""Option A - Payment pages as separate app supplied with\npre-generated access token"", because\n* Further refinements to access token delivery mechanism in Option A do not\nsignificantly affect the rest of the payments app.\n* Doesn't preclude an upgrade to Option B in the future - i.e. once [Issue\n#640](https://github.com/mozilla/fxa/issues/640) is resolved.\n* Doesn't preclude Option C as a future option - e.g. offering embedded\nsubscription widgets to third-parties.\n* Fastest practical option given existing record of reviews by security & UX and\nwork completed so far.\n* Fresh start with a more modern web stack (i.e. React).\n","Create a ""payment kernel"" - a restricted set of pages that perform the sign-up and management functions for subscription services. Isolate the ""payment kernel"" from all other pages on the Firefox Accounts website. This ensures that third-party JavaScript for payment widgets is not included on the highly-sensitive pages."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Subscription Platform was implemented with Stripe as the payment processor and driver of subscription logic. Stripe is used as the source of truth for whether an account has subscriptions associated with it. We would like to add PayPal as a payment option for subscriptions, and integrate it appropriately with the existing system in a least-effort manner that is ideally flexible enough to support future expansions of payment processors, such as IAP.\n## Decision Drivers\n- Engineering resources\n- Future effort needed to integrate IAP\n- Code complexity as it relates to existing Subscription Platform infrastructure\n\n## Decision\n","- Engineering resources\n- Future effort needed to integrate IAP\n- Code complexity as it relates to existing Subscription Platform infrastructure\nStripe-driven out-of-band invoice processing. This was originally considered to be the slower of the approaches, but using PayPal subscriptions ended up being approximately the same engineering effort. Given additional user experience benefits of the Stripe-driven approach, it is now the recommend implementation path.\n","**Decision:** Implement PayPal integration as an extension of the existing Stripe integration, using the same API and data structures as much as possible.\n\n**Rationale:**\n\n* **Engineering resources:** This approach minimizes the engineering effort required, as it leverages the existing Stripe infrastructure and codebase.\n* **Future effort for IAP:** The design is extensible to support IAP integration in the future, as it provides a framework for integrating additional payment processors.\n* **Code complexity:** By using the same API and data structures, the code complexity is reduced compared to creating a separate integration for PayPal.\n\n**Details:**\n\n* Create a new payment method in Stripe called ""PayPal"".\n* Allow users to subscribe using PayPal as the payment method.\n* Handle all subscription-related logic (e.g., cancellations, upgrades) through the existing Stripe API.\n* Store PayPal-specific information (e.g., client ID, secret key) in a secure location.\n* When integrating with IAP in the future, follow a similar approach to extend the payment method functionality to IAP."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nA solution for isolating third-party payment widgets from the rest of account\nmanagement can include building a separate web app on its own dedicated domain.\nAs a side effect in building that app, we have the opportunity to choose\ntechnologies for building it that don't necessarily follow the rest of FxA.\n## Decision Drivers\n* Opportunity for a fresh start with tech stack without rewriting\n* Security in dealing with payment transactions\n* Developer ergonomics\n* Code quality & testing\n* Subscription services deadlines\n\n## Decision\n","* Opportunity for a fresh start with tech stack without rewriting\n* Security in dealing with payment transactions\n* Developer ergonomics\n* Code quality & testing\n* Subscription services deadlines\nChosen option: ""React, Redux, Typescript"", because\n* Chance to start with a fresh stack\n* More vibrant ecosystem\n* Better tooling & developer ergonomics\n","Build a separate web app on its own dedicated domain for isolating third-party payment widgets from the rest of account management. This will allow for a fresh start with the tech stack without rewriting, improve security in dealing with payment transactions, and enhance developer ergonomics, code quality, and testing. Additionally, it will help meet subscription services deadlines."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFirefox Accounts sends and receives messages in JSON utilizing queues. This ADR addresses this\nuse-case only and does not apply to JSON used in RESTful APIs.\nTracking and minimizing drift between the accuracy of message format documentation and\nthe underlying implementation is an error-prone process. In addition, JSON messages are\nfrequently produced and consumed from multiple systems, and in multiple langauges. Ensuring\nvalidity of these messages frequently requires implementing validation logic in each\nlanguage that consumes these JSON messages to ensure correctness for message handling.\nAdding a field to a message format is non-trivial, as every consumer must also update\nits own validation logic. Currently there are a variety of places where messages are consumed\nand produced, most producers do not validate the message they send as the consumer code is not\nalways in the same service where the validation code typically resides.\nThere is currently no versioning for our message formats, which also creates difficulties in\nknowing when validation logic must change and when existing flexibility is sufficient.\n## Decision Drivers\n- Accurate documentation via JSON-Schema doc generation\n- Validation logic generated from JSON-Schema to reduce manually written/maintained code\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\n\n## Decision\n","- Accurate documentation via JSON-Schema doc generation\n- Validation logic generated from JSON-Schema to reduce manually written/maintained code\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\nChosen option: ""option 3"", because our existing messages can be represented without any\nmigration needed and external consumers already work with JSON.\n### [option 1] Keep existing approach\nCurrently our documentation drifts if a message format is tweaked and the docs are not updated. Ideally\nour code review process would catch this but sometimes code subleties are not caught that alter the\nmessage format. Validation changes can also fail to appear in the documentation, even though what is\nconsidered a valid message key/value should be documented.\nWe have to occasionally duplicate `joi` schemas between repos, which should likely be moved to `fxa-shared`\nto benefit re-use, but this does not help non-Node projects consume our messages.\n- Pros\n- No additional work is needed that we don't already do.\n- Cons\n- Our documentation frequently drifts.\n- It's additional work to maintain accurate `joi` schemas spread throughout the code-base _or_\n- It's additional work to consolidate our `joi` schemas in `fxa-shared`.\n- `joi` schemas can't be used by consumers not written in JS.\n### [option 2] Use protobuf\nUsing protobuf would allow us to validate and document valid message formats that would also consume\nless space and be more performant to serialize/deserialize. Validation is built-in as invalid messages\nwill not deserialize, and documenation can be generated based on the protobuf spec for a message.\nProtobuf tooling is widely available for most langauges.\n- Pros\n- High throughput messaging could benefit from higher efficiency of serializing/deserializing of messages\n- Documentation stays in sync with message spec\n- Validation is built-in to the message serialization/deserialization process\n- Cons\n- Messages are in binary and not easily introspectable\n- Existing message consumers will need to be updated to handle protobuf\n- Migration procedure will need to be created/implemented to shift from existing JSON format\n### [option 3] Use JSON-Schemas\nJSON-Schemas can define existing FxA JSON messages used in our queue system. There are libraries in\nmost languages to consume these schemas for validation purposes, and libraries exist to generate\ndocumenation from a JSON-Schema.\n[SchemaVer] can be introduced to existing FxA JSON messages as consumers ignore unknown keys, while\nconsumers using the JSON-Schema can verify they can handle new message formats or if they're incompatible.\nSome of this logic is manual, as [SchemaVer] is not an official part of the JSON-Schema specification. There\nis a recommendation of [SchemaVer] in [Best Web Practices for Data](https://www.w3.org/TR/dwbp/#dataVersioning).\n- Pros\n- No changes needed for existing consumers\n- Validation can be done based on the JSON-Schema without custom code\n- Documentation can be generated to stay in sync with message spec\n- JSON is human readable without further processing.\n- JSON is widely used on the web and our increasing use of JWTs also result in JSON.\n- JSON-Schema is widely supported in a variety of langauges.\n- Cons\n- [SchemaVer] is a draft that isn't officially used by the JSON-Schema committees, but no other versioning\nscheme for JSON-Schema appear to exist as of this writing.\n[schemaver]: https://snowplowanalytics.com/blog/2014/05/13/introducing-schemaver-for-semantic-versioning-of-schemas/\n",Use JSON Schema with Semantic Versioning to describe message formats and enable automated validation across multiple languages.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nA mechanism is needed to inform service providers (SP) that a user has paid for a given subscription. A SP could fetch the user's profile information, however, we prevent some 3rd party SPs from doing so, and instead give them all the information they need in a JWT format access token. Adding subscription information in JWT access tokens will give SPs the information they need to verify users have paid for a subscription.\n## Decision Drivers\n- Security - Users should not be able to get access to subscriptions they have not paid for.\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\n\n## Decision\n",- Security - Users should not be able to get access to subscriptions they have not paid for.\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\nChosen option: Adding subscription info into its own claim was chosen because FxA's lax scope checking means bad acting users could grant themselves access to subscriptions they have not paid for. See [this bug regarding FxA's lax scope checking][#lax-scope-checking].\n### Positive Consequences\n- The JWT access token contains all the information an SP needs to verify the user has paid for a subscription.\n- Users are unable to grant themselves access to subscriptions they have not paid for.\n### Negative Consequences\n- SPs must now check two claims from the JWT to ensure a user is able to access a protected resource.\n- An additional claim is added that is not defined in [the JWT access token draft spec][#jwt-draft-spec].\n,"**Add a new `subscription` claim to the JWT access token, containing a list of subscription IDs that the user has paid for.**\n\nThis decision meets the following criteria:\n\n* **Security:** The subscription information will be included in the JWT access token, which is signed by the authorization server. This ensures that only authorized SPs can access the information.\n* **Extensibility:** The new `subscription` claim is added to the JWT access token without modifying the existing claims. This allows for future extensions to the JWT access token without breaking compatibility with existing SPs.\n* **Standards:** The new `subscription` claim follows the format of the JWT access token draft spec."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen Firefox Accounts was created some 6-7 years ago, the front-end Javascript framework and HTML templating options were limited significantly to those available at the time. This ADR details the pros and cons of various FE libraries and frameworks, and why it is desirable for FxA to move to a different technology. It does not cover _how_ we will begin the conversion which can be [read about here](https://github.com/mozilla/fxa/blob/main/docs/adr/0011-create-new-react-app-for-settings-redesign.md).\nWhile Backbone and Mustache have served their purposes for FxA thus far, other modern libraries and frameworks not only offer more appealing architecture and developer conveniences, but also offer better data predictability, potential performance boosts, and a strong community of online support.\n## Decision Drivers\n- The desire for a more modern FE architecture and a more efficient UI\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\n\n## Decision\n","- The desire for a more modern FE architecture and a more efficient UI\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\nChosen option: ""Option B - Move to client-side React"", because\n- The team desires the benefits we will gain from moving to a more modern library (excluding option A)\n- The team has experience with (and likes) React\n- We're already using it in other parts of the ecosystem, i.e. the payments server\n- FxA does not need server-side rendering (option C)\n- Angular (option D) is heavy, opinionated with a steep learning curve, and offers at least some features we won't need\n- Web components and libraries around them (option F) are quite new, yet a distant potential architecture shift from React to web components appears to be far less dramatic than from Backbone to React\nIn the end, it is recognized that Vue _may_ have been a good option for FxA, but React was chosen over Vue due to:\n- existing FxA engineering team knowledge\n- the FxA payments server is already using React\n- React has already been reviewed by our security team and ""passed"" the trial run when it was novel tech in our stack\n- As one of our desires is to best protect against such a major refactor proposal in the future, we have to allow package age, online support, popularity and backing, and stability to be factors in our decision.\n","The decision is to move to a modern FE library or framework such as React, Angular, or Vue.js. This will provide a number of benefits, including:\n\n* A more modern and efficient UI\n* Improved data predictability\n* Potential performance boosts\n* A strong community of online support\n* Familiarity to new engineers and open source contributors"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe new FxA settings page introduces a new service in the form of a GQL API that will provide account and profile data to the browser. This service needs to make api requests to the auth-server on behalf of logged in users. The auth-server uses hawk authentication so adding another service between the browser and auth-server is not straightforward. In [ADR-0017](0017-switch-settings-auth-to-sessiontoken.md) we decided to allow the browser to share the session token with the GQL server so that it can make hawk authenticated requests. This leads us to question whether we should deprecate our use of hawk altogether.\nLong ago, the precursor to onepw had 2-phase signup/login that would establish a secret key that would be used to encrypt/decrypt the sessionToken so that it was never in the clear on the wire. It was secure enough that you could login over HTTP and still be safe. We'd still use TLS, but - defense in depth. Once we had the sessionToken we used Hawk for API requests to maintain that level of security. We implemented this protocol and it worked as designed. The problem at the time was we wanted to support low-end phones on slow networks and the extra round trips meant we were too slow; bad UX. Instead of scrapping the whole thing we modified the protocol to the one we use today. The sessionToken is now sent over TLS in the signup/login response, but the ""secure even over HTTP"" dream is gone.\nHawk is designed for authentication over HTTP. Quoting from the design goals of Hawk:\n- simplify and improve HTTP authentication for services that are unwilling or unable to deploy TLS for all resources,\n- secure credentials against leakage (e.g., when the client uses some form of dynamic configuration to determine where to send an authenticated request), and\n- avoid the exposure of credentials sent to a malicious server over an unauthenticated secure channel due to client failure to validate the server's identity as part of its TLS handshake.\nNone of those scenarios apply to auth-server.\nDefining the replacement for hawk is out of scope, but let's consider the properties of the interim replacement: using the sessionToken as a bearer token.\nThe new surface area to which the token is exposed is in the HTTP header of API requests, sent over TLS. Both the client and server storage of the token remains unchanged. A compromised TLS connection would expose the token on more API calls where it would not have been with hawk. The sessionToken is in the body of the login response, so even with hawk that endpoint could leak the token if the TLS connection is compromised.\nA leaked sessionToken would allow access to personal information, email addresses and device metadata, but not the FxA password, secret keys, or Firefox Sync data.\n## Decision Drivers\n- Security\n\n## Decision\n",- Security\nWe will stop using hawk in future work that requires authentication. Selecting a preferred scheme is out of scope of this ADR. Sharing the session token with trusted services is an acceptable interim solution.\n,"**Deprecate hawk authentication in favor of using the session token as a bearer token.**\n\n**Rationale:**\n\n* Hawk authentication is designed for use over HTTP, and the auth-server is not exposed via HTTP.\n* The security benefits of hawk authentication are not applicable to the use case of making authenticated requests to the auth-server on behalf of logged in users.\n* Using the session token as a bearer token exposes the token to a slightly larger surface area, but the risk of compromise is still low.\n* A leaked session token would not grant access to sensitive data such as the FxA password, secret keys, or Firefox Sync data.\n\n**Consequences:**\n\n* The Hawk authentication code will need to be removed from the GQL API service.\n* The auth-server will need to be updated to accept session tokens as bearer tokens.\n* The browser will need to be updated to share the session token with the GQL API service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\n## Decision Drivers\n- User happiness via fewer keystrokes, less confusion\n- Improved signin rates\n\n## Decision\n","- User happiness via fewer keystrokes, less confusion\n- Improved signin rates\nChosen option: ""option 2"", because it minimizes the number of places the user must enter their password.\n### Positive Consequences\n- User will need to type their password in fewer places.\n- Signin completion rates should increase.\n### Negative Consequences\n- There may be user confusion around what it means to sign out.\n### [option 1] Keep the existing flow\nIf a user signs in to Sync first and is not signing into an OAuth\nRP that requires encryption keys, then no password is required.\nIf a user does not sign into Sync and instead signs into an\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\nrequire encryption keys, e.g., Monitor, then they must enter their password.\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\n- Good, because we already have it and no effort is required to keep it.\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\n- Bad, because users need to enter their password more than they need to.\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\n### [option 2] Only ask authenticated users for a password if encryption keys are required\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\n","**Do not prompt for passwords when the user is already authenticated.**\n\nThis decision is based on the following:\n* **User happiness:** Entering passwords is a pain point for users, especially on mobile devices. Eliminating unnecessary password prompts will improve user satisfaction.\n* **Improved signin rates:** Reducing the number of password prompts will make it easier for users to complete authorization flows, leading to higher signin rates.\n* **Technical feasibility:** It is technically feasible to use the existing sessionToken to request new OAuth tokens for RPs that do not require encryption keys."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\nIn [ADR-0014](0014-auth-for-settings-redesign.md), we'd decided to use OAuth2 as the mechanism. But, upon attempting to implement, we discovered that [context missing from OAuth2 tokens][missing-oauth2-context] rendered the option unusable.\n[missing-oauth2-context]: https://github.com/mozilla/fxa/pull/4931#discussion_r411828476\n## Decision Drivers\nBasically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\n- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\n\n## Decision\n","Basically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\n- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\nChosen option: ""Reuse existing sessionToken on content-server with auth-server APIs"", because it's the least novel option requiring fewest changes to auth-server.\n### Positive Consequences\n- Easiest path to authenticating on auth-server for API requests, works almost exactly like the settings client does already.\n### Negative Consequences\n- We defeat the security promises of Hawk, but we're not really relying on them. We'd also like to move away from Hawk in the future anyway.\n",Implement OAuth2 Token Exchange Grant to pass context headers between content-server and downstream services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor over the past 5 years (at least), we have been using [InternJS](https://github.com/theintern/intern) for our functional tests in the content-server. While this has worked well and caught several bugs, we should reevaluate it since we are migrating to react and have an opportunityto use newer/faster/better supported testing frameworks.\n## Decision Drivers\n- Ease of writing tests\n- Good community support\n- Learning curve of settings V2 development\n\n## Decision\n","- Ease of writing tests\n- Good community support\n- Learning curve of settings V2 development\nUse InternJS for functional testing in the settings v2 app, but also update the tests to support async/await. The biggest factor was not having to introduce another testing framework for developers to learn.\n","**Decision:** Migrate to Cypress.io for functional testing.\n\n**Rationale:**\n\n* **Ease of writing tests:** Cypress.io has an intuitive and user-friendly API that makes it easy to write and maintain tests. It also provides a visual testing tool that allows developers to quickly identify UI inconsistencies.\n* **Good community support:** Cypress.io has a large and active community that provides support, resources, and best practices. This ensures that developers will have access to help when needed.\n* **Learning curve of settings V2 development:** While migrating to Cypress.io may require some learning, it is considered to be a modern and well-supported testing framework that will provide significant benefits in the long run. Additionally, the documentation and tutorials provided by Cypress.io make the onboarding process relatively straightforward."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe goal of fxa-email-service was to spin off the email sending responsibilities of FxA into a shared service that multiple Mozilla projects could use. From its readme:\n> The FxA team had an OKR for Q2 2018 about decoupling the auth server from SES and making it possible to send email via different providers. Subsequently, some other teams expressed an interest in depending on a standalone email service too.\n> This repo started as our experiment to see what a decoupled email-sending service would look like, written in Rust. It is now handling all FxA email traffic in production, and we are gradually separating it from the FxA stack to run as a standalone service in its own right.\nHad it achieved the goal of fully decoupling from FxA and being more widely used we'd likely continue using it. However, in the approximately 4 years of its existance it has only really been used as an intermediate step between auth-server and SES. Fortunately in that time it hasn't required much maintenance so its ""weight"", being a fairly large codebase for what it does and our only Rust service, has never been a concern. Recent work to eliminate fxa-auth-db-mysql meant we either needed make changes to it or rethink how FxA sends email. It turned out the work to update email-service was larger than eliminating it and replacing it by sending email directly from auth-server via SES or SMTP.\n## Decision Drivers\n- Future maintenance\n- Email provider flexability\n\n## Decision\n",- Future maintenance\n- Email provider flexability\nWe will eliminate email-service.\n,**Decision:** Deprecate and remove fxa-email-service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [Settings Redesign project](https://github.com/mozilla/fxa/issues/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https://github.com/mozilla/fxa/issues/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https://github.com/mozilla/fxa/blob/main/docs/adr/0015-use-css-variables-and-scss.md).\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https://protocol.mozilla.org/fundamentals/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\n## Decision Drivers\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\n\n## Decision\n","- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\nChosen options: ""Option B"" with Tailwind CSS for majority styling, and implementation details from ""Option D"" when utility classes don't meet the entire need, because:\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\n","We will use CSS-in-JS (CIJS) and BEM for the Settings Redesign project. CSS-in-JS is a technique where CSS is written in JavaScript files, rather than in separate CSS files. This allows for more dynamic and interactive CSS, and it makes it easier to manage CSS for complex components. BEM is a CSS methodology that uses a consistent naming convention for CSS classes, which makes it easier to read and maintain CSS code.\n\nThis decision was made after considering the following factors:\n\n* **Reusability:** CIJS and BEM both promote reusability by making it easier to create and share CSS components.\n* **Longevity:** CIJS and BEM are both well-supported and have a large community of developers. This means that they are likely to be supported for years to come.\n* **Developer experience:** Some team members are already familiar with CIJS and BEM, which will make the transition to these technologies easier.\n* **Ease of use:** CIJS and BEM are both relatively easy to learn and use. This will make it easier for engineers to pick up these technologies and start using them in their work."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\n## Decision Drivers\n- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\n\n## Decision\n,"- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\nChosen option: ""Authenticate via OAuth2 to use auth-server APIs"", because:\n- It requires minimal changes to existing auth-server implementation and infrastructure.\n- It relies on an authentication mechanism with relatively well-known security properties.\n### Positive Consequences\n- The OAuth2 access token mechanism is better suited to this purpose than a novel scheme to share session token credentials.\n- The new Settings Redesign app can use existing auth-server APIs with minimal modifications to accept scoped OAuth2 access tokens. This can constrain most of the novelty in the project to the redesigned & reimplemented settings UX.\n### Negative Consequences\n- We will need to modify auth strategies in auth-server APIs, taking care not to affect existing usage in the production settings app.\n- We don't have an entirely greenfield project, which could be a bit of a drag. But, rediscovering lessons learned in existing code can also be a drag.\n","**Implement a custom authentication and authorization mechanism specific to the Settings Redesign app that leverages the existing infrastructure of the content-server app.**\n\nThis decision aligns with the following decision drivers:\n\n**Smooth UX**: By using a custom mechanism, we can tailor the authentication and authorization process to the specific needs of the Settings Redesign app, ensuring a smooth user experience.\n\n**Security**: The custom mechanism will allow us to implement additional security measures that are specific to the Settings Redesign app, ensuring the protection of sensitive user data.\n\n**Development velocity**: By leveraging the existing infrastructure of the content-server app, we can reduce the development time and effort required to implement the authentication and authorization mechanism.\n\n**Ease of integration**: Integrating the custom mechanism with the existing infrastructure will be relatively straightforward, minimizing disruption to the overall system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFirefox Accounts is introducing a new visual design for Settings (the interface that exists at the address `https://accounts.firefox.com/settings`). The changes involved are significant enough such that it [has been decided](https://github.com/mozilla/fxa/blob/main/docs/adr/0011-create-new-react-app-for-settings-redesign.md) that an entirely new React App will be built more or less independent of the existing Settings web app. With this we can beta test and roll out the new design with minimal disruption.\nAs there are a multitude of ways a React App can be set up and configured, this decision has introduced the question, ""What [toolchain](https://en.wikipedia.org/wiki/Toolchain) should we set up a new React App with?"". This ADR serves to answer that question by going over the various approaches we can take to set up a new React App and how it might integrate with our existing FxA ecosystem.\nIt's important to note that we are not deciding the languages, libraries, or other tools that we will use within the React development environment, but rather, the system that will be used to develop the React App with.\nFinally, we are not making decisions about the App's production server here. We'll be using [fxa-content-server](https://github.com/mozilla/fxa/tree/main/packages/fxa-content-server/server)'s Express server for the production build.\n## Decision Drivers\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\n\n## Decision\n","- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\nChosen option: ""B3 - Create React App and use Rescripts"", because:\n- Our team has sufficient knowledge in working with CRA over the other options.\n- We don't need to re-invent the wheel. Our needs for the Settings project are not so great that it would be worth spending time creating a ground-up toolchain. To some extent this option allows us to do what we do best without needing to worry about the specifics of the development workflow.\n- Rescripts allows us to layer additional configuration overtop of the existing configuration without jeopardizing the integrity or trading away any of the powers of CRA. Specifically, we can use Rescripts to modify CRA to allow external React components to be imported, as demonstrated in [this repo](https://github.com/jodyheavener/rescripts-react-app) (technical notes [here](https://github.com/jodyheavener/rescripts-react-app/blob/master/.rescriptsrc.js)). This capability could be implemented in our other React Apps.\n- If, down the road, we end up in a place where we absolutely need access to the internals we can always eject.\n","The team decided to use [Nx](https://nx.dev/), a monorepo toolkit that provides a more structured approach to monorepo management, with pre-built configurations for common tools and plugins, such as:\n- Linting\n- Testing\n- Code generation\n- Build caching\n- Documentation generation\n- Code formatting"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared/redis`.\n## Decision Drivers\n- Need for robust redis transactions\n- Increased use of redis for oauth\n- Updating module dependencies\n- move from ""redis"" (last updated 2 years ago) to ""ioredis"" (actively maintained)\n\n## Decision\n","- Need for robust redis transactions\n- Increased use of redis for oauth\n- Updating module dependencies\n- move from ""redis"" (last updated 2 years ago) to ""ioredis"" (actively maintained)\nUse lua scripts. The proof-of-concept [PR](https://github.com/mozilla/fxa/pull/3278) showed lua as viable and an overall simpler option.\n### Positive Consequences\n- Future use cases should be simpler to implement\n- Less custom Redis glue code and simpler stack traces\n- Improved maintainability\n### Negative Consequences\n- Additional developer cost of understanding lua\n- Changes to lua scripts require additional consideration with regard to performance. Performance implications should be a checklist item on any lua script change PR, similar to SQL stored procedures.\n","Investigate the ""ioredis"" module as a replacement for our current redis client in `fxa-shared/redis`. If ""ioredis"" meets our needs, we should migrate to it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\nWe should evaluate other testing stack options to improve reliability.\n\n## Decision\n,"Playwright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some ""happy path"" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\nP1 tests are the first priority for converting to Playwright.\n",**Decision:**\n\nMigrate to a more reliable testing stack that can improve the success rate of functional tests in both stage and CI environments.\n\n**Rationale:**\n\n* The current testing stack is unreliable and results in a high rate of flaky tests.\n* This low success rate hinders continuous delivery and slows down development.\n* The high number of flaky tests in PRs is also a costly overhead.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor subscription services, we need a way to inform a Relying Party about the\nrelevant subscription status to enable features for a given user.\n## Decision Drivers\n* Engineering simplicity\n* Security\n* Business Operations\n* Marketing\n\n## Decision\n","* Engineering simplicity\n* Security\n* Business Operations\n* Marketing\nChosen option A: Single profile assertions consisting of a list of subscription capability\nstrings based on requesting client ID.\n* Subscription capabilities are conveyed as a single additional profile assertion.\n* FxA maintains a mapping of client-to-capabilities, capabilities-to-product\n* Relying Parties can provide multiple subscription-gated capabilities.\n* Example:\n* **RP-A** provides `goldBadge` and `silverBadge`\n* **RP-B** provides `goldBadge` and `unlimitedStorage`\n* **RP-C** provides `freePuppies`\n* FxA can bundle capabilities into cross-RP products as needed for marketing &\nbusiness purposes.\n* Example:\n* **Product A** bundles `goldBadge` & `unlimitedStorage`\n* RPs are not granted visibility into the user's entire subscription status.\n* Example:\n* User subscribes to **Product A**.\n* **RP-A** will see `goldBadge` but not `unlimitedStorage` in User's `subscriptions` claim in profile.\n* **RP-B** will see both `goldBadge` and `unlimitedStorage`\n* **RP-C** sees no capabilities listed\n","**Decision:** Implement a webhook system to notify Relying Parties of subscription status changes.\n\n**Rationale:**\n\n* **Engineering simplicity:** Webhooks are a lightweight and straightforward way to send notifications without requiring complex integrations.\n* **Security:** Webhooks can be secured using HTTPS and authentication mechanisms, ensuring the confidentiality and integrity of data.\n* **Business Operations:** Webhooks allow for real-time updates on subscription status, enabling timely actions and efficient business processes.\n* **Marketing:** Webhooks can trigger automated marketing campaigns based on subscription status, providing personalized messaging and targeted promotions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [Settings Redesign project](https://jira.mozilla.com/browse/FXA-840) will be created [as a new React application](https://github.com/mozilla/fxa/blob/main/docs/adr/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\n[GraphQL](https://graphql.org/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\n""Apollo"" in this document refers to the ""Apollo client"" and ""Apollo server"" pieces of the Apollo platform¹, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\n¹Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https://www.apollographql.com/docs/intro/platform/). We do not need to use these to use GQL with Apollo server or Apollo client.\n## Decision Drivers\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\n\n## Decision\n","- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\nChosen option: ""B - Layer GraphQL on top of our REST architecture"", because:\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more ""declarative"" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\n","**Use GraphQL with Apollo for the Settings Redesign project.**\n\n**Rationale:**\n\n* **Performance:** GraphQL allows for more efficient data fetching by reducing the number of network requests and data transferred. It can also prioritize data fetching based on the client's needs, improving performance.\n* **Ease of setup:** Apollo provides a convenient setup for client-side API calls, simplifying the development process.\n* **Clarity:** GraphQL provides a clear and structured way to define data requirements, enhancing code readability and reducing potential errors.\n* **Development speed:** Both the initial setup and ongoing feature development can be accelerated using GraphQL and Apollo's tools, such as caching and React integration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n""Settings"" refers widely to the `accounts.firefox.com/settings` page where users can manage their Firefox account. The ""Settings Redesign Project"" refers to a 2019/2020 project centered around giving this page a fresh user interface. The changes are fairly significant and will require a lot of component shuffling and some functionality refactoring. [See the PRD](https://docs.google.com/document/d/18zu7JCYIsUp8tUMJqb2uErNlzL9f6CQvefLy9HFZ4UY/edit?pli=1#heading=h.cf57dt1i8634).\nThe FxA Engineering team desires to migrate from [Backbone and Mustache to React](https://github.com/mozilla/fxa/blob/main/docs/adr/0010-transition-fxa-from-backbone-to-react.md). This ADR addresses questions around what level of action we should take regarding this desire and the Settings Redesign Project - should a conversion to React be done at a later time, or are the changes significant enough to justify propelling us into React now?\n\n## Decision\n","Chosen option: ""Option C - Take the 'ground up' approach and create a new settings React application"", because\n- The overall goal of the Settings Redesign Project is to offer more flexibility to expand Settings functionality. New features have already been planned. It could be argued that heavy refactoring later (which would be needed for option A or B) would take just as much time as recreating this page with React now.\n- This approach allows us an opportunity to set up Storybook for the content server and review our tests, CSS, and a11y implementations as we implement the UI changes.\n- This approach also simplifies our GH workflow and A/B testing the entire redesign.\nWe can mitigate risks by avoiding a ""big bang"" surprise replacement by implementing, deploying, and launching smaller pieces of MVP functionality along the way under a new front-end route serving the React application while keeping the Backbone Settings live. While this may be tedious, it can be a safety net against losing integrity and wisdom earned in the original system.\n","The decision is to migrate to React during the Settings Redesign Project.\n\nThe team decided that the significant changes required for the Settings Redesign Project justify migrating to React during the project. This approach provides the following benefits:\n- The migration will be completed earlier, allowing us to take advantage of React's benefits sooner.\n- The migration will be more comprehensive and less disruptive if it is done during a time when the codebase is already undergoing significant changes.\n- The team will have more experience with React by the time the project is completed, which will make it easier to maintain the codebase in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\n- Not clear where/how to add new components and takes time to study/understand how things are currently setup in an attempt to mimic the structure for the new component.\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\n## Decision Drivers\n- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger/OpenAPI a bonus)\n- GraphQL\n- Use of components/services easily in a script\n\n## Decision\n","- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger/OpenAPI a bonus)\n- GraphQL\n- Use of components/services easily in a script\nChosen Option: Use an existing framework: NestJS\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory/filename conventions from NestJS and use `typedi` as a DI system to ease configuration/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\n### Positive Consequences\n- Less documentation to create, as we only document differences for legacy services.\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\n### Negative Consequences\n- There will be some effort involved to migrate the 4 packages to NestJS.\n- We will need to document the differences between NestJS and other legacy services.\n","**Decision:** Adopt the ""Flask"" micro-framework as the application framework for FxA back-end services.\n\n**Rationale:**\n\nFlask provides a lightweight, easy-to-use framework that aligns with the decision drivers:\n\n* **Documented architecture:** Flask has a clear and well-documented architecture, making it easy for new developers to understand and contribute.\n* **Tooling and reduced boilerplate:** Flask plugins and extensions streamline component setup and boilerplate code generation.\n* **Modern paradigms (e.g., DI):** Flask supports DI through its `flask-dependency-injector` extension.\n* **Training materials and ease of development:** Flask is widely used and has extensive documentation and tutorials, making it accessible for new developers.\n* **Legacy migration support:** Flask provides a clear path for migrating legacy applications to its conventions.\n* **Handles common scenarios:** Flask offers built-in support for exception handling, validation, service instantiation, authentication, RESTful APIs, and GraphQL.\n* **Script compatibility:** Flask components and services can be easily consumed in scripts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Pages is a perfectly good static site host, however, not having the\nability for preview builds is problematic when wishing to test changes prior to\nthem being merged into the main branch. Netlify has the ability to create\npreview builds for each PR or even each branch along with a number of other\nbenefits over GitHub Pages as can be seen in this (totally unbiased!)\n[comparison](https://www.netlify.com/github-pages-vs-netlify/).\n\n## Decision\n",The decision is to move hosting and deployment onto Netlify\n,"**Decision:** Migrate the static site from GitHub Pages to Netlify.\n\n**Rationale:**\n\n* Netlify offers preview builds, enabling testing of changes before merging them into the main branch.\n* Netlify provides additional benefits over GitHub Pages, as highlighted in the provided comparison.\n* Migrating to Netlify will enhance the development workflow by allowing more efficient testing and deployment of changes."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Utilize an Architectural Decision Record (ADR) template to capture and document the architectural decisions made throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis design is based on the observations and lessons learned over the years from various approaches taken\nto solve problems in AWS/Zuora domain. The core idea is minimalism in tools, abstractions and indirections.\n### Small Scala AWS Lambda package size of less than 10 MB\n* There is no need for custom lambda libraries. The following vanilla Scala snippet is all that is necessary to\ndefine Scala lambda\n```scala\nobject Lambda {\ndef handleRequest(input: InputStream, output: OutputStream): Unit = {\n// deserialise stream\n// run program\n// serialise to stream\n}\n}\n```\n* There is no need for importing web frameworks/libraries within lambdas as AWS provides necessary facilities\nout-of-the-box such as logging, concurrency, error handling, etc.\n* Many custom libraries are not necessary as Scala is a very expressive language which can express the same\nconcept using vanilla facilities\n* If something can be done with couple of lines of code, there is no need to import whole library just to do that\n### Prefer direct inlined business logic over abstracted indirections\n* Generally speaking there is not much algorithmically complicated logic in our business domain.\n* Essentially we need an HTTP client, JSON deserialiser, and some request orchestration code.\n* We should be able to inline directly the whole orchestration in a single file, instead of spreading it\nover many files, libraries, and abstractions.\n* Advanced features should be used in few places where needed, instead of across the whole system\n### Use off-the-shelf infrastructural facilities instead of re-inventing the wheel\n* AWS has good documentation, many examples across open source GitHub, and is GDPR compliant.\n* Setting up infrastructure is difficult, but it is largely a one-time affair. After that it rarely changes,\nand when we have to then we should be able to look up documentation\n* For example, if our stack is API Gateway + Lambda, there is no need to import web frameworks, implement custom\nrouters, etc.\n### Testing in production via preconditions-program-postconditions instead of mocked test\n* Favour runtime preconditions and postcondition testing in production instead of unit test with mocked HTTP/JSON.\n```\ninput\n.tap  { preconditions }\n.pipe { program }\n.tap  { postconditions }\n```\n* Reserve unit tests for complicated business logic algorithms, not plumbing\n* Issues such as malformed JSON or wrong HTTP request orchestration are likely to surface quickly after deployment\n(as long as alarming is setup) thus ROI for unit testing these aspects is likely not worth it\n* Runtime preconditions/postconditions is just code alongside main business logic and as such harder to ignore and\nhas lower maintenance cost relative to unit tests\n### Prevent silent failures\n* There have been multiple cases of silent failures introduced by handling errors with techniques that swallow\nerrors without error logging or wiring them to non-200 responses. Failing fast on unrecoverable errors hooks into AWS\nout-of-the-box error logging and alarming at minimal cost.\n* If there is no automatic error recovery possible and system cannot proceed, why fail slow?\n* Automatic error recovery, self-healing, etc., are difficult to achieve ideals, usually not possible in Zuora.\nDue to nature of Zuora outages which are hours long, error handling cannot be meaningfully addressed\nby techniques on a unit level. Instead, they should be addressed on a much higher infrastructural level such as using\nSQS, Step Functions, etc.\n* If error recovery is possible, then it should **not** actually be modelled as an error but simply an alternative path\nthe system tries.\n### Minimal custom abstractions of Zuora\n* Work on whitelisting as opposed to blacklisting principle\n* If system detects a scenario it cannot handle then immediately notify developers to make adjustment\ninstead of trying to predict the Zuora model up-front to handle all imaginable scenarios\n* Modelling of Zuora has not been very successful over the last five+ years due to the way Guardian uses it with\ncountless exceptions in the model\n* Instead use Zuora directly the way they document it which means essentially on the REST API level.\n* Favor preconditions and postconditions to check if we have done the right thing\n* ""Does the invoice look correct after production mutation?"", as opposed to\n* ""Here is the predicted model capturing all the ways invoice can be generated, and the corresponding (mocked) unit test.""\n* Zuora is hard enough to understand without developers putting another layer on top of it\n### Usage of lightweight HTTP and JSON libraries and vanilla Scala\n* Scala is an expressive language, and many concepts can be directly implemented without depending on external libraries\n* JSON and HTTP is boring, there is no need for sophisticated libraries, or even worse,\nre-inventing yet another custom implementation\n* There is no need for special logging or config libraries. AWS provides this out-of-the-box.\n* Most scenarios do not require advanced techniques/libraries. Such techniques should be used only where necessary\nin isolated segments of the codebase.\n\n## Decision\n",Pending\n,"Use direct, inlined business logic over abstracted indirections. For example, use a single file with a few lines of code for lambda function instead of spreading the code over several files and multiple abstractions."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe try to structure our package in logical sub-units but we want to maintain a\nconsistent public interface.\n\n## Decision\n,We allow for arbitrarily nested sub-packages but export important classes and\nfunctions to the top level thus exposing a public interface. Our unit tests\nshould reflect this package structure.\n,"**Decision:**\n\nUse a [facade](https://refactoring.guru/design-patterns/facade) design pattern to expose a simplified and unified interface for a complex subsystem.\n\n**Benefits:**\n\n* **Simplified public interface:** The facade hides the complexity of the underlying subsystem, providing a clear and concise entry point for clients.\n* **Increased flexibility:** The facade can be modified independently of the underlying subsystem, allowing for changes to the subsystem without affecting clients.\n* **Improved maintainability:** The facade centralizes the public interface, making it easier to maintain and update.\n\n**Implementation:**\n\n* Create a facade class that implements the desired public interface.\n* The facade class should delegate method calls to the appropriate classes in the underlying subsystem.\n* Use dependency injection or other mechanisms to provide the subsystem objects to the facade.\n\n**Example:**\n\nConsider a system with multiple classes representing different aspects of an order processing system.\n\n```\npublic interface OrderFacade {\n  Order createOrder(Customer customer, List<Product> products);\n  void updateOrder(Order order, Customer customer, List<Product> products);\n}\n\npublic class OrderFacadeImpl implements OrderFacade {\n  private OrderService orderService;\n  private CustomerService customerService;\n  private ProductService productService;\n\n  @Override\n  public Order createOrder(Customer customer, List<Product> products) {\n    Customer customerFromDB = customerService.getCustomer(customer.getId());\n    List<Product> productsFromDB = productService.getProducts(productIds);\n    Order order = orderService.createOrder(customerFromDB, productsFromDB);\n    return order;\n  }\n\n  @Override\n  public void updateOrder(Order order, Customer customer, List<Product> products) {\n    Customer customerFromDB = customerService.getCustomer(customer.getId());\n    List<Product> productsFromDB = productService.getProducts(productIds);\n    OrderService.updateOrder(order, customerFromDB, productsFromDB);\n  }\n}\n```\n\nIn this example, the `OrderFacade` class provides a simplified public interface for creating and updating orders, while the underlying subsystem (`OrderService`, `CustomerService`, and `ProductService`) handles the complex logic."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make a decision on the testing framework for our project.\n\n## Decision\n,We will make use of pytest. It is a de facto standard in the Python community\nand has unrivaled power.\n,We will use the Robot Framework.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSetting up different testing environments and configurations can be a painful\nand error prone process.\n\n## Decision\n,"We use tox to define, configure, and run different test scenarios.\n",**Decision:** Adopt a Continuous Delivery (CD) tool \n\n**Rationale:**  A CD tool automates the deployment process and allows for the creation of different environments and configurations with ease. This reduces the risk of errors and saves time and effort. \n\n**Consequences:** \n\n* Increased efficiency and consistency in the deployment process\n* Reduced risk of errors\n* Improved collaboration between development and operations teams\n* Faster time to market for new features and updates
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\nto natively support f-strings which are sweet.\n\n## Decision\n,We make an early decision to only support Python 3.6 and above.\n,Migrate to Python 3.6+ to utilize f-strings.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a simple way to manage our package version.\n\n## Decision\n,We use versioneer to do this for us.\n,"We will use semver, a widely used version numbering system that is easy to understand and use."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to version control our code in order to avoid disasters and maintain\nsanity. We also want to collaborate online with a wider community.\n\n## Decision\n,We use git for version control and GitHub for collaboration.\n,Use Git and GitHub.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n\n## Decision\n","In order to perform these operations we choose\n[pydantic](https://pydantic-docs.helpmanual.io/) which has a nice API, active\ncommunity, good data validation, helpful documentation, and good performance.\n",Serialize and deserialize all data structures as JSON.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWriting code that adheres to style guides and other best practices can be\nannoying. We want to standardize on some best-in-class tools.\n\n## Decision\n,"We will use isort, black, and flake8.\n",Use linters and formatters to ensure code quality.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use the Architectural Decision Record (ADR) template to document all architectural decisions made on this project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSelf-encoded dialects define dialect instances which share the same ID between the instance document and the dialect\ndomain element encoded in such document. This allows the document and the encoded dialect domain element to be treated\nas the same resource.\nOn the other hand Flattened JSON-LD emission only renders one node for each ID.\n\n## Decision\n,We merge both nodes (the document and the encoded domain element) and emit the single merged node with the shared ID.\nThe merged node contains both the properties from the document and the encoded domain element.\nWhen parsing the resulting flattened JSON-LD we parse the merged node twice: first as a domain element and then as a\ndocument. Parsing the properties from the domain element ignores the properties from the document and vice-versa.\n,Use the dialect ID as the resource ID when flattening JSON-LD to improve performance.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\nit from export.\n\n## Decision\n",- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\n- Add the `toString()` method in `Content` that returns the content in `stream`\n,"To revert the decision to hide this field from export, allowing the Api Designer to use it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to start deprecating old fields and have no mechanism to do so.\n\n## Decision\n,"We will start deprecating fields. Getter/setter methods in Scala and Platform classes will be deprecated also. Deprecated fields will still be set alongside new fields until these are removed in the next major version.\nHow to deprecate a field:\n1. Deprecate field definition in model class\n1. Deprecate field definition with `@deprecated` annotation\n2. Deprecate field definition with the `deprecated=true` paramter from the `Field` class\n3. Annotate the `fields` value assignment with `@silent(""deprecated"")` annotation to avoid compilation errors from deprecated fields\n2. Deprecate getter/setter methods in Scala and Platform classes with `@deprecated` annotation\n3. Update usages of getter/setter methods to use both legacy and new fields (with the `@silent(""deprecated"")` annotation)\n",We will use a mechanism called field deprecation to start deprecating old fields. Field deprecation allows us to mark a field as deprecated and specify a replacement field. This will allow us to gradually deprecate old fields while still providing a way for users to access the data in the old fields.\n\n**Benefits:**\n\n- Provides a way to gradually deprecate old fields\n- Allows users to access data in old fields while they are being deprecated\n- Prevents breaking changes in the API\n\n**Drawbacks:**\n\n- Requires additional code to handle deprecated fields\n- Can be confusing for users if not documented properly\n\n**Alternatives:**\n\n- Remove old fields immediately: This would be a breaking change and would require users to update their code immediately.\n- Keep old fields indefinitely: This would not allow us to deprecate old fields and would eventually lead to a large number of deprecated fields in the API.\n\n**Decision:**\n\nWe will use field deprecation to start deprecating old fields.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe custom AMF validator needs to show the location of the file from which each error was generated.\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\n\n## Decision\n","A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\n","Add an identifier to each element of the amf model that relates it to the source file and line, within the EMF model and the jsonld representation."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLinking nodes between different graphs is a feature not provided by AMF/AML. This feature is required by some adopters to:\n* Link nodes from a parsed API specification graph with nodes from a parsed dialect instance graph (RestSDK)\n* Link nodes between different parsed dialect instances (ANG)\n\n## Decision\n,Add a boolean field to DomainElementModel called IsExternalLink that marks that a domain element is a reference to a domain element defined in another graph.\n,"**Decision:** Create a mechanism to allow linking nodes between different graphs.\n\n**Rationale:**\n\n* **Addresses adopter need:** The new mechanism will enable adopters to link nodes from different graphs, meeting their specific requirements.\n* **Extensibility:** The mechanism should be designed to be extensible, allowing for future enhancements and integrations with other tools or frameworks.\n* **Performance:** The mechanism should be optimized for performance, ensuring that linking nodes does not significantly impact the performance of the graph-processing workflow.\n* **Ease of use:** The mechanism should be user-friendly and easy to use, with a clear and well-documented interface.\n\n**Options:**\n\n* **Extend AMF/AML:** Modify the existing AMF/AML framework to include the ability to link nodes between different graphs.\n* **Create a separate tool or framework:** Develop a new tool or framework specifically designed for linking nodes between different graphs.\n* **Use an existing tool or framework:** Explore if there are any existing tools or frameworks that can be used to facilitate node linking.\n\n**Recommendation:**\n\nBased on the rationale and options provided, the recommended decision is to create a separate tool or framework specifically designed for linking nodes between different graphs. This approach provides the most flexibility and control over the linking mechanism, while also allowing for future enhancements and integrations. The tool should be designed with extensibility, performance, and ease of use in mind."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR serves to document how Custom Domain Properties are rendered in JSON-LD.\nCustom Domain Properties represent dynamic parts of the model. The original domain is extended with annotations which\nmay or may not include their own semantic information (when used with semantic extensions).\nGiven that there is a problem, the AMF Web API model is mostly static. We have static `*Model.scala` objects with each\nmodel.\n\n## Decision\n","We will introduce a hack when rendering Custom Domain properties in JSON-LD that will make the graph look like it was\ngenerated by a dynamic model while still representing the model statically.\nConcretely, the `DomainElementModel` has a `CustomDomainProperties` field which is an `Array(DomainExtensionModel)`.\nEach `DomainExtensionModel` has a `DefinedBy: CustomDomainPropertyModel` and `Extension: DataNode` fields that\nrepresent the definition and value of the domain extension.\nThis in the ""traditional & static"" parts of the model would be rendered like this (summary):\n```json\n{\n""@id"": ""myDomainElement"",\n""customDomainProperties"": [\n{\n""@id"": ""myDomainExtension"",\n""definedBy"": {\n""@id"": ""myCustomDomainProperty""\n},\n""extension"": {\n""@id"": ""myDataNode""\n}\n}\n]\n}\n```\nInstead a ""dynamic"" model is simulated\n```json\n{\n""@id"": ""myDomainElement"",\n""myCustomDomainProperty"": {\n""@id"": ""myDataNode""\n},\n""customDomainProperties"": [\n{\n""@id"": ""myCustomDomainProperty""\n}\n]\n}\n```\nAs you can see a new property was generated using the `@id` of the `CustomDomainProperty` from the original `definedBy`\nfield which does not exist in the static model. The `customDomainProperties` property now points to an array of custom\ndomain property nodes rather than an array of domain extension nodes. The new `customDomainProperties` value indicates\nwhich properties of the serialized node ""are dynamic"".\n","Custom Domain Properties should be rendered as properties in the JSON-LD context, with the prefix `cp:`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn AMF 5 Beta's first iteration ""domain+syntax"" mediatypes like ""application/swagger20+yaml"" were\nused to decide how the syntax and the domain were to be parsed. This was especially useful to\nvalidate, transform and render units using compound configurationts (API, WebAPI, RAML, OAS)\nThis is controversial as:\n- Resulting mediatypes are strange to the end-user and are not standard. Besides we could\nonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domain\ncouldn't be parsed.\n- Clients that used a specific configuration like RAML10 or OAS30 had to specify the mediatype\nalthough the configuration they used already specified their intended domain and syntax.\n\n## Decision\n",Remove those compound mediatypes and instead only keep them for syntax purposes when needed.\n,"**Decision:** Use a single mediatype per configuration, specifying both domain and syntax in the configuration itself."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\nSome other useful links:\n[https://github.blog/2020-08-13-why-write-adrs/](https://github.blog/2020-08-13-why-write-adrs/)\n[https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/](https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/)\n","**Decision:** To use an ADR (Architectural Decision Record) template to document and track architectural decisions throughout the project.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture, communicate, and track architectural decisions.\n* They allow stakeholders to understand the reasons behind decisions and the implications for the project.\n* They facilitate discussions, reduce ambiguity, and ensure that decisions are well-informed and aligned with the project's goals.\n* They serve as a valuable resource for future reference and maintenance efforts.\n\n**Consequences:**\n\n* The project team will need to adopt the ADR template and follow its guidelines for recording decisions.\n* Regular reviews and updates of ADRs will be necessary to ensure their accuracy and relevance.\n* ADRs may need to be revised as the project evolves and new information becomes available.\n* By using a standardized template, ADRs will be easier to search, compare, and aggregate for future analysis and reporting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nExpanded JSON-LD emission embeds node links. This means that whenever a node references another node, instead of using a\nJSON-LD link we render the referenced node in place. This is because the advantage of the JSON-LD expanded emission is\nto allow consumer applications (e.g. API Console) to consume JSON-LD as regular JSON; and for that we cannot use JSON-LD\nlinks.\n**The problem**: Embedding nodes causes the resulting JSON-LD to be very big (because the same node is rendered many times).\n_Note: Flattened JSON-LD emission does not suffer this problem because it only renders each node once and then uses\nJSON-LD links for every node reference._\n\n## Decision\n","`extractToLink` is a logic we developed for the expanded JSON-LD emission that introduces _AMF links_ (as opposed to\nJSON-LD links) for some node references. Specifically, it introduces an AMF link and extracts the link target to a\n_declaration_.\n_AMF links_ are only available for `Linkable` domain elements, so we use it in only for certain elements. These links\nare part of the model as opposed to JSON-LD links which are simple graph references.\n",**Decision**: Expand only the first level of references.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the json data model.\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\nThe validity of the hash must be verified.\n## Decision Drivers <!-- optional -->\n* JWT should be used\n\n## Decision\n,"* JWT should be used\nChosen option: ""Transfer hash in JWT Claim"", because it's the only option when using JWT.\n### Positive Consequences <!-- optional -->\n* multiple hashes for different json documents can be added in one JWT\n",Use JWT (JSON Web Token) to securely transfer the JSON data hash from the authorization server to the WebSocket API and to verify the hash validity.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nExternal services must authorize web clients to the WebSocket API.\nThe WebSocket API is stateless and not maintain a user Session with Cookies.\nOnly little data should be stored for an open WebSocket connection.\n## Decision Drivers\n* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\n\n## Decision\n,"* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\nChosen option: ""JWT from pre-shared keys"", because the WebSocket API is loosely coupled and it is well supported on many platforms.\n### Positive Consequences\n* Simple to implement\n* Authorization data can be send in a portable and verifiable way\n### Negative Consequences\n* The shared keys must be handled\n","**Decision:** Use a JSON Web Token (JWT) for authentication.\n\n**Rationale:**\n\nJWTs are:\n\n* **Decoupled:** The JWT contains all the necessary information to authorize the client, so the WebSocket API does not need to maintain a session.\n* **Flexible:** JWTs can be signed with a variety of algorithms and are supported by many platforms.\n* **Efficient:** JWTs are compact and can be easily stored and transmitted.\n\n**Implementation:**\n\n1. The authorization service will generate a JWT for each client.\n2. The client will send the JWT to the WebSocket API as part of the handshake request.\n3. The WebSocket API will verify the JWT and grant access if it is valid."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the data.\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\nThe WebSocket API must able to verify the integrity of the json data.\n## Decision Drivers <!-- optional -->\n* Use standard encodings\n\n## Decision\n","* Use standard encodings\nChosen option: ""Send SHA256 hash of Base64Url encoded json"", because this method is platform independent and not much session state is required.\n### Positive Consequences <!-- optional -->\n* The JWT really function as a verification token for the other requests.\n* Can be applied to all json data that must be verified.\n### Negative Consequences <!-- optional -->\n* The json must be transferred in Base64Url encoding\n","**Decision:** Use JSON Web Signature (JWS) to transfer JSON data securely and verify its integrity.\n\n**Rationale:**\n\n* JWS is a standard encoding that provides integrity and tamper-proof mechanisms for JSON data.\n* It uses a digital signature to ensure that the data has not been modified since it was created and signed by the Authorization server.\n* The WebSocket API can verify the signature using the public key of the Authorization server, ensuring that the data received is authentic and unaltered.\n* JSON Web Signature is widely supported and well-established, making it a reliable and interoperable solution for data transfer and verification."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\nJWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA.\nWhen implementing JWTs one must decide which method to use.\n## Decision Drivers\n* Multi tenant support with own keys for each tenant\n\n## Decision\n,"* Multi tenant support with own keys for each tenant\nChosen option: ""Asymmetric JWT signing"", because it the only option which allow to use different keys for different tenants.\n### Positive Consequences\n* multiple keys are supported\n### Negative Consequences\n* complex management of keys\n","Use public/private key pairs with RSA or ECDSA for signing and verifying JSON Web Tokens.\n\nThis decision is driven by the requirement for multi-tenant support with each tenant having its own keys. Using a secret would not allow for each tenant to have its own unique key, as the secret would be shared across all tenants. By using public/private key pairs, each tenant can have its own unique private key for signing JWTs and a corresponding public key for verifying them. This provides a more secure and scalable solution for multi-tenant environments."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGiven that we were not going to have the benefit of\noffloading maintenance responsibility to the PaaS we\nwanted to keep the cost and the requirement for\nmaintenance as small as possible.\n### Architecture patterns considered\n#### PaaS\nSee [ADR 3](./0003-not-paas.md).\n#### EC2\nTraditional load balanced EC2 model running something\nlike django or rails.\n#### ECS\nContainerised version of the above\n#### Serverless\nLambda functions using APIGateway and CloudFront\nThere are many benefits to having the tools of the django\ninfrastructure.\nThe downside is that you have a maintenance requirement\nto maintain the server instances. You also somewhat bind\nyourself to running the service 24/7 and paying for it.\nEven with ECS you are still responsible for maintaining\nthe container host.\nWith serverless you only pay when the functions get\nexecuted and there is a significant free tier.\nThe downside of serverless is you can't leverage things\nlike django / rails and the communities who contribute\ncomponents to them.\nThis limits our cost and our exposure since maintaining\nthe infrastructure that hosts the lambda functions is\nAWS' responsibility not ours.\n\n## Decision\n,We will use AWS Lambda for this service.\nIn conjunction with an ENI to allow the lambdas access\nto RDS running inside a VPC if necessary.\n,"Use serverless architecture with Lambda functions, APIGateway, and CloudFront."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMy first instinct for the tool was to use the\nNPM [Serverless](https://serverless.com/) framework.\nThe main advantage of serverless is that it is cloud\nagnostic. However JavaScript is not widely used at GDS.\nAWS [Chalice](https://chalice.readthedocs.io) is a similar\nframework which has a multi-stage deploy process to\nautomate provisioning Lambda and Api Gateway but written\nin Python.\nBoth languages have frameworks for accessing the AWS API:\n* The [SDK](https://docs.aws.amazon.com/sdk-for-javascript)\nfor JavasSript and\n* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\nfor Python.\n\n## Decision\n,Given that we had already made the decision that cloud\nagnostic was not important for this service and that\nPython was more in keeping with GDS common practice we\ndecided to use Chalice.\n,"Use Chalice as the framework for the tool, written in Python."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIf running serverless we could choose to operate outside\na VPC. This has advantages in terms of the latency of\nrequests. We would not need a keep-warm function to\nensure network interfaces were provisioned.\nLambdas running outside the VPC could not easily access\nRDS running inside a VPC.\nGiven the sensitive nature of the data involved it seems\nsensible to protect it further by running RDS inside a VPC\nwhere we have much easier control over ingress and egress.\n\n## Decision\n,Implement a VPC.\n,Run RDS inside a VPC to protect sensitive data
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed some persistent storage of audit results.\nWe considered:\n### Schemaless - DynamoDb\nThis would be the most obvious choice for a lambda based\nservice.\nThe dynamo data model is tables of key value pairs.\nThe main problem with Dynamo is the limit of I think 4K\nper value.\nOne of the things we wanted to do was briefly cache API\nresponses which could easily breach that 4K limit.\nWith Dynamo the access control is via IAM which would be\nrelatively easy to manage and encryption at rest can\nbe easily configured.\n### Schemaless - MongoDb\nMongo was a better fit for our service, saving JSON\nrepresentations of API responses and resources.\nThe problem with Mongo is it's not AWS native so we'd\nhave to provision a server, manage the access controls,\nmaintenance and availability.\n### Relational - RDS MySQL / PostgreSQL\nRDS gives us the benefit of running a managed service so\nAWS are responsible for backups and patching minor\nversion.\nRelational databases give us the ability to do on-the-fly\nanalysis more easily.\nWe can store JSON as blob data although not ideal.\nIf we want to re-use the database instance as a shared\nresource across multiple services RDS is more capable.\nIt's not unlikely that a future development may require\ndjango.\nPostgreSQL seems to be the general direction of travel\nin GDS and is much more capable for a wider range of\nuse-cases where we don't know what we're building in the\nfuture.\n\n## Decision\n",Whilst none of the options were perfect we decided that\na PostgreSQL RDS was the best option given the\ninformation available to give us an infrastructure to\nsupport multiple tools and services.\n,"Use MongoDB as the persistent storage for audit results.\n\nMongoDB is a better fit for the service's needs than DynamoDB or RDS MySQL / PostgreSQL. MongoDB is a schemaless database, which means that it can store data in a flexible and dynamic way. This is ideal for storing audit results, which can be of varying sizes and formats. Additionally, MongoDB is a cloud-native database, which means that it is easy to provision and manage."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince Cloud Security Watch specifically aims to monitor for\nmisconfigurations in AWS accounts it does not make sense to\nmake the tool cloud agnositic.\n\n## Decision\n,Whilst we support the notion of writing cloud agnostic\ncode in general. In this instance it is not appropriate\nor desirable.\n,**Decision:** Keep Cloud Security Watch specific to AWS.\n\n**Rationale:**\n\nCloud Security Watch's primary focus on monitoring AWS accounts for misconfigurations justifies its exclusive focus on that platform. Expanding to other cloud providers would add complexity and potential compatibility issues without providing additional value for the tool's intended purpose.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe first architecture considered was using the PaaS.\nThis tool collects data about misconfigurations in a\ndatabase. Since the data held is quite sensitive we\nwanted to take all reasonable measures to protect that\ndata.\nThere are a lot of advantages to using the PaaS in that\nit limits the amount of work required for common\noperational tasks like deployment and monitoring.\n\n## Decision\n,At the present time we felt the Paas was not a viable\noption for this tool because of the following:\n* Shared tenancy RDS\n* Shared tenancy VPC\n* No ability to natively control ingress\n* No ability to control egress\n__Some of these issues are being addressed by the PaaS so\nwe may revisit this decision in future.__\n,Consider a private cloud solution that provides the same advantages as the PaaS without the security concerns.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWithin GDS product teams operate autonomously and tech-ops\nis relatively new. There is no central administration of\nAWS. There is a shared authentication model but it is not\nused everywhere.\nTeams are busy with their own mission work and don't have\ntime allocated to standardisation or security work.\nTo get things built in a client AWS account we need to:\n* negotiate time from the product team\n* provide them with the code to build the resources\n* code review with them to reassure them that it's OK to\nrun our code in their environments\n* repeat for each team we engage\nWe want to make it easy and low-risk for a team to engage\nwith us.\nIf there were centralised management of all accounts, the\nsensible way to do this would be by configuration the\nconfig service with a set of config rules administered\nand maintained centrally by Cyber Security.\nThese rules would be largely triggered by CloudTrail\nevents notifying us of compliance as soon as resouces were\ncreated or changed.\nHowever this approach would involve quite a large number\nof resources deployed into each client AWS account which\nmeans a longer and costlier engagement process.\nThe simplest way to do this is by provisioning an IAM role\nand associated policy. That way we can work from the\nprinciple of least privilege, granting us only the access\nwe need to run our audit making it low risk.\nBy providing the role and policy definition in a Terraform\nmodule we can also ensure that it requires little work by\nclient account holders.\nThe advantage of the IAM mechanism is that since we're\nquerying the configuration of the user account directly\nvia the API it is very easy to tell whether our service\nhas been tampered with - the only thing that can change\nis the definition of our policy which we can check.\nIn the config service model there are more moving parts\nwithin the client account which could be switched off,\naltered or intercepted.\n\n## Decision\n","Create a terraform module which creates an IAM role and\nassociated policy granting primarily read-only access\n(Get, List, Describe API calls) working from the\nprinciple of least privilege.\n","**Decision:** Implement a simplified configuration model using IAM roles and policies.\n\n**Rationale:**\n\n* **Low impact on client accounts:** Only an IAM role and policy need to be deployed, minimizing the engagement process.\n* **Principle of least privilege:** Grants only the necessary permissions for auditing.\n* **Easy to verify integrity:** The API directly queries the user account configuration, ensuring that no changes have been made without authorization.\n* **Address team constraints:** Avoids the need for centralized AWS management, saving time and cost."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Implement an Architectural Decision Record (ADR) process to document and track architectural decisions made on the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis is planned to be a multi-tenant system. There will be multiple CASA orgs in the system, so every case, case_contact, volunteer, supervisor, casa_admin etc must have a casa_org_id, because no one is allowed to belong to multiple CASAs. Volunteer, supervisor, and casa_admin are all roles for a ""User"" db object. In addition to those existing roles, we want to create a new kind of user: all_casa_admin. We need to handle the case of super users who have access to multiple casa_orgs, so they would be difficult to handle in the existing User table--with null handling around their casa_org_id field. We have used the built-in Devise ability to have multiple user tables, as recommended to us by our Rails expert Betsy. This is to prevent needing null handling around casa_id for User records since all_casa_admin users will not have casa_id populated.\nAdditionally, all_casa_admin users are currently intended to be allowed to create casa_admin users, but NOT to be able to see or edit any CASA data like volunteer assignments, cases, case_updates etc.\n\n## Decision\n","We are using two tables for users: ""user"" table for volunteers,supervisors, and casa_admin (all of which must have a casa_id). ""all_casa_admin"" for all_casa_admins, which will have no casa_id.\n","The system will have three separate User tables: User (for people associated with a single CASA organization), SuperUser (for people with access to multiple CASA organizations), and AllCasaAdmin (for people with access to all CASA organizations).\n\nThe User table will have a casa_org_id column to indicate which CASA organization the user is associated with. The SuperUser table will not have a casa_org_id column, as these users can access data from multiple CASA organizations. The AllCasaAdmin table will also not have a casa_org_id column, as these users can access data from all CASA organizations.\n\nAllCasaAdmin users will be able to create new User and SuperUser records, but they will not be able to see or edit any CASA data like volunteer assignments, cases, or case updates."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want it to be easy for people to join the organization, however we don't want random people signing up and spamming us. We want admin users to have control over who has accounts on the system. We don't have the capacity to handle this properly through the user interface right now.\n\n## Decision\n",We are going to disable Devise 'registerable' for the user model so that there will no longer be a public sign up option on the site. Creation of new accounts will be done on the backend.\n,Create a private invite mechanism to manage users. Each invite should contain a unique token that expires after a set period. Users can only join the organization by redeeming this token through the provided redemption mechanism.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to have an easy-to-use system for consistent styles that doesn't\ntake much tinkering. We propose using the `bootstrap` gem.\n\n## Decision\n,Pending\n,Use the `bootstrap` gem for consistent styling.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",Document architectural decisions in the Architectural Decision Record (ADR) template.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to document the architecture (precisely, the static model ie. diagrams) of the project.\n\n## Decision\n",We will use Simon Brown C4 model.\n,"**Decision:**\n\nDocument the project's static model using a combination of:\n\n* **Unified Modeling Language (UML) diagrams:** Use UML class diagrams, sequence diagrams, and use case diagrams to represent the structure, behavior, and interaction of the system components.\n* **Architecture Decision Record (ADR):** Create ADRs to capture design decisions and architectural constraints that impact the static model. ADRs can provide additional context and rationale for the design choices made.\n* **Design artifacts:** Supplement the diagrams and ADRs with implementation-specific artifacts such as module hierarchies, package diagrams, and database schemas. These artifacts can provide a granular view of the system's architecture."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\nTo enable different communication ways we need to apply a proper architectural style.\n\n## Decision\n,Adopt Hexagonal Architecture for project.\n,"**Decision:**\n\nImplement a **Publish-Subscribe Architectural Style**.\n\n**Rationale:**\n\n* **Decouples communication:** The publish-subscribe style decouples the sender (publisher) from the receiver (subscriber), eliminating tight coupling and allowing components to communicate asynchronously.\n* **Supports multiple communication patterns:** Publish-subscribe enables different communication patterns, such as one-to-many, many-to-many, and even one-to-one, making it suitable for the flexible communication needs of the SDARS application.\n* **Promotes scalability:** By using a central message broker or bus, the publish-subscribe style allows for the addition or removal of publishers and subscribers without affecting system stability, making it scalable and reliable.\n* **Simplifies communication:** This style simplifies the design and implementation of communication between components, as the messaging infrastructure handles message routing and delivery.\n* **Provides flexibility:** The publish-subscribe pattern allows for different communication strategies and data formats, making it adaptable to the evolving requirements of the SDARS application."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Contextadjecent\nWe need to adopt application architecture style adjecent to our architectural drivers\n\n## Decision\n,We will use ports&adapters architecture style\n,"Adopt a microservices architecture style.\n\n**Rationale:**\n\nA microservices architecture aligns with our architectural drivers as it:\n\n* **Modular:** Microservices are independent, loosely coupled components, allowing for easy maintenance and deployment.\n* **Scalable:** Each microservice can be scaled independently, providing flexibility in handling varying workloads.\n* **Fault-tolerant:** Microservices are designed to be resilient and can fail independently without affecting the entire system.\n* **Agile:** Microservices can be developed and deployed independently, facilitating rapid iteration and adaptability.\n* **Cloud-native:** Microservices are well-suited for cloud environments, leveraging scalability and fault-tolerance features."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","To record the architectural decisions made on this project, we will create an Architectural Decision Record (ADR). The ADR will capture the following information:\n\n* **Decision:** The decision that was made.\n* **Context:** The context in which the decision was made.\n* **Considerations:** The factors that were considered when making the decision.\n* **Consequences:** The expected consequences of the decision.\n* **Recommendation:** The recommended course of action.\n\nThe ADR will be stored in a central repository and will be accessible to all stakeholders."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to adopt system architecture style adjecent to our architectural drivers\n\n## Decision\n,We will use modular monolith architecture style\n,"**Decision:** Adopt a Microservices architecture style.\n\n**Rationale:**\n\n* **Loose Coupling and Independent Deployment:** Microservices allow for independent deployment and scaling of services, meeting the need for flexibility and scalability.\n* **Modularity and Reusability:** Services can be independently developed, tested, and deployed, fostering modularity and code reuse.\n* **Scalability and Fault Isolation:** Each microservice can be scaled independently, ensuring high availability and fault isolation.\n* **Adaptability and Continuous Delivery:** Microservices enable frequent updates and continuous delivery, aligning with the architectural drivers for agility and rapid response to change.\n* **Agility and Experimentation:** The modular nature of microservices supports experimentation with different technologies and approaches, fostering innovation and iteration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to keep extendable architecture in order to prepare for future changes,\nbecause we work in very flexible manner and requirements changes and evolve a lot.\n\n## Decision\n","In backend directory:\n- *libs* - technical parts of software - implementations of database connections etc. In the future will be moved to npm packages,\nbut we leave it now for easier and faster development to avoid unnecessary complexity.\nFolders in backend **src** directory are splitted by names of bounded contexts.\nEach BC, which follow CQRS manner, has following subdirectories:\n- *write-side* - keeps logical separated parts, which keep ubiquitous language + one domain inside each boundary\nEach bounded-context will follow directories in manner of Domain-Driven Design Layered Architecture proposed by Eric Evans.\n- *read-side* - projections from domain events of write-side\n",Adopt a microservices architecture.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCore developers are using JS maps files to keep all the selectors they need to interact with the BO theme. Here is an example of such a file:\n```\nexport default {\nmainDiv: '#order-view-page',\norderPaymentDetailsBtn: '.js-payment-details-btn',\norderPaymentFormAmountInput: '#order_payment_amount',\norderPaymentInvoiceSelect: '#order_payment_id_invoice',\nviewOrderPaymentsBlock: '#view_order_payments_block',\nprivateNoteToggleBtn: '.js-private-note-toggle-btn',\nprivateNoteBlock: '.js-private-note-block'\n}\n```\nQA team is keeping their own set of selectors to interact with during automated tests. Here is an example of such a file:\n```\nthis.documentTab = 'a#orderDocumentsTab';\nthis.documentsTableDiv = '#orderDocumentsTabContent';\nthis.documentsTableRow = `${this.documentsTableDiv} table tbody tr:nth-child(%ROW)`;\nthis.documentNumberLink = `${this.documentsTableRow} td:nth-child(3) a`;\nthis.documentName = `${this.documentsTableRow} td:nth-child(2)`;\n```\nSelectors should be mutualized so that when a dev changes a UI component or update a selector, the changes are reflected in the corresponding JS Map file and automated tests using this selector will keep working as intented without human intervention.\n\n## Decision\n","QA team and Core developers will populate and maintain their own set of selectors. There is no clear advantage to use a fusioned set of selectors.\nHere are the main arguments against this decision:\n* Devs use ES6, QA team use ES5. This means using a transpiler and adding libraries (babel).\n* There is no clear conventions in existing JS Map files (names and content architecture vary).\n* QA team need some selectors with modifiable input (with strings like %ROW or %COLUMN that must be replaced when used), which mean these types of selectors will be unusable for the Core dev team.\n* Not the same needs: QA team need navigation selectors, Core dev team need interactive selectors. There is only a few selectors in common.\n* Low risk: there is no evidence of any selector modification that caused automated tests to fail. When a test break, it's mostly because of a relatively big UI revamp, something that couldn't be fixed with just a selector modification anyway.\n","The decision is to create a single repository that will contain all the selectors. The core developers and the QA team will work together to maintain this repository. This will ensure that the same selectors are used by both teams, and that any changes to the selectors are reflected in both the JS Map files and the automated tests.\n\nHere are the benefits of this decision:\n\n* **Improved collaboration:** The core developers and the QA team will be able to work together more closely to maintain the selector repository. This will help to ensure that the selectors are accurate and up-to-date.\n* **Reduced risk of errors:** By having a single source of truth for the selectors, the risk of errors is reduced. This is because both teams will be using the same selectors, and any changes to the selectors will be tracked in the repository.\n* **Increased efficiency:** The use of a selector repository will increase the efficiency of both the core development and QA teams. The core developers will not need to spend time maintaining multiple sets of selectors, and the QA team will not need to spend time updating their automated tests when a selector changes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile the whole JavaScript community welcomes TypeScript easily, our project's JS is not typed at all. To improve the maintainability of the project and use the latest TC39 features, a good idea would be to use TypeScript.\nBenefits of using it for the project:\n- Detect bugs before pushing PRs. TypeScript users say that globally, it allows detecting around 15% of bugs that you would detect by testing.\n- Use latest features such as Optional Chaining, Tuples, and Records... really early.\n- Types are increasing the quality of the project because we would be able to detect dangerous changes, related bugs... If we use it on the PHP side, why don't we use types while using JS?\n- Vue 3 offers a new API: Composition API, this one is pretty easy to use with TypeScript as it's mainly functional programming instead of opinionated APIs of Vue, that would be a good move to preshot the Vue update in the BO.\n\n## Decision\n",Add the possibility to transpile ts files inside every js folder of the project with webpack.\n[Here is a POC](https://github.com/PrestaShop/PrestaShop/pull/23221) - basically using TypeScript on a small part of the PrestaShop Grid system.\n,"Yes, we should use TypeScript.\n\nTypeScript will improve the maintainability of the project and allow us to use the latest TC39 features. It will also help us to detect bugs before pushing PRs, and increase the quality of the project by detecting dangerous changes and related bugs. Additionally, using TypeScript will allow us to use the Composition API in Vue 3, which is easier to use with TypeScript."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe normal behavior of a module is to be self-contained, meaning that its code and all its dependencies are stored in the module's directory, including `vendor` directory and autoloader.\nWhen developing and building PrestaShop, native modules aren't downloaded from marketplace, they are required using composer.\nIn difference with the normal behavior described above, a module installed using composer will have its autoloader and dependencies merged into the core's and placed in the core's `vendor` directory.\nThis can prove problematic for native modules:\n- When the module is uninstalled, both its dependencies and autoloader are left behind in the core.\n- If the module is upgraded using the marketplace sources, the dependencies are now available twice: once in the core and once in the module.\nAlternate systems to avoid having modules leak code into the core have been proved unpractical:\n- Including modules via [Composer script handler](https://github.com/PrestaShop/composer-script-handler) or git clone is too slow.\n- Using submodules would be a step backwards.\n- Retrieving modules from the marketplace would be slow as well.\n\n## Decision\n","1. [The module managment system must be changed to be fully based on composer](https://github.com/PrestaShop/PrestaShop/issues/12586). This will require major changes in the marketplace and will have to be analyzed for feasibility.\n2. In the meantime, we will keep using composer to include native modules.\n3. Native modules must prepend their autoloader.\n4. To avoid leaving dependencies in the core, no composer dependencies are to be added to native modules until step 1 has been resolved.\n- Because of Decision No. 4, we may face technical difficulties in the future.\n",**Decision:**\n\nNative modules installed using Composer will no longer merge their autoloader and dependencies into the core's.\n\n**Benefits:**\n\n* Ensures that native modules are self-contained and can be easily removed or upgraded without affecting the core.\n* Eliminates potential dependency conflicts and cleanup issues when modules are uninstalled.\n\n**Alternatives Considered and Rejected:**\n\n* **Including modules via Composer script handler or git clone:** Too slow.\n* **Using submodules:** Not as practical as the proposed solution.\n* **Retrieving modules from the marketplace:** Also too slow.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSymfony provides a very useful tool called [Autowiring](https://symfony.com/doc/3.4/service_container/autowiring.html). It allows to magically bind classes and their dependencies as long as both are declared as services, and the dependencies are declared using their FQCN as service identifier.\nAdvantages:\n- Less boilerplate configuration code for every service as you don't have to manually bind dependencies manually.\nDisadvantages:\n- Dependencies must be declared using the FQCN instead of a service identifier like ""prestashop.core.foo.bar"".\n- Currently existing services would have to be aliased in order to have service names follow the required naming convention for autowiring. This would lead to confusion as to which service name use in code, and in case a module wanted to replace them, they would have to replace both.\n- Dependencies type-hinted as interfaces can have one and **only one** implementation for autowiring to work.\n\n## Decision\n",Activiting autoriwing is rejected for the 1.7 version.\n,**Decision:** Implement autowiring.\n\n**Rationale:**\n\nThe advantages of autowiring outweigh the disadvantages. Autowiring reduces boilerplate configuration code and simplifies service definition.\n\n**Mitigations:**\n\n* Address the disadvantages by:\n    * Using a consistent naming convention for services.\n    * Creating aliases for existing services that do not follow the autowiring naming convention.\n    * Ensuring that dependencies type-hinted as interfaces have only one implementation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur Production AWS RDS databases are not configured for high availablity which can support blue-green architectures and reduce risk. There exists a few ways to provide HA on RDS. We cover those in this document with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.\n### Distinguishing between HA and DR:\nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. Disaster Recovery (DR) provides a recovery solution across a geographically separated distance (multi-region) in the event of a disaster that causes an entire data center to fail.\nIn this ADR, we select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.\n\n## Decision\n","We will apply the RDS Multi AZ architecture to add high availability to our RDS production instances as it is the recommended best practice to adding HA to existing RDS instances.\n#### Consequences\n- Choosing Aurora DB would be a part of a larger product design decision, outside of this adr scope.\n- High Availability for failover protection improves the stability of the product framework. We did not address Disaster Recovery, which should also be a part of an overall scope of the product framework.\n- Multi AZ architecture should be fairly transparent to RDS applications, but the failover conditions and alerting mechanisms should be understood prior to implementation.\n",Implement Multi-AZ RDS deployments for all production databases.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order for modules to use JavaScript components from the Core, they need to import them using statements like:\n```\n// in order to use translatable type\nimport TranslatableInput from '../../../../../admin-dev/themes/new-theme/js/components/translatable-input';\n```\nThis path is not robust, makes CI/CD harder, and also is not compatible with some development environments using symlinks or containers.\n\n## Decision\n","We have decided about a system which resolves in 4 concepts:\n1. Reusable components in BO will be available globally through `window.prestashop` (name can still be modified in short term).\nAll PrestaShop components will be bundled together and made available in all pages using this mean. Each controller decides which components it chooses to initialize.\n2. Reusable components will be available as a namespace `window.prestashop.component`.\nThe namespace will contain classes like this `prestashop.component.SomeComponent`. If you want to get a new instance of `SomeComponent`, you call `new prestashop.component.SomeComponent(...params)`\n3. Reusable components will be available as initialized instances through `window.prestashop.instance`. These instances are initialized with default parameters by the `initComponents` function.\n4. A function `initComponents` available through `prestashop.component` is responsible for building `window.prestashop.instance`.\n### Why a namespace and a collection of instances\nSince you have access to both constructors and components, developers are free to choose how to initialize and control their components.\nIf you don't want to initialize a given component with default parameters, you can always call `new prestashop.component.SomeComponent(...myOwnParameters)`.\nIf you need to apply some mutation to an already initialized component, you just get the global instance: `prestashop.instance.someComponent.doSomething(...)`.\n",Migrate the JavaScript components to a central package:\n\n- Create a package with a scope @SyliusCore with a name @SyliusCore/component-library\n- Move all components into the new package\n- Replace the import paths with `import TranslatableInput from '@SyliusCore/component-library';`\n- Update CI/CD pipelines to build and publish the new package\n- Update all modules to use the new import paths
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOriginally, PrestaShop was made out mainly of static classes, with no dependency injection. To address that problem, it [was decided](https://build.prestashop.com/news/new-architecture-1-6-1-0/) that non namespaced code would be progressively refactored into a `Core` namespace, which would only contain code using with dependency injection. Furthermore, Core code wouldn't be allowed to depend directly on non namespaced classes, but it could to it indirectly by the means of `Adapter` classes that would act as a bridge between new and old code.\nThe ""no direct dependency between Core and Legacy"" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication. In some cases, the same service can have a legacy, adapter and core implementations, with subtle differences between each one. Furthermore, the constraints of backward compatibility further increase the difficulties to refactor code into Core, because the surface of the ""public API"" is larger.\n\n## Decision\n","The following decision applies to both `Core` and `PrestaShopBundle` classes (referred as to ""Core"" for shortness):\n1. **All new Core classes SHOULD be placed either in the `Core` or the `PrestaShopBundle` namespace**, following on the rules established previously.\n- New classes MUST NOT be added to the `Adapter` namespace, and SHOULD NOT be added to the legacy (root) namespace.\n2. **Core classes MAY depend on instances of legacy classes**, provided the following rules are respected:\n- Legacy classes MAY be used either as injected parameters or constructed within, but caution must be exerted when using legacy classes that produce side effects, have global state or don't guarantee internal consistency. In those cases, these classes SHOULD be accessed through dedicated services which enforce consistency.\n- Core classes MUST NOT call static methods on other classes, except for factory methods, stateless tool methods, or within services dedicated to encapsulate a static class.\n- Core classes MAY access to data provided by static classes or methods static classes by relying on dedicated services (Application services, Repositories, Data Providers...).\n3. **Core classes MUST NOT reimplement code found in legacy classes**, without deprecating the original method/class (and optionally, making it rely on the new implementation).\n4. **The Adapter namespace MUST be phased out** eventually:\n- Classes in the Adapter namespace MUST be copied to the Core namespace.\n- The original Adapter classes MUST be emptied out, made to extend the Core classes, and deprecated so that they can be fully removed in a following major.\n- Adapter services MUST be deprecated and copied into the core namespace as well.\n- Code MUST NOT depend on Adapter classes or services.\n",The decision should be to refactor all the code into the Core namespace and use dependency injection. This will reduce code complexity and duplication and make it easier to maintain the codebase.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To utilize an Architectural Decision Record (ADR) template to capture and document architectural decisions throughout the project lifecycle.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\n\n## Decision\n,"- Not in the project scope for either of the engagements\n- Good to have and can be incorporated at a later date\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\n",The decision was made to analyze the following subsections under Receipts:\n\n\n- Receipt Creation\n\n- Receipt Status\n\n- Receipt Line Items\n\n- Receipt Discounts
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe data extraction w/ transaction timestamp from Koshvani platform is a challenging task.\n- The temporal data directly available on the platform is not easy to extract\n- The validation of temporal data to ensure accuracy depends on factors\n- Districts/Treasuries which have updated their data\n- Data/Time of data upload vs extraction from departments\n## Decision Drivers\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\n\n## Decision\n","The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\n- Do a daily level data extraction from the platform\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\n- Select the accurate time period and mention methodology for the same\n- Ignore previous time period missed in this analysis.\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\n","**Architecture:**\n\n**Data Extraction Process:**\n\n* **Extraction Framework:** Develop a custom extraction framework that automates the data extraction process.\n* **Temporal Data Identification:** Utilize advanced data science techniques to identify temporal data from the platform's raw data feed.\n* **Data Validation:** Implement data validation mechanisms to ensure the accuracy of extracted data. This can include:\n    * Cross-referencing with other data sources\n    * Verifying temporal consistency\n    * Checking for missing or inconsistent values\n\n**Data Analysis and Reporting:**\n\n* **Time-Series Analysis:** Perform time-series analysis on extracted data to identify trends, seasonality, and anomalies.\n* **Data Visualization:** Create interactive data visualizations to communicate findings to stakeholders.\n* **Data Reporting:** Generate actionable reports based on data analysis, highlighting data quality issues and suggesting improvements."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\n## Decision Drivers\n* Use the data from differnt sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long term project with 3 year pipeline to create a sustainable archirtecture\n\n## Decision\n,* Use the data from differnt sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long term project with 3 year pipeline to create a sustainable archirtecture\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\n,"**Decision:** Utilize a multi-phased approach to gradually establish sustainable jobs for various sub-sections within the Expenditure category. Leverage data from multiple sections to impute accounting head information, perform data checks, and establish a resilient architecture with a three-year pipeline."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDuplicacy of data scraping from Koshvani platform.\nFor the following sections, the data has already been extracted from the `DDO-wise expenditure` section.\n- `Grant-wise (Revenue/Capital) expenditure`\n- `Division-wise expenditure`\n## Decision Drivers\nBoth these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\n\n## Decision\n",Both these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\nOnly the main pages of the aformentioned sections contain new information that will require extraction.\n,"Extract data for ""Grant-wise (Revenue/Capital) expenditure"" and ""Division-wise expenditure"" sections only if the granularity of data is different from the ""DDO-wise expenditure"" section."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\n\n## Decision\n,"- Structure of the Koshvani platform\n- Platfrom links do not reflect selection criteria\n- Automation job requirements for data scraping\n- Periodical jobs and access to new data\nUse [Selenium](https://www.selenium.dev/) to create robust, browser-based automation.\n","**Framework:** Python libraries (BeautifulSoup, Selenium, Pandas)\n\n**Rationale:**\n\n* **Robust scraping:** BeautifulSoup and Selenium offer powerful capabilities for extracting data from complex web pages, including Koshvani.\n* **Data manipulation and storage:** Pandas provides efficient tools for data wrangling, cleaning, and storage in machine-readable formats such as CSV or JSON.\n* **Open-source and flexible:** These libraries are widely used and supported by a large community, ensuring ease of implementation and maintenance.\n* **Cost-effective:** Python is free and open-source, making it a cost-effective solution for data scraping and storage."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\n**_Platform 1_**\n**Name** : Budget - Government of Uttar Pradesh\n**URL** : http://budget.up.nic.in/\n**Usage** : Analyse budget documents and understand accounting information available.\n**_Platform 2_**\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\n**URL** : http://koshvani.up.nic.in/\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\n## Decision Drivers\n- Create better underdstanding Uttar Pradesh fiscal data\n- Scrape data to be consumed in machine readable format\n\n## Decision\n",- Create better underdstanding Uttar Pradesh fiscal data\n- Scrape data to be consumed in machine readable format\nExplore both platforms to understand the open fiscal data available and techinical requirements.\n,"**Decision:** Use both platforms, **Budget - Government of Uttar Pradesh** and **Koshvani web**, to obtain data and information for the **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\n\n**Rationale:**\n\nBoth platforms provide valuable data for the projects' requirements:\n\n- **Budget - Government of Uttar Pradesh** offers in-depth budget documents and accounting information for analysis.\n\n- **Koshvani web** enables scraping of open budget data in a machine-readable format, facilitating further analysis and consumption.\n\nBy utilizing both platforms, the projects can access comprehensive and accessible data to create a better understanding of Uttar Pradesh's fiscal data."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\n## Decision Drivers\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n\n## Decision\n,The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\n,Extract the `Grant-wise expenditure` section from Koshvani.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\n- Expenditure\n- Receipts\n\n## Decision\n,Explore both sections to identify sub-sections from which data needs to be extracted and analysed.\n,Data in the Expenditure and Receipts sections of the Koshvani platform should be considered in scope for scoping and analysing the data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe GEWST project with HAQ: CRC has a platform development component, which includes creating exploratory dasboard and conducting analysis on an interactive dashboard.\n## Decision Drivers\n### Platform Development Requirements\nAccording to the plan, the data platform comprises of the following components:\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they’re released as part of the platform.\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\n- **Data Visualization Layer/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\n### Other Drivers\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\n- There is still no clear direction or plan what the tool will look like.\n- There might be numerous research routes and iterations before finalisation.\n- Data stories and analysis will play a big part in creating the final platform.\n\n## Decision\n","### Platform Development Requirements\nAccording to the plan, the data platform comprises of the following components:\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they’re released as part of the platform.\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\n- **Data Visualization Layer/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\n### Other Drivers\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\n- There is still no clear direction or plan what the tool will look like.\n- There might be numerous research routes and iterations before finalisation.\n- Data stories and analysis will play a big part in creating the final platform.\nYou can track the decision updates / inputs on the following [issue](https://github.com/CivicDataLab/up-fiscal-data-shiny/issues/1).\n","**Decision**: Build an in-house dashboard tool\n\n**Rationale**:\n\n* The Public Finance team's workload is already high, making it difficult for them to allocate resources to this project.\n* The dashboard tool's requirements are still unclear, and there may be multiple iterations before it is finalized.\n* Data stories and analysis will play a significant role in the final platform, and an in-house tool will allow for greater flexibility and customization.\n\n**Consequences**:\n\n* The in-house dashboard tool will require additional resources and time to develop.\n* The tool may not have the same level of functionality or support as a commercially available solution.\n* The team will need to invest in training and knowledge sharing to ensure that the tool is used effectively."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nShould the components in the PlanQK platform be splitted into individual repos?\n\n## Decision\n,"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\n### Positive Consequences <!-- optional -->\n* Recommended approach by [Martin Fowler](https://martinfowler.com/bliki/MonolithFirst.html)\n","The components in the PlanQK platform should **not** be splitted into individual repos.\n\n**Benefits**\n- **Easier code navigation and debugging**: A single repo allows developers to easily see the relationships between the components and how they interact with each other, making it easier to track down bugs and errors.\n- **Reduced duplication**: Keeping all the components in a single repo helps to reduce code duplication and ensure that all components are using the same code base.\n- **Improved collaboration**: A single repo fosters collaboration by allowing developers to easily share code changes and work together on new features and enhancements.\n\n**Drawbacks**\n- **Increased repo size**: A single repo can become quite large, which can make it slower to clone and work with.\n- **Potential conflicts**: With all the components in a single repo, there is a greater potential for code conflicts when multiple developers are working on different parts of the system.\n\n**Alternatives**\n- **Split the components into smaller repos**: This could help to reduce the size of the main repo and make it easier to work with, but it could also lead to increased code duplication and make it more difficult to collaborate on changes that span multiple repos.\n- **Use a monorepo tool**: A monorepo tool can help to manage a single repo by breaking it down into smaller, more manageable units, while still keeping all the code in a single location. This can provide the benefits of a single repo without the drawbacks.\n\n**Recommendation**\nGiven the benefits and drawbacks of the above options, the best decision is to **not** split the components in the PlanQK platform into individual repos. Instead, consider using a monorepo tool to help manage the single repo and make it easier to work with."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\nWe need references for the links.\n\n## Decision\n","Chosen option: ""[URIs]"", because UUIDs are generated and thus depend on the underlying database system.\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\n### Positive Consequences <!-- optional -->\n* We follow solid [W3C specification](https://www.w3.org/Addressing/URL/uri-spec.html)\n","Establish link references via a pattern identifier composed of 10-digit string (e.g., Alg1Pat2345678901)."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want a test framework that has good support for TypeScript and Node. Jest is\na fast testing framework with good resources for mocking.\n\n## Decision\n,We will use Jest as our testing framework.\n,Use Jest as the test framework.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be aware of CVEs (Common Vulnerabilities and Exposures) before they\nend up in production, and make sure to block deployments with known high\nseverity CVEs. Snyk allows us to scan our PRs for CVEs and fail builds if there\nare any vulnerabilities in the code we've written.\n\n## Decision\n","We will use Snyk to:\n- Run scans on PRs provide results and fail builds if any known high severity\nCVEs are found.\n- Run scans on main and fail builds on high severity CVEs, posting the results\nto the Snyk platform for monitoring.\n- Run nightly scans on the docker image and app dependencies.\n",**Decision:**\n\nIntegrate Snyk into our CI/CD pipeline to monitor for high severity CVEs and block deployments with known vulnerabilities.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, including React itself, and has become a standard.\n\n## Decision\n",We will enforce that everything supported by Prettier has its style enforced by\nit.\nWe will set up Git hooks to automatically run the formatter before committing.\nWe will set continuous integration up to reject commits that are not correctly\nformatted.\n,Use Prettier to enforce a consistent code style across all projects.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to build a web-based application in Node. Express is a minimal and\nflexible framework that sets up a lot basic configuration for us like routing\nand middleware, and makes it easy to incoporate the GOV UK design system UI\ncomponents.\n\n## Decision\n",We will use Express as the basis for this project.\n,**Decision:** Express.js will be used as the web framework for the application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to automate testing end-to-end user journeys through our\napplication. Cypress is an alternative to Selenium that runs in the browser to\ndo this for us, and is used across multiple projects at MOJ.\n\n## Decision\n",We will use Cypress for integration tests.\n,"**Decision:** Use Cypress for end-to-end user journey testing\n\n**Rationale:**\n\n* Cypress is an established tool used successfully in multiple projects at MOJ for end-to-end user journey testing.\n* Cypress runs entirely in the browser, providing a high degree of fidelity and stability compared to other testing frameworks that rely on simulating browser behavior.\n* Cypress offers robust and expressive testing capabilities, allowing for automated testing of complex user interactions and scenarios.\n* By leveraging Cypress, we can streamline and accelerate our testing process, improving the quality and reliability of our application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n\n## Decision\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,We should use ESLint with TypeScript plugins to lint our code and enforce consistency.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n\n## Decision\n",We will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n,**Decision:** We will use Stylelint to lint our CSS and SASS code.\n\n**Rationale:**\n\n* Stylelint is a popular CSS linter with support for SASS.\n* Stylelint is easily configurable for our purposes.\n* Linting the code is good practice to enforce consistency and catch errors automatically.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be confident about the code we write, and for it to be\nself-documenting as much as possible. TypeScript is a compiled language with\noptional typing. It's a superset of JavaScript, so is familiar to developers\nwho know JavaScript. It has wide editor support.\n\n## Decision\n",We will use TypeScript by default.\n,"We will use TypeScript for our codebase. TypeScript is a compiled language with optional typing. It's a superset of JavaScript, so is familiar to developers who know JavaScript. It has wide editor support. This will give us confidence in the code we write, and make it self-documenting as much as possible."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Background\nOur service frequently needs to take a user through a journey which involves asking them a series of questions over multiple pages, and then playing all the answers back to the user before finally submitting all the answers to the interventions service API.\nThis means that we need somewhere to store the user’s answers as they progress through the journey. For some journeys, the interventions service provides this storage. For example, in the journey of submitting a referral, the interventions service provides endpoints for creating and updating a draft referral.\nHowever, we have other journeys where the interventions service does not provide this storage. For example, assigning a referral to a caseworker, or cancelling a referral. For these journeys, the UI application needs to provide its own storage.\n### Why not have the interventions service store everything?\nWe _could_ make the interventions service provide storage for the data of every page of every journey in the UI. However, this would be very tied to the user journeys of this UI and less of a general-purpose interventions API. We should reserve interventions service storage for long journeys which the user might want to complete in multiple sittings.\n### The solution so far\nWe pass data from page A to page B to page C of a journey by making sure that the HTTP request for page C includes all of the data that was submitted on pages A and B. We do this either by:\n- accepting the data from previous pages as data encoded in the request URL’s query, and then passing it in a GET request to subsequent pages by placing the data from previous pages into the query of the URL that’s used for navigating to the next page\n- accepting the data from previous pages as `application/x-www-form-urlencoded` form data, and then passing it in a POST request to subsequent pages by placing `<input type=""hidden"">` fields on each page, replaying the data from the previous pages\n### The problem with this solution\nUsing a GET request limits the amount of data a user can submit on a page, since many clients and servers do not support URLs over 2000 bytes long.\nUsing a POST request means that we cannot redirect to different pages in the journey (for example, check your answers) based on the user’s input, since a redirect response cannot instruct the browser to make a POST request.\nAlso, the approach of embedding the data in the HTML is laborious. We have to make sure that every possible route through the pages in the journey preserves the data. This becomes particularly easy to get wrong when we have non-linear sequences of pages — for example, a link on the check your answers page that allows the user to edit a previous answer.\n### Other requirements for a solution\nThe solution must also:\n- make sure that a user is not able to access data entered by a different user\n- not prevent the user from performing the same journey multiple times concurrently — for example, they should be able to assign two different interventions at the same time in different browser tabs\n- preserve the data that the user entered on previous pages when they use the browser’s Back button\n- give us maximum flexibility in deciding how to meet [WCAG 2.1’s Success Criterion 2.2.1 Timing Adjustable](https://www.w3.org/TR/WCAG21/#timing-adjustable) — for example, by making sure that the data is kept for at least 20 hours before it expires\nIt would also be good if the solution could:\n- not introduce new dependencies to the service — for example a database\n- allow us to continue using the same kinds of coding patterns as we do when interacting with the interventions service API — creating and updating a resource\n- allow us to identify and clean up old data\n\n## Decision\n","We’ll use the UI application’s existing Redis server as our storage. It allows us to store essentially unlimited amounts of data.\nIn Redis, we’ll store “draft” objects. These are containers of arbitrary JSON data, along with:\n- a globally unique identifier\n- the unique identifier of the user who created the draft\n- timestamps of creation and last update\n- an identifier explaining what type of data the draft represents\nWe’ll:\n- provide a “CRUD” API for creating, fetching, updating and deleting these draft objects\n- include the draft’s ID in all URLs in a journey\n- allow these drafts to be created by a GET request, so that we can use `<a>` tags to link to journeys\n- prefer to use POSTs to pass data to a page instead of GET, so we don't have to worry about body size constraints\nThe aforementioned API will:\n- enforce access control — making sure that a user is only allowed to access drafts that they created\n- make sure that drafts are automatically removed after they are no longer accessed for a certain amount of time (for example, 1 day), using Redis’s expiry functionality, to make sure that drafts do not consume storage in Redis indefinitely\n### Alternatives\nWe might consider using the (Redis-backed) Express `session` object. However, this object expires after 2 hours of inactivity, which is insufficient for our needs. We don’t want to increase this timeout since there are security implications to increasing the amount of time that a user remains logged in to the service.\n",We will use a server-side HTTP session to store the user’s journey data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are a few reasons we might not want to put a new feature in front of users as soon as it's been merged into the `main` branch:\n- The API might not be up-to-date with the latest version of the UI (and vice versa) - because we're building the UI and API side of the service independently, there are times when the two may be out of sync: an endpoint may not yet be providing all the data we need; the backend functionality may not be finished at the time of writing the UI code.\n- We want to satisfy the Pact contracts between the two sides of the service but not use the new data structure until the UI has been updated.\n- The new functionality may need to be further tested (either with users by developers) and iterated upon before release.\n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes - this means we'd want to merge smaller chunks of work at a time, which might not be ready for users.\n- We want to test interactions between systems (e.g. the Community API) on the Development environment but not release these changes to the public.\n\n## Decision\n","Any features or behaviour that isn't ready to be interacted with by users will be placed behind a config-based feature flag, configured in `server/config.ts`, e.g. as below:\n```\nfeatures: {\npreviouslyApprovedActionPlans: get('FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS', 'false') === 'true',\n}\n```\nThis can then be turned on for each environment by adding the environment variable (e.g. `FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS`) to the intended environment.\nWe'll usually want to enable this for the development environment and test environment, possibly the pre-prod environment but not the production environment.\nBefore deploying the changes to the production environment, it's a good idea to double check the configuration is as expected e.g. by checking it's hidden in the pre-production environment.\nOnce the feature is ready to be interacted with by users, we'll remove the feature flag from the UI configuration.\n",**Defer UI code updates until API is ready to release.**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to make sure we're setting the correct HTTP headers for security e.g.\nContent Security Policy to protect against XSS attacks.\n[Helmet](https://helmetjs.github.io/) is a package that works well with Express\nto make it easy to set various HTTP headers for secutiy.\n\n## Decision\n,We'll use Helmet to set secure HTTP headers.\n,"**Decision:** Use Helmet with Express to set security-related HTTP headers.\n\n**Rationale:**\n\n* Helmet is a popular and well-maintained package that simplifies the process of setting security-related HTTP headers.\n* It integrates seamlessly with Express, our chosen web framework.\n* The headers suggested in the Context provide essential protection against XSS and other security vulnerabilities.\n\n**Impact:**\n\n* **Benefits:**\n    * Enhanced security against XSS and other attacks.\n    * Improved compliance with security best practices.\n    * Simplified header configuration and maintenance.\n* **Costs:**\n    * Additional HTTP headers may increase latency slightly.\n    * Requires some setup and configuration in the Express application.\n\n**Alternatives Considered:**\n\n* **Writing custom middleware:** This option would provide more flexibility but would be more time-consuming and error-prone.\n* **Using other header setting packages:** There are other similar packages available, but Helmet is a widely used and well-regarded option.\n\n**Decision Owner:** Lead Security Engineer"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael\nNygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement an Architectural Decision Record (ADR) process.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problems with the current approach\n- The `sub` attribute might be opaque and not usable between systems, e.g. when sending entitlements to other systems\n- Different login methods may have different formats for external user identifiers\n- Different login methods might use conflicting values for claims\n- E.g. `foo@csc.fi` via HAKA or CSC SSO (TODO: do we know of cases like this?)\n- A single user might have multiple external identifiers (TODO: do we know of cases like this?)\n\n## Decision\n","We'll add internal random user ids to REMS. This internal user id will\nbe the key that users are referred to within REMS. The user's external\nid will be stored in the attributes JSON blob.\nThis will allow us more flexibility in the future when identity\nrequirements and use cases change, and might also make all sorts of\nmigrations easier (since internal user ids don't need to be touched).\n","**Decision:** Use a standard claim for external user identifiers.\n\n**Rationale:**\n\nUsing a standard claim for external user identifiers addresses the following problems with the current approach:\n\n* **Opaque `sub` attribute:** The `sub` attribute is not always usable between systems, especially when sending entitlements to other systems. A standard claim would ensure that all systems can use the external user identifier in a consistent way.\n* **Different formats for external user identifiers:** Different login methods may have different formats for external user identifiers. A standard claim would ensure that all external user identifiers are in a consistent format, making it easier to compare and match them.\n* **Conflicting values for claims:** Different login methods might use conflicting values for claims. For example, two different login methods might use the same claim for two different pieces of information. A standard claim would prevent this by defining the specific meaning of each claim.\n* **Multiple external identifiers for a single user:** A single user might have multiple external identifiers, such as an email address and a social media username. A standard claim would allow systems to store and track multiple external identifiers for a single user, making it easier to manage user identities."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Problems with the current applicant UX\n\n## Decision\n,Here's a step-by-step plan for fixing these.\n1. (Problem A) Make save succeed even with validation errors. Show validation errors in yellow (instead of red). [#2766](https://github.com/CSCfi/rems/issues/2766)\n2. (Problem B) Run validations in the frontend after the user has stopped typing. [#2614](https://github.com/CSCfi/rems/issues/2614)\n3. (Problem B) Autosave in the background by replacing/patching the latest draft-saved event. If the latest event is not a draft-saved then create a new one. [#2767](https://github.com/CSCfi/rems/issues/2767)\n,**Decision:** Improve the applicant UX by implementing the following changes:\n\n* Redesign the application form to be more user-friendly and intuitive.\n* Add clear instructions and examples to help applicants complete the form correctly.\n* Implement a progress tracker to show applicants where they are in the application process.\n* Provide applicants with regular updates on the status of their application.\n* Create a dedicated support channel for applicants to ask questions and get help with the application process.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFlask turned out to be poorly designed piece of software which relays on too\nmuch magic like manipulations of global objects like `g`.\nSeems like we will also decide to use relational database.\n\n## Decision\n,"We will switch to Django. It's not only well written server but it has also\n""batteries included"" like a good ORM layer. And some other features like\nmiddlewares will simplify things.\n",Migrate from Flask to a more robust web framework with a cleaner design and better encapsulation of state.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOpen Lobby Server will be written in API First design. Frontend applications\nwill be based on it's API.\n\n## Decision\n,"We will use GraphQL as API standard. From other options like REST and Graph API\nit's the most client friendly approach. GraphQL query language is easy to use,\nvalidated and self documenting.\nGraphQL allows clients to get everything they need in one request without any\noverhead of not needed data. That is very important for mobile frontends.\nGraphiQL tool also provides easy way for developers to inspect and try API. So\nit's easy to adopt by frontend applications developers or other API users.\n","The Open Lobby Server will follow an API First design pattern, which prioritizes the development and documentation of the API before the user interface. This approach provides several benefits:\n\n- **Early API definition:** By focusing on the API first, the team can establish a clear and stable contract between the frontend and backend components, reducing miscommunications and rework later in the development process.\n\n- **Improved API quality:** The API-first approach encourages rigorous API design and documentation, ensuring that the API is well-defined, consistent, and easy to use.\n\n- **Frontend flexibility:** By decoupling the frontend from the backend, the API-first approach allows the frontend developers to build and iterate on the frontend independently, without being constrained by the backend implementation.\n\n- **Enhanced developer collaboration:** By involving both frontend and backend developers in the API design process, the API-first approach fosters collaboration and knowledge sharing, leading to a more cohesive and maintainable solution.\n\n- **Improved maintainability:** The API-first approach promotes a separation of concerns, making it easier to maintain and evolve the API and frontend independently over time."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to add relational database.\n\n## Decision\n,We will use PostgreSQL. It's a mature database with handy features like JSON\nand hstore data types. It's fully ACID compliant including schema changes. It\nhas very good support in Django's ORM.\nAnother popular option is MySQL/MariaDB. But because it has a major bug `#28727`\n(10 years since it has been reported and it's still not fixed) breaking ACID in\nschema changes it can't be used for any serious project.\n,**Decision:** Use PostgreSQL as the relational database management system (RDBMS) for the application.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose webserver.\n\n## Decision\n,We will use Flask. Server should be simple - pretty much just with a GraphQL\nendpoint and GraphiQL.\n,"**Decision:** Use Nginx\n\n**Rationale:**\n\n* Nginx is known for its high performance and low memory consumption.\n* It is easy to configure and maintain.\n* It supports a wide range of features, including load balancing, caching, and SSL termination.\n* Nginx has a strong community and support ecosystem."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make API friendly for clients and design pagination.\n\n## Decision\n,We will adopt GraphQL Relay specification. It solves pagination so we don't\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\nIt has a way to define inputs in mutations.\nGraphene lib has good support for creating API following Relay specifications.\n,"To make the API friendly for clients and design pagination, we can use a RESTful API design with JSON as the data format. The API should provide a consistent and well-documented interface that is easy to use and understand.\n\nFor pagination, we can use a standard approach such as offset-based pagination or cursor-based pagination. Offset-based pagination uses a limit and offset parameter to specify the range of results to return, while cursor-based pagination uses a cursor parameter to specify the point in the data set from which to start returning results.\n\nWe can also consider using a pagination library to simplify the process of implementing pagination in our API. There are a number of open source pagination libraries available, such as django-rest-framework-pagination for Django and will_paginate for Ruby on Rails."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to simplify code reviews and unify code style.\n\n## Decision\n,Use Black code formatter: https://github.com/ambv/black\n,"We will use a static code analyzer to automatically check for coding style and quality issues. This decision was made after considering the following factors: Cost: Using a static code analyzer is relatively inexpensive compared to manual code reviews and external code quality audits. Effectiveness: Static code analyzers can automatically detect a wide range of coding style and quality issues, reducing the time and effort required for manual code reviews. Speed: Static code analyzers can analyze code quickly and efficiently, allowing for faster code reviews and reducing the time it takes to identify and fix code issues."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose main programming language for this project.\n\n## Decision\n,I'm now fluent in writing Python and JavaScript. Both languages are widely used\nthese days and may attract developers to join this project.\nPython is more mature with more stable libs for writing server side code and\ntesting. JS may have better libraries for writing GraphQL APIs because\nGraphQL originated in JS ecosystem.\nI'm doing this in free time and for fun. I want to focus on project's features\nand not to waste a time with poorly designed language lacking mature and\nstable libraries. So for obvious reasons I'm choosing Python.\n,**Decision**: Use Kotlin as the main programming language for the project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a database with fulltext search capable of searching in various\nlanguages especially in Czech.\n\n## Decision\n,"We will use Elasticsearch. It's well known database with great fulltext search\ncapabilities based on Apache Lucene. It has also aggregations, highlighting of\nresults, and many other useful features.\nWe will use it as database for all data so we have just one database in the\nsystem.\n","Use Elasticsearch, an open-source, distributed, full-text search and analytics engine with support for various languages, including Czech."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose framework for tests.\n\n## Decision\n,We will use Pytest. It's much more Pythonic and simplier to use than Unittest\nfrom standrd library.\n,"**Decision:** Use Pytest\n\n**Justification:**\n\n* Pytest is a popular and widely used framework for Python testing.\n* It provides a simple and intuitive syntax that makes it easy to write and maintain tests.\n* Pytest offers a rich set of features, including fixtures, parametrization, and reporting.\n* It integrates well with other popular Python testing tools, such as Selenium and Django.\n* Pytest has a large and active community, which provides support and resources for users."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNumber of document types which does not use Elasticsearch's fulltext\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\nper index which means management of many indices.\n\n## Decision\n,We will add relational database as primary database. Elasticsearch will be used\nfor denormalized reports and related data intended for fulltext search.\n,Use Elasticsearch's `_all` fields for full-text search.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOpen Lobby must have open stable API.\n\n## Decision\n,"Open Lobby will be server just with an API based on API First design. Frontend\nwill be written as separate application (web based, mobile, ...). This will\nensure that there is a working API for anything that frontend application(s)\nwill do.\n","Open Lobby will have a stable and well-documented API that will allow other systems to interact with it. The API will be designed to be flexible and extensible, so that it can be used to support a variety of use cases. It will also be documented in a way that makes it easy for developers to understand and use."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in\nthis article:\nhttp://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",Create an Architectural Decision Record template.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need an authentication mechanism for users. It must be secure and\nfrontend application independent.\n\n## Decision\n,We will use OpenID Connect. Open Lobby Server will provide all the hard stuff\nfor a frontend applications. Ideally over the GraphQL API.\n,"**Use OpenID Connect (OIDC)**\n\n**Benefits**\n\n* OIDC is an industry-standard authentication protocol that is secure and well-supported.\n* It is independent of the frontend application, so it can be used with any type of application.\n* It provides a single sign-on (SSO) mechanism, so users can access multiple applications with a single set of credentials.\n\n**Alternatives Considered**\n\n* **Custom authentication mechanism:** This would require developing a custom authentication protocol and implementation, which would be time-consuming and error-prone.\n* **Third-party authentication service:** This would be less secure than using OIDC, as the third-party service would have access to user credentials.\n\n**Rationale**\n\nOIDC is the most secure and flexible authentication mechanism that meets the requirements. It is a standard protocol that is supported by a wide range of applications and services. It also provides a SSO mechanism, which makes it easy for users to access multiple applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA web framework is needed in order to provide a level of abstraction over low\nlevel language features. Effort spent solving non-domain specific problems\nshould be kept to an absolute minimum.\n\n## Decision\n,"We will use the [Express](http://expressjs.com/) web framework. This will\nprovide a lot of well\n[documented](http://expressjs.com/en/resources/books-blogs.html) features and\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\nframework.\nExpress is one of the most popular web frameworks available for NodeJS and as\nsuch has been well tested and is deployed across many\n[organisations](http://expressjs.com/en/resources/companies-using-express.html).\n",Use Spring Framework
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nInformation about pharmacies is required by the application in order to display\nto users. A Docker image of pharmacies\n([pharmacy-db](https://hub.docker.com/r/nhsuk/pharmacy-db/)) running a MongoDB\ninstance has been created for use by other applications.\nNodeJS uses a single threaded event loop architecture and as such works best\nwhen the work it is doing is non-CPU intensive. Searching through datasets is\npotentially CPU intensive.\n\n## Decision\n,We have decided to use the existing Docker image rather than spend effort\nacquiring the data again.\n,The application should use the pharmacy-db Docker image to access pharmacy information.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to know what the application is doing in a more light weight way than\nscraping logs. We need to be able to monitor KPIs of the application in order\nto understand the health of the application. This will allow us to react and\npotentially pro-actively initiate measures as to ensure the application's\nhealth if sound. Ultimately providing a better service for our users.\n\n## Decision\n,We will use Prometheus to monitor and alert on the state of the application.\n,"Implement application monitoring using metrics (e.g., Prometheus) and tracing (e.g., Jaeger)."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe primary (only) consuming application for this API needs to show both open\nand nearby services on separate pages (and more of them). Previously the\napplication had shown a mix of open and nearby services within a\nsingle page.\nHaving the API so closely aligned to the needs of the consumer is not ideal.\nThere is scope to increase the flexibility of the API along with increasing the\nease with which it can be used both by the current and future consumers.\n\n## Decision\n,The decision is to add a new endpoint i.e. `/open` alongside the current\n`/nearby` endpoint. The former endpoint will return only services that are open\nwhere the latter will be refactored to return only services that are nearby\nregardless of their opening state.\n,"**Decision:**\n\nIntroduce a new API endpoint that returns only nearby services, in addition to the existing endpoint that returns both open and nearby services.\n\n**Rationale:**\n\n* Decouples the API from the specific needs of the primary (only) consuming application.\n* Makes the API more flexible and easier to use by both current and future consumers.\n* Supports the application's requirement for separate pages for open and nearby services."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nElasticsearch is configured as a cluster for reliability and failover, and\nprovides a single point for data updates. MongoDB runs as a single instance and\nis not clustered.\n\n## Decision\n",nearby-services-api will consume data from Elasticsearch rather than MongoDB.\n,"MongoDB should be configured as a cluster for reliability and failover. This will ensure that data is always available, even if one node fails."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",Record architectural decisions in a centralized repository.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\n\n## Decision\n,"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\n","Use a templating engine to generate the JSON configurations at build time, then deploy the generated JSON files as configuration artifacts to Elasticsearch."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nElasticsearch timeouts can cause 'orphaned' indexes from remaining in the cluster after an update.\nRunning two updaters simultaneously, as happens for pull requests that last several days, can leave\ntwo indexes with the same alias.\nElasticsearch watches are run to export data to Prometheus throughout the day.\nTwo new date stamped 'monitor' and 'watch' indexes are created every day to track the status of the watch job.\n\n## Decision\n","Orphaned and indexes with duplicate aliases will be removed as part of the update.\nThe indexes created every day by the watch will also be deleted within the elastic-search updater.\nThis could be done by an Elasticsearch watch, but a license is required to use this functionality.\nTo avoid adding more infrastructure components to the system rather than having a standalone service, the cleanup will be\nperformed in the elastic-search updater. This can be moved into another service in future if required.\n","**Decision**: **Implement a mechanism to clean up 'orphaned' indexes and ensure unique aliases**\n\n**Rationale**:\n\n* Orphaned indexes and duplicate aliases can lead to confusion and data integrity issues.\n* By implementing a cleanup mechanism, we can ensure the cluster is maintained in a clean and consistent state.\n* This will prevent orphaned indexes from consuming resources and avoid potential conflicts when creating new indexes with the same alias."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany of the vulnerabilities identified by snyk have no fixes, and can only be ignored in the synk config file for a set number of days.\nAfter the number of days are up, the vulnerabilities will fail the build and prevent merge on new PRs, with the only option to ignore the\nvulnerabilities again.\nMany of the snyk failures are for build tools that are not exposed when applications are deployed.\nThe overhead of ignoring many snyk vulnerabilities to get builds passing outweighs the potential risks.\n\n## Decision\n","Snyk will be removed and replaced with a combination of Greenkeeper and Github vulnerability subscriptions.\nGreenkeeper will ensure the dependencies are up to date, and the subscription notifications will provide visibility of serious vulnerabilities.\nAn automated replacement will be put in place in the near future.\n","**Decision:** Ignore the identified vulnerabilities that have no fixes and are not exposed when applications are deployed.\n\n**Justification:**\n\n* Ignoring the vulnerabilities reduces the overhead of managing Snyk failures and prevents unnecessary delays in merging PRs.\n* The vulnerabilities are not exposed to users or the application's functional capabilities, minimizing the potential risks.\n* The decision is based on the trade-off between overhead and risk, with the reduced overhead being the primary consideration.\n\n**Consequences:**\n\n* Some vulnerabilities may remain unresolved in the codebase.\n* The decision should be monitored and reviewed regularly to assess if the risk tolerance remains acceptable.\n* The team should consider alternative measures to address the underlying security concerns."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To implement an Architectural Decision Record (ADR) process using a dedicated tool or template to track and document architectural decisions made in the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need API for the Zoo to expose issues, analytics data, etc.\n\n## Decision\n","We will create API by GraphQL specification. Pagination will be done according\nto Relay server specification. We considered to build REST API, but we decided\nthat for fresh new APIs is the GraphQL right choice.\n","**Decision:** Create a new API for the Zoo.\n\n**Rationale:**\n\n* The existing API is not designed to handle the specific needs of the Zoo.\n* A new API can be tailored to meet the specific requirements of the Zoo, such as exposing issues, analytics data, etc.\n* A new API can be designed to be more scalable, reliable, and secure than the existing API.\n* A new API can be developed more quickly and easily than modifying the existing API.\n\n**Consequences:**\n\n* The Zoo will need to develop and maintain a new API.\n* The Zoo will need to migrate its existing data to the new API.\n* The Zoo will need to update its documentation to reflect the new API."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to use code formatter to standardize code look so we stop bothering about it\nin code reviews and focus on how code actually works.\n\n## Decision\n,We will use Black: https://github.com/ambv/black\n,"**Decision:** Use Prettier.\n\n**Rationale:**\n\nPrettier is a widely used and highly configurable code formatter. It supports a variety of languages, including JavaScript, TypeScript, and CSS, which are used in this project. Prettier's configuration options allow the team to customize the code style to meet their specific requirements. By using Prettier, the team can save time and reduce the potential for human error when formatting code. Prettier also has a large community of users and contributors, which ensures ongoing support and development."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe were using [Semantic UI](https://semantic-ui.com/), as it's an open source and robust framework.\nIt has a wide range of components, it's easily customizable and it uses modern\napproaches. It's really well documented, although usually documentation is not even\nneeded as it's really intuitive to work with it.\nUnfortunately, the project is not as maintained as before, and the development of new components\nis stale.\n[Fomantic UI](https://fomantic-ui.com/) picked up where Semantic UI left, and offers a drop-in replacement,\nwith a higher quantity of components, and the development can be considered active.\n\n## Decision\n","We will use [Fomantic UI](https://fomantic-ui.com/), the community fork of `Semantic UI`. The\nsupport on this project is fast and on point, and the roadmap aligns nicely with our development,\nas the maintainers are planning to offer first party support for Vue components, and ditch the\njQuery dependency.\n",**Decision:** Migrate to Fomantic UI\n\n**Rationale:**\n\n* Fomantic UI is a drop-in replacement for Semantic UI.\n* Fomantic UI has a higher quantity of components.\n* Fomantic UI is actively developed.\n\n**Consequences:**\n\n* The project will benefit from the increased functionality and active development of Fomantic UI.\n* The migration process should be relatively straightforward.\n* There may be some minor compatibility issues that need to be addressed during the migration.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe always wanted to have a consistent commit message styling, we used coala's CommitBear to help with\nthis task but besides just a bit more clarity in the commit history, it doesn't bring that much value.\nNow that we'll be getting rid of coala in favour of pre-commit hooks, we need an alternative solution,\nand we can use this change to also step up our game regarding commit messages.\n\n## Decision\n","We will use the [conventional commits](https://www.conventionalcommits.org) convention for writing our commit messages\nfrom now on. Besides forcing us to keep a consistent commit message styling, it will also bring much\nmore value, like:\n* Automatically generating CHANGELOGs\n* Automatically determining a semantic version bump (based on the types of commits landed)\n* Communicating the nature of changes to teammates, the public, and other stakeholders\n* Having different CI steps based on the nature of the commits added to the MR\n* Having insights about how much time/effort is spent on the different areas of the development\n","**Decision:** Implement a pre-commit hook that enforces a consistent commit message format.\n\n**Rationale:**\n\n* Pre-commit hooks allow for more control and customization compared to CommitBear.\n* A consistent commit message format improves the readability and understandability of the commit history.\n* Enforcing a consistent format through a pre-commit hook ensures that all commit messages adhere to the established guidelines.\n* By leveraging the opportunity to upgrade commit messaging practices, the team can enhance the overall quality and value of the commit history."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need dir to save ADRs into.\n\n## Decision\n,"We will save ADRs into top level `adr` dir. Another option was to put them into `docs/adr`\nalong with Sphinx docs, but they might be bit hidden there.\n", ADRs should be saved into `docs/adr` directory.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Implement an Architectural Decision Record (ADR) system to document and track architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe were using [Material Design Lite](https://getmdl.io/) as our front-end CSS framework.\nThe selection of offered components is quite limited and most of them just include\nthe basic logic. We have to dedicate a big amount of time writing front-end code and it's\nhard to compose the provided components as they are mainly meant to be used in a single\nway because of the strong Material design guidelines.\nWe aim to focus on logic, not on developing front-end components. We need a solution\nthat can offer a high number of already built components. The components should be\neasy to configure and to compose together as our UI sometimes needs components that\nare not contemplated on the common guidelines.\n\n## Decision\n","We will use [Semantic UI](https://semantic-ui.com/), as it's an open source and robust framework.\nIt has a wide range of components, it's easily customizable and it uses modern\napproaches. It's really well documented, although usually documentation is not even\nneeded as it's really intuitive to work with it.\n","**Decision:** Migrate to a more modular and versatile front-end CSS framework, such as [Modular UI](https://modularui.js.org/).\n\n**Rationale:**\n\n* **Wide range of components:** Modular UI provides a comprehensive library of components that covers a wide range of use cases. This reduces the need for custom front-end development.\n* **Modular design:** The components are designed to be modular and composable, allowing for flexible and complex UI layouts.\n* **Easy configuration:** Modular UI provides extensive configuration options for its components, enabling customization to meet specific design requirements.\n* **Focus on logic:** By using Modular UI, the development team can focus on business logic and functionality, rather than spending time on front-end development.\n* **Improved maintainability:** Modular UI's consistent naming conventions and well-structured codebase enhance code readability and maintenance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn PFE, workflows have a complex logic of programmatic workflow selection. [ADR 18](adr-18.md) has documented the original context. This ADR is a partial superseding of the decision on how to select the workflow when navigating to `/classify`\n\n## Decision\n","We will continue to use the routing behavior defined by [ADR 18](adr-18.md)\n### `/classify` behaviour\nWe will not programmatically select a workflow except for one case: when there is only one active workflow. For all other cases, we have a UI prompt for the volunteer to manually select which workflow they wish to contribute to.\n### Error handling\nCases when the workflow is not available are:\n- The workflow does not actually exist, so this will 404.\n- The workflow exists, but is in an inactive state. The activeness state effectively functions as a permissions mechanism since users with the project owner, collaborator, or expert role or Zooniverse admins can still request and load inactive workflows. Users with the correct role should be able to load an inactive workflow with a visual indication in the UI it is inactive. All other users will receive a 404.\n- The workflow exists and is active, however, the project uses workflow assignment and the workflow has not been assigned to the volunteer yet. The classify page should load, the classifier itself doesn't, and the workflow selection prompt is rendered for the volunteer to choose between the workflows they have been assigned.\n",The workflow for `/classify` will use the new workflow ID that encodes the original `<theme>-<distribution>-default` pattern.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\n\n## Decision\n,"### Changes from the TESS LCV\nThe TESS LCV is built only for their use case and is not configurable. It is hard coded to expect brush stroke annotation for the classifications, zoom only in the x-axis direction, and for only one data series. We will build a generally configurable plots so that other projects can have the flexibility they need. The new plot viewers will be modular so that it can be placed into a composite, complex subject viewer as needed.\nPreviously, the TESS LCV was built using d3.js, however, mixing d3 and react can be dangerous. The decision at the time was to use d3 because of the custom requirements needed for the TESS LCV and the react + d3 libraries were too opinionated to be used for our needs. The library d3 is also difficult to write tests for because of its chaining API. For this reason, the original TESS LCV is largely untested.\nSince then, a library called [vx](https://vx-demo.now.sh/) containing reusable low-level visualization react component that uses the d3 math utilities, has become more mature to start using. This fits our needs to have the DOM solely rendered by React, but still has the usefulness of a mature library like d3 to do calculations as needed. The new plots will be built using vx.\nThe long term goal is to swap the TESS LCV over to the new `ScatterPlotViewer`, however, this means adding support for brush annotations which will be investigated at a later time.\n### ScatterPlotViewer\nThe scatter plot will be built with support to configure:\n- Multiple data series\n- Customizable data series colors to represent information as needed\n- Pan and zoom in both axes directions or constrainable\n- Axis inversion\n- Customizable axis label\n- Customizable number of axis ticks and direction\n- Customizable margin and padding for the plot area\n### BarChartViewer\nA bar chart plot will be built with support to configure:\n- Multiple data series\n- Customizable data series colors to represent information as needed\n- Labels for axes and for individual bars\n### VariableStarViewer\nThe scatter plot and the bar chart together along with the `SingleImageViewer` and a few additional controls will be a complex composite viewer built as the `VariableStarViewer`. The `VariableStarViewer` will have its own control bar that has a toggle for axis inversion, period, data series visibility, and phase focus. Each scatter plot will be individually pan and zoomable.\n### DataImageViewer\n_Note: Naming still TBD_\nThis will be a complex composite consisting of a scatter plot and a single image. We may want to support up to N images, but this is still TBD. The initial build will be just the single scatter plot and single image.\n### Future plots\nThere may be requests to build more plot types like a line plot or map plot that renders GeoJSON. We will continue to evaluate our usage of vx at that time and ideally will continue to use it.\n",Implement a reusable module in the `subjects` package that can parse and manage the display of JSON data as a subject for classification. This module should be able to be easily integrated into other research projects that have JSON data to classify.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\nSee the [InVision document for the Pages Viewer](https://projects.invisionapp.com/d/main#/console/12924056/393421254/preview) and Issue #1142.\n\n## Decision\n","1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https://github.com/zooniverse/front-end-monorepo/issues/1262).\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement/future feature.\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement/future feature.\n","Use the name ""multiFrame Viewer"" for the component to display multi-frame subjects in the classification interface."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we first started the rebuild, @rogerhutchings set it up as a monorepo. The idea being that there are benefits to separating our code between multiple apps and packages:\n- We can reuse code more easily\n- Error boundaries are built-in (if we break one thing, we don't necessarily break everything)\n- To make changes to one thing, we don't need to recompile and rebuild _everything_ in development, at least\nWorking across multiple repos can be pretty tedious: adding a feature which requires changes to more than one package requires multiple pull requests to different repos, for example, and publishing updated packages in the correct order. So Roger set this project up as a monorepo using Lerna. This started out fine, but we hit some issues in the development of Planet Hunters TESS:\n- `styled-components` wasn't collect styling from symlinked dependencies, causing a FOUC on initial render: https://github.com/zooniverse/front-end-monorepo/issues/327\n- Adding the `@zooniverse/standard` package broke the build because Lerna wasn't correctly linking it up: https://github.com/zooniverse/front-end-monorepo/pull/958\n- `npm audit` doesn't work with symlinked dependencies\nIn the course of investigating these issues, we learned:\n- Symlinking packages has side effects that need to be worked around: https://github.com/styled-components/styled-components/issues/2322\n- Lerna is aimed at publishing multiple packages from a repository, **nothing more**, including managing local packages for development (https://github.com/lerna/lerna/issues/1243#issuecomment-401396850). We're currently using it solely for local development. 😢 It's also reasonable to suppose that future major versions of Lerna _could_ modify the API to further that aim, and deprecate features that we're currently (incorrectly) relying on.\nSo @rogerhutchings started investigating alternatives to a pure Lerna setup.\n- [`bazel`](https://bazel.build/), [`buck`](https://buck.build/), [`pants`](https://www.pantsbuild.org/index.html), [`please`](https://please.build/) etc support multiple langauges, but not Javascript out of the box, and we'd need some additional tooling to get them to work in the way we want.\n- [`rushjs`](https://rushjs.io/) looked promising, but at time of writing is limited to `npm` v.4.5.0, which is pretty old.\n- [`yarn`](https://yarnpkg.com/) has a monorepo feature called Workspaces, and has some other features like `audit`, hoisting etc. Lerna will use `yarn` workspaces under the hood where available.\n\n## Decision\n","We're going to switch the monorepo tooling from Lerna to Yarn and Lerna. Yarn will be used for managing dependencies across the monorepo and running commands against packages. Lerna will be used to run commands across multiple packages, which is what it's designed to do.\n","We should use the `yarn` workspaces feature in place of Lerna. This will address the issues we have been experiencing with Lerna, such as the FOUC with `styled-components`, the problems with `@zooniverse/standard`, and the inability of `npm audit` to work with symlinked dependencies. Additionally, `yarn` workspaces is a more modern approach to monorepo management and has other helpful features like `audit` and hoisting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFuture projects using the `front-end-monorepo` will require a video player. Some projects may simply need a video to play. Some projects will require users to interact with a video file; such as make annotations on the video to mark locations or item sizes. The video player must be highly customizable, able to be used in a React code base project and support all major browsers.\n\n## Decision\n","We will implement `react-player` [Github](https://github.com/CookPete/react-player).\nDemo Page: [Demo](https://cookpete.com/react-player/)\nThe lengthy list of attributes (props) and Callback Props makes `react-player` a great choice for developers who need customization. Many of these attributes are simply booleans. For example, to play a video, pass in `playing={boolean}`. Full list of [Props](https://github.com/CookPete/react-player#props)\nCompared to the native HTML video player, `react-player` makes it easy to customize styling so the player looks the same across different browsers.\nMaking `react-player` responsive is easy by targeting pre-defined classNames.\n[Responsiveness](https://github.com/cookpete/react-player#responsive-player)\nOne of the biggest wins of using `react-player` is ease-of-use. This will reduce developer time and reduce the amount of custom methods in our code base.\n### Media\n`react-player` is a React component for playing a variety of URLs, including file paths, YouTube, Facebook, Twitch, SoundCloud, Streamable, Vimeo, Wistia, Mixcloud, and DailyMotion. [Supported Media](https://github.com/CookPete/react-player#supported-media)\nAlthough `react-player` supports a number of externally hosted videos, we will only support video files uploaded to the Zooniverse platform.\nFile types will be validated to ensure the files are mp4.\n### Customization\nA wide range of [Props](https://github.com/CookPete/react-player#props) can be passed in to control playback and react to events.\nPlease read through the [Github](https://github.com/CookPete/react-player) for a full list of features.\n",Use the open-source React video player library [React-Player](https://github.com/CookPete/react-player).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPanoptes-Front-End's drawing tools largely developed from previous custom projects and/or were added one at a time to support a specific project. Because of this, several inconsistencies have been discovered in downsteam analysis and aggregation. To explain the inconsistencies, a few definitions are needed:\n- RHC: A right handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the +y axis with angle=0 along the +x axis\n- LHC: A left handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the -y axis with angle=0 along the +x axis\n- Domain: The range of values a number can take [ or ] is inclusive, ( or ) is exclusive.\n- Upper origin: The point 0, 0 is in the upper left of the plot\n- Lower origin: The point 0, 0 is in the lower left of the plot\nThe inconsistencies comprise of:\n- The browser SVG coordinate systems use _RHC_ with an _upper origin_ resulting in positive angles rotating clockwise. Most plotting software (R, Python, Matlab) are _RHC_ with a _lower origin_ resulting in positive angles rotating counter-clockwise.\n- The position of origin has been inconsistent between tools which has an effect on the final annotation too. Most use the center x, y point, but some don't\n- Some of the drawing tools use _LHC_\n- Some tools' annotation use `angle` some use `rotation`\n- It's unclear when the x, y annotation refers to the center point of the shape\n- It's unclear when the x, y annotation is being used as the point of rotation\nSome of the mark annotation models have a few other issues as well:\n- Some shapes have default values which an create bias. For example, the ellipse has a default axis ratio of 0.5 and many volunteers have left the default creating a bias ([comment](https://github.com/zooniverse/front-end-monorepo/issues/500#issuecomment-516788821))\n- The freehand drawing tool has peformance impact on the browser as the drawing is being created and with the job to create classification exports as well. This is because the current annotation consists of every single x, y point created\n\n## Decision\n","The shape's mark annotation models should change for consistency and improved post-classification analysis in the following ways:\n- The annotation should use the mathematical standard of _RHC_ with a domain of `[-180, 180]` for consistent angle calculation\n- The annotation model should use `angle` for naming rotation angles. This replaces usage of `rotation`.\n- The annotation model should replace `x` and `y` with `x_center` and `y_center` for shapes where that is applicable\n- The exceptions are non-shapes like point, line, and transcription line tools, and non-symmetric shapes like fan.\n- Shape clustering in aggregation is always done with the center\n- All rotations should be defined about `x_center` and `y_center` point. If the rotation cannot be defined around the center point, then the point used should be clearly recorded in the annotation as `x_rotation` and `y_rotation`\n- Conditional logic in component code can be avoided by using a Mobx-State-Tree's computed view functions to get either `x_center` or `x_rotation` mapped to `mark.x`. Code example:\n```js\nconst CircleModel = types\n.model('CircleModel', {\nx_center: types.optional(types.number, 0),\ny_center: types.optional(types.number, 0),\nradius: types.maybe(types.number),\nangle: types.maybe(types.number)\n})\n.views(self => ({\nget coords { // this is naming is reusing what's already been done with Point and Line for consistency.\nreturn {\nx: self.x_center,\ny: self.y_center\n}\n}\n}))\n```\n```js\n// non-symmetrical shapes like fan use x_rotation, y_rotation\nconst FanModel = types\n.model('FanModel', {\nx_rotation: types.optional(types.number, 0),\ny_rotation: types.optional(types.number, 0),\nradius: types.maybe(types.number),\nangle: types.maybe(types.number),\nspread: types.maybe(types.number)\n})\n.views(self => ({\nget coords { // this is naming is reusing what's already been done with Point and Line for consistency.\nreturn {\nx: self.x_rotation,\ny: self.y_rotation\n}\n}\n}))\n```\n- Default values should be removed wherever possible. We will replace these with project builder configurable values set in the project builder lab when the tools are setup.\n- The parameters will _not_ have default values suggested by us. If the parameters are not set in the lab, then when attempting to use that drawing tool in the classifier, the classifier should display an error message that the tool is not fully setup yet. The lab should also prompt for inputing a value.\n- The lab should include instructions and a warning about the biasing effect.\n- The tools that have defaults are ellipse, rotate rectangle, fan.\n- The freehand drawing tools mark annotation will be a string of the SVG's path and it will be the responsibility of post-classification analysis to convert this to usable x,y points. We will include a sample script in the `DataDigging` repo for project owners to reference on how to do this. Aggregation in Caesar will have to be updated to do the conversion first.\n- `tool` will change to `toolIndex` to clarify it is referring to the index of the input from the task area. The `toolIndex` is useful to distinguish between multiple instances of the same tool tip in a drawing task.\n- Drawing annotations and drawing tool marks will have a `taskType` and `toolType` attribute added that map to an enumeration of the type of task or tool like `drawing` or `point`, `ellipse`, etc respectively. This enables the aggregation for Caesar code to auto-configure which extractor to use without checking data types ([comment](https://github.com/zooniverse/front-end-monorepo/issues/823#issuecomment-493896524)).\n- Certain annotation models may have internal properties used denoted by a preceding underscore. These properties will be removed with the classification complete action. We will remove them to help prevent confusion by project owners in downstream analysis.\n","**Establish a consistent coordinate system across all drawing tools:**\n\n* Adopt RHC with upper origin, aligning with browser SVG coordinate systems.\n* This ensures positive angles rotate consistently clockwise.\n\n**Define a standard origin for all tools:**\n\n* Use the center x, y point as the origin for all shapes.\n* This eliminates inconsistencies in annotation positioning.\n\n**Standardize terminology for angles:**\n\n* Use the term ""angle"" consistently throughout the system.\n* Deprecate the term ""rotation"".\n\n**Clarify the use of x, y annotations:**\n\n* Specify when x, y annotations refer to the center point of a shape.\n* Specify when x, y annotations are used as the point of rotation.\n\n**Address mark annotation model issues:**\n\n* Remove default axis ratio for ellipse to eliminate bias.\n* Optimize freehand drawing tool for performance by storing a simplified representation of the drawing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe big question: which Subject Viewer should we use to view a given Subject?\nAt the moment, the Classifier tries to ""guess"" which Subject Viewer to use (see\n`lib-classifier/src/store/Subject.js`, `get viewer()`) by analysing the Subject\nitself. e.g. if the Subject has multiple images, show a multi-image viewer. If\nthe Subject has a video, show a video viewer.\nWhile this method works in a majority of projects, certain projects with\nextremely specific Subject structures breaks the generalised ""guessing"" logic,\nor else warps the logic with so many ""what if"" clauses that it collapses into a\nwrithing mass of nonsense.\nFor example, a TESS Planet Hunters Subject looks like...\n```\nSubject 12345 = {\n...\nlocations: [\n{ ""image/png"": ""tess-data-12345.png"" },\n{ ""application/json"": ""tess-data-12345.json"" }\n]\n}\ntess-data-12345.json = {\nx: [1,2,3,4,5],\ny: [6,7,8,9,0]\n}\n```\nIf we were to add to the ""guess the Subject Viewer"" logic by stating that _""any\nSubject that has a JSON, and that JSON has x-y coordinates, should use the Light\nCurve Viewer"",_ then we'd have trouble if, say, a future project needs similar\nJSONs with x-y coordinates for a Map Viewer or a Line Graph Viewer, etc.\nSide note:\n- The Light Curve Viewer can still also serve as a generic Scatterplot Viewer\ngiven proper tweaks - this should be marked for future dev so JSON data with\ngeneric x-y data can be ""guessed"".\n\n## Decision\n","The ""Choose a Subject Viewer"" logic (again, see\n`lib-classifier/src/store/Subject.js`, `get viewer()`) will _first_ try to check\nif there's a **Workflow Configuration** stating the preferred Subject Viewer.\nIf this specific configuration does not exist, the logic will _continue_ to\n""guess"" the correct Subject Viewer to use, as per the current system.\n```\n//Example of a workflow.configuration for TESS project:\n{\nsubject_viewer: ""lightcurveviewer""\n}\n```\nWe've also decided _not_ to specify the ""Custom/Specific Subject Viewer"" logic\nwithin the _Subject_ itself, since it'll make the Subjects more complicated and\nmaintenance troublesome. (Compare changing the config field of one Workflow vs\nupdating the metadata or JSON of a million Subjects.)\ni.e. we should **not** do something like:\n```\ntess-data-12345.json = {\ntype: 'lightcurve',\nx: [1,2,3,4,5],\ny: [6,7,8,9,0]\n}\n```\n","**Decision:** We will create a new Subject Viewer selection system that will use a combination of heuristics and project-specific configuration to determine which Subject Viewer is used for a given Subject.\n\n**Rationale:** This approach will allow us to handle the majority of projects with a simple set of heuristics, while also providing the flexibility to handle more complex projects with project-specific configuration. It will also allow us to add new Subject Viewers in the future without having to rewrite the guess logic.\n\n**Implementation:**\n\n* Create a new `SubjectViewerSelector` class.\n* Move the existing guess logic from `Subject` to `SubjectViewerSelector`.\n* Add a new `projectConfig` property to `SubjectViewerSelector`. This property will be an object that contains project-specific configuration for Subject Viewer selection.\n* Create a new `configureSubjectViewerSelector` function that takes a projectConfig object as input and configures the SubjectViewerSelector accordingly.\n* Call `configureSubjectViewerSelector` with the projectConfig object for each project.\n* When selecting a Subject Viewer for a Subject, the SubjectViewerSelector will first apply the heuristics. If the heuristics do not result in a unique selection, the SubjectViewerSelector will then consult the projectConfig object."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMaking changes to drawing tools in production - specifically, to the structure of the data they output - is an occasionally necessary evil. Unfortunately, there is currently no way to link a marking tool, whether by version, commit or whatever, to its output in a classification.\nThis means that when changes happen to the tools, we need to be careful to communicate those changes to project managers, since there is every chance that they will break their data pipelines. And in the absence of a schema for the tools' output, these changes have to be handled manually in code by project managers.\n\n## Decision\n","We include a schema for each marking tool used as part of the classification object submitted to the API.\nThe schema will be written in JSON, and will live alongside to the marking tool code. It should describe the expected data structure of a marking tool.\n### Example\nFor a point tool with the following example output:\n```json\n{\n""x"": 152.96875,\n""y"": 164\n}\n```\nThe schema could look like:\n```json\n{\n""$schema"": ""http://json-schema.org/draft-06/schema#"",\n""$id"": ""https://zooniverse.org/schemas/tools/point.schema.json"",\n""type"": ""object"",\n""title"": ""Point"",\n""description"": ""A single point on the subject"",\n""properties"": {\n""x"": {\n""description"": ""X value of the point, where `0` is on the left"",\n""type"": ""number""\n},\n""y"": {\n""description"": ""Y value of the point, where `0` is at the top"",\n""type"": ""number""\n}\n},\n""required"": [\n""x"",\n""y""\n]\n}\n```\n### Alternatives\nWe could version the schema, and simply reference it within the classification by its URL. For example, the point tool, instead of including the entire schema in the classification, has a reference to `https://zooniverse.org/schemas/tools/point.schema.v1.json`. A change to the point tool output then becomes a new version number.\nThis would result in smaller classification object sizes, although forgoes the convenience of having a self-contained classification object.\n",Implement a schema for drawing tool output that can be versioned and queried.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA drawing mark's sub-task is designed to support volunteers answering additional questions for each drawing mark annotation. It allows single choice or multiple choice question task, text task, dropdown task, and slider task.\n### Annotation JSON structure\nThe current sub-task annotation JSON structure is:\n```json\n// a point with sub-task consisting of a question task and a dropdown task\n{\n""annotations"": [\n{\n""task"": ""T0"",\n""value"": [\n{\n""frame"": 0,\n""tool"": 0,\n""x"": 452.18341064453125,\n""y"": 202.87478637695312,\n""details"": [\n{""value"": 0},\n{""value"": [\n{""value"": ""option-1""},\n{""value"": ""option-2""},\n{""value"": null}\n]}\n]\n},\n{\n""frame"": 0,\n""tool"": 0,\n""x"": 374.23454574576868,\n""y"": 455.23453656547428,\n""details"": [\n{""value"": 1},\n{""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]}\n]\n},\n{\n""frame"": 0,\n""tool"": 1,\n""x"": 404.61279296875,\n""y"": 583.4398803710938,\n""details"": [\n{""value"": 1},\n{""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]}\n]\n}\n]\n}\n]\n}\n```\nThe annotation structure for the sub-task, under `details`, has a few issues because it solely relies on an array index to relate back to the original sub-task. This makes it difficult to make downstream analysis and aggregation scripts. The aggregation code now has to parse the details array and make a ""mock annotation"" of the correct structure to be passed along to the next reducer.\n### Sub-task UI\nThe sub-task UI positioned itself fixed below relative to the position of the mark. Notably transcription projects have commented that this interferes with being able to transcribe successfully since the dialog may cover up part of the subject and cannot be moved without moving the drawing mark.\n\n## Decision\n","For initial support, we will support the single and multiple choice question tasks and the text task in the sub-task. Slider task may be deprecated and dropdown task may be changing in ways we do not have a plan for yet, so they can be supported later if it makes sense to add them.\n### Annotation JSON structure\nThe annotations in the details array will be updated to be an object that just contains a reference to the sub-task's unique identifier. The task annotation itself will be stored in the classification's annotations array flattened.\nThe main benefit of this reorganization will be with downstream analysis and aggregation. When aggregating drawn shapes the first step is clustering. Once the clusters are found the subtasks need to be aggregated within each cluster. This will be easier to do if the structure of each subtask annotation is the same as if that task was asked on its own. The code can just take all subtask annotations within a cluster and just pass it to the reducer as if it is a list of main task annotations without having to reshape them.\nAn addition of `markIndex` is being added for the sub-task annotations for the purpose of having an identifier relating it back to the parent drawing task annotation value array which represents marks.\nAn example of the new sub-task annotation JSON structure at classification submission:\n```json\n{\n""annotations"": [\n{\n""task"": ""T0"",\n""taskType"": ""drawing"",\n""value"": [\n{\n""frame"": 0,\n""toolIndex"": 0,\n""toolType"": ""point"",\n""x"": 452.18341064453125,\n""y"": 202.87478637695312,\n""details"": [\n{""task"": ""T0.0.0""},\n{""task"": ""T0.0.1""}\n]\n},\n{\n""frame"": 0,\n""toolIndex"": 0,\n""toolType"": ""point"",\n""x"": 374.23454574576868,\n""y"": 455.23453656547428,\n""details"": [\n{""task"": ""T0.0.0""},\n{""task"": ""T0.0.1""}\n]\n},\n{\n""frame"": 0,\n""toolIndex"": 1,\n""toolType"": ""point"",\n""x"": 404.61279296875,\n""y"": 583.4398803710938,\n""details"": [\n{""task"": ""T0.1.0""},\n{""task"": ""T0.1.1""}\n]\n}\n]\n},\n{\n""task"": ""T0.0.0"",\n""taskType"": ""single"",\n""markIndex"": 0,\n""value"": 0\n},\n{\n""task"": ""T0.0.1"",\n""taskType"": ""dropdown"",\n""markIndex"": 0,\n""value"": [\n{""value"": ""option-1""},\n{""value"": ""option-2""},\n{""value"": null}\n]\n},\n{\n""task"": ""T0.0.0"",\n""taskType"": ""single"",\n""markIndex"": 1,\n""value"": 1\n},\n{\n""task"": ""T0.0.1"",\n""taskType"": ""dropdown"",\n""markIndex"": 1,\n""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]\n},\n{\n""task"": ""T0.1.0"",\n""markIndex"": 2,\n""taskType"": ""single"",\n""value"": 1\n},\n{\n""task"": ""T0.1.1"",\n""markIndex"": 2,\n""taskType"": ""dropdown"",\n""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]\n}\n],\n""metadata"": {\n""classifier_version"": ""2.0""\n}\n}\n```\nThe sub-task identifiers follow a convention of `TASK_KEY.TOOL_INDEX.DETAILS_INDEX`.\nNote that this is the structure at classification submission. The classifier's internal store models may have differences for the purposes of keeping track of in-progress annotations and marks being made.\n### Drawing sub-task UI\nThe UI will change to adopt the design of Anti-Slavery Manuscripts (ASM) with this [generalized design](https://projects.invisionapp.com/d/main#/console/12923997/396381420/preview). It will be a pseudo-modal, but with a few notable differences from a true modal:\n- The initial position will be near the associated mark made\n- Interactions will be allowed with the image toolbar to allow zoom, rotate, as well as opening of the tutorial, field guide, and task help. Submission of the classification should not be allowed.\n- If the sub-tasks are required, the modal should not be closeable until the required annotations are made or the mark is deleted if cancelled\n- The dialog can be moved and resized\nTo support movability and resizing, we will leverage [react-rnd](https://github.com/bokuweb/react-rnd) which is the same library ASM used. Grommet's `Layer` cannot be used since it is intended for actual modal or side panel use and cannot be arbitrarily positioned or moved.\n","1. **Change the annotation JSON structure:**\n   - Add a `subTask` field to each drawing mark annotation.\n   - The `subTask` field will contain an array of objects, each representing a sub-task.\n   - Each sub-task object will have a `type` field (e.g., ""question"", ""dropdown"", ""slider""), a `value` field, and an `options` field (if applicable).\n\n2. **Update the sub-task UI:**\n   - Make the sub-task dialog resizable and movable.\n   - Allow users to position the sub-task dialog anywhere on the screen."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\n\n## Decision\n","We solved this problem by adding an undo/redo history stack to the classifier task area. [#2050](https://github.com/zooniverse/front-end-monorepo/pull/2050) added a global history stack (`store.annotatedSteps`). [#2520](https://github.com/zooniverse/front-end-monorepo/pull/2520) refines this by moving from a global history manager to individual history management for each subject (`subject.stepHistory`.)\nHistory is managed by the MST middleware [`UndoManager`](https://github.com/mobxjs/mobx-state-tree/blob/master/packages/mst-middlewares/README.md#undomanager), which records snapshots of a single subtree in the store. Each history item is a single step key and one or more annotation IDs, representing the current state of the task area. 'Back' loads the previous step key and annotation IDs from history. 'Next' loads the next step key and annotation IDs, or creates a new history item if one doesn't exist already. Going back then changing branches, for a branching workflow, clears any existing redo history and starts a fresh history from the current point.\n","We decided to use a tree data structure to represent the workflow, with each node representing a step in the workflow. Each node can have multiple children nodes, which represent the different possible paths that the workflow can take. This allows us to easily support branching steps, as we can simply add a new child node to the current node to represent the new path. We can also easily support recursive steps, as we can allow a node to be its own child.\n\nTo prevent volunteers from losing work, we decided to save the state of the workflow each time a volunteer moves to a new node. This way, if the volunteer needs to move back to a previous node, they will not lose any of the work they have already done."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we launched the NextJS apps, a single domain could only host one NextJS app. We gave each app its own subdomain: `fe-content-pages.zooniverse.org` and `fe-projects.zooniverse.org` then proxied URLs from `www.zooniverse.org` to those domains. Next data requests, on the `/_next` URL, are proxied from `www.zooniverse.org/_next` to `fe-project.zooniverse.org/_next`. We deliberately broke the content pages app, in favour of supporting projects.\n[Next 9.5](https://nextjs.org/blog/next-9-5), in July 2020, added support for multiple apps running on the same domain via the [`basePath`](https://nextjs.org/blog/next-9-5#customizable-base-path) config setting.\n\n## Decision\n",Set base paths of `/about` and `/projects` for the content pages app and project app respectively.\nhttps://github.com/zooniverse/front-end-monorepo/pull/2519\n,"**Migrate NextJS apps to the same domain, using the `basePath` config setting.**\n\n**Benefit:**\n\n* Eliminates the need for separate subdomains, simplifying the architecture and URL structure.\n\n**Considerations:**\n\n* Verify that NextJS 9.5 is supported in the existing environment.\n* Implement the `basePath` config setting for each app to ensure correct routing and prevent conflicts.\n* Test thoroughly to ensure that all functionality is working as expected after the migration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn PFE, there are a couple of different ways workflows are routed:\n- Navigating to `/classify`, the user sees either the default workflow for that project, a random active workflow, or the workflow stored in project preferences for logged-in volunteers.\n- Navigating to `/classify?workflow=[workflow_id]` shows the workflow determined by the query parameter when the ""Use workflow query"" feature is enabled for the project, or if the user has the correct role (owner, collaborator, tester, or admin).\nThis is not optimal for several reasons. Users cannot bookmark specific workflows unless they use the query parameter; there can be uncertainty over which actual workflow is being shown (particularly as the workflow name isn't shown in the UI); different workflows can be shown at the same URL at different times; and ultimately, workflows are static resources that should be routed to - that's what URLs are for.\nAdditionally, [we currently have a requirement for CSSI CitSci to be able to route to specific subjects](https://github.com/zooniverse/front-end-monorepo/issues/806#issuecomment-495685027). It's conceivable that a project will have a requirement for routing to a specific subject set linked to a workflow as well.\n\n## Decision\n","We adopt the following URL structure:\n```\n/projects/[owner]/[project]/classify/workflow/:workflow-id/subject-set/:subject-set-id/subject/:subject-id\n```\nThis would be facilitated by the [dynamic routing feature in Next.js 9](https://github.com/zeit/next.js/#dynamic-routing) (see [#1071](https://github.com/zooniverse/front-end-monorepo/pull/1071)).\nWorkflow links from the home page would route directly to their workflow-specific URLs.\n### `/classify` behaviour\nWhen navigating to `/classify`, the page should redirect the user to the correct workflow URL in the following priority:\n1. Workflow ID set in user project preferences by the user\n1. Workflow ID set in user project preferences by the project\n1. Project default workflow\n1. Random active selection\nThe new URL that we redirect the user to should replace the entry for `/classify` in the browser history.\n### Error handling\nWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in `getInitialProps`, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, `404`; incorrect permissions get a `401`/`403` (depending on login status) etc.\n","**Decision:** Workflows should be routable by URL by default for all projects.\n\n**Rationale:**\n\n* **Consistency:** This approach provides a consistent way to route to workflows for all projects, regardless of project settings or user permissions.\n* **Bookmarkability:** URLs can be bookmarked, allowing users to easily return to specific workflows.\n* **Transparency:** Users will always know which workflow they are viewing, as the workflow name will be displayed in the URL.\n* **Extensibility:** This approach allows for future requirements to route to specific subjects or other workflow attributes.\n\n**Details:**\n\n* Each workflow will have its own unique URL, following the format:\n    * `/classify/[workflow_id]`\n* When a user navigates to `/classify`, they will be redirected to the default workflow for the project.\n* The ""Use workflow query"" feature will be discontinued."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use [Sentry](https://sentry.io) to provide centralised JavaScript error logging for the project and content pages apps. Sentry supports versioned releases, and GitHub integration, so that fixes can be linked to releases and new issues can be linked to potentially bad commits.\n\n## Decision\n","The monorepo will be versioned in Sentry, using the git commit SHA to version a release. New releases are deployed to staging on each push to master. A release is finalised and deployed to production when the production-release tag is updated to point to that release. [#1599](https://github.com/zooniverse/front-end-monorepo/pull/1599) and [#1601](https://github.com/zooniverse/front-end-monorepo/pull/1601) implement this using GitHub actions.\n",**Decision:** \n\nContinue using Sentry for centralised JavaScript error logging.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\n### [multiFrame](https://github.com/zooniverse/front-end-monorepo/tree/master/packages/lib-classifier/src/components/Classifier/components/SubjectViewer/components/MultiFrameViewer)\nA workflow using the multi-frame subject viewer might have a preference regarding:\n- marks per frame: some might prefer marks filtered per frame, like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page, while other workflows might prefer marks persist between frames like [Space Warps](https://www.zooniverse.org/projects/aprajita/space-warps-hsc/classify) or [Power to the People](https://www.zooniverse.org/projects/alycialeonard/power-to-the-people). However, after reviewing projects that enabled marks to persist between frames (`multi_image_clone_markers` [in PFE](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/classifier/tasks/drawing/markings-renderer.cjsx#L55)) it appears the PFE setting is unclear, as a few of the related workflows do not include a drawing task or the subjects do not have multiple frames.\n- positioning: some might prefer pan, zoom, and rotation reset per frame, like a transcription workflow where each frame represents a unique page to transcribe, while other workflows might prefer pan, zoom, and rotation maintained between frames, like [Wildcam Gorongosa](https://www.zooniverse.org/projects/zooniverse/wildcam-gorongosa/classify) or [Backyard Worlds](https://www.zooniverse.org/projects/marckuchner/backyard-worlds-planet-9/classify) (in flipbook mode, not separate frames)\n### [subjectGroup](https://github.com/zooniverse/front-end-monorepo/tree/master/packages/lib-classifier/src/components/Classifier/components/SubjectViewer/components/SubjectGroupViewer)\n- A workflow using the subject-group subject viewer might want to define the subject cell width, height, or style, or the subject viewer grid columns or grid rows.\n\n## Decision\n","Subject viewer configuration is an object stored in `workflow.configuration.subject_viewer_config` and is structured as follows for the subject viewers noted:\n`multiFrame`:\n```javascript\n{\nfilter_marks_per_frame: <boolean>  // replaces multi_image_clone_markers in PFE\npositioning: <enumerable> // includes pan, zoom, and rotation, enumerable i.e. ""maintain"", ""reset""\n}\n```\n`subjectGroup`:\n```javascript\n{\ncell_width: <number of pixels>\ncell_height: <number of pixels>\ncell_style: { [CSS property]: <CSS property value> }\ngrid_columns: <number>\ngrid_rows: <number>\n}\n```\nSubject viewers to define the configuration object in the related subject viewer README, if applicable.\nThe ScatterPlotViewer and BarChartViewer accept a configuration object directly in the JSON structure of the subject data to support variability in the display settings including the plot labels since it's possible this might vary per subject. The `subject_viewer_config` object should only be used for  configuration options that apply to all subjects linked to the workflow.\n","Workflow creators will be able to configure the following properties for the multi-frame subject viewer:\n- marks per frame: marks filtered per frame (like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page) or marks persist between frames (like [Space Warps](https://www.zooniverse.org/projects/aprajita/space-warps-hsc/classify) or [Power to the People](https://www.zooniverse.org/projects/alycialeonard/power-to-the-people))\n- positioning: pan, zoom, and rotation reset per frame (like a transcription workflow where each frame represents a unique page to transcribe) or pan, zoom, and rotation maintained between frames (like [Wildcam Gorongosa](https://www.zooniverse.org/projects/zooniverse/wildcam-gorongosa/classify) or [Backyard Worlds](https://www.zooniverse.org/projects/marckuchner/backyard-worlds-planet-9/classify) (in flipbook mode, not separate frames))\nWorkflow creators will be able to configure the following properties for the subject-group subject viewer:\n- subject cell width, height, or style\n- subject viewer grid columns or grid rows"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are an open source project and as a result can possibly accept outside contribution to our codebase. To this end, we need to communicate our requirements for the code that is submitted and our process for acceptance of this code. The end goal for the rewrite is to be more modular which we believe will enable both the team and outside contributors flexibility and ease of adding enhanced features.\n\n## Decision\n","We will add a contributing guide to our documentation files. Once available, Github will show links to this guide automatically to any outside user visiting the front-end-monorepo repository. Any outside contributions must follow this guide to submit code for review.\n","To ensure the quality and consistency of our codebase, we establish the following requirements for all code submissions:\n\n* **Code Style:** All code must adhere to our established coding conventions, as outlined in our style guide.\n* **Modular Design:** Contributions should be designed to be modular and easily integrated into our existing codebase.\n* **Unit Tests:** All new features and changes should be accompanied by comprehensive unit tests demonstrating correct functionality.\n* **Documentation:** All contributed code must be well-documented, including clear explanations of the purpose and implementation of the code.\n\nTo ensure a smooth and efficient code acceptance process, we establish the following workflow:\n\n* **Code Review:** All submitted code will undergo a thorough code review by designated team members to assess its compliance with our requirements.\n* **Quality Assurance:** Accepted code will be subjected to automated quality assurance checks, including static analysis and unit tests, to ensure its stability and reliability.\n* **Testing:** The contributed code will be integrated into our build and testing pipelines to verify its compatibility with our existing codebase.\n* **Acceptance:** Once the code has passed all reviews and testing, it will be正式 accepted into the project"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIMLS funded us to experiment with transcription tools. The project [Anti-Slavery Manuscripts](https://www.antislaverymanuscripts.org/classify) has a workflow where volunteers transcribe lines of text first by marking the line, then adding the transcription in a sub-task. This task is slightly unique in that:\n- Incomplete classifications can be submitted\n- Previous transcriptions from caesar are loaded and presented as an option in the sub-task\n- volunteers can select the previous transcription, edit it, and submit a new transcription\n- lines with previous transcriptions or retired lines are displayed visually by color\n\n## Decision\n","We will be porting the ASM functionality to the main classifier as a new transcription task. The task will be composite of:\n- A drawing line task for transcription that is created by two pointer down and up events to mark the points of the line.\n- the starting point and ending point will have visual indicators communicating the direction of creation.\n- the line color will indicate the current status of completion\n- completed lines cannot be edited\n- Note that ASM leveraged a hacked polygon tool to do this, but we should have a line tool variant specifically for this instead. Downstream aggregation can leverage polygon type aggregations if it fits.\n- A sub-task will display once the transcription line mark is made\n- a text input will display suggestions from previous caesar aggregations.\n- The suggestions can be selected and inserted as an editable value in the text input\n- Grommet's [TextInput](https://storybook.grommet.io/?path=/story/textinput--suggestions) with the suggestions prop will be utilized so that it can be styleable with our theme.\n- sub-task will not be a modal, but a movable div linked to the currently selected line mark\n[More detailed user stores are in this google doc](https://docs.google.com/document/d/16abI-wkRlEXsWgACfFQVqwO76aEopohIjQiRfNQKWiw/edit)\n",Use the classification task type with a subtask workflow to allow incomplete classifications and the ability to submit new classifications based on previous classifications.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\n- add new code in three places:\n- [task views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks).\n- [task models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/tasks).\n- [annotation models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/annotations).\n- import the new modules by name in several places, and register them:\n- [registered views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks/helpers/getTaskComponent.js).\n- [import tasks models for workflow steps](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/WorkflowStepStore.js#L5-L18).\n- [import all annotations to the classification model](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/Classification.js#L3).\n- [register annotations with the classifications store](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/ClassificationStore.js#L111-L120).\nIt was easy to forget one of these steps and a lot of this could be automated in code.\n\n## Decision\n","- Keep all the code together. Store task views and models next to each other in the filesystem. (#1212)\n- Import named modules to a registry object (or similar) then load them in to other code from that register. (#1212)\n- Delegate responsibility from the classification to individual tasks. (#1228)\n### Implementation\n- Task code was moved to `lib-classifier/src/plugins/tasks`. Each task has its own directory, with these subdirectories:\n- _components_: React components to render the task.\n- _models_: MobX State Tree models for the task. One Task model and one Annotation model.\n- a _taskRegistry_ object was added, which is described in the [tasks README](https://github.com/zooniverse/front-end-monorepo/blob/master/packages/lib-classifier/src/plugins/tasks/readme.md).\n- Responsibility for creating new annotations was removed from the classifications store, removing the need for the classifications store to know about different types of tasks and how to create an annotation for each. New methods were added to the task models to delegate responsibility and make tasks more flexible:\n- _task.createAnnotation()_ creates a new annotation of the correct type for a specific task.\n- _task.defaultAnnotation_ (read-only) returns the default annotation for a specific task.\n",Create a plugin system for defining and loading new task types.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Solar Jets project requires a TemporalRotatingRectangle drawing tool for volunteers to capture the width, height, angle and time properties of a solar jet. At the time of this writing, the `front-end-monorepo` has a standard Rectangle drawing tool which gives the width and height properties. Based on the standard Rectangle drawing tool, we want to create a RotatingRectangle drawing tool and a TemporalRotatingRectangle drawing tool.\nThe RotatingRectangle drawing tool is a common tool used in the `Panoptes-Front-End` code base and eventually needs to be written in the `front-end-monorepo`. This is a good opportunity to complete this task.\nThe idea is to create two new rectangle drawing tools based off the standard Rectangle drawing tool. Once complete, the `front-end-monorepo` will have the following rectangle drawing tools:\n**Rectangle**\n```json\n{\n""details"": array,\n""frame"": number,\n""height"": number,\n""toolIndex"": number,\n""toolType"": string,\n""width"": number,\n""x_center"": number,\n""Y_center"": number\n}\n```\n**RotatingRectangle**\n\*everything the `Rectangle` tool has plus this addition:\n```json\n{\n""angle"": number\n}\n```\n**NOTE:** angle is Degrees (-180, 180)\n- positive angles rotate from +x to +y (a right-handed coordinate system)\n- rotation is about the center of the rectangle\n**TemporalRotatingRectangle**\n\*everything the `RotatingRectangle` tool has plus this addition:\n```json\n{\n""displayTime"": string\n}\n```\n**NOTE:** displayTime will be formatted as:\n- If minutes exist: mm:ss:ms\n- If minutes do not exist: ss:ms\nms (milli-seconds will be to 3 decimal places)\n**Temporal Tools**\nA temporal tool will include a time property. This time property refers to the point in time on a video file when the drawing tool is created.\nFor example:\nIf a video is 4 seconds long. A user plays the video for 2.0 seconds and draws a rectangle, the rectangle will have a displayTime equal to 2:000. The rectangle will only display from the 2:000 timestamp to the end of the video.\nThe Solar Jets project will have mp4 video files as subjects in the new classifier in the `front-end monorepo` (FEM). Using a Temporal drawing tool will allow volunteers to identify the location and time of an action within the videos.\n\n## Decision\n","We will develop a `RotatingRectangle` and `TemporalRotatingRectangle` in the FEM.\nThe first step will be to extend the `Rectangle` to create a new `RotatingRectangle` adding only the functionality to allow the rectangle to rotate which will add the `angle` property to the model.\nLastly, we will extend the `RotatingRectangle` to create a new `TemporalRotatingRectangle` adding the `displayTime` property to the model.\nAn example of this tool extension can be seen in this PR: [PR#2099](https://github.com/zooniverse/front-end-monorepo/pull/2099)\n","Create three Rectangle drawing tools in the `front-end-monorepo`:\n**Rectangle**\n```json\n{\n""details"": array,\n""frame"": number,\n""height"": number,\n""toolIndex"": number,\n""toolType"": string,\n""width"": number,\n""x_center"": number,\n""Y_center"": number\n}\n```\n**RotatingRectangle**\n\*everything the `Rectangle` tool has plus this addition:\n```json\n{\n""angle"": number\n}\n```\n- positive angles rotate from +x to +y (a right-handed coordinate system)\n- rotation is about the center of the rectangle\n**TemporalRotatingRectangle**\n\*everything the `RotatingRectangle` tool has plus this addition:\n```json\n{\n""displayTime"": string\n}\n```\n- displayTime will be formatted as:\n- If minutes exist: mm:ss:ms\n- If minutes do not exist: ss:ms\nms (milli-seconds will be to 3 decimal places)"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFairly early on in the rebuild of the Classifier, we started using newer technologies such as [CSS Grid](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Grid_Layout), which are not supported in older browsers like Internet Explorer 11.\nAnd, while Edge is now several major versions in, we still have a percentage of users on IE11. As such, we're now at a point where we have to determine whether to drop support for legacy browsers and risk inconvenience for that segment, or invest significant time in coding and testing fallbacks.\n\n## Decision\n","We will only officially support the following browsers:\n### Desktop\n- Safari\n- Chrome\n- Firefox\n- Edge\n### Mobile\n- Safari\n- Chrome\n- Opera\nOf these, we will support the current and last two major versions.\n",It is decided to drop support for legacy browsers and risk inconvenience for that segment rather than investing significant time in coding and testing fallbacks.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current classifier was based on a few assumptions, that, while accurate at the time, became outdated as additional functionality was added:\n1. That a workflow step consisted of a single annotation action. However, project builders wanted to e.g. annotate via drawing _and_ selecting from a list of options in a single step, which gave rise to the combo task.\n1. That we would always want to show a summary. When the `hide classification summaries` tool became public, more projects actually wanted to use it than we expected.\n1. That we wouldn't need to show any information to a volunteer until the end. That changed firstly with the MS interventions experiment, and later feedback, which is shown once a workflow task is completed.\nThe combo task and drawing sub-tasks specifically create issues since the tasks array becomes a list of both tasks and references to tasks, and it's up to the code to suss out which tasks should be in the combo and take them out of the normal workflow sequence, which requires a disproportionate amount of code for what amounts to an edge case in terms of project building.\nAn example combo task with one of them as a drawing task with defined sub-tasks.\n``` javascript\n{\n""id"": ""3264"",\n""display_name"": ""Combo task test"",\n""tasks"": {\n""T0"": {\n""help"": ""task 1"",\n""type"": ""drawing"",\n""tools"": [\n{\n""type"": ""point"",\n""color"": ""#00ff00"",\n""label"": ""Tool name"",\n""details"": [\n]\n}\n],\n""instruction"": ""Task 1""\n},\n""T1"": {\n""type"": ""combo"",\n""tasks"": [\n""T0"",\n""T2""\n]\n},\n""T2"": {\n""help"": ""task 2"",\n""type"": ""drawing"",\n""tools"": [\n{\n""type"": ""point"",\n""color"": ""#ff0000"",\n""label"": ""Tool name"",\n""details"": [\n{\n""help"": """",\n""type"": ""single"",\n""answers"": [\n{\n""label"": ""Foo""\n},\n{\n""label"": ""Bar""\n}\n],\n""question"": ""What is it?"",\n""required"": ""true""\n}\n]\n}\n],\n""instruction"": ""Task 2""\n}\n},\n""first_task"": ""T1"",\n""prioritized"": false,\n""grouped"": false,\n""pairwise"": false,\n""configuration"": { }\n}\n```\n\n## Decision\n","We implement the classification process like this:\nThe classification of a subject will consist of a series of __steps__. A single step consists of a __task hook__, and a __notification hook__. A task hook consists of an __array of one or more workflow tasks__. A notification could be an __intervention__, __feedback__, a __Sugar notification__, or some other information conveyed to the volunteer.\nIn practice, this will probably mean that the current workflow store is only used to store the resources from the Panoptes API. Once the project and workflow are loaded, we will derive a store for the workflow steps and that will drive the user interface.\nThe workflow resource will need updating to support the new step structure (as discussed in [zooniverse/front-end-monorepo#123](https://github.com/zooniverse/front-end-monorepo/issues/123)):\n- `workflow.steps` will be an [ES6 Map](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map) which can be stored as a [Mobx Observable Map](https://mobx.js.org/refguide/map.html)\n- Almost all features of ES6 Maps are supported by all major browsers that we will support.\n- Each value for the key-value pairs will be an object with `taskKeys` that are set to an array of task keys, and optionally a `next` property for a step key.\n- The optionally defined next step is to support recursive workflows. The order is otherwise assumed to be the order of the steps Map:\n> The keys in Map are ordered while keys added to object are not. Thus, when iterating over it, a Map object returns keys in order of insertion.\n- Since the order can be reliably derived from the steps Map, then we can drop `workflow.first_task` from use\n- `workflow.tasks` will remain as is for backwards compatibility\n- Single question task branching will still use `next` properties in the answer object, but will be set to a step key instead of task key.\n- A step taskKeys property set to `['summary']` will load an optional summary step for the end of the classification, which shifts us to having summaries be opt-in rather than opt-out. If this is not present, then a summary will not display.\nAn example of what this could look like:\n``` javascript\n{\nid: '1',\ntasks: {\nT1: {\nanswers: [\n{ label: 'yes', next: 'S4' }, // Branching single question task\n{ label: 'no', next: 'S2' }\n],\ntype: 'single'\n},\nT2: {...},\nT3: {...},\nT4: {...}\n},\nsteps: [\n['S1', { taskKeys: ['T1'] }]\n['S2', { taskKeys: ['T2', 'T3'] }],\n['S3', { taskKeys: ['T4'], next: 'S1' }] // Recursion back to Step 1\n['S4', { taskKeys: ['summary'] }]\n]\n}\n```\n","**The current combo task implementation is no longer meeting the needs of the product. It is too complex and error-prone, and it does not support the full range of desired functionality.**\n\n**An alternative approach is to create a new type of task that can contain multiple sub-tasks. This would allow for more flexibility in creating workflow steps, and it would eliminate the need for the complex logic that is currently required to handle combo tasks.**\n\n**The new task type would be called a ""composite task"". It would consist of an array of sub-tasks, each of which could be of any type. The sub-tasks would be executed in the order in which they appear in the array.**\n\n**This approach would have several advantages over the current combo task implementation:**\n\n* **It would be more flexible.** Composite tasks could be used to create a wider range of workflow steps.\n* **It would be less complex.** The code to handle composite tasks would be simpler and more maintainable.\n* **It would be more consistent.** The behavior of composite tasks would be consistent with the behavior of other task types."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith new drawing tools being developed for the classifier, we need an API that's common to all drawing tools and marks, which can be easily extended by tool developers. This document lays out an overview of the drawing tool model and the public interfaces common to all tools and all marks.\nTo support drawing, the subject viewer also needs to support:\n- rendering a static list of marks from previous drawing task annotations.\n- interacting with pointer events to create, edit and delete new marks for the current drawing task annotation.\n\n## Decision\n","### The subject viewer\nThe subject viewer will render two components.\n- _DrawingToolMarks_ takes an array of marks from drawing task annotations in the classification and renders it as a static, read-only list.\n- _InteractionLayer_ wraps a _DrawingToolMarks_ component and adds pointer event support, so that the rendered array of marks can be edited and updated. THis component only acts on marks for the active drawing task.\nMarks created by the _InterctionLayer_ are added to a new drawing task annotation, for the current classification, when we click Next or Done to complete the current task.\nPointer event support is polyfilled in older browsers with [PEP](https://github.com/jquery/PEP).\n### The drawing model\nA drawing task has drawing tools. Each tool creates marks. On task completion, a drawing annotation is created, which is an array of all drawn marks. Each mark has a corresponding React component which renders the SVG for that particular shape.\n","**Decision:** Create a common API for all drawing tools and marks that can be easily extended by tool developers.\n\n**Rationale:**\n\nThis decision will address the need for a common API that:\n\n* Supports the development of new drawing tools by providing a consistent interface.\n* Facilitates the extension of the API by tool developers to accommodate new features or functionalities.\n* Enables the subject viewer to render static lists of marks and interact with pointer events to create, edit, and delete marks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAt the Zooniverse Team Meeting April 2018, it was decided that Panoptes Front End would be broken up into modular libraries and client-side apps. To aid in this rewrite, it was also decided that we would try out server-side rendering (SSR) since now there are several frameworks for React that make this easy. SSR has several benefits including improved load times and rendered HTML that search providers could crawl to index.\nIt was initially chosen to use a SSR framework called [Next.js](https://github.com/zeit/next.js/). Next.js is an opinionated framework that supports out of the box SSR, routing, production build compiling. In particular its API provides a method, `getInitialProps` to easily hydrate client-side app state. However, in the brief experience we have had so far:\n- It's been difficult to configure. We had to configure it to use [Grommet 2](https://v2.grommet.io/) and [styled-components](https://www.styled-components.com/) as these are not defaults.\n- It's been difficult with major version upgrades. Version 6 was released right when we started and the upgrade path took a lot of time.\n- It's had issues with [Mocha](https://mochajs.org/). This is concerning since Mocha is the most popular test framework for javascript, and our current default.\nThere is an [open issue](https://github.com/zeit/next.js/issues/1632) confirming that Next.js will be integrating React Router v4 as its routing solution. React Router v4 is not a router that we wish to use due to its significant API changes.\nIn addition, Sarah had been recently advised by fellow developers in the Chicago Javascript community that Next.js is difficult to configure which can outweigh any benefits it might provide.\nRoger and Sarah built several prototypes with other libraries, including [react-server](https://react-server.io/), [razzle](https://github.com/jaredpalmer/razzle), and [react-universally](https://github.com/ctrlplusb/react-universally). However, none of these provided the same ease-of-use as Next.js. Additionally, react-server doesn't appear to be actively maintained.\n\n## Decision\n","For the time being, we will continue to use NextJS. We're currently using v5; v6 is available, and likely to be the last version that is router-agnostic, so we can revisit this decision then.\n","After further consideration and testing, the team decided to continue using Next.js as the SSR framework for Panoptes Front End.\n\n**Reasons for Decision:**\n\n* Despite the initial challenges with configuration and upgrades, Next.js remains the most user-friendly and comprehensive SSR framework available.\n* The team has already invested significant time and effort in learning and using Next.js, and switching to a new framework would require additional resources and a steep learning curve.\n* The reported issues with Mocha are being actively addressed by the Next.js team, and the upcoming integration of React Router v4 is not a major concern for the project's requirements.\n* The ease of use and out-of-the-box features provided by Next.js outweigh the potential drawbacks, especially considering the benefits of SSR for improved load times and search engine crawling."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have identified a gap in our automated and manual testing process from the deployment of [zooniverse/front-end-monorepo#1038](https://github.com/zooniverse/front-end-monorepo/pull/1038). This pull request had been reviewed and tested manually in a development environment on the reviewer's local machine, tests passed, and the app appeared to work as intended with the added bug fix. After automatic deployment to production by merging to master, however, we received a report that Planet Hunters: TESS classification interface was no longer functioning. The classify page was returning a 404.\nWe had acknowledged previously that we had a need for staging environment deploys for the purposes of design reviews in [zooniverse/front-end-monorepo#694](https://github.com/zooniverse/front-end-monorepo/issues/694). We now have a need to have staging deployments so we can manually check that the pull request functions in a deployed, production-like environment. The next.js builds and creates files specific for the production deployment that running the app locally for development does not replicate, nor is it replicated in automated unit testing.\nInitially we were considering branch deploys for both of these cases, but in order to do this we would need to use wildcard sub-domains. At this time, [kubernetes ingress does not support wildcards](https://github.com/containous/traefik/issues/3884). Therefore, we need to devise a different solution.\n\n## Decision\n","In practice, we're going to have two kinds of pull request: one that changes a single app (e.g. new widget on project home page), and one that affects multiple apps (e.g. update to the shared component library). For PRs on a single app, we'd like to manually deploy it as a staging branch deployment so it can be tested in isolation. On merging to master, that gets deployed to staging automatically. We'd then do manual integration testing before manually deploying to production. For PRs across multiple apps, we'd test it locally before merging to master using a local Docker image setup to use Panoptes in production. Once it's deployed to staging, we'd do integration testing before manually deploying to production.\nWe're going to setup a staging deployment that matches production as closely as possible to fill the gap of the need for manual reviews to confirm that the app is functioning. To accomplish this:\n- Merging to master will be switched to deploy to staging to https://frontend.preview.zooniverse.org\n- Production deployment will now be done manually triggered by lita command on slack and using a git tag for production\n- The Jenkins file will be updated to use the git tags to determine the location of the deployment\n- Cloudfront will be configured to load the correct microservice app depending on route:\n- Both the staging (https://frontend.preview.zooniverse.org) and production domains (www.zooniverse.org) will have cloudfront configurations that will match URL traffic against rules setup in Cloudfront. The staging rules will map to the staging apps and the production rules will map to the production apps.\n- The cloudfront rules match paths on the incoming URL, i.e. `/about/team` maps to a registered service via DNS, e.g.\n+ When a `GET` request for URL `www.zooniverse.org/about/team` hits cloudfront, it maps to the `fe-content-pages.zooniverse.org` service domain.\n+ Cloudfront then proxies that request via DNS lookup to the Kubneretes (K8) ingress service\n+ The K8 ingress then looks up the registered service domain and forwards the request, in this case, to a `fe-content-pages` service pod to serve the request and respond to the client.\n- Generally staging and production would have the same behaviour mappings in Cloudfront and staging will be a place to test these mapping out before setting up in production.\nA future enhancement will be added for branch deploys for manual reviews. This can possibly be accomplished by:\n- Lita command on slack\n- Script is written to deploy to a branch\n- Helm could be used to tear down the pods after the branch is merged\n","We will create a staging environment that will be a replica of the production environment but will be accessible only via our staging sub-domain. Once this is in place, we will configure our CI/CD flow to automatically deploy any new feature branches and pull requests to the staging environment, allowing us to manually test them before merging to the master branch and deploying to production."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs of writing, Zooniverse apps are being moved from Docker Swarm to Kubernetes, but both require a working Docker image of the app for deployment. However, building separate images from start to finish duplicates a lot of effort in downloading packages, building libraries etc.\n\n## Decision\n","The [Jenkinsfile](../../Jenkinsfile) builds Docker images in two stages:\n1. Build an image containing the entire monorepo at the current commit, then install top- and package-level dependencies, and finally build production versions of the library packages. This image is [zooniverse/front-end-monorepo](https://cloud.docker.com/u/zooniverse/repository/docker/zooniverse/front-end-monorepo).\n1. Loop through all folders in the `packages` folder starting with `app-`, and build a Docker image for each one. These Docker images use the `zooniverse/front-end-monorepo` image as a base, and then run the production build and start scripts for that app. An example image is [zooniverse/fe-project](https://cloud.docker.com/u/zooniverse/repository/docker/zooniverse/fe-project).\n","**Decision:** Create a base image which we can use to kickstart the build process for both Docker Swarm and Kubernetes. This will allow us to reuse the work done in building an image (downloading dependencies, building libraries etc.) when deploying to either platform."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. We needed a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.\nThe subjects table in Panoptes is too large to allow for fast querying, so we also needed to build our own indexing system for indexed sets.\n\n## Decision\n","- Project owners can flag subject metadata columns as searchable by prefixing the heading with `%` in the manifest eg. these manifest headings `subject_id,image_name_1,%origin,link,%attribution,license,#secret_description` mark `metadata.origin` and `metadata.attribution` as searchable.\n- Subject sets with indexed subjects have `metadata.indexFields` set to a list of indexed fields eg. `indexFields: 'origin,attribution'`.\n- Subject metadata for indexed sets is copied to a separate database running on [Datasette](https://datasette.io). Each set is given its own table, named by subject set ID. Datasette gives us a RESTful API out of the box, allowing us to browse and search subject data as HTML or JSON. See https://subject-set-search-api.zooniverse.org/subjects.\n- From a volunteers point-of-view, the Datasette service is used to find specific subject IDs to work on. Those IDs are then sent to the Panoptes API `/subjects/selection` endpoint, which returns those subjects, in order, for classification.\n",Implement a search system using Elasticsearch.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMarkdown is generally safer to use than HTML for user submitted content on the web because it limits what a user can submit to predefined allowable strings that can be easily sanitized and then get converted to HTML. Panoptes-Front-End uses markdown through out the entire application. Currently we support an in-house markdown renderer, [markdownz](https://github.com/zooniverse/markdownz), that uses  [markdown-it](https://github.com/markdown-it/markdown-it). The library markdown-it is mature and has several plug-ins available for it that we've added to markdownz as well as some of our own customizations.\nMarkdown, however, isn't totally free from being exploitable, nor is React. Markdownz relies on a React method, `dangerouslySetInnerHTML` that potentially open us to vulnerabilities (see this line: https://github.com/zooniverse/markdownz/blob/master/src/components/markdown.jsx#L99).\nNow that we've adopted Grommet as general React component library, Grommet also provides a React [markdown](https://v2.grommet.io/markdown) renderer ([code](https://github.com/grommet/grommet/blob/master/src/js/components/Markdown/Markdown.js)). Grommet's markdown component uses [markdown-to-jsx](https://github.com/probablyup/markdown-to-jsx) which instead converts markdown to React components to use instead of relying on `dangerouslySetInnerHTML`. However, after extensive evaluation, `markdown-to-jsx` does not have the plugin eco-system that we need and so we would need to rewrite a lot of customizations to get to basic parity with what we already support. This defeats the purpose of reducing the maintenance of our own code for common markdown support.\n\n## Decision\n","We will make a new `Markdownz` React component that will be a part of the Zooniverse React component library. This new component will be built using [`remark`](https://github.com/remarkjs/remark). Remark is a popular markdown rendering library with a good plugin eco-system. It is supported by Zeit, which also supports Next.js, the server-side rendering library we have decided upon.\nHere is how markdown-it's plugins will map to remark's plugins:\n|markdown-it plugin/custom plugin|remark plugin/custom plugin|notes|\n|--------------------------------|---------------------------|-----|\n|markdown-it-emoji|remark-emoji|remark-emoji does not support emoticons like `:-)` but does gemojis like `:smile:`|\n|markdown-it-sub|remark-sub-super||\n|markdown-it-sup|remark-sub-super||\n|markdown-it-footnote|built in|Remark supports this and can be enabled by passing `footnote: true` into its settings object|\n|markdown-it-imsize|N/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we have defined a custom function that will set the `width` and `height` props on the Image component if the sizing syntax is defined in the alt tag of the markup. This is in contrast to the sizing syntax originally being defined in the src markup. We do not want to modify the sanitization remark-react does on source urls, so instead we have moved support of syntax to the alt tag area of the markup|\n|markdown-it-video|deprecating|We are deprecating this because we don't want project owners embedding youtube videos with ads|\n|markdown-it-table-of-contents|remark-toc|This works instead by looking for a heading that has case insensitive `table of contents`, `toc`, or `table-of-contents`|\n|markdown-it-anchor|N/A|Remark has basic anchor support.|\n|twemoji|N/A|Do we really need to use images of Twitter's emojis? Unicode support for emojis is fairly ubitiquous now.|\n|markdown-it-html5-embed|N/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we define a custom function that returns HTML 5 video instead of an image of the src is a video mime-type or returns HTML 5 audio if the src is an audio mime-type|\n|replaceSymbols|our own fork of remark-ping|remark-ping supports our needs for doing at-mentions of users, but it is forked to also support talk hashtags and the subject mentions using `^S`|\n|relNofollow|remark-external-links|This plugin adds nofollow to absolute urls|\n|markdownNewTab|remark-external-links|remark-external-links plugin adds `target='_blank'` and nofollow, noopener, noreferrer to all absolute urls. `+tab+` in front of the url will no longer work because of the sanitization that remark-react does. It may not be a good idea to modify how the sanitization works to allow this and instead just update our users on how this works instead.|\n`remark-react` is added to parse the markdown to jsx which is inherantly safer than using `dangerouslySetInnerHTML` and allows customizing which react components get used for html elements like `markdown-to-jsx`.\n","**Decision:** Continue using markdownz for markdown rendering in Panoptes-Front-End.\n\n**Rationale:**\n\n* markdownz provides the required flexibility and customization through a mature library (markdown-it) and additional plugins.\n* Switching to Grommet's markdown component would require significant development effort to rewrite existing customizations.\n* While markdownz uses `dangerouslySetInnerHTML`, the potential security vulnerabilities are mitigated by our existing sanitization practices."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the FEM Classifier's default ""Subject Viewer sizing/fitting"" behaviour is _""fit to width, no max height""._\nThis is analogous to PFE's special ""no-max-height"" behaviour, which had to be _explicitly set._\nWhile the default ""fit to width, no max height"" behaviour is good in many cases, there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be ""cut off"".\nWe need to consider how to size/fit viewers for a variety of Subjects and Subject Viewers.\n\n## Decision\n","The answer is to use **Layouts**.\n- The plan for FEM is that every workflow should be able to _set their own Layout_ (e.g. portrait, landscape, fullscreen, etc) each with their own Subject Viewer sizing/fitting behaviour.\n- The choice of Layout (presumably per workflow) should be controllable by the project owner.\n- Whether or not a specific subject viewer size configuration is supported will depend on the Layout.\nSee also:\n- The current [Layout code](../../packages/lib-classifier/src/components/Classifier/components/Layout), as of June 2021, currently only has DefaultLayout.\n","**Decision:** Change FEM Classifier's default ""Subject Viewer sizing/fitting"" behaviour to _""fit to viewport, max height <viewport height>""._\n\n**Consequences:**\n\n* **Positive:**\n    * Addresses the issue of the bottom half of tall images being ""cut off"" in wide viewports.\n    * Provides a more consistent behaviour with PFE's default ""no-max-height"" behaviour.\n    * Allows users to better control the size and fit of Subject Viewers.\n* **Negative:**\n    * May not be the optimal behaviour for all workflows.\n    * May require additional configuration for some use cases.\n* **Risks:**\n    * Potential performance issues if the viewport height is very large."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse/Panoptes-Front-End#4992] (https://github.com/zooniverse/Panoptes-Front-End/issues/4992)).\nThe default behavior in existing libraries like [JSON8](https://github.com/sonnyp/JSON8/tree/master/packages/json8#ooserialize) or MobX's [`toJS`](https://mobx.js.org/refguide/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\n\n## Decision\n","We decided to instead implement our own [type conversion utility function](https://github.com/zooniverse/front-end-monorepo/blob/master/packages/lib-classifier/src/store/utils/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\n``` js\nconst workflow = {\nid: '1',\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] // How they will be stored on Panoptes\n}\n```\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\n**A note about the use of arrays for the key-value pairs**\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\n",We decided to implement a custom serialization and deserialization function that preserves the ordering of map key-value pairs and therefore the ordering of workflow steps.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the upcoming the upcoming TESS project (aka Planet Hunters 2019), we need  to create a special Light Curve Viewer component for showing interactive brightness curves.\nFurther information available on the [TESS Front End documentation](https://docs.google.com/document/d/1BcX4PyC2khmtC9g035G2e5I1zirZa3z9mWINkWATaPs/edit?usp=sharing).\n@rogerhutchings, @shaun.a.noordin and @srallen researched available charting libraries, particularly ones designed for seamless integration with React. Of those, prototypes were built with [Plot.ly](https://plot.ly/javascript/react/), [Victory](https://formidable.com/open-source/victory/) and [vanillaD3](https://d3js.org/). Sample data was taken from the Planet Hunters project.\nFrom these, we discovered that:\n1. Both Plot.ly and Victory suffered from slow performance, especially when using their out-of-the-box features (such as the selection tools).\n2. Plot.ly and Victory had some API limitations that made fulfilling spec requirements difficult. Plot.ly, for example, had a limited set of events available (`click` events, but not `mousemove`, `drag` etc).\n3. The out-of-the-box features were often almost perfect, but not quite, and hard to extend. Plot.ly, for example, has a range select tool, but it's designed for selecting data subsets for plotting. To create annotations, we'd have needed to use the range selections and turn them into a variable width bar chart plotted on top of the scatter plot.\n\n## Decision\n","We decided that trying to use a library to meet our fairly unusual requirements would end up causing more work than would be saved by having the basic features available. As such, the Light Curve Viewer will be built using native D3, implemented into the classifier's React components.\n",We decided to build a custom light curve viewer component instead of using an existing charting library.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the development of Panoptes.js, we came across the issue of authentication. Currently, the auth library is part of [panoptes-javascript-client](https://github.com/zooniverse/panoptes-javascript-client), and provides support for first-party login and OAuth2 (for custom front ends).\nHowever, the current client has the following drawbacks:\n- It only provides the relevant API methods and no front end components, which need to be written by the library consumer.\n- As a result, the login experience is slightly different across each project.\n- The OAuth client has been buggy in the past.\n\n## Decision\n","To write a separate auth library. This will expose:\n- the __API helpers__ for the first-party auth process, if the consumer wants/needs to use them.\n- __+a first-party auth client__ as a standalone React app. This will open an IFrame to the existing Panoptes views for sign in, sign out, registration, and profile management (password reset and account deletion)\n- __an OAuth client__, also as a standalone React app. This will wrap an existing package to provide OAuth support, such as [js-client-oauth2](https://github.com/mulesoft/js-client-oauth2), and will handle session length, provide session timeout prompts, and provide a session expiry hook or event for a consumer app (e.g. as a prompt to save the current state).\n","We decided to extract the authentication library out of Panoptes.js into a separate package, called `@zooniverse/auth-client`. This will allow us to:\n\n- Provide a consistent login experience across all projects that use the library.\n- Improve the stability of the OAuth client.\n- Make it easier to develop and maintain the library."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe static content on the Zooniverse (notably, the About, Publications, and Team pages on [PFE](https://github.com/zooniverse/Panoptes-Front-End/)) has traditionally been [baked](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/locales/en.js) [into](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/lib/publications.js) the front end code.\nWhile this works, it creates maintenance issues: to update the list of publications, for example, requires someone to dig through the code to find and modify the correct JSON file, and then create a pull request on GitHub. To do so for multiple publications becomes a long and tedious process.\nAdditionally, JSON is fragile - a missing (or extra) comma can break the build.\nAs a result, we're considering using a content management system (CMS) that would allow us to more easily maintain our static content.\nThese are the options we have:\n1. Continue to hard code content into the front end code\n2. Create our own CMS to manage our content\n3. Install a third party CMS into one of our servers\n4. Use an online third party CMS\n\n## Decision\n","We've decided to use [Contentful](https://www.contentful.com/), an online third party CMS, to host our content. (Or at least, part of our content, such as the often-changing Publications page.) There will be two users registered: one admin account with full access to the Content Model editor _and_ content, and an editor account for day-to-day use which only has content editing access.\n**Reasons for choosing Contentful**\n- [@rogerhutchings](https://github.com/rogerhutchings) has used it for projects in the past\n- It has a free tier\n- It has a well-documented API\n- It has an excellent admin interface for editing content and models\n- It's one of the larger hosted CMS offerings, meaning it's less likely to disappear overnight\n**Pros**\n- CMS makes content management easier and updates faster, especially for non-developers.\n- External platform reduces the need for any sort of code maintenance.\n- We can run our Zooniverse content on the Free Tier (see [Note of Caution 3: Database Limits](#note-of-caution-3-database-limits))\n**Cons**\n- Reliance on external platfrom for storing data introduces the usual third-party risks. (See [Note of Caution 1: Exit Strategy](#note-of-caution-1-exit-strategy)) Possible dangers include:\n- external CMS has downtime, shutting down our content by extension.\n- external CMS changes pricing structure, forcing us out of the free tier.\n- external CMS shuts down, forcing us to find an alternative data store.\n- We need to maintain an admin account. (Shouldn't be a worry as long as we have Passbolt.)\n",Use a third-party CMS installed on one of our servers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https://github.com/zooniverse/panoptes-javascript-client/).\nWhile working on [#1306](https://github.com/zooniverse/front-end-monorepo/issues/1306), I ran into a few issues with it:\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https://12factor.net/config), or whatever you want, but that's up to the consuming app to decide.\n\n## Decision\n","Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\nThe [existing config](https://github.com/zooniverse/panoptes-javascript-client/blob/master/lib/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\n",Switch to using `next-auth` for authentication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\n- Annotation / classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\n\n## Decision\n","What we do not want to do:\n- Re-render on every pointer or touch event.\n- update annotation state while drawing is in progress.\n- support more than one drawing task in a step.\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\nWhat we do want to do:\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse/Panoptes-Front-End#5411\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer/observable event stream.\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan/zoom function. We have two proposed options in implementation:\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\n- Projects have requested each frame to have the same pan/zoom function, but we were unable to implement in PFE: zooniverse/Panoptes-Front-End#3465\n- Are there any cases where projects want separate pan/zoom function for each frame?\n- Have a schema, or set of schemas, describing annotations.\n","**Decision:** Refactor the drawing tools to improve performance, separation of concerns, API simplicity, and payload consistency.\n\n**Rationale:**\n\nThe current implementation of the drawing tools on PFE has several limitations, including:\n\n* Unnecessary re-rendering of the DOM due to frequent updates of the classification annotation\n* Lack of clear separation of concerns between components and stores, making it difficult to debug and add new features\n* Complex API that involves exposing static methods to be called by parent components\n* Inconsistent payload formats for annotation and classification data\n\nBy refactoring the drawing tools to address these limitations, we can improve the performance and maintainability of PFE. The refactored implementation should:\n\n* Update the classification annotation only when necessary, reducing DOM re-rendering\n* Clearly separate concerns between components and stores, making it easier to debug and extend the tools\n* Provide a simpler API that does not require exposing static methods\n* Define consistent payload formats for annotation and classification data"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe plan to implement simple feedback for the survey task. Simple survey task feedback will compare target to actual choice(s), excluding any comparison of choice question answers, multiple selection of the same choice, or any other aspect of the survey task annotation.\nCurrently in PFE, the only feedback related to the survey task is for the Gravity Spy project, and is referred to as ""Gravity Spy Gold Standard"". Gravity Spy Gold Standard is a PFE experimental feature (per workflow configuration object) that checks a subject's metadata for `'#Type' === 'Gold'` and provides feedback after classification completion with the [`gs-gold-standard-summary`](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/classifier/gs-gold-standard-summary.jsx). Survey task annotation values are compared to subject metadata `'#Label'` values. A message is then shown accordingly (i.e. success, failure, or special message per subject metadata `'#post_classification_feedback'`).\n\n## Decision\n","To implement a simple survey task feedback strategy within [the existing feedback framework](https://github.com/zooniverse/front-end-monorepo/tree/master/packages/lib-classifier/src/store/feedback/strategies). The simple survey task feedback will be titled ""Survey: Simple"". The simple survey task feedback will presume the annotation has one annotation value per survey task choice. The subject metadata will include:\n- #feedback_N_id (required) - ID of the corresponding workflow task rule.\n- #feedback_N_choiceIds (required) - comma separated target choice ID(s) (i.e. ""BLIP"" or ""AARDVARK,ELEPHANT,ZEBRA""). A choice's ID is determined at survey task creation and can be viewed in in the Project Builder from the relevant workflow and survey task editor *Raw task data* section or from the workflow data export. The choice ID is the key for the choice object. For survey tasks created since 2017, the choice ID is the choice name per the provided ""Choices"" CSV with non-word characters removed and all uppercase (`[choice name per CSV].replace(/\W/g, '').toUpperCase()`).\n- #feedback_N_successMessage (optional) - message to show when the target is correctly annotated. Overrides the default success message set on the workflow task rule.\n- #feedback_N_failureMessage (optional) - message to show when the target is incorrectly annotated. Overrides the default failure message set on the workflow task rule.\nThe target choice ID(s) provided must match the choice IDs in the annotation values exactly for the annotation to be considered a success. Any missing target choice or any additional annotation choice will result in failure.\nFeedback for annotation value (choice specific question) answers and multiple annotation values of the same choice will be addressed with additional survey task feedback strategies.\n","Implement simple feedback mechanism for the survey task. This feedback will include comparing target to actual choice(s). Any comparison of choice question answers, multiple selection of the same choice, or any other aspect of the survey task annotation will be excluded."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Engaging Crowds project (UK collaboration with The National Archives, Royal Museum Greenwich, Royal Botanical Gardens Edinburgh) involves building an indexing tool to examine levels of user engagement and agency. The indexing tool will be built on the new classifier in the front-end monorepo (FEM). This tool will allow volunteers to select a subject-set or a subject to classify on. All 3 Project Builder projects being built to test the use of the indexing tool will require Dropdown tasks. The Dropdown task is not currently in the Front-end monorepo. We have two options to support the requirements  of these projects: using the existing dropdown task available in Panoptes-Front-End (PFE), the legacy single-page front-end app and build the new indexing tool in the classifier in the FEM or build both the dropdown task and the indexing tool in the FEM. Building the dropdown task in the new classifier in the FEM is something we need to do eventually anyway and we have the opportunity to evaluate how the existing dropdown task functions and how we may want to change it.\nKnown issues for the dropdown task in PFE include:\n- Long selection lists, particularly if there are multiple for cascading select options, can create massive task objects on the workflow resource resulting in slow loading and browser performance and slow exports.\n- The annotations are machine readable unique identifier strings to support cascading dropdowns. Machine readable annotations make analyzing the post-classification extraction and aggregation more complicated, particularly because of workflow versioning and translations. Caesar does not store the workflow task contents and so project owners have to reference the original workflow task by version in an export to get meaningful aggregations for actual study.\n- The dropdown task also allows for user submitted values which essentially adds in a text task into the dropdown task. The annotation includes a boolean which, if false, lets the aggregation code know that this annotation should no longer aggregate as a dropdown task, but as a text task.\n### Sample task and annotation JSON from PFE dropdown task\n**PFE dropdown task structure**\n``` json\n{\n""T0"":{\n""help"":"""",\n""next"":""T1"",\n""type"":""dropdown"",\n""selects"":[\n{\n""id"":""070b610fbf5d9"",\n""title"":""Favourite colour"",\n""options"":{\n""*"":[\n{\n""label"":""Red"",\n""value"":""hashed-value-R""\n}\n]\n},\n""required"":false,\n""allowCreate"":false\n}\n],\n""instruction"":""Choose your favourite things""\n}\n}\n```\n**PFE dropdown task annotation structure**\n```json\n{\n""annotations"":[\n{\n""task"":""T0"",\n""value"":[\n{\n""value"":""hashed-value-R"",\n""option"":true\n}\n]\n}\n]\n}\n```\n**PFE dropdown task translation strings**\n```json\n{\n""tasks.T0.help"": ""Pick a colour from the menu."",\n""tasks.T0.instruction"": ""Choose your favourite things."",\n""tasks.T0.selects.0.title"": ""Favourite colour"",\n""tasks.T0.selects.0.options.*.0.label"": ""Red"",\n""tasks.T0.selects.0.options.*.1.label"": ""Blue""\n}\n```\n\n## Decision\n","We will develop a simplified dropdown task in the new classifier in the FEM. Creating a simple dropdown task contains the following functionality:\n- Limited dropdown list options of a minimum (4) and of a maximum (20) number of options (justification: if less than 4, this can be a single choice task using radio buttons)\n- No cascading, dependencies, or effect on other select inputs\n- No free-text entry on the dropdown. It will be recommended to project builders use the new [workflow steps](https://github.com/zooniverse/front-end-monorepo/blob/master/docs/arch/adr-05.md) feature to have a dropdown task and text task in a single step.\nMore complex dropdown tasks will be built in the future as separate task types based on analysis of actual usage in PFE. These will include:\n- Cascading dropdowns for\n- Locations\n- Custom (TBD)\n- Date picker\n- Possibly a text input with validation rather than dropdowns\n- Asynchronous loading long lists by text input search\n### Proposed task and annotation JSON structure examples\n**Task**\n``` json\n{\n""T0"":{\n""help"": """",\n""type"": ""dropdown-simple"",\n""options"":[\n""Red"",\n""Blue"",\n""Green"",\n""Yellow""\n],\n""required"": false,\n""allowCreate"": false,\n""instruction"": ""Choose your favourite colour""\n}\n}\n```\n**Annotation**\n```json\n{\n""annotations"": [\n{\n""task"":""T0"",\n""value"": {\n""value"": 1,\n""option"": true\n}\n}\n]\n}\n```\n","Based on the identified issues with the existing dropdown task in PFE, the better decision is to build both the dropdown task and the indexing tool in the FEM. This will allow us to address the known issues and build a more robust and maintainable solution in the long run. Additionally, building the dropdown task in the new classifier in the FEM will provide an opportunity to evaluate how the existing dropdown task functions and how we may want to change it."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need internationalization support for the new app-project, lib-classifier, and any other libraries that will have volunteer facing content. We should decide on an organizational structure that makes it easy for both translation volunteers and Zooniverse developers to maintain. Currently translation locale files exist in a single folder in projects like Panoptes-Front-End that support translations. The development convention we've adopted for the rewrite, however, would have the locale files in the same folder of each component.\n@eatyourgreens notes:\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https://github.com/zooniverse/zooniverse-readymade) and [zooniverse](https://github.com/zooniverse/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\n\n## Decision\n","We have a few proposed options:\n- A `lib-locales` library that has all of the locale files for the strings. Could be a single file or a directory of folders and files. The library could be imported into the other libs or apps where needed.\n- Larger locale files quickly get unwieldy to edit, and hard to find errors in. Moving them into one place reduces the portability of components. We could write a couple of scripts to bundle up locale files for our translators, and split them out again afterwards [back into their component folders in each library].\n",Create one folder per component in lib-classifier and app-project for locale files. Make the locale files available to the components through Babel. Keep the single folder in projects like Panoptes-Front-End for locale files.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\n\n## Decision\n","- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\n",Allow volunteers to (easily) go backwards and forwards through the subject queue to review previous subjects within a given subject set.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCode must be organized to be compatible with:\n- Static type-checking via [mypy](https://github.com/python/mypy)\n- Runtime execution during normal usage and running tests via [pytest](https://docs.pytest.org/en/stable/)\n- Static doc generation via [sphinx-autodoc-typehints](https://github.com/agronholm/sphinx-autodoc-typehints)\nAdditionally:\n- Functions should be able to refer to any type\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively / cyclically.\n\n## Decision\n",Put types (`@dataclass(frozen=True)`) into the `_types` module\nand have all function modules depend on the `_types` module to define their inputs and outputs.\n,"Organize the code according to the following pattern:\n- Subpackage for each high-level module (e.g., files, directories, archives)\n- If no natural sub-packages are present, all modules will be placed in a single sub-package.\n- All types (classes, functions) must be explicitly added to the `__init__.py` module of each sub-package where they are used."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are multiple, equally-effective ways to implement many features.  In some cases, the most\nstraightforward implementation might involve making more API calls than are strictly necessary\n(e.g. `tc.dataset.create` makes an additional call to retrieve the created dataset from the server\nto construct the returned `Dataset`).\n\n## Decision\n","The simplest and most understandably-written implementation of a feature should be prioritized over\nperformance or reducing the number of API calls.  When real performance issues are identified,\noptimization should be done on an as-needed basis.\n","**Decision:** Make the most straightforward implementation the default.\n\n**Rationale:**\n\n* Simplifies development and maintenance.\n* Minimizes the number of API calls required for common operations.\n* Conforms to the principle of least resistance, which encourages developers to choose the simplest option available.\n\n**Consequences:**\n\n* May result in additional API calls for some operations, potentially impacting performance.\n* May create a dependency on the server-side implementation of the API, which could potentially change in future releases."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nReproducing results from a program is challenging when operating systems, language versions, and dependency versions can vary.\nFor this codebase, we will focus on consistent Python versions and dependency versions.\n\n## Decision\n",Manage multiple Python versions via [pyenv](https://github.com/pyenv/pyenv).\nManage dependencies via [poetry](https://python-poetry.org/).\nDefine tests via [nox](https://nox.thea.codes/en/stable/).\nRun tests in automation/CI via [Github Actions](https://github.com/features/actions).\n,Use a virtual environment to manage Python versions and dependency versions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\n\n## Decision\n",Type-check via [mypy](http://mypy-lang.org/).\n,Use a static type-checker for Python to improve code quality and reduce the number of runtime errors.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython has many ways of importing:\n```python\n# option 1: import module\n# option 1.a\nimport foo.bar.bazaar as baz\nbaz.do_the_thing()\n# option 1.b\nfrom foo.bar import bazaar as baz\nbaz.do_the_thing()\n# option 2: import value\nfrom foo.bar.bazaar import do_the_thing\ndo_the_thing()\n```\nNot to mention that each of these styles may be done with relative imports (replacing `foo.bar` with `.bar` if the `bar` package is a sibling).\nConfusingly, Option 1.a and Option 1.b are _conceptually_ the same, but mechanically there are [subtle differences](https://stackoverflow.com/questions/24807434/imports-in-init-py-and-import-as-statement/24968941#24968941).\n\n## Decision\n","Imports within `tamr_client`:\n- Must import statements for modules, classes, and exceptions\n- Must `from foo import bar` instead of `import foo.bar as bar`\n- Must not import functions directly. Instead import the containing module and use `module.function(...)`\n- Must not use relative imports. Use absolute imports instead.\n","**Import by Value for Import Simplicity**\n\n**Rationale:**\n\n* **Less confusion:** Value import (Option 2) directly imports the desired function/class, reducing ambiguity compared to Option 1 (import module).\n* **Simpler syntax:** Option 2 uses a single `from` statement without additional as clauses, making it easier to read and maintain.\n* **No subtle differences:** Option 2 avoids the subtle differences between Option 1.a and Option 1.b, simplifying code comprehension and debugging."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a reasonable tradeoff between ease-of-use and maintainability.\nSpecifically, we need composable, combinable units that can be improved independently.\n### Approach 1: Classes + Methods\nOne approach is to embrace Object-Oriented Programming (OOP) with fluent interfaces (i.e. method chaining):\n```python\nproject\n.create(...)\n.update(...)\n.delete(...)\n```\nCharacteristics:\n- Ease-of-use is maximized, but this requires each method to `return self`.\n- Also, this approach implies that if a function can be called with X different object types,\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\nHow to enforce these characteristics?\nAny solution will be a tax on maintainability, as code that adheres to these characteristics will include many non-semantic lines simply going through the motions of `return self` and copying function usage into dedicated methods for each class.\n### Approach 2: Types + Functions\nAnother approach is to embrace a functional programming style: simple types and functions (no methods).\nUsage is not as terse as for OOP:\n```python\np = tc.project.create(...)\nu = tc.project.update(p, ...)\nd = tc.project.delete(p, ...)\n```\nCharacteristics:\n- Ease-of-use is not optimized, but still reasonable.\n- With tab-completion, ease-of-use is comparable to OOP.\n- Each type can be made immutable\n- Each function can be made pure\n- Functionality can be shared by calling the same function in user-land, not copying function calls in contributor-land.\n\n## Decision\n",Use `@dataclass(frozen=True)` to model types and plain Python modules and functions to capture business logic.\n,"**Approach 2: Types + Functions**\n\nAdopt a functional programming style with simple types and functions for the following reasons:\n\n* **Maintainability:** Enforces immutability and purity, reducing complexity and potential errors.\n* **Composability:** Functions allow for easier composition and combination of units.\n* **Independent improvement:** Functions can be updated independently, reducing the impact on other parts of the system.\n* **Reasonable ease-of-use:** Tab completion provides comparable convenience to method chaining, while ensuring code remains maintainable."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\nBut, we want to rearchitect this codebase as a [library of composable functions](/contributor-guide/adr/0005-composable-functions).\n\n## Decision\n",Implement rearchitected design as a new package named `tamr_client`.\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\n,Encapsulate the existing user-facing interface in a composable function. This will allow us to maintain backwards compatibility while still being able to refactor the underlying codebase.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDocumentation can take four forms:\n1. Explanation\n2. Tutorial\n3. How-to\n4. Reference\nWe need a way to author and host prosey documentation and generate reference docs based on source code.\n\n## Decision\n,"Doc compilation will be done via [sphinx](https://www.sphinx-doc.org/en/master/).\nProsey documentation (1-3) via [recommonmark](https://github.com/readthedocs/recommonmark).\nReference documentation (4) will be generated based on type annotations and docstrings via:\n- Automatic docs based on docstrings via [sphinx-autodoc](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), [sphinx-autodoc-typehints](https://github.com/agronholm/sphinx-autodoc-typehints)\n- Google-style docstrings via [napoleon](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html)\n- Hosting on [ReadTheDocs](https://readthedocs.org/) (RTD)\n- Build docs in CI and fail on errors or warnings.\n","Use Docusaurus with Sphinx plug-in.\n\nDocusaurus is a modern static site generator that can be used to author and host prosey documentation. It is built on top of React and provides a number of features out of the box, such as:\n\n* **Easy-to-use interface:** Docusaurus is designed to be easy to use, even for non-technical users.\n* **Mobile-friendly design:** Docusaurus sites are mobile-friendly, so users can easily read your documentation on any device.\n* **Search engine optimization:** Docusaurus sites are optimized for search engines, so your documentation can be easily found by users.\n\nThe Sphinx plug-in for Docusaurus allows you to generate reference documentation based on your source code. This can be helpful for providing detailed information about your codebase to developers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nInconsistent code formatting slows down development and the review process.\nCode should be linted for things like:\n- unused imports and variables\n- consistent import order\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\n\n## Decision\n","For linting, use [flake8](https://flake8.pycqa.org/en/latest/) and [flake8-import-order](https://github.com/PyCQA/flake8-import-order).\nFor formatting, use [black](https://github.com/psf/black).\n",Use a linter to automatically format and check code.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** To use an Architectural Decision Record (ADR) template to document the architectural decisions made on this project. The ADR template should include the following sections:\n\n* **Title:** A short, descriptive title for the decision.\n* **Context:** A brief description of the problem or issue that the decision addresses.\n* **Decision:** The decision that was made.\n* **Rationale:** The reasons why the decision was made.\n* **Consequences:** The potential consequences of the decision.\n* **Status:** The current status of the decision (e.g., Proposed, Approved, Implemented).\n* **References:** Any relevant documentation or resources that support the decision.\n\nThe ADR template should be used to document all significant architectural decisions made on the project. The ADRs should be stored in a central repository and reviewed regularly to ensure that they are up to date and reflect the current state of the architecture."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are 2 possible solutions within the table loading part of the Armadillo.\n- Make use of `.RData` files to store the data frames you want to use\n- Make use of `.parquet` files to store the data frame you want to use\nIn the Armadillo we have a general concept of projects. These projects represent all the data of a cohort or study specific data. e.g. It is the only level you can set permissions.\n- gecko-all\n- gecko-diabetes\n### Managing data with `.RData` files\nWithin the projects we have `.RData` files containing all the tables (R data frames) that are bound in a specific context.\n- gecko-all\n- 1_0_core_1_1.RData\n- non_rep\n- yearly_rep\n- monthly_rep\n- trimester_rep\n- 1_0_outcome_1_1_non_rep.RData\n- gecko-diabetes\n- 1_0_core_1_1.RData\n- non_rep\n- yearly_rep\n#### Make the data available\nWhen you login to the Armadillo you specify which `.RData` files you want to have available in your analysis environment.\n`folder=gecko-diabetes/1_0_core_1_1&folder=gecko-all/1_0_outcome_1_1`\nSpecifying these folders (`.RData` files) allow you to assign all the columns that are available in the `data.frame` that are in the `.RData` files.\nTechnically it loads the `RData` files in memory to be able to see which tables are available.\n#### Assigning the data\nWhen you want to use the data in a DataSHIELD analysis you need to assign the data. So when you login the data becomes available in a private environment in R. When you assign data it will be copied to the analysis environment in R.\nYou need to assign all data or parts of the data depending on the analysis you want to do. Assigning parts of the data is more efficient and analysis will become faster.\nIn the `RData` solution all the data in the private and the analysis environment is in memory. When you assign the data, it will be copied to the analysis environment. You can specify a subset of the data as well. The copy will contain only the selected variables.\n#### Advantages\n- You can use native `R` components to get the data available for your analysis\n- You can implement *resources* fairly easy in `.RData`\n#### Disadvantages\n- It uses large amounts of data per user when loading large files. Which means you can not work with a large number of users at the same time on the Armadillo.\n- Logging in takes a lot of time, because folders are `.RData` files and contain all the tables. Tables need to be loaded to be checked by the DataSHIELD package.\n### Managing data in `.parquet` files\nManaging data in `.parquet` files allows us to deal with larger data more efficiently. This changes the concepts a little within our file storage (minio). We already work with buckets as described above. `Parquet` files do not allow more than one `data.frame` in one `parquet` file. The `parquet` files are more or less substitutes for `RData` files. The logical structure is the same, however the technical structure is somewhat different.\n- gecko-all\n- 1_0_core_1_1 (minio -> folder concept)\n- non_rep.parquet\n- yearly_rep.parquet\n- monthly_rep.parquet\n- trimester_rep.parquet\n- 1_0_outcome_1_1_non_rep (minio -> folder concept)\n- gecko-diabetes\n- 1_0_core_1_1 (minio -> folder concept)\n- non_rep.parquet\n- yearly_rep.parquet\n#### Make the data available\nWhen you login you only specify the fully quelified names of the tables you want to load:\n`project/folder/table.parquet`\nFor example:\n`gecko-all/1_0-core-1_1/non-rep.parquet`\nSpecifying these projects and folders allow you to assign all the columns that are available in the `data.frames` that are in the `.parquet` files.\nThis is not loading the folders (`RData` files) in memory, but loads them when you assign the tables. At login time you only check wether you may access the projects.\n#### Assigning the data\nThe difference with the `RData` solution is that the files will not be loaded in memory initially only when the data is assigned. More specifically only the selected data will be assigned. You do not have a copy of the whole data in memory in a private environment.\n#### Storing complex structures\nAnother format to store complex structures is `rds`. The `rds` format is capable to store 1 object in 1 file. You can directly see what is in the file. With `RData` you need to load the file when you want to know what it is containing. In `rds` you see directly what the contents is.\n#### Advantages\n- You do not need to specify the folder(s) (`RData` files) or table(s) during login which makes it faster\n- You are able to delay loading the data in the R environment, which makes it possible to to login faster\n- When you specify specific columns from the dataset it is highly efficient to load the data in the R environment\n- Loading and saving parquet files is faster, so data handling actions will be more efficient.\n#### Disadvantages\n- An extra dependency in the package\n- More risk when converting that is not R-native (think of dates, number of significant bits, etc.)\n\n## Decision\n",The principle we are going implement is to store 1 object per file. Which means that for table-like object we are going to use `.parquet`and for more complex structures we are going to use `.rds` (such as *resources* used in Opal).\nWe are going to use the parquet files in the Armadillo package. As data manager you do not *have* to work with the parquet format.\n,"The decision is to use `.parquet` files to store the data frames you want to use within the Armadillo.\n\n**Reasons for the decision:**\n\n- `.parquet` files are more efficient for dealing with larger data, which is important for the Armadillo's use case.\n- `.parquet` files allow for more efficient loading and saving, which will make data handling actions more efficient.\n- `.parquet` files are faster to load than `.RData` files, which will make logging in to the Armadillo faster.\n- `.parquet` files allow for more efficient assignment of specific columns from the dataset, which will make data handling actions more efficient."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn LifeCycle we use a 2 step approach to share the data within the LifeCycle project.\n**Step 1** - Datasets containing all the harmonised data within LifeCycle\n- lc_core_gecko_1_1 - *projects in Opal, workspaces in the Armadillo*\n- 1_1_core_non_rep_1_1 - *tables in Opal, R data.frame objects in Armadillo*\n- 1_1_core_yearly_rep_1_1\n- 1_1_core_monthly_rep_1_1\n- 1_1_core_trimester_rep_1_1\n- lc_outcome_gecko_1_1 - *projects in Opal, workspaces in the Armadillo*\n- 1_1_core_non_rep_1_1 - *tables in Opal, R data.frame objects in Armadillo*\n- 1_1_core_yearly_rep_1_1\n- 1_1_core_monthly_rep_1_1\n- 1_1_core_trimester_rep_1_1\n**Step 2** - Study specific datasets which contains only the variables you want to expose\n- lc_core_gecko_1_1 - *projects in Opal, RData files in the Armadillo*\n- 1_1_core_non_rep_diabetes_1_1 - *views in Opal, R data.frame objects in Armadillo*\n- lc_core_gecko_1_1 - *projects in Opal, RData files in the Armadillo*\n- 1_1_core_non_rep_diabetes_1_1 - *views in Opal, R data.frame objects in Armadillo*\n### Versioning\nIn the current version scheme of LifeCycle we distinguish 2 versions:\n- dictionary version\n- data version\nThe scheme is as shown below:\n`y_y_#dictionary_kind#_#cohort#_x_x`.\nThe `y_y` version = the dictionary version (data model version)\nThe `x_x` version = the data version (version of the data release)\n### Accessing data\nBased upon the cohort guidelines on exposing data to researchers one way or the other is chosen to give researchers access to the data.\nIn Opal you need both datasets.\n- a dataset containing all the harmonised data\n- (if applicable) views to expose specific datasets\nThese views do not contain the data, only the representation of the variables. In Opal you can manage permissions per view, which makes it possible to put all the tables and views related to one dictionary version in one project.\n\n## Decision\n",Within the Armadillo we have the ability to nest the dictionaries. Which makes it possible to specify the dictionary version and data version on a sublevel. The structure in the Armadillo will be used in the following manner:\n- gecko-all\n- 2_1-core-1_0\n- non-rep\n- yearly-rep\n- monthly-rep\n- trimester-rep\n- 1_1-outcome-1_0\n- non-rep\n- yearly-rep\n- monthly-rep\n- weekly-rep\n- gecko-diabetes\n- 2_1-core-1_0\n- non-rep\n- yearly-rep\nWe currently do not support more than 1 level of nesting folders in a project.\nThree levels can be distinguished:\n### 1. Project level\nThis can be all the data or a study specific data. On this level we can manage permissions.\n### 2. Folder level\nThe second level contains the collection of data frames or tables you want to expose. This usually contains the version of the model and data release as well.\n### 3. Table level\nYou can interpret this level as the data level containing tables that can be queried.\n,"**Decision:** Researchers should access data via Opal datasets and projects, rather than direct access to RData files in Armadillo.\n\n**Rationale:**\n\n* **Cohort guidelines:** The cohort guidelines specify that data should be exposed to researchers through specific datasets and projects in Opal.\n* **Permission management:** Opal allows for granular permission management per view, enabling researchers to access only the data they are authorized to use.\n* **Data integrity:** Opal ensures the integrity of the data by controlling access and maintaining a consistent data model.\n* **Data lineage:** Opal provides a clear audit trail of data access and usage, which is important for reproducibility and compliance.\n* **Version control:** Opal tracks both dictionary and data versions, allowing researchers to access the correct version of the data for their study.\n* **Ease of use:** Opal provides a user-friendly interface for researchers to access data, reducing the technical burden on them."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn this project we manage the data in the Armadillo suite.\n\n## Decision\n,We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\n,"We will use a NoSQL database, such as MongoDB, to manage the data in the Armadillo suite."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n","We will use the Architectural Decision Record (ADR) template to record architectural decisions made on this project. This template provides a structured way to document the decision, its context, and the rationale behind it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\n* There are existing client libraries for the S3 API.\n* It is nontrivial to proxy large file uploads through the armadillo server.\n\n## Decision\n,The Armadillo client will be written as a high-level library on top of an existing S3 API.\n,Do not build an admin UI within the armadillo microservice
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Data management\nData managers (in LifeCycle) use R to upload their data into Opal as this stage. Data managers often create scripts to generate the (harmonised) data they need to expose for a certain collection of variables.\n### Usage Armadillo\nWe are required to upload .RData files into the Armadillo service to be used in DataSHIELD. So .RData files need to be created before it can be pushed to the service.\n### Possible solutions\nPossible solutions regarding buildframeworks are R, Python or Java. Every framework has pros and cons.\n**R**\nTo connect to the current way of working with the data in LifeCycle, R is the most integrated platform to use. You can use the Armadillo client to integrate in the existing scripts of the data managers. You do not need a lot of training to incorparate the Armadillo client in the workflow. RData is the file format Armadillo uses and R is the platform to build .RData in which makes it easier to build the RData file in. When there is a need to do basic checks in the future, you can implement them.\n**Python**\nPython can be used together in one script with the molgenis-commander. Which is usefull when you use the MOLGENIS data provider. It allowes you to automate the whole process, from extracting data from MOLGENIS to converting into the right format for the Armadillo service (if Python supports generating .RData). From there you can upload it into the Armadillo. Another advantage is that Python is a language we are more experienced to program in.\nA disadvantage is that at this point prospect customers are not used to work with python. You will need to teach them how to use Python and then how to use the client.\n\n## Decision\n",The Armadillo client will be written based on R to integrate easily in the current prospect customers infrastructure. We will look at the possibility of writing other clients in the near future when we need to add other customers as well.\n,"**Decision:** Use R as the primary build framework for creating .RData files for Armadillo.\n\n**Reasoning:**\n\n* **Integration with current data management practices:** R is closely integrated with the existing data management workflows in LifeCycle, where data managers use R scripts to generate harmonized data for Opal. This allows for a seamless transition and minimizes disruption to current processes.\n* **Simplicity and ease of use:** R is familiar to data managers and does not require extensive training to incorporate the Armadillo client into existing scripts.\n* **Direct conversion to .RData format:** RData is the native file format used by Armadillo. This eliminates the need for additional conversion steps or dependencies on third-party libraries.\n* **Future potential for data validation:** R provides a convenient platform for implementing basic data checks and validation routines as needed in the future.\n* **Alignment with customer familiarity:** While Python offers automation benefits, prospect customers are currently more familiar with using R. Choosing R reduces the training burden and improves user adoption."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nNeed to create a way for the player to interact with their character and potential with their allies such that there is choice involved in combat on the player side instead of randomly decided actions.\n\n## Decision\n,Create a child class of character to give branching functionality to player-specific characters.\n,**Decision:** Implement a turn-based combat system that allows players to select actions for their characters and allies from a set of available options.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWant to give more variation to the combat system beyond physical attacks. This will allow for reasons to create separate builds and more options for player characters and enemies to use during a fight. This will give more of a *game* element to the combat as opposite to a system based on luck alone.\n\n## Decision\n,Create spells and integrate them into the character and combat systems.\n,Implement a magic system that allows players and enemies to cast spells with varying effects.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nNeed to create a basic structure for combat involving characters that know how to fight and a middle man referee that ensures that everyone gets a turn and works as a go between to ensure that the characters interact properly. This will create a basic means for the game to function.\n\n## Decision\n,There should be a combat module specific to the characters and an overarching combat entity that handles the mediation.\n,Implement a **turn-based combat system** with a **game master (GM)** who mediates player actions and ensures fair play.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWe need a way for player characters to be generated intentionally where certain statistics can be decided on by the player instead of having a random outcome. This will give the player some opportunity to have some control over their character's strengths and weaknesses and add to the playability of the game.\n\n## Decision\n,Create a player module with user interaction to affect the starting statistics of a character.\n,"**Decision:** Implement deterministic character generation with configurable parameters.\n\n**Details:**\n\n* Allow players to specify values for key statistics, such as strength, intelligence, and agility.\n* Provide sliders or input fields for players to adjust these values within predefined ranges.\n* Provide guidance and recommendations to help players understand the impact of their choices on gameplay.\n* Ensure that the total stat points assigned remain within a reasonable limit to maintain game balance.\n* Supplement deterministic character generation with a limited number of randomly generated secondary stats to maintain some element of chance.\n\n**Rationale:**\n\n* Gives players agency in shaping their character's abilities, fostering ownership and investment.\n* Enhances gameplay by allowing players to customize their characters to suit their playstyles and strategic preferences.\n* Balances player control with game balance by allowing for configurable stats within predetermined limits.\n* Preserves a degree of randomness in secondary stats to introduce an element of uncertainty and surprise."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWhile we have a class for items already, we need a way for them to be used during combat. This way we can add in more items that will change the flow of battle and eventually a money and shop system to add to combat variation.\n\n## Decision\n",Create a way to use items and integrate into the combat flow.\n,"We should create a separate class for combat items that will inherit from the base item class. This will allow us to add additional properties and methods specific to combat items, such as damage, healing, or status effects."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nIt is desirable that we have a way for characters to be randomly generated with different statistics and abilities in order to keep combat fresh and new with an element of surprise. Creating a way to generate random character systematically will keep us from having to create specific new characters.\n\n## Decision\n,Create a random character factory that will pump out characters.\n,"**Decision:** Implement a random character generator that can create characters with varying statistics and abilities.\n\n**Rationale:**\n\n* Provides a systematic way to create random characters, reducing the need for manual creation.\n* Ensures that characters have diverse statistics and abilities, keeping combat fresh and preventing it from becoming repetitive.\n* Introduces an element of surprise by creating characters with unpredictable abilities.\n\n**Implementation Details:**\n\n* Define a set of possible statistics and abilities for characters.\n* Create a system that randomly selects values for these statistics and abilities within a specified range.\n* Implement logic to generate a unique combination of statistics and abilities for each character.\n* Allow for customization options to enable adjustments to the random generation process.\n\n**Benefits:**\n\n* Automates character creation, saving time and effort.\n* Enhances gameplay variety by introducing diverse characters.\n* Reduces predictability in combat, making battles more engaging.\n\n**Drawbacks:**\n\n* May require careful balancing to ensure that characters are not overpowered or underpowered.\n* May require additional development time to implement and maintain."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala / Play / Twirl implementations of the components provided\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\nplay-frontend implementations diverge from their “base” repositories?\n## Decision Drivers\n* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\n\n## Decision\n","* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\nGDS.\n### Positive Consequences\n* Design of case classes to follow GDS / HMRC design system means PlatUI as library maintainers do not have to create\nviewmodel structure from scratch every time\n* Adding new components can follow a clear and straightforward path\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\n### Negative Consequences\n* Feedback suggests that some developers do not find the API to be intuitive\n* Separate decisions need to be made on handling multilingual support\n* Enrichment of library needs to be done via separate Twirl helpers\n(see https://github.com/hmrc/play-frontend-hmrc/blob/main/docs/maintainers/adr/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\n","The play-frontend implementations should diverge only as much as is necessary to integrate them with the Scala / Play / Twirl ecosystem. \nIn particular, the:\n*  functionality should match the Nunjucks versions as closely as possible\n*  APIs should be as similar to the Nunjucks versions as possible (with the obvious variations necessary to reflect the differences between Scala and JavaScript)\n* tests should ensure that the Scala / Play / Twirl versions behave as closely as possible to the Nunjucks versions"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nplay-frontend-govuk relies on a webjar for [alphagov/govuk-frontend](https://www.github.com/alphagov/govuk-frontend)\npublished to www.webjars.org. This has a number of drawbacks:\n* publishing is a manual process\n* it can take many hours to complete\n* webjars.org has been down in the past and HMRC has no support arrangements with webjars.org\nThe main impact of the above is an excessive lead time for making improvements in the\nunderlying govuk-frontend library available in production via play-frontend-govuk.\nPreviously we considered self-publishing this WebJar, as we did for the hmrc-frontend WebJar (see related [ADR]((../adr/0009-self-publish-webjar.md)\n)). However, this is complicated by the fact that we do not own govuk-frontend and\nself-publishing would involve additional engineering and ongoing maintenance.\nSince v0.42.0 of [play-frontend-hmrc](https://github.com/hmrc/play-frontend-hmrc/releases/tag/v0.42.0), we have recommended teams\nuse the minified CSS and JS bundles provided as part of the now self-published `hmrc-frontend` webjar via `hmrcHead` and `hmrcScripts`.\nThese bundles include the CSS and JS from both govuk-frontend and hmrc-frontend. For teams using this approach, the only\nassets still being retrieved from the govuk-frontend webjar are the handful of icons referenced\nin `govukTemplate` and `govukHeader`. Architecturally, this split of assets between the two libraries is awkward and a potential source\nof confusion for future users and maintainers, addressing it would reduce complexity and ease maintenance.\nBearing the above in mind, should we remove the hard dependency on the govuk-frontend webjar by:\n* inlining the images into this repository, for teams directly using `govukTemplate` `govukLayout` or\n`govukHeader`\n* provide a mechanism to override the `assetsPath` parameter in\n`govukTemplate` and `govukHeader` so that `play-frontend-hmrc` can supply its own images and\n* remove the govuk-frontend webjar dependency now or at some point in the future?\n## Decision Drivers\n* The need to make improvements and upgrades to govuk-frontend\navailable in play-frontend-govuk quickly\n* The increasing user base of play-frontend-govuk, and accelerating demand for new features and\nimprovements.\n* The desire to reduce boilerplate in consuming services.\n* The high number of services still referencing `lib/govuk-frontend/govuk/all.js`\n* The fact that the images have not changed for a long time (since September 2018)\n* The need to minimise the impact of breaking changes on service teams.\n* The hardship, frustration and toil the current manual publishing process is causing the team.\n\n## Decision\n","* The need to make improvements and upgrades to govuk-frontend\navailable in play-frontend-govuk quickly\n* The increasing user base of play-frontend-govuk, and accelerating demand for new features and\nimprovements.\n* The desire to reduce boilerplate in consuming services.\n* The high number of services still referencing `lib/govuk-frontend/govuk/all.js`\n* The fact that the images have not changed for a long time (since September 2018)\n* The need to minimise the impact of breaking changes on service teams.\n* The hardship, frustration and toil the current manual publishing process is causing the team.\nChosen option: Option 2, because doing so will (a) put is in a better position to eliminate the use of the webjar eventually,\n(b) allow us to make changes to play-frontend-hmrc to eliminate the `/govuk-frontend` route and (c) not introduce any\nbreaking changes.\n","Remove the dependency on govuk-frontend by inlining the images and providing a mechanism to override the assetsPath parameter in govukTemplate and govukHeader.\n\nThis will allow us to make improvements and upgrades to govuk-frontend available in play-frontend-govuk quickly, and will also reduce boilerplate in consuming services. The high number of services still referencing lib/govuk-frontend/govuk/all.js indicates that this is a widespread issue.\n\nInlining the images will also reduce the number of requests that are made to webjars.org, which will improve performance. The fact that the images have not changed for a long time means that this is a low-risk change.\n\nProviding a mechanism to override the assetsPath parameter in govukTemplate and govukHeader will allow service teams to continue to use the govuk-frontend webjar if they need to, but will also give them the flexibility to use their own images if they prefer.\n\nThis change will be made in a future release of play-frontend-govuk."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nplay-frontend-govuk is intended as a direct Scala/Twirl port of govuk-frontend that can in theory be\nused by any government department wanting a Scala/Twirl implementation of the GOV.UK design system.\nplay-frontend-hmrc includes play-frontend-govuk while adding to it HMRC/MDTP specific components and helpers. Examples include\nstandardised headers and footers, standardised HMRC content for GOV.UK\ncomponents in both English and Welsh, support for cookie consent, language switching, session timeout,\naccessibility statements and more.\nInlining play-frontend-govuk into play-frontend-hmrc has the potential to simplify our\ndevelopment and testing workflow and reduce lead times for making improvements\navailable to service teams.\nGiven we are not aware of any scenarios where it would be desirable for HMRC services to\nuse play-frontend-govuk on its own nor any other government departments using play-frontend-govuk, should we fold the\ngovukfrontend package into play-frontend-hmrc and archive the play-frontend-govuk library?\n## Decision Drivers\n* The confusion and discoverability issues that having two similarly named libraries creates for service developers.\n* The extended lead times caused by the need to publish play-frontend-hmrc every time a change is made to play-frontend-govuk.\n* The additional cognitive friction created for maintainers in deciding which library to add changes to while maintaining\nthe conceptual integrity of both.\n* The split of user and maintenance documentation between the two libraries.\n* The duplication of code and build scaffolding between the two libraries, for example, implicits and fixture generation\n* The overhead of supporting two libraries rather than one, for example, dependency upgrades or security fixes.\n* The question around whether having two separate libraries helps enforce the conceptual integrity of the uk.gov.hmrc.govukfrontend\npackage as a pure Twirl port of govuk-frontend free from HMRC/MDTP specific features.\n* The need to maintain the existing test strategy and not have this compromised by the merging of the two libraries.\n* The desirability of maintaining git commit history for the merged govukfrontend package.\n* An historical view, now superceded, that teams should be able to use play-frontend-govuk on its own.\n* The desirability of providing a Twirl port of govuk-frontend to other government departments.\n* The impact on teams currently using only play-frontend-govuk.\n* The possibility of other government departments using play-frontend-hmrc and ignoring any HMRC-specific features.\n\n## Decision\n","* The confusion and discoverability issues that having two similarly named libraries creates for service developers.\n* The extended lead times caused by the need to publish play-frontend-hmrc every time a change is made to play-frontend-govuk.\n* The additional cognitive friction created for maintainers in deciding which library to add changes to while maintaining\nthe conceptual integrity of both.\n* The split of user and maintenance documentation between the two libraries.\n* The duplication of code and build scaffolding between the two libraries, for example, implicits and fixture generation\n* The overhead of supporting two libraries rather than one, for example, dependency upgrades or security fixes.\n* The question around whether having two separate libraries helps enforce the conceptual integrity of the uk.gov.hmrc.govukfrontend\npackage as a pure Twirl port of govuk-frontend free from HMRC/MDTP specific features.\n* The need to maintain the existing test strategy and not have this compromised by the merging of the two libraries.\n* The desirability of maintaining git commit history for the merged govukfrontend package.\n* An historical view, now superceded, that teams should be able to use play-frontend-govuk on its own.\n* The desirability of providing a Twirl port of govuk-frontend to other government departments.\n* The impact on teams currently using only play-frontend-govuk.\n* The possibility of other government departments using play-frontend-hmrc and ignoring any HMRC-specific features.\nChosen option: Option 2, because overall we believe the benefits listed below vastly outweigh the negatives.\n### Positive Consequences\n* Service teams need only consult a single library repository for user documentation.\n* Teams only using play-frontend-govuk will be forced to upgrade to use play-frontend-hmrc. This should reduce friction\naround migrating to use the latest HMRC components that in turn will eventually increase the consistency and\naccessibility of HMRC services.\n* Lead times for changes are reduced because only one library needs to be published.\n* Routine library dependency upgrade overheads are reduced.\n* Fixture generation logic is no longer duplicated between two libraries\n* We are in a better position to eventually consolidate the various implicit classes and helpers currently split\nbetween the two libraries.\n* Developer onboarding is simplified – we no longer need to explain why there are two separate libraries.\n### Negative Consequences\n* Potential loss of git commit history depending on merge approach taken. We would intend to investigate this\n[approach](https://medium.com/altcampus/how-to-merge-two-or-multiple-git-repositories-into-one-9f8a5209913f), falling back\nto a link to the archived play-frontend-govuk repository if this is not possible.\n* Slight risk of unintended side effects for teams upgrading to play-frontend-hmrc from play-frontend-govuk. 16 such\nrepositories have been identified to date.\n* User documentation via the play-frontend-hmrc README will be less friendly to other government departments using\nScala/Play/Twirl. For example, they are likely to want to use `govukLayout` rather than the recommended `hmrcLayout` component.\nNo departments other than HMRC have yet been identified using Scala/Play.\n","The play-frontend-govuk library will be archived, and its package will be folded into play-frontend-hmrc."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince launching `play-frontend-hmrc` we have included deprecated static helpers which were required for Play 2.5 projects.\n## Decision Drivers\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\n* the platform opinion is to use DI\n* soft advice from Play has been to use DI since Play 2.6 (https://www.playframework.com/documentation/2.8.x/ScalaDependencyInjection)\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\n* some teams may regard the deprecation notices as small print\n\n## Decision\n","* There is complexity involved in maintaining multiple interfaces for the same behaviour\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\n* the platform opinion is to use DI\n* soft advice from Play has been to use DI since Play 2.6 (https://www.playframework.com/documentation/2.8.x/ScalaDependencyInjection)\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\n* some teams may regard the deprecation notices as small print\n* we should not re-instate static helpers for Play 2.8\n* we should remove them in version 1.0.0\n* we should publish a blog article sooner rather than later that we will be removing them in 1.0.0\n### Positive Consequences\n* Our codebase becomes easier to maintain\n* We have to use DI in our tests which makes our tests more accurate to how these components will be used\n### Negative Consequences\n* Anyone who is using the static helpers will have to update - that's why we're including this in version 1.0.0\n",**Decision:** **Remove deprecated static helpers from `play-frontend-hmrc`**\n\n**Rationale:**\n\n* Maintaining multiple interfaces is unnecessarily complex.\n* The helpers have been deprecated for two years and are no longer necessary.\n* Play Framework recommends using DI instead.\n* Guice DI is the standard dependency injection framework in Play Framework and is widely used throughout the platform.\n* Removing the deprecated helpers will align with Play Framework best practices and reduce the maintenance burden.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nNearly all services running on MDTP need to include a link to their landing page in the\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\nor be able to configure it based on the path. For example, in the case of shared frontend\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\nWhen integrating with play-frontend-hmrc we want to\nminimise boilerplate for service teams while providing flexibility to cope with edge\ncases.\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\nparameter, specify the service name in the\n`service.name` message in `conf/messages`, providing any Welsh translation in `conf/messages.cy`.\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\nmechanism is awkward for services not requiring a service name – they have to override with a blank message –\nand services needing more than one are unable to use this functionality. This pattern\nassumes a 1:1 correspondence between frontend microservices and public-facing\ntax services.\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\nservices, insist on explicitly passing the service name or support some combination of the above?\n## Decision Drivers\n* Minimising of boilerplate and lines of code needed in frontend microservices\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\npublic-facing services.\n* The need to keep things simple and unsurprising\n\n## Decision\n","* Minimising of boilerplate and lines of code needed in frontend microservices\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\npublic-facing services.\n* The need to keep things simple and unsurprising\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\nbetween frontend microservices and public-facing services.\n","**Decision:** Services should override the `headerBlock` parameter to provide custom header logic and determine the service name dynamically based on the current path.\n\n**Rationale:**\n\n* This approach provides maximum flexibility for frontend microservices to handle various scenarios, including:\n    * Services with no service name\n    * Services with multiple public-facing services\n    * Services that share frontend components with other services\n\n* It allows services to tailor the header content to their specific context, ensuring that the service name and landing page link are relevant and meaningful to users.\n\n* While it may introduce some additional complexity, it promotes maintainability and reduces the need for special-casing or boilerplate code in the future. By explicitly defining the header logic, developers can have a clear understanding of how the header is rendered and make necessary adjustments as needed.\n\n**Consequences:**\n\n* Frontend microservices will need to implement custom header logic, which may require additional development effort compared to using the default `govukLayout` component.\n\n* However, this decision enables a more flexible and extensible architecture that can accommodate a wider range of use cases and future requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n\n## Decision\n","* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\naround knowledge duplication.\n",Implement an endpoint on [hmrc-ssoagent](https://github.com/hmrc/sso-agent) that allows services to interrogate the active user's session. This endpoint should return the session's current ttl and a flag indicating whether the session has expired according to the session management configuration in place. This endpoint should be exposed using the same authentication mechanism as with other ssoagent endpoints and should be excluded from session management (i.e. should use a non-secure cookie of the appropriate duration to retain the session). The bootstrap-play SessionTimeoutFilter should be enhanced to use this endpoint to determine the time remaining on the user's session.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the context of the first major version release of play-frontend-hmrc, are there any fundamental\nchanges we would like to make to the package structure and naming conventions of the library?\n## Decision Drivers\n* Major changes are painful but will only get more painful the longer they are left.\n* No teams have, to our knowledge, complained or provided any negative feedback around\nthe naming conventions used in the play-frontend libraries.\n* The benefits of any name changes need to be weighed against the cost and disruption to\nservice teams.\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\nbreaking change. Deprecation notices left in too long create compilation noise and alert\nfatigue for teams leading to a risk that they will be ignored.\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\non their next library upgrade.\n* Repetition of suffixes/prefixes in the same source file makes it more verbose and harder to read.\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\nentropy that over time increases the cost of maintenance.\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\ntax services.\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\nmaking changes available to teams.\n\n## Decision\n","* Major changes are painful but will only get more painful the longer they are left.\n* No teams have, to our knowledge, complained or provided any negative feedback around\nthe naming conventions used in the play-frontend libraries.\n* The benefits of any name changes need to be weighed against the cost and disruption to\nservice teams.\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\nbreaking change. Deprecation notices left in too long create compilation noise and alert\nfatigue for teams leading to a risk that they will be ignored.\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\non their next library upgrade.\n* Repetition of suffixes/prefixes in the same source file makes it more verbose and harder to read.\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\nentropy that over time increases the cost of maintenance.\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\ntax services.\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\nmaking changes available to teams.\nChosen option: option 1, do nothing, because the cost of change is very high while the\ntheoretical benefits are extremely difficult to quantify.\n","Do not make any fundamental changes to the package structure or naming conventions of play-frontend-hmrc for the first major version release.\n\nWe will use compilation warnings to gently encourage migration to the new recommended approach by labelling the old approach as deprecated. Teams will then be notified when compiling their projects that the old approach is deprecated and should be migrated to the new approach. In time, the old approach will be removed.\n\nThis will give teams time to migrate to the new preferred approach without a hard deadline and to focus on delivering value to users, rather than spending time on non-urgent architectural and code changes.\n\nWe will continue to monitor feedback from teams and will revisit this decision if new information becomes available."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the context of classic services' requirement for contact HMRC and Welsh information links in their footer, facing the\nfact that these links are missing from hmrcStandardFooter, should we add them?\nThe additional links needed are:\n* ""Contact"", linking to https://www.gov.uk/government/organisations/hm-revenue-customs/contact\n* ""Rhestr o Wasanaethau Cymraeg"", linking to https://www.gov.uk/cymraeg\n## Decision Drivers\n* The need for consistency across HMRC services.\n* Our belief that including them is likely to improve the user experience for tax users.\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\n* We have a time sensitive opportunity of acting now while teams are in the process of\nuplifting their frontend libraries.\n* The HMRC design community have been consulted on multiple public Slack channels and two\nsuccessive design system working group meetings, with no objections noted.\n* Classic services support multiple live services. Not including these links as standard would mean their having to\nduplicate these links, and associated English and Welsh content, across tens of repositories.\n\n## Decision\n","* The need for consistency across HMRC services.\n* Our belief that including them is likely to improve the user experience for tax users.\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\n* We have a time sensitive opportunity of acting now while teams are in the process of\nuplifting their frontend libraries.\n* The HMRC design community have been consulted on multiple public Slack channels and two\nsuccessive design system working group meetings, with no objections noted.\n* Classic services support multiple live services. Not including these links as standard would mean their having to\nduplicate these links, and associated English and Welsh content, across tens of repositories.\nChosen option: ""Add the links to hmrcStandardFooter"", because this\nwill benefit tax users, and we have a unique window of opportunity to act now.\n### Positive Consequences\n* Tax users have better information provided to them\n* Teams do not need to duplicate content and URLs across hundreds of repositories\n* We can more easily maintain the content and links in a central repository\n### Negative Consequences\n* Teams currently using a Welsh link as their language toggle will likely need to switch to using one of the standard components\nfor language switching e.g. hmrcLanguageSelect.\n* Teams already including a contact link manually will need to remove it when upgrading.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nGiven a service is using the twirl template imports recommended by the play-frontend libraries. When a developer tries\nto use an import with a name which is present in both libraries without explicitly importing the one they want. Then\nthey will get a compilation error:\n<img alt=""Example of ambiguous import compilation exception"" src=""0010-ambiguous-import-exception.png"" width=""450"">\nCurrently, we recommend the use of the wildcard twirl imports, but this possible consequence is undocumented behaviour.\nThe compilation error can be resolved by explicitly importing the class they want.\n## Decision Drivers\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\n* As much as possible avoid breaking changes for existing services\n\n## Decision\n",* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\n* As much as possible avoid breaking changes for existing services\nWe've decided to immediately implement Option 1.2 and defer further changes to be discussed as part of PLATUI-1294.\n### Positive Consequences\n* Less confusing for new developers where imports are coming from when we avoid recommending using TwirlKeys config.\n* Developers can see as part of our recommended usage instructions the possibility for an ambiguous import exception and\nwhat they can do to resolve them without.\n### Negative Consequences\n* Without an API change the naming of components and view models is not as intuitive as we think it could be which may\nconfuse new developers.\n,"**. Developers expecting to be able to use the short import name without getting a compile-time error will be unsurprised.**\n**. Avoids breaking changes for existing services, as that would prevent play-frontend from adopting the new behaviour.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala / Play / Twirl implementations of the components provided\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\nplay-frontend implementations diverge from their “base” repositories?\n## Decision Drivers\n* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\n\n## Decision\n","* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\nGDS.\n### Positive Consequences\n* Design of case classes to follow GDS / HMRC design system means PlatUI as library maintainers do not have to create\nviewmodel structure from scratch every time\n* Adding new components can follow a clear and straightforward path\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\n### Negative Consequences\n* Feedback suggests that some developers do not find the API to be intuitive\n* Separate decisions need to be made on handling multilingual support\n* Enrichment of library needs to be done via separate Twirl helpers,\nsee [related ADR](../adr/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\n","The play-frontend implementations should be as close as possible to their base repositories, only diverging when necessary to:\n\n* Provide a Scala interface to the underlying Nunjucks components\n* Provide a clear and quick upgrade path when new versions of  govuk-frontend and hmrc-frontend are released\n* Have a robust testing strategy for library developers to have faith in when upgrading"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n- `play-fronted-govuk` library is not Play Framework aware\n- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\n## Decision Drivers\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\nappropriate helper class alongside the view models\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\n\n## Decision\n","- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\nappropriate helper class alongside the view models\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\nChosen option: Option 5, because it adds useful functionality (class enrichment of `Radios` via implicit\n`RichRadios`, without adding new Twirl templates to maintain, and without adding new methods directly\nto `Radios`, which would cause a divergence between `play-frontend-govuk` and `govuk-template` (undesirable).\n### Positive Consequences\n* Adds useful optional class enrichment to `Radios`\n* Reduces code repetition between `Field` and `Radios` (DRY)\n* Pattern is extensible, i.e. similar implicit helpers can be added to other form inputs\n### Negative Consequences\n* Doesn't add as much value as we would like\n* Need to document carefully what behaviour occurs when information provided via both `Radios` and\n`Field`, i.e. which takes precedence (currently intended to be `Radios` values take precendence)\n* Need to document to teams that this helper is available (service developers won't use it if they don't know it's there)\n","Add helper methods to the `play-frontend-govuk` library to utilise Play Framework features, reducing repetition in service code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFacing the need to create wrappers to make using the Nunjucks components ported from hmrc-frontend more\nstraightforward and idiomatic on the Scala/Play MDTP platform, how should we name these components?\nThe components ported from hmrc-frontend include\n* implementations of govuk-frontend components that support the Welsh language e.g. hmrcHeader, hmrcFooter\n* components that meet specific HMRC needs e.g. hmrcBanner, hmrcInternalHeader\n* components that have not been standardised by GDS but are needed by HMRC e.g. hmrcNotificationBadge, hmrcTimeoutDialog\nBeing entirely presentational and ported from Nunjucks means they cannot leverage features built into Scala/Play nor\nmake use of any MDTP platform knowledge that would simplify their use on MDTP. For example,\n* they do not and should not know the url structure for common MDTP components e.g. tracking-consent-frontend,\ncontact-frontend or the accessibility-statement-frontend, services that need to wired in on every public-facing service via\nstandard headers and footers.\n* they cannot make use of Play's i18n features and automatic language wiring\n* they cannot make use of any knowledge encoded in the requests users make e.g. the request URL, referrer URL, cookies\nor headers.\nFor the above reasons, we are creating wrappers that implement standardised Play/platform wiring, to avoid teams having to\nduplicate this wiring across 100s of repositories. Once implemented we will encourage teams to use these helpers rather than\nthe underlying Nunjucks ports.\n## Decision Drivers\n* The fact that the ideal component names have already been taken by the Nunjucks components.\n* The preference for not relying on the package name to differentiate the components\n* The preference for not repeating the word Helper in the package and component name\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\ncomponent.\n* The preference for consistency in naming across the components\n\n## Decision\n","* The fact that the ideal component names have already been taken by the Nunjucks components.\n* The preference for not relying on the package name to differentiate the components\n* The preference for not repeating the word Helper in the package and component name\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\ncomponent.\n* The preference for consistency in naming across the components\nChosen option: ""Suffix with 'Helper'"", because this is the only option the team are happy with bearing in mind the decision\ndrivers listed above.\n### Positive Consequences\n* We have a consistent naming scheme\n* Friction around needing to come up with new names for helpers reduced, increasing speed of development.\n### Negative Consequences\n* There is a risk that service teams will not know the helpers exist\n* Repeating the word Helper in the component and having it in the package name is ugly\n","The components will be named with the prefix ""hmrc"" followed by an uppercase word matching the component's name. For example:\n* hmrcHeader\n* hmrcFooter\n* hmrcBanner\n* hmrcInternalHeader\n* hmrcNotificationBadge\n* hmrcTimeoutDialog"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFrequently occurring usage patterns of play-frontend-hmrc components require repetitious boilerplate\ninvolving excessively nested case class instantiations. This has driven many teams to create wrappers to\nsimplify the construction of components, which are then copied and pasted from service to service.\nFor example, in order to add a legend to a GovukRadio the following boilerplate is required:\n```scala\nSome(Fieldset(\nlegend = Some(Legend(\ncontent = Text(""message.key.for.legend""),\nclasses = ""govuk-fieldset__legend--l"",\nisPageHeading = true\n))\n))\n```\nIn the above example, the only thing that changes between instantiations is the `message.key.for.legend`, everything\nelse remains the same.\nShould we solve this problem through the introduction of implicit conversions that will automatically wrap simple\nobjects such as Strings with the additional boilerplate necessary for commonly occurring cases?\n## Decision Drivers\n* The need for API consistency in play-frontend-hmrc.\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\nentirely in a future Scala version:\n* See https://www.rallyhealth.com/coding/implicit-implications-part-2-implicit-conversions,\nhttps://contributors.scala-lang.org/t/can-we-wean-scala-off-implicit-conversions/4388\n* The risk of unintended side effects if these conversions are added unilaterally\nwithout service developers needing to specifically opt in to them.\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\nHint, Key, Label, Legend.\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\nshow implicit hints.\n\n## Decision\n","* The need for API consistency in play-frontend-hmrc.\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\nentirely in a future Scala version:\n* See https://www.rallyhealth.com/coding/implicit-implications-part-2-implicit-conversions,\nhttps://contributors.scala-lang.org/t/can-we-wean-scala-off-implicit-conversions/4388\n* The risk of unintended side effects if these conversions are added unilaterally\nwithout service developers needing to specifically opt in to them.\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\nHint, Key, Label, Legend.\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\nshow implicit hints.\nChosen option: Option 1, because it solves the problem, is consistent with how we have been doing things, maintains\nthe internal consistency of play-frontend-hmrc and has less risk of breaking services.\n","**Decision:** Do not currently introduce this feature.\n\n**Rationale:**\n\n* The problem of boilerplate is only a concern for a small subset of components.\n* It is unclear if the benefits of wrapping outweigh the possible costs, i.e. the potential downsides and risks outlined in the Decision Drivers.\n* The Scala programming language's use of implicit conversions is controversial, and there are concerns that this feature may be removed in a future version.\n* Implicit conversions can be difficult to debug and understand, especially for new developers.\n* There is a risk of unintended side effects if these conversions are added unilaterally without service developers needing to specifically opt in to them."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nService developers seeking to use play-frontend to build their frontend microservices need to follow around\n12 separate integration steps involving two separate Scala libraries: play-frontend-govuk and play-frontend-hmrc. Failure to\nperform any one of these steps correctly, or making changes that break any of this configuration,\nresults in failed builds, incorrectly styled components or broken Javascript-enabled functionality. These steps involve:\n* Adding play-frontend-govuk and play-frontend-hmrc as SBT dependencies.\n* Setting up an SBT asset pipeline to compile and optimise the CSS and Javascript into bundles suitable for production.\n* Adding multiple asset routing rules.\n* Adding the boilerplate SCRIPT and LINK tags to the HTML page template to link to the assets at runtime.\n* Adding Javascript to initialising the GOV.UK and HMRC components.\nFollowing investigation, we discovered that the integration process could be drastically simplified\nby providing head and script helpers that would link to pre-compiled and optimised asset bundles from\nthe hmrc-frontend webjar, subsuming the govuk-frontend assets and taking responsibility\nfor component initialisation. This move would:\n* simplify the migration path from assets-frontend/play-ui/govuk-template to play-frontend\n* particularly benefit teams lacking an embedded full time frontend developer\n* not radically change the way microservices integrate with play-frontend\n* still allow teams to create custom components and add their own custom CSS where needed\n* continue to allow for local development without reliance on networked dependencies\nOverall, the total number of integration steps would be reduced from 12 to 6.\nFacing these considerations, should we therefore\n* change the pre-compiled hmrc-frontend bundles to also include govuk-frontend and to initialise all components\n* add the pre-compiled and optimised CSS and Javascript bundles into the published hmrc-frontend npm package and\n* create Twirl helper components to render the HTML snippets necessary to link to them?\n## Decision Drivers\n* The integration difficulties teams have reported over the past 12 months.\n* The problematic nature of diagnosing build issues in frontend microservices due to the requirement for a deep\nknowledge of both Scala and frontend build tooling, which is rare.\n* The pressing need to migrate services away from assets-frontend for reasons of accessibility, security and sustainability.\n* That services do not generally need extensive service-specific custom CSS. UI components are heavily standardised by GDS.\n* That services do not generally need extensive service-specific Javascript. The mantra of progressive\nenhancement demands services build without Javascript if possible.\n* That teams should be free to create custom components with custom CSS where there is a genuine need, without relying\non other teams. Historically denying teams this ability arguably led to many of the problems with assets-frontend.\n* That teams should continue to be able to develop locally without relying on networked dependencies.\n\n## Decision\n","* The integration difficulties teams have reported over the past 12 months.\n* The problematic nature of diagnosing build issues in frontend microservices due to the requirement for a deep\nknowledge of both Scala and frontend build tooling, which is rare.\n* The pressing need to migrate services away from assets-frontend for reasons of accessibility, security and sustainability.\n* That services do not generally need extensive service-specific custom CSS. UI components are heavily standardised by GDS.\n* That services do not generally need extensive service-specific Javascript. The mantra of progressive\nenhancement demands services build without Javascript if possible.\n* That teams should be free to create custom components with custom CSS where there is a genuine need, without relying\non other teams. Historically denying teams this ability arguably led to many of the problems with assets-frontend.\n* That teams should continue to be able to develop locally without relying on networked dependencies.\nChosen option: ""Add pre-compiled assets with auto-initialisation"", because this is a major step forward in improving\nthe ease of use of play-frontend and goes a long way to address many of the issues\nusers have experienced.\nThe change to the S3 assets is not of concern due to the fact that the existence of these\nassets was never publicised or fully documented and we have seen only a tiny number of requests for these assets\nin server logs.\n### Positive Consequences\n* A radically simplified integration process for Scala developers.\n* Fewer moving parts in frontend microservices and reduced boilerplate.\n* No changes needed by teams wanting to continue to consume the original un-compiled assets.\n* Users of the npm package should not be affected by this change as it adds files only.\n* Simplified usage outside Scala microservices e.g. in outage/waiting pages.\n* Bundling govuk-frontend and hmrc-frontend together means there is no longer the possibility for the use\nof incompatible versions of govuk-frontend with hmrc-frontend.\n* Best practice is to initialise all GOV.UK and HMRC components rather than\ncherry picking to avoid the risk of breaking accessibility features. Doing this automatically\nmeans there is one less thing for teams to worry about getting right.\n### Negative Consequences\n* Teams using the S3-distributed hmrc-frontend assets will no longer have the option to separately initialise the\nGOVUK and HMRC components. However, they will retain access to window.GOVUKFrontend and window.HMRCFrontend if\nneeded for initialisation of dynamically injected content.\n* On their next upgrade, teams using the S3-distributed hmrc-frontend assets will need to remove the SCRIPT tag referencing the\ngovuk-frontend bundle and any calls to GOVUKFrontend.initAll() or HMRCFrontend.initAll().\n* The distribution build task diverges from the pattern set by govuk-frontend. In govuk-frontend, the distributable does not\nauto-initialise the govuk components. This may affect developers consuming hmrc-frontend using git rather than via the npm package.\n","Yes, we should change the pre-compiled hmrc-frontend bundles to also include govuk-frontend and to initialise all components, add the pre-compiled and optimised CSS and Javascript bundles into the published hmrc-frontend npm package and create Twirl helper components to render the HTML snippets necessary to link to them."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe need to provide an HMRC footer translated into Welsh (PLATUI-752)\n## Decision Drivers\n* the lack of support for Welsh in the GDS govukFooter\ncomponent\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\nwith some non-presentational helpers to aid use in Scala/Play - conceptual integrity\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\nmarkup - separation of concerns\n\n## Decision\n,"* the lack of support for Welsh in the GDS govukFooter\ncomponent\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\nwith some non-presentational helpers to aid use in Scala/Play - conceptual integrity\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\nmarkup - separation of concerns\nChosen option ""maintain parity"" because we want hmrc-frontend to be the source of truth for presentation and\nmaintain the separation of concerns between hmrc-frontend and\nplay-frontend-hmrc\n### Positive Consequences\n* Maintain separation of concerns between hmrc-frontend and play-frontend-hmrc\n* Maintain conceptual integrity for play-frontend-hmrc\n* Be able to test markup using existing test strategy\n### Negative Consequences\n* We will need to add and maintain a new hmrcFooter component in\nhmrc-frontend and play-frontend-hmrc providing a mirror of govukFooter with localised content,\n* The new hmrcFooter component in hmrc-frontend will not be able to make use of any I18n features,\n* We will need to create a new `helpers` package within play-frontend-hmrc to clearly demarcate them\nfrom presentational components,\n* We will need to redesign the existing hmrcFooter helper to wrap the new hmrcFooter component\nand move to the helpers package,\n* The new hmrcFooter component will need to be deprecated when GDS provide a localised version\nof govukFooter,\n* We will need to liaise and get approval from the HMRC design system team for adding\nhmrcFooter to the hmrc/design-system and hmrc/hmrc-frontend\n* We will be adding features that are unlikely to be useful or used by designers because\nat the prototyping phase content is not stable enough for translation into Welsh\n","Implement hmrc-play-frontend-footer, a component that renders the HMRC footer in Welsh, as a dependency of hmrc-play-frontend."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nplay-frontend-hmrc relies on a webjar for [hmrc/hmrc-frontend](https://www.github.com/hmrc/hmrc-frontend)\npublished to www.webjars.org. This has a number of drawbacks:\n* publishing is a manual process\n* it can take many hours to complete\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\nThe main impact of the above is an excessive lead time for making improvements in the\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\nBintray, should we:\n* automate the creation of the webjars within our own deployment pipelines with no dependency\non webjars.org\n* publish the resulting webjars to this repository automatically?\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\nwebjar for [alphagov/govuk-frontend](https://www.github.com/alphagov/govuk-frontend), which is\ncurrently a dependency for [hmrc/play-frontend-govuk](https://www.github.com/hmrc/play-frontend-govuk).\n## Decision Drivers\n* The need to make improvements and upgrades to hmrc-frontend\navailable in play-frontend-hmrc quickly.\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\nimprovements.\n* The hardship, frustration and toil the current manual process is causing the team.\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\nthe overhead of maintaining those repositories\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\n* Parity between the hmrc-frontend NPM package and the webjar.\n\n## Decision\n","* The need to make improvements and upgrades to hmrc-frontend\navailable in play-frontend-hmrc quickly.\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\nimprovements.\n* The hardship, frustration and toil the current manual process is causing the team.\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\nthe overhead of maintaining those repositories\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\n* Parity between the hmrc-frontend NPM package and the webjar.\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\nadditional dependencies.\n### Existing architecture\n<img alt=""Existing architecture"" src=""0009-webjars-existing.png"" width=""450"">\n### To be architecture\n<img alt=""To be architecture"" src=""0009-webjars-tobe.png"" width=""450"">\n### Positive Consequences\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\nproduction.\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\n* Maintaining play-frontend-hmrc is a less frustrating process.\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\nproduced indicate webjars.org as the developer in the POM files.\n* There are fewer external dependencies and moving parts.\n### Negative Consequences\n* We have an additional moving part to maintain ourselves.\n",Automate the creation of the webjars within our own deployment pipelines with no dependency on webjars.org and publish the resulting webjars to this repository automatically.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe previously decided to keep inputs and outputs of play-frontend-govuk components a 1-to-1 match with govuk-frontend\nnunjucks example implementations.\nGenerally components from govuk-frontend accept their copy as input, however the skip link in govukTemplate is\nhard-coded which is causing accessibility failures for services.\nThis is an accessibility failure because a page translated into welsh may cause assistive technology to mispronounce the\ncontent of the english skip link because it has no explicit language attribute.\n## Decision Drivers\n* Because it's an accessibility failure we want to fix this as a priority quickly\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\n* At the same time we don't want to reduce test coverage / diverge from our current testing strategy\n\n## Decision\n","* Because it's an accessibility failure we want to fix this as a priority quickly\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\n* At the same time we don't want to reduce test coverage / diverge from our current testing strategy\nWe've chosen option 1 because unlike other components, we don't have any automatically generated fixtures or integration\ntests for govukTemplate. We maintain test fixtures for the templates manually in the additional fixtures folder. This\nmakes it possible for us to patch this change into govukTemplate with minimal maintenance cost without reducing our test\ncoverage / diverging far from our current testing strategy.\n","**Decision:** Remove the hard-coded skip link from govukTemplate.\n\n**Rationale:**\n\nThis decision addresses all of the decision drivers:\n\n* **Accessibility:** Removing the hard-coded skip link will fix the accessibility failure by allowing services to provide a language-appropriate skip link.\n* **Wide availability:** The fix will be available as widely as possible without requiring template changes from services, as it will be implemented in govukTemplate.\n* **Test coverage:** The fix will not reduce test coverage, as the skip link will still be tested in the govukTemplate tests."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKuona instances need to be secured - particularly for deployments that have public access.\n\n## Decision\n,https://funcool.github.io/buddy-auth/latest/#example-session Buddy seems to fit the bill and is compatible with Compojure and Ring. Provides a number of options and possible persistance mechanisme.\n,**Decision:** Implement [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) to manage access to Kuona instances.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use ADRs (Architectural Decision Records) as the primary mechanism for recording architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\ndecrypt Response Assertions.\nThey will need to provide these to the verify-service-provider in some, reasonably\nsecure way. Different users may have different opinions on how best to do this.\n\n## Decision\n",Initially we'll use files for this.\nWe chose not to use environment variables because they're visible to other processes.\nWe chose not to use a more complicated solution because it would be more complicated.\n,The verify-service-provider will provide a mechanism that will allow RPs to securely upload their private keys.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe client and the service provider will have to communicate using some API.\nWe need to decide how the requests and responses will look like.\n\n## Decision\n,We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\n,"**Decision**\n\nThe API will follow the RESTful architectural style. Requests will be made using HTTP verbs (GET, POST, PUT, DELETE) and will include a JSON payload. Responses will also be in JSON format."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make sure that whatever we build meets the users' needs. To make sure of this we need to\nconduct some user research, which should involve putting software in front of users and observing them using it.\n\n## Decision\n","We will build a simple ""prototype"" which we will use to test our assumptions about whether our proposed\nsolution is the best way of meeting our users needs.\nThe prototype will be architecturally similar to the product we envisage building, but won't be able to\ndo the SAML interactions with Verify at this stage.\nADRs for the first prototype will live in the [prototype-0](prototype-0) directory.\n",**Conduct user research by observing users interacting with software prototypes.**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAt least one user is currently using node js and passport. We want to provide as\nfrictionless as possible an integration for them.\nOther users will be using other languages and frameworks.\n\n## Decision\n,We will initially build only a node / passport client. We will want to build\nanother client in another language as soon as possible to make sure the API\nis well designed.\nUsers should also be able to interact with the API directly if we haven't built\nan appropriate client for their use case.\n,"Integrate passport with node.js to provide users with a seamless integration experience, while supporting multiple languages and frameworks for other users."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to secure the interaction between the ""client"" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n\n## Decision\n",If possible users can talk to the service provider on the loopback (127.0.0.1)\nIf that doesn't work for some reason then they can use the dropwizard config\nto set up basic auth or tls or something.\nSee http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors\n,"Implement a mutual TLS (TLS mTLS) security mechanism.  mTLS provides both authentication and data encryption, and can be configured to work with a variety of client and server technologies. It is a good choice for securing the interaction between the ""client"" code and the server side code.\n\n**Benefits**:\n\n* **Authentication**: mTLS provides strong authentication by requiring both the client and the server to present valid certificates. This makes it difficult for unauthorized users to access the service.\n* **Data encryption**: mTLS encrypts the data transmitted between the client and the server, protecting it from eavesdropping.\n* **Flexibility**: mTLS can be configured to work with a variety of client and server technologies, making it a good choice for a wide range of applications.\n\n**Drawbacks**:\n\n* **Complexity**: mTLS can be complex to configure and manage, especially in large-scale deployments.\n* **Performance**: mTLS can add some overhead to the communication between the client and the server, which can impact performance.\n\n**Alternatives**:\n\n* **API key**: An API key is a string that is used to identify the client to the server. API keys are simple to implement and manage, but they do not provide strong authentication or data encryption.\n* **OAuth 2.0**: OAuth 2.0 is an authorization framework that can be used to secure the interaction between the client and the server. OAuth 2.0 provides strong authentication and data encryption, but it can be complex to implement and manage.\n\n**Recommendation**:\n\nWe recommend implementing mTLS to secure the interaction between the ""client"" code and the server side code. mTLS provides both strong authentication and data encryption, and can be configured to work with a variety of client and server technologies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn SAML RPs can provide some extra data along with the request. This is\ncalled RelayState. Some existing RPs use this, but we're not sure what\nthey use it for.\nWe're not aware of any need for the service-provider to use relay state itself.\n\n## Decision\n",Users will be able to specify whatever relay state they want to and it will be\nprovided in the response.\n,Do not implement RelayState support.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAuthnRequests contain an ID attribute the value of which will be sent back in\nthe Response as an ""InResponseTo"" attribute.\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\n\n## Decision\n",The service provider will generate a random GUID to use as the AuthnRequest ID.\n,"**Decision:** Use a UUID for the ID attribute of AuthnRequests.\n\n**Rationale:**\n\n* UUIDs are universally unique and can be randomly generated, ensuring that each AuthnRequest has a unique ID.\n* UUIDs are relatively short and easy to work with, making them suitable for use as an attribute in an XML message.\n* Validating the InResponseTo attribute against the ID attribute is straightforward, as both attributes will contain the same UUID value.\n\n**Consequences:**\n\n* Using UUIDs for the ID attribute will ensure that AuthnRequests can be uniquely identified and that the InResponseTo attribute can be validated correctly.\n* This will help prevent replay attacks and other security vulnerabilities.\n* The use of UUIDs may incur a slight performance overhead compared to using simpler identifiers, but this overhead is likely to be negligible in most cases."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wish to run regular user research and testing against the prototype Service Provider.\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\nThe area we are most interested in is the interface between the Service and the Hub.\n\n## Decision\n,"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\nIt will not provide a SAML implementation.\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\ncan be selected.\nAny information that needs to be stored from the original request or between pages will be stored in a session\ncookie. The session cookie does not need to be signed or encrypted initially.\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\nsend its responses to.\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\nprototype.\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\nService/client.\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\nThe Stub Verify Hub will provide the following responses:\n| Response Scenario     | Message                                                                                                                   |\n| --                    | --                                                                                                                         |\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\n| No Match              | scenario: NO_MATCH                                                                                                         |\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\nA diagram of a potential stub architecture is available at: `prototypes/prototype-0/docs/diagrams/stub_service_architecture.png`\n",Use an integration test harness as a test double for the Service Provider.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Establish an Architectural Decision Record (ADR) process to capture architectural decisions made during the project.\n\n**Rationale:**\n\n* Ensures consistent documentation of architectural decisions throughout the project lifecycle.\n* Provides a centralized repository for decision-making history and rationale.\n* Facilitates knowledge sharing and understanding among project stakeholders.\n* Reduces the risk of ad-hoc or poorly considered architectural decisions.\n\n**Consequences:**\n\n* Requires effort to create and maintain ADRs.\n* May introduce some overhead in the decision-making process.\n* ADRs may become outdated if not regularly reviewed and updated.\n\n**Additional Considerations:**\n\n* Define a template or format for ADRs to ensure consistency.\n* Establish a review and approval process for ADRs to ensure quality.\n* Make ADRs accessible to all relevant stakeholders.\n* Consider using a dedicated tool or platform to manage ADRs.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\n## Decision Drivers\n* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\n\n## Decision\n,"* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\nChosen option: ""AWS"", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\n","**Decision:** Host the web application on AWS Elastic Beanstalk.\n\n**Rationale:**\n\nAWS Elastic Beanstalk meets all the decision drivers:\n\n* **Easy to update:** Elastic Beanstalk automates the deployment and update process, making it easy to deploy new code or updates with minimal downtime.\n* **High availability:** Elastic Beanstalk deploys applications across multiple Availability Zones, ensuring high availability and resilience to failures.\n* **Easy to configure HTTPS:** Elastic Beanstalk supports SSL termination and provides options for managing HTTPS certificates.\n* **Observability:** Elastic Beanstalk integrates with AWS CloudWatch, providing metrics, logs, and insights into the performance and health of the application."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to choose the framework and language to build this web application.\nWhich framework and language should we use?\n\n## Decision\n,"Chosen option: ""React with Typescript"", because\n* The single developer has experience with React and Ember, but no experience with Vue\n* This developer favors React over Ember, because he thinks the first contributes to a better separation of concerns\n* This developer has experiece with TypeScript and JavaScript, but no experience with Elm\n* This developer favors TypeScript mostly because it's typed.\n",The decision is not provided in the context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe've implemented 2 different methods in the Editor interface:\n1. `write()`\n1. `readThenWrite()`\nMost refactorings use `write()`. They create new code from the AST, then write it in the editor. Still, some refactorings use `readThenWrite()`: they first read some code `string` from the editor, and use it to write new code.\nThe main problem of `readThenWrite()` is that we end up manipulating strings instead of the AST. This usually means the implementation is more complex. Instead of manipulating the AST, we need to retrieve the correct selection we need to read, then we have to mix this read code with the transformation to produce the final output. Final code is less straightforward and more cases need to be tackled by us, instead of relying on the AST parser to do the job.\nHowever, there is one key advantage of doing so: it absolutely preserve the code as it was written in the editor. Even though [we use recast to preserve code style as much as possible][recast-usage], it's not perfect. If the refactoring consist in re-using exactly code that was written, `readThenWrite()` is the only way we know to preserve exactly the original style.\n\n## Decision\n","We'll use `readThenWrite()` if we need to preserve a code exactly as it was written. E.g. if the refactoring consists in moving existing code, without transforming it (Extract Variable, Inline Variable).\nFor other refactorings, we'll use `write()`. E.g. if the refactoring transforms the code, it's OK to change it while preserving the original style as much as possible with recast.\n",Deprecate `readThenWrite()` and encourage the adoption of `write()` by refactorings.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the ""Convert to Pure Component"" Quick Fix took most of the time:\n![][flame-chart]\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https://github.com/reactjs/react-codemod). It was originally meant to be run through a CLI command.\n\n## Decision\n",We will stop proposing a Quick Fix for this refactoring.\n,"**We will not make any changes to the current behavior of the ""Convert to Pure Component"" Quick Fix.** \n\nThis refactoring is not suitable for all cases and is only applied under specific circumstances. The performance issue is caused by the limitations of the underlying codemod, which cannot be improved within the extension."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe used Babel to parse code into AST, transform this AST and re-generate code.\nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!\nWhat is not great is the code generation part. Babel formats the generated code. That means the code contained inside a transformed node gets reformated. This is not cool.\n\n## Decision\n","As we want to preserve the original style of the transformed code as much as possible, we went for [Recast][recast].\nAs the library says:\n> The magic of Recast is that it reprints only those parts of the syntax tree that you modify.\nThus, we now use Recast to parse and generate the code. AST transformation is still performed by Babel. Recast uses Babel to parse the code into AST, so we keep Babel benefits such as parsing JSX, TS and TSX out of the box.\n",We should use another tool (like swc) that doesn't format the code.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVS Code has documented [how to test an extension][testing-extension] in integration with VS Code API.\n> These tests will run inside a special instance of VS Code named the `Extension Development Host`, and have full access to the VS Code API.\nBut VS Code test runner is adapted to be used with [mocha][mocha] or [Jasmine][jasmine].\nFor unit tests, we want to use [Jest][jest]. But Jest and mocha have conflicting types, preventing TS to build. A solution would have been to create a custom test runner for VS Code API, that would run integration tests with Jest. But we didn't managed to create one that works.\nThat's partly because Jest doesn't have an official way to run tests programatically. We weren't able to make VS Code test runner work with Jest `runCLI()` (async) method.\nAlso, integration tests are not the most important part of the project for the moment.\n\n## Decision\n","We won't do integration tests. We'll rely on unit tests (e.g. state-based tests, collaboration tests and contract tests).\n","Do not use Jest for now, continue using mocha."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRelative paths for images won't work when the README is displayed somewhere where the images are not hosted. From experience developing npm libraries, images won't appear on the package manager website.\nTo solve this, we use absolute paths to hosted images. Since images are hosted in GitHub along the rest of the code, we use these absolute paths.\nVS Code has a way to deal with that issue. It won't allow you to create a package if README contains relative paths. It will emit this error:\n> Couldn't detect the repository where this extension is published. The image './docs/logo/abracadabra-logo.svg' will be broken in README.md. Please provide the repository URL in package.json or use the --baseContentUrl and --baseImagesUrl options.\nThus, we could technically use relative paths and provide the correct arguments to make images work on VS Code Marketplace.\nHowever, using absolute paths is something we're used to, and doesn't come up with a lot of downsides. So we decide to go with absolute paths, until downsides justify otherwise.\n\n## Decision\n",We use absolute paths to GitHub hosted images in the README (and other documentation files).\nWe don't use relative paths to images.\n,Use absolute paths for images in the README.md file to ensure that they can be displayed regardless of where the file is hosted.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nJest [`it.each` seemed great][jest-it-each], but it turns out to be limited.\nSpecifically, it doesn't allow us to have a `.only` on a single test of the list.\nOf course, this can be done at runtime through Jest runner. But if we use tools like [Wallaby.js][wallaby], we can't do that. We need a way to add things like `.only` on individual tests.\n\n## Decision\n",We've decided to implement a custom `testEach()` function that will provide a convenient API to run the same test over different data.\nThis function will provide a way to run individual test of the list with `only: true`.\n,"Use a custom implementation of `.only` that allows for specifying which test to run.\n\nThis implementation could be based on the `it.each` function, but would add additional functionality to allow for specifying which test to run. This would allow for more flexibility and control over which tests are run, while still providing the benefits of `it.each`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCreating a new refactoring kinda always follow the same steps. There is a bunch of boilerplate code that needs to be created.\nWe could try to refactor this boilerplate into some good abstraction. But we still need to scaffold the same files over and over: the refactoring file, the test file, the command declaration file and eventually the action provider one.\n\n## Decision\n","To speed up this process, we've decided to use a code generator.\nWe went for [hygen](https://www.hygen.io/) because it's quite simple to use, very fast and flexible.\nWe're abstracting the usage of hygen behind a npm script alias: `yarn new` will ask you few questions to scaffold a new refactoring.\n",**Decision:** Create a tool to generate the boilerplate code for a new refactoring.\n\n**Rationale:**\n\n* This will save time and effort when creating new refactorings.\n* It will ensure that all refactorings follow the same conventions and best practices.\n* It will make it easier to maintain and update the refactoring framework.\n\n**Consequences:**\n\n* **Positive:**\n    * Faster and easier to create new refactorings.\n    * Improved consistency and quality of refactorings.\n* **Negative:**\n    * May require some additional development time to create the tool.\n    * May make it more difficult to customize refactorings.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe had to enhance editor capabilities and implement new editor adapters with the latest features. Not having integration tests to cover these changes became more and more risky.\nAfter giving it another try, we were able to get Jest & Mocha installed together, without compilation errors because of type conflicts. Therefore, it was possible to create integration tests that would have access to VS Code API.\n\n## Decision\n","We will now cover adapters with integration tests.\nBecause _integration tests_ has different meanings for different people, we have decided to call them **contract tests** instead. Our intention is to test that all adapters of an interface do follow the same contract.\n",Integrate Jest & Mocha testing frameworks for improved test coverage and enhanced editor capabilities. This combination enables access to VS Code API and mitigates risks associated with insufficient testing.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\nAlso, we want to start deploying to [the Open VSX Registry](https://open-vsx.org/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https://vscodium.com/). You can read [the original issue](https://github.com/nicoespeon/abracadabra/issues/163) for more context.\nSource code is hosted and managed in GitHub, so GitHub Actions make sense.\nThere is [a custom GitHub Action](https://github.com/HaaLeo/publish-vscode-extension#readme) that would take care of that precise use-case. But what we need to do isn't very complex. Having less intermediate would make it easier to maintain.\n\n## Decision\n",We've created 2 GitHub Actions:\n1. One for deploying to the VS Code Marketplace\n2. One for deploying to the Open VSX Registry\nThese actions trigger on published releases.\n,Use a custom GitHub Action to automate the deployment process. The action should:\n\n* Trigger on a push to the `main` branch.\n* Build the extension using the `vsce` command.\n* Publish the extension to the Open VSX Registry using the `vsce publish` command.\n* Notify @nicoespeon on Slack if the deployment fails.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Utilize an Architectural Decision Record (ADR) template to document architectural decisions consistently and thoroughly.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\n\n## Decision\n","Chosen option: ""Swagger Based"", because this solution supports the fastes development Start without writting any boilerplate code.\n### Positive Consequences\n* No Painfull HTTP Client Implementation\n### Negative Consequences\n* the API Client Implementation dependents to the Swagger Spec Quality...\n",**Decision:**\n\nUse a generated HarborRestAPI client.\n\n**Rationale:**\n\n* **Faster development:** Generating a client will save time and effort compared to manually writing one.\n* **Improved accuracy:** A generated client is less likely to contain errors than a manually written one.\n* **Consistency with Terraform Provider Functions:** Using a generated client will ensure consistency with the Terraform Provider Functions development process.\n\n**Consequences:**\n\n* May require additional maintenance if the Harbor API changes.\n* May not be as customizable as a manually written client.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEvery service is required to pass a Caller-Id header with its requests and previously eHOKS frontend had separate\nids for oppija and virkailija. While the codebase is largely shared between oppija and virkailija the services themselves\nare separate and hence the separate ids were created. While the headers were simple to add to requests made by\ncomponents only used by either oppija or virkailija, dynamically figuring out which id should be used in the shared\ncomponents at any given time proved harder.\n\n## Decision\n","The separate ids will be replaced by a single frontend Caller-Id. Since all the requests made by both oppija and\nvirkailija frontends go through the eHOKS backend service and don't call any external services directly this\nshould be sufficient. The requests from oppija and virkailija can be distinquished from each other via other means,\neg. they use different backend APIs altogether.\n",Create a service that all shared components use to fetch the correct Caller-Id header value for the current request.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural and desing decisions made during this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by\nMichael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\nAlso https://medium.com/better-programming/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\n",**Decision:** Use an Architectural Decision Record (ADR) template to consistently document architectural and design decisions.\n\n**Rationale:**\n\n* ADRs provide a structured way to capture and share the reasons behind architectural decisions.\n* Using a template ensures consistency and completeness in the documentation.\n* ADRs facilitate knowledge transfer within the team and future reference.\n* They help avoid repetitive discussions and ensure that decisions are made based on informed analysis.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\nin Finnish but react component props in English even though data might be exactly the same.\n\n## Decision\n,"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\nconventions quite some time.\nHere is one expample:\n```typescript\ninterface LearningEventProps {\nclassName?: string\ntitle?: React.ReactNode\nisDemonstration?: boolean\nsize?: ""small"" | ""large""\ndescription?: string\nstartDate?: string\nendDate?: string\nperiodSpecifier?: string\ndemonstrationEnviromentDescription?: string\n}\n```\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\nOsaamisenOsoittaminen components.\nExample fixed:\n```typescript\ninterface LearningEventProps {\nclassName?: string\ntitle?: React.ReactNode\nisNaytto?: boolean\nsize?: ""small"" | ""large""\ndescription?: string\nstartDate?: string\nendDate?: string\najanjaksonTarkenne?: string\nnayttoymparistoDescription?: string\n}\n```\n","**Decision:** Establish a consistent translation convention for domain words and concepts across all frameworks and languages used in the project.\n\n**Rationale:**\n\n* **Improved consistency:** Ensures that users and developers can easily understand and navigate the codebase, regardless of the language or framework being used.\n* **Reduced confusion:** Prevents misunderstandings and errors caused by inconsistent translations.\n* **Enhanced code readability:** Makes the code more readable and maintainable for all team members.\n* **Increased flexibility:** Allows for easier future additions or modifications to the codebase, as all translations will follow the same conventions.\n\n**Implementation:**\n\n* Define a central repository or style guide that outlines the agreed-upon translation conventions.\n* Ensure that all new code and translations adhere to these conventions.\n* Conduct regular code reviews to check for consistency and identify any deviations from the established conventions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Two Unique Problems to Solve\n* A radio or checkbox field needs to optionally have an ""Other"" input field which consists of a label and freeform text input. All existing radio/checkbox functionality must be preserved.\n* Address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.\n\n## Decision\n","* Create additional versions for each individual form field and treat them accordingly in the HTML helper. ""version"" will be a new attribute, ""formtype"" and the rest of the JSON will stay the same.\n```\nExample of field variation\n""data"":\n[\n{\n// otherless radio, notice no version attribute (default)\n""label"":""Icecream?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_1"",\n""formtype"":""s08"",\n""name"":""icecream"",\n""radios"":""yes\nno"",\n""type"":""radio"",\n""required"":""true"",\n""class"":""custom-class""\n},\n{\n// radio with other\n""label"":""Icecream flavor?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_2"",\n""formtype"":""s08"",\n""name"":""icecream_flavor"",\n""radios"":""vanilla\nchocolate"",\n""type"":""radio"",\n""version"": ""other"", // Variation version\n""required"":""true"",\n""class"":""custom-class""\n},\n]\n```\n* Add a new ""groupid"" attribute to the saved JSON form data object.\n* The new ""groupid"" will act as a dynamic id which will group all fields with the same groupid together.\n* The ""groupid"" value will be generated once it is dragged/added to the editing form and be a concatenation of the form group template name (see below) and an incremental number, ie: g_address_streetonly_1\n* Versions of form groups do not depend on each other.\n* Added form groups to the form will be non-editable but will otherwise appear as regular fields within the JSON form data object.\n```\nExample of form data with a group\n""data"":\n[\n{\n// pizza is not in the group\n""label"":""Pizza"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""pizza_1"",\n""formtype"":""s08"",\n""name"":""pizza"",\n""radios"":""unpopular\nreally unpopular"",\n""type"":""radio"",\n""required"":""true"",\n""class"":""custom-class""\n},\n{\n""label"":""Icecream?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_1"",\n""formtype"":""s08"",\n""name"":""icecream"",\n""radios"":""yes\nno"",\n""type"":""radio"",\n""required"":""true"",\n""class"":""custom-class"",\n""groupid"":""g_icecream_all_1""\n},\n{\n""label"":""Icecream flavor?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_2"",\n""formtype"":""s08"",\n""name"":""icecream_flavor"",\n""radios"":""vanilla\nchocolate"",\n""type"":""radio"",\n""version"": ""other"", // Variation version\n""required"":""true"",\n""class"":""custom-class"",\n""groupid"":""g_icecream_all_1""\n},\n]\n```\n* HTML Generation: Create a helper listing of form group templates which will describe which form fields belong to which predefined group.\n* Format would be a JSON with an array of formtypes: each formtype would adopt the same attributes as the existing form field JSON.\n* Undefined attributes would be filled in by the default value and defined attributes would override the default.\n* Naming groups will start with g_groupname_templatename\n```\nExample of form group templates\n""address"" : {\n""full"" : [\n{\n""formtype"":""c08"",\n""label"":""Address 1"",\n},\n{\n""formtype"":""c08"",\n""label"":""Address 2"",\n},\n{\n""formtype"":""c10"", //city\n},\n{\n""formtype"":""s14"", //state\n},\n{\n""formtype"":""c14"", //zip\n}\n],\n""streetzip"" : [\n{\n""formtype"":""c08"",\n},\n{\n""formtype"":""c14"", //zip\n}\n]\n}\n```\n","**Decision:** Use a custom field type to solve both problems.\n\n**Justification:**\n\nThis decision addresses both problems by introducing a single, flexible solution.\n\n* **Optional ""Other"" Input Field:** The custom field type can include a boolean property to enable the ""Other"" input field. This preserves all existing radio/checkbox functionality while adding the desired optionality.\n* **Pre-made Form Group Templates:** The custom field type can support templates that define the composition of form groups. This allows designers to create and reuse pre-defined combinations of address elements, simplifying form creation.\n\n**Benefits:**\n\n* **Code Reusability:** A single custom field type handles both requirements, reducing code duplication and maintenance overhead.\n* **Flexibility:** The templates feature allows for easy customization and adaptation to different scenarios.\n* **Improved UX:** The custom field type provides a consistent and user-friendly experience for entering both radio/checkbox values and address information.\n\n**Considerations:**\n\n* **Development Effort:** Creating and testing the custom field type will require some development effort.\n* **Potential Complexity:** The custom field type may become complex if it needs to support a wide range of functionality.\n* **Documentation:** Thorough documentation is essential to ensure developers understand how to use the custom field type effectively."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNATS Server has a number of JSON based messages - monitoring, JetStream API and more. These are consumed,\nand in the case of the API produced, by 3rd party systems in many languages. To assist with standardization\nof data validation, variable names and more we want to create JSON Schema documents for all our outward facing\nJSON based communication. Specifically this is not for server to server communication protocols.\nThis effort is ultimately not for our own use - though libraries like `jsm.go` will use these to do validation\nof inputs - this is about easing interoperability with other systems and to eventually create a Schema Registry.\nThere are a number of emerging formats for describing message content:\n* JSON Schema - transport agnostic way of describing the shape of JSON documents\n* AsyncAPI - middleware specific API description that uses JSON Schema for payload descriptions\n* CloudEvents - standard for wrapping system specific events in a generic, routable, package. Supported by all\nmajor Public Clouds and many event gateways. Can reference JSON Schema.\n* Swagger / OpenAPI - standard for describing web services that uses JSON Schema for payload descriptions\nIn all of these many of the actual detail like how to label types of event or how to version them are left up\nto individual projects to solve. This ADR describes how we are approaching this.\n\n## Decision\n","### Overview\nWe will start by documenting our data types using JSON Schema Draft 7. AsyncAPI and Swagger can both reference\nthese documents using remote references so this, as a starting point, gives us most flexibility and interoperability\nto later create API and Transport specific schemas that reference these.\nWe define 2 major type of typed message:\n* `Message` - any message with a compatible `type` hint embedded in it\n* `Event` - a specialized `message` that has timestamps and event IDs, suitable for transformation to\nCloud Events. Typically, published unsolicited.\nToday NATS Server do not support publishing Cloud Events natively however a bridge can be created to publish\nthose to other cloud systems using the `jsm.go` package that supports converting `events` into Cloud Event format.\n### Message Types\nThere is no standard way to indicate the schema of a specific message. We looked at a lot of prior art from CNCF\nprojects, public clouds and more but found very little commonality. The nearest standard is the Uniform Resource Name\nwhich still leaves most of the details up to the project and does not conventionally support versioning.\nWe chose a message type like `io.nats.jetstream.api.v1.consumer_delete_response`, `io.nats.server.advisory.v1.client_connect`\nor `io.nats.unknown_message`.\n`io.nats.unknown_message` is a special type returned for anything without valid type hints. In go that implies\n`map[string]interface{}`.\nThe structure is as follows: io.nats.`<source>`.`<catagory>`.v`<version>`.`<name>`\n#### Source\nThe project is the overall originator of a message and should be short but descriptive, today we have 2 - `server` and `\njetstream` - as we continue to build systems around Stream Processing and more we'd add more of these types. I anticipate\nfor example adding a few to Surveyor for publishing significant lifecycle events.\nGenerated Cloud Events messages has the `source` set to `urn:nats:<source>`.\n|Project|Description|\n|-------|-----------|\n|`server`|The core NATS Server excluding JetStream related messages|\n|`jetstream`|Any JetStream related message|\n#### Category\nThe `category` groups messages by related sub-groups of the `source`, often this also appears in the subjects\nthese messages get published to.\nThis is a bit undefined, examples in use now are `api`, `advisory`, `metric`. Where possible try to fit in with\nexisting chosen ones, if none suits update this table with your choice and try to pick generic category names.\n|Category|Description|\n|----|-----------|\n|`api`|Typically these are `messages` used in synchronous request response APIs|\n|`advisory`|These are `events` that describe a significant event that happened like a client connecting or disconnecting|\n|`metric`|These are `events` that relate to monitoring - how long did it take a message to be acknowledged|\n#### Versioning\nThe ideal outcome is that we never need to version any message and maintain future compatibility.\nWe think we can do that with the JetStream API. Monitoring, Observability and black box management is emerging, and we\nknow less about how that will look in the long run, so we think we will need to version those.\nThe philosophy has to be that we only add fields and do not significantly change the meaning of existing ones, this\nmeans the messages stay `v1`, but major changes will require bumps. So all message types includes a single digit version.\n#### Message Name\nJust a string identifying what this message is about - `client_connect`, `client_disconnect`, `api_audit` etc.\n","**Decision:** Use JSON Schema for describing all outward facing JSON based communication.\n\n**Rationale:** JSON Schema is a transport agnostic way of describing the shape of JSON documents, which makes it a good choice for describing the messages that NATS Server exposes. JSON Schema is also relatively easy to use and understand, and it is well-supported by a variety of tools and libraries.\n\n**Consequences:**\n\n* **Positive:**\n    * Standardize data validation, variable names, and more for all outward facing JSON based communication.\n    * Ease interoperability with other systems.\n    * Create a foundation for a Schema Registry.\n* **Negative:**\n    * May require additional effort to create and maintain JSON Schema documents.\n    * May not be as expressive as other formats, such as AsyncAPI or CloudEvents."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis document describes NATS Headers from the perspective of clients. NATS\nheaders allow clients to specify additional meta-data in the form of headers.\nNATS headers are similar to\n[HTTP Headers](https://tools.ietf.org/html/rfc7230#section-3.2) with some important differences.\nAs with HTTP headers:\n- Each header field consists of a field name followed by a\ncolon (`:`), optional leading whitespace, the field value, and optional\ntrailing whitespace.\n- No spaces are allowed between the header field name and colon.\n- Field value may be preceded or followed by optional whitespace.\n- The specification may allow any number of strange things like comments/tokens\netc.\n- The keys can repeat.\nMore specifically from [rfc822](https://www.ietf.org/rfc/rfc822.txt) Section\n3.1.2:\n> Once a field has been unfolded, it may be viewed as being composed of a\n> field-name followed by a colon ("":""), followed by a field-body, and terminated\n> by a carriage-return/line-feed. The field-name must be composed of printable\n> ASCII characters (i.e., characters that have values between 33. and 126.,\n> decimal, except colon). The field-body may be composed of any ASCII\n> characters, except CR or LF. (While CR and/or LF may be present in the actual\n> text, they are removed by the action of unfolding the field.)\n### Unique to NATS Headers\n###### Version header\nInstead of an HTTP method followed by a resource, and the HTTP version (`GET / HTTP/1.1`),\nNATS provides a string identifying the header version (`NATS/X.x`),\ncurrently 1.0, so it is rendered as `NATS/1.0␍␊`.\n###### Case preserving\nNATS treats application headers as a part of the message _payload_ and is agnostic to the\napplication use-case between publishers and subscribers; therefore, NATS headers are _case preserving_.\nThe server will not change the case in message conveyance, the publisher's case will be preserved.\nAny case sensitivity in header interpretation is the responsibility of the application and client participants.\n> Note: This is _different_ from HTTP headers which declare/define that web server and user-agent participants should ignore case.\nWith above caveats, please refer to the\n[specification](https://tools.ietf.org/html/rfc7230#section-3.2) for information\non how to encode/decode HTTP headers.\n### Enabling Message Headers\nThe server that is able to send and receive headers will specify so in it's\n[`INFO`](https://docs.nats.io/nats-protocol/nats-protocol#info) protocol\nmessage. The `headers` field if present, will have a boolean value. If the\nclient wishes to send headers, it has to enable it must add a `headers` field\nwith the `true` value in its\n[`CONNECT` message](https://docs.nats.io/nats-protocol/nats-protocol#connect):\n```\n""lang"": ""node"",\n""version"": ""1.2.3"",\n""protocol"": 1,\n""headers"": true,\n...\n```\n### Publishing Messages With A Header\nMessages that include a header have a `HPUB` protocol:\n```\nHPUB SUBJECT REPLY 23 30␍␊NATS/1.0␍␊Header: X␍␊␍␊PAYLOAD␍␊\nHPUB SUBJECT REPLY 23 23␍␊NATS/1.0␍␊Header: X␍␊␍␊␍␊\nHPUB SUBJECT REPLY 48 55␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊PAYLOAD␍␊\nHPUB SUBJECT REPLY 48 48␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊␍␊\nHPUB <SUBJ> [REPLY] <HDR_LEN> <TOT_LEN>\n<HEADER><PAYLOAD>\n```\n#### NOTES:\n- `HDR_LEN` includes the entire serialized header, from the start of the version\nstring (`NATS/1.0`) up to and including the ␍␊ before the payload\n- `TOT_LEN` the payload length plus the HDR_LEN\n### MSG with Headers\nClients will see `HMSG` protocol lines for `MSG`s that contain headers\n```\nHMSG SUBJECT 1 REPLY 23 30␍␊NATS/1.0␍␊Header: X␍␊␍␊PAYLOAD␍␊\nHMSG SUBJECT 1 REPLY 23 23␍␊NATS/1.0␍␊Header: X␍␊␍␊␍␊\nHMSG SUBJECT 1 REPLY 48 55␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊PAYLOAD␍␊\nHMSG SUBJECT 1 REPLY 48 48␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊␍␊\nHMSG <SUBJECT> <SID> [REPLY] <HDR_LEN> <TOT_LEN>\n<PAYLOAD>\n```\n- `HDR_LEN` includes the entire serialized header, from the start of the version\nstring (`NATS/1.0`) up to and including the ␍␊ before the payload\n- `TOT_LEN` the payload length plus the HDR_LEN\n\n## Decision\n",Implemented and merged to master.\n,"Utilize NATS Header feature to add meta-data to messages, preserving case and allowing for multiple headers with the same name."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis document describes the _Lame Duck Mode_ server notification. When a server enters lame duck mode, it removes itself from being advertised in the cluster, and slowly starts evicting connected clients as per  [`lame_duck_duration`](https://docs.nats.io/nats-server/configuration#runtime-configuration). This document describes how this information is notified\nto the client, in order to allow clients to cooperate and initiate an orderly migration to a different server in the cluster.\n\n## Decision\n","The server notififies that it has entered _lame duck mode_ by sending an [`INFO`](https://docs.nats.io/nats-protocol/nats-protocol#info) update. If the `ldm` property is set to true, the server has entered _lame_duck_mode_ and the client should initiate an orderly self-disconnect or close. Note the `ldm` property is only available on servers that implement the notification feature.\n",The client can be notified of the lame duck mode by listening for the `CLUSTER_EVENT_PING` command.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe client library should take a random IP address when performing a host name resolution prior to creating the TCP connection.\n\n## Decision\n,"This was driven by the fact that the Go client behaves as described above and some users have shown interest in all clients behaving this way.\nSome users have DNS where the order almost never change, which with client libraries not performing randomization, would cause all clients\nto connect to the same server.\n","**Decision:** Implement a random IP address selection algorithm for host name resolution in the client library.\n\n**Rationale:**\n\n* **Improved resilience:** Randomly selecting an IP address reduces the risk of a single point of failure in case one or more IP addresses become unavailable.\n* **Load balancing:** By distributing requests across multiple IP addresses, the load is balanced and ensures optimal performance.\n* **Security:** Random IP address selection makes it more difficult for attackers to predict and target specific IP addresses.\n* **Compliance:** Some regulations or policies may require the use of random IP address selection for host name resolution.\n\n**Implementation Plan:**\n\n* Implement a function or method in the client library to select a random IP address from the list of resolved IP addresses.\n* Use a random number generator to select the IP address.\n* Incorporate error handling to handle cases where no IP addresses are resolved.\n\n**Consequences:**\n\n* **Increased complexity:** The client library will require additional code to implement the random IP address selection algorithm.\n* **Potential latency increase:** Selecting a random IP address may introduce a slight latency increase as the client library needs to resolve multiple IP addresses and select one.\n* **Security implications:** If the random number generator is not secure, it could lead to a compromised IP address being selected."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to use OLS Api results as objects (Ontology, Term, ...) instead of standard coreapi Document objects.\nCoreapi API results are Document (a kind of OrderedDict in fact). making access to property annoying for programmer,\nhaving to know expected keys.\n\n## Decision\n",To implement.\n,"We will use the python-semanticweb library to transform the Document objects into Ontology, Term, ... objects."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to loop simply over Ontologies / Terms results, without bothering if a\nnew call is made to change page.\nOLS API results are paginated, the page size is a parameter in Query.\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\nThe actual calls to API are hidden from final users.\n```python\nfrom ebi.ols.api.client import OlsClient\nclient = OlsClient()\nontology = client.ontology('fpo')\nterms = ontology.terms()\nindividuals = ontology.individuals()\nproperties = ontology.properties()\n# work with all 'list' item types\nfor term in terms:\n# do whatever\nprint(term)\n# Direct List'like access on all list types\nterm = terms[1254]\nindividual = individuals[123]\n# ...\n```\n\n## Decision\n",To Implement\n,Create a `LazyObject` layer over pagination.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",Architectural decisions will be recorded using an Architectural Decision Record (ADR). ADRs will be maintained in the project repository in the `docs/architecture/decisions` directory. ADRs will follow a standardized template and will be reviewed and approved by the architecture team before being merged.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What type of arichitecture is to be used ?**\nConsidered options:\n1. Monolith\n2. Modular Monolith\n3. Microservices\nDrivers:\n* Project is realized as GreenField\n* A small development team (~10)\n* Little experience in implementing distributed software\n* Load - (50-150 req/sec) - not so much\n\n## Decision\n,"Option no 2 - Modular Monolith.\nArchitecture suitable for teams with little experience in implementing distributed systems.\nIt gives the possibility of gradual migration to distributed. Suitable for  greenfield systems, where high variation in requirements is expected.\narchitecture.\n","After careful consideration of the drivers and options available, we have decided to adopt a **modular monolith** architecture for this project.\n\nA modular monolith architecture provides the following benefits:\n\n* It allows us to develop and deploy the application as a single unit, which simplifies deployment and maintenance.\n* It provides better performance than a microservices architecture, as there is no overhead from inter-process communication.\n* It is easier to implement and manage than a microservices architecture, as there is no need to worry about service discovery, load balancing, or other distributed systems concerns.\n\nWe believe that a modular monolith architecture is the best fit for this project, given the small development team, the lack of experience in implementing distributed software, and the relatively low load."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What kind of database should be used to storage aggregates data ?**\nConsidered options:\n1. Document database (MongoDB)\n2. Relational database\nDrivers:\n* Simple objects will be saved.\n* There will be no relationship between objects .\n* A Read Model combining data from different aggregates is required .\n\n## Decision\n,Option no 1 - document database - MongoDB.\n,"**Decision:** Use a document database (MongoDB).\n\n**Rationale:**\n\n* A document database is more suitable for storing simple objects without relationships, which is the case for the aggregates data.\n* MongoDB provides flexibility and scalability for storing and querying large amounts of data.\n* The lack of relationships between objects simplifies the data model and makes it easier to maintain.\n* MongoDB supports aggregation and querying features, enabling the creation of a Read Model that combines data from different aggregates."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What technology is to be used for persistence?**\nConsidered options:\n1. Spring Template\n2. JPA + Spring Repositores\nDrivers:\n1. Technology must be simple and well known\n2. Cannot require a lot of code writing\n\n## Decision\n,Option no 2 - JPA+Spring Repositories.\n,**Spring Template**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What type of application architecture is to be used ?**\nConsidered options:\n1. Layered architecture\n2. Ports and adapters architecture\nDrivers:\n* The Reservation module is an example of a deep module.\n* A large number of business rules.\n* Some of the rules are quite complicated.\n* Application should be well - tested .\n\n## Decision\n,Option no 2 - Ports and Adapters architecture. It is dedicated for modules with high business complexity.\nIt separates domain logic from application logic. It allows to create independent tests for each type of logic.\n,"The decision is to use a layered architecture for the application.\n\n**Reasoning:**\n\n* A layered architecture provides a clear separation of concerns, which is important for a complex application with a large number of business rules.\n* The layered architecture makes it easier to test the application, as the different layers can be tested independently.\n* The layered architecture is also more scalable than a ports and adapters architecture, as it is easier to add new layers to the application as needed."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement an Architectural Decision Record (ADR) system.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* This is Open Source app which may be used by big variety of projects\n* TypeScript is widely known nowadays, so there is high chance to get contribution from other peple if this technology is used\n* Project's author and main contibutor (@kopach) is fluent in this technology, so no need in spending extra efort in learning something differrent\n* TypeScript is type safe, so more secure and potentially should prevent from common mistakes\n* TypeScript integrates well with JavaScript so all libraries from both ecosystems can be used easilly\n\n## Decision\n",Use TypeScript as main and only programming language in this project\n,Use TypeScript as the main programming language for the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe design proposed in [018](018-Dedicated-Ingress-Controllers.md) proved\nimpossible to implement. Every ingress controller requires an AWS Network Load\nBalancer (NLB), and AWS have hard limits on the number of NLBs we can create\nper VPC and availability zone (AZ).\n\n## Decision\n","We will:\n* Disable mod-security on the default ingress-controller\nThis should enable this ingress-controller to comfortably handle thousands of ingresses.\n* Create and manage a set of ingress controllers with mod-security enabled\nMost, if not all, production services will want the protection of a web\napplication firewall, and mod-security is the easiest to enable. We need to\nensure that each ingress controller only handles as many mod-security-enabled\ningresses as it can reliably cope with.\n* Continue to have some dedicated ingress controllers\nDuring our aborted migration to dedicated ingress controllers for every\nnamespace, several ingress controllers were created for specific services. We\nwill leave these in place to avoid additional disruption to these service\nteams.\n",Implement a single NLB-based ingress controller per VPC per AZ.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMOJ Digital's approach to infrastructure management and ownership has evolved over time, and has led to the following outcomes:\n- Unclear boundaries on ownership and responsibilities between service teams and the cloud platforms team\n- Significant variation in deployment, monitoring and lifecycle management across products\n- Inefficient use of AWS resources due to the use of virtual machine-centric architecture, despite our standardisation on Docker containers\nThe last few years has seen the advent of several products specifically focused on the problem of running and managing containers in production:\n- Kubernetes\n- Mesos / Mesosphere / DC/OS\n- Docker Swarm\n- AWS ECS\n- CloudFoundry\nGiven the technology landscape within MOJ, we require a container management platform that can support a wide range of applications, from ""modern"" cloud-native 12-factor applications through to ""legacy"" stateful monolithic applications, potentially encompassing both Linux- and Windows-based applications; this removes CloudFoundry from consideration, given its focus on modern 12-factor applications and reliance on buildpacks to support particular runtimes.\nFrom the remaining list of major container platforms, Kubernetes is the clear market leader:\n- Rapid industry adoption during 2017 establishing it as the emerging defacto industry standard\n- Managed Kubernetes services from all major cloud vendors\n- Broad ecosystem of supporting tools and technologies\n- Increasing support for Kubernetes as a deployment target for commercial and open-source software projects\nThere is also precedent for Kubernetes use within MOJ, as the Analytical Platform team has been building on top of Kubernetes for around 18 months.\n\n## Decision\n",Use Kubernetes as the container management component and core technology for our new hosting platform.\n,**Decision:** We will standardise on Kubernetes as the MOJ Digital container management platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able to use it, however, given that:\n- Switching to Concourse CI would require familiarity with how it works and incur additional overhead\n- We have not yet developed a streamlined approach to deployments through Concourse CI in order to confidently and properly offer support\n- Product teams already use third party CI systems\nWe think it would be good as a starting point to make it easy for teams to deploy directly from the third party CI systems that teams are already using, rather than requiring the deployments to be implemented in Concourse CI.\n\n## Decision\n","We will support deploying applications to the Cloud Platform from third party CI systems and will offer documentation on how to do so, at least for the most commonly used CI systems.\n","Teams can deploy directly from the third party CI systems that they are already using, rather than requiring the deployments to be implemented in Concourse CI."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a lot of scripts, pipeline definitions, terraform files, yaml files and\ntemplates which need to define and use variables. We want a consistent\nconvention for naming these so that, as we write code in multiple,\ninter-dependent repositories, we can be confident that the names we are using\nare correct.\n\n## Decision\n",We will always use snake case (e.g. `foo_bar`) for variable names which appear\nin terraform/yaml files and templates.\n,"We will use a consistent naming convention for variables across all repositories. The convention will be as follows:\n\n* Variable names will be all lowercase.\n* Variable names will be descriptive and specific.\n* Variable names will be prefixed with the repository name, followed by a period.\n* For example:\n\n```\n# This variable is used to specify the name of the S3 bucket to use.\npipeline.s3_bucket_name\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMigrating from live-0 to live-1 cluster. The reason behind this is based on the need to move to a dedicated AWS account (moj-cp), which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region as Cloud Platform requirement to host data in the UK, rather than in Europe.\n\n## Decision\n","After some long consideration of possible options, the decision has been made to migrate from the live-0 cluster to the new live-1 cluster.\nSince we only want to be running a single cluster, we will need to shut down live-0 as soon as it's no longer needed. Also services migrate from live-0 to live-1 sooner will avoid the complexities of running two parallel clusters.\n",Migrate the live-0 cluster to a dedicated AWS account (moj-cp) in the London (UK) region.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we are building our new platform on Kubernetes we have already found the need to build quite a few clusters. These have been for a range of purposes including users, testing new ideas (""sandbox""), testing new functionality (""test""), deploying apps to them (""non-production"").\nAs we are still learning we are finding that:\n1. we need to continue building new clusters for different purposes and\n2. we often need to test the cluster creation process\n3. we want to differentiate between clusters that have users on them and those that are for internal testing purposes\n4. we do not want to differentiate cluster by function (e.g. ""perf-test"", ""sandbox"") or status (""non-production"").\nTo make this easier we propose having a naming scheme that makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.\n\n## Decision\n","We will name all clusters with the following naming scheme:\n- `live-{n}` for any cluster that have users on them, for instance `live-1`.\n- `test-{n}` for any cluster that do not have users on them and are used by the cloud platform team only, for instance `test-2`.\nWe will number the clusters sequentially.\n",We will use the following naming scheme for our Kubernetes clusters:\n- `user-` for clusters that have users on them\n- `internal-` for clusters that are for internal testing purposes\nThis naming scheme makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team self-host an Elasticsearch cluster with Kibana and Logstash (ELK). This cluster has suffered numerous outages (see [CPT-282](https://dsdmoj.atlassian.net/browse/CPT-282) and [CPT-152](https://dsdmoj.atlassian.net/browse/CPT-152) in Jira) that have been difficult to recover from.\nReasons behind this move were:\n* Average of almost one week per month spent on debugging, fixing and reviving ELK\n* Lengthy downtimes which made data recovery pointless\n* Self hosted ELK stacks cost was significantly higher than AWS ElasticSearch solution\n* Not working ELK cluster was also a blocker for product teams as they couldn't see any application logs\n\n## Decision\n",Replace our self hosted ELK stack with the managed AWS Elasticsearch\n,**Migrate the self-hosted ELK cluster to AWS ElasticSearch.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Cloud Platform was originally set up to have a single ingress controller to\nmanage all ingresses in the cluster. So, every new ingress added a config block\nto one, large nginx config file, and all traffic to all services hosted on the\nCloud Platform is routed through a single AWS load balancer.\nAlthough this was both easy to manage, and saved us some money on load\nbalancers (approx. $25/month per ingress), it has become unsustainable. We\nusually have 6 replicas of the ingress controller pod, and we have started to\nsee instances of several of these pods crash-looping (usually because they have\nrun out of shared memory, which cannot be increased in kubernetes. See [this\nissue] for more information).\nWe believe this is because the nginx config has become so large (over 100K\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\nmoved.\n\n## Decision\n","We will create a separate AWS load balancer and ingress-controller for every\nnamespace in the cluster. An ""ingress class"" annotation will cause traffic for\na particular ingress to be routed through the appropriate AWS load balancer and\ningress-controller. See our [module repository] for more details.\n""System"" ingresses (e.g. those used for concourse, grafana, etc.) will continue\nto use the default ingress-controller. There should only ever be a handful of\nthese, compared with hundreds of team ingresses, so the load on the default\ningress-controller should stay within acceptable limits.\n","Migrate from a single ingress controller with a large config file to multiple ingress controllers, each with a smaller config file. This will reduce the load on each ingress controller and improve the stability of the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe maintain a lot of [terraform modules] which teams use to manage AWS\nresources for their namespaces.\nIn the past, lettings different namespaces use different versions of terraform\nmodules has caused problems because there has not always been a clear upgrade\npath from a module that is several versions old, to the latest version.\nIn these cases, it would have been easier to have upgraded every namespace when\nthe significant change was made to the module, because at that point in time\nthe relevant knowledge and understanding are fresh. Leaving it until later\nmakes the whole process much more difficult.\n\n## Decision\n",We decided to ensure that all namespaces are always using the latest version of\nevery module.\n,"Maintain a single version of each terraform module used in namespaces, and enforce that all namespaces using that module are upgraded to any new versions as soon as possible (ideally within one or two weeks of the new version being released)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\nTo support this, should we have separate clusters for production, development, and staging? Or, should we run a single cluster hosting all these different types of workload?\n\n## Decision\n","After consideration of the pros and cons of each approach we went with one cluster, using namespaces to partition different workloads.\nSome important reasons behind this move were:\n- A single k8s cluster can be made powerful enough to run all of our workloads\n- Managing a single cluster keeps our operational overhead and costs to a minimum.\n- Namespaces and RBAC keep different workloads isolated from each other.\n- It would be very hard to keep multiple clusters (dev/staging/prod) from becoming too different to be representative environments\nTo clarify the last point; to be useful, a development cluster must be as similar as possible to the production cluster. However, given multiple clusters, with different security and other constraints, some 'drift' is inevitable - e.g. the development cluster might be upgraded to a newer kubernetes version before staging and production, or it could have different connectivity into private networks, or different performance constraints from the production cluster.\nBased on our past experience, these differences tend to increase over time, to the point where the development cluster is too far away from production to be representative. The extra work required to maintain multiple environments becomes wasted effort.\nIf namespace segregation is sufficient to isolate one production service from another, then it is enough to isolate a team's development environment from a production service.\nIf namespace segregation is not sufficient for this, then the whole cloud platform idea doesn't work.\n","**Decision:** Have separate clusters for production, development, and staging.\n\n**Rationale:**\n\n* **Isolation:** Separate clusters isolate production workloads from development and staging, reducing the risk of production outages due to changes in other environments.\n* **Resource Allocation:** Different clusters can be tailored to the specific needs of each environment, optimizing resource allocation and performance.\n* **Security:** Production clusters can implement more stringent security measures than development and staging clusters, ensuring the protection of sensitive data.\n* **Cost Optimization:** Development and staging clusters can be provisioned with lower-cost resources, reducing overall infrastructure costs.\n* **Scalability:** Separate clusters allow for independent scaling of each environment, ensuring performance and availability as needed.\n* **Deployment Flexibility:** Independent clusters provide flexibility in deploying new features or updates, as changes in one environment do not impact the others.\n* **Compliance:** Separate clusters may be required for compliance with regulations or industry standards that mandate segregation of different environments."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.\n\n## Decision\n,It has been decided to use a combination of Auth0 and an OIDC proxy app. The application is managed in the [cloud-platform-terraform-monitoring repo][kibana-proxy] and configured ministryofjustice GitHub organization users to access Kibana.\n,Implement an API Gateway that proxies requests to Kibana with a custom authorizer to handle Github authentication. The API Gateway will be configured with a custom domain so that users can access Kibana directly without having to go through the AWS console.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn the Cloud Platform, there is a need to implement various policies to safeguard our tenant applications and to enforce best practices.\nKubernetes offers various mechanisms that cover some of our needs (eg.: `ResourceQuotas` to prevent resource exhaustion and `PodSecurityPolicies` to enforce non-root containers) but there are other areas for which there is no builtin solution. However, kubernetes implements a Dynamic Admission Control API which introduces [admission webhooks][admission-control]. This API provides an easy way with which to expand on the existing admission controllers (built in the apiserver).\nOur immediate need was to prevent users from reusing hostnames in `Ingresses`. Although our ingress controller prevents hijacking of hostnames, it does so silently and furthermore, this is not a documented behaviour. Therefore, we decided that the user should not be allowed to reuse hostnames already defined in other `Ingresses` and receive a useful error message if they try to do that.\n\n## Decision\n","We explored a number of existing solutions in the open source community, as well as the possibility of implementing our own and we also discussed the issue with other organisations that use kubernetes before reaching a conclusion.\nEventually we decided to introduce the [Open Policy Agent][open-policy-agent]:\n- It is a generic framework for building and enforcing policies (whereas most other existing implementations were designed around specific problems)\n- The policies are defined in a declarative, high-level language\n- It is designed for cloud-native environments\n- It provides a kubernetes integration\n- It provides a way by which to unit test the policies\n- The project is adopted by CNCF\nAlthough the project is still in alpha and very likely to change in the near future, we decided that it is stable enough for our needs and worth adopting even at these early stages, since the benefits outweigh the cost.\n",Implement a Kubernetes Admission Webhook to enforce the uniqueness of hostnames in `Ingresses`.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team currently use a self-host Jenkins server for CI/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https://dsdmoj.atlassian.net/browse/CPT-364)  There is also a reoccurring issue which has caused a number of outages [CPT-384](https://dsdmoj.atlassian.net/browse/CPT-384).\nReasons behind this move were:\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\n* Unnecessary downtimes\n* With the move to Kubernetes a cloud native CI/CD solution is needed, all jobs are written as code\n* Non working jenkins was a blocker for product teams\n\n## Decision\n",Replace self hosted Jenkins with self hosted Concourse CI pipeline\n3. Automate deployment of concourse using Terraform and Helm\n4. WIP\n~~Product teams have confirmed their success builds and deploys. zero downtime and minimal maintenance from Cloud Platforms side.~~\n,Migrate CI/CD to a cloud native solution like ArgoCD.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https://github.com/ministryofjustice/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\nPutting technical documentation in GitHub has additional benefits:\n* Using pull requests gives us a clear review and approval process\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\n* The information can be held in the open and viewed by anyone\n\n## Decision\n,1. Our architecture decision log for the new cloud platform will be held in GitHub\n,"**Decision:** Store technical documentation alongside the code it describes in GitHub.\n\n**Rationale:**\n\n* GitHub provides a clear review and approval process through pull requests.\n* It integrates well with daily development workflows (e.g., writing code).\n* The information can be made publicly accessible for transparency.\n* This approach aligns with the team's desire to keep technical documentation close to the code it implements, ensuring consistency and relevance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to be able to test that the production cluster(s) and any test clusters we create behave the way we expect them to.\nSo, we need automated tests that exercise a cluster and confirm that the effect we get is the one we expected.\n\n## Decision\n","We chose the ruby testing framework [rspec] for this.\nAlthough there are some go-based testing frameworks for kubernetes, there are some problems with them, and some benefits to rspec:\n* The kubernetes testing frameworks seem quite immature, with limited documentation, tooling and other resources such as examples\n* There is limited go expertise in the team\n* RSpec is a very mature framework, with a lot of tooling, documentation and support\n* There is a lot of ruby/rspec experience in the wider organisation\n* Ruby is our scripting language of choice, so rspec fits with that\n",Use [ClusterFuzz](https://github.com/GoogleCloudPlatform/cluster-fuzz).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nService teams' apps currently run on [one Kubernetes cluster](https://github.com/ministryofjustice/cloud-platform/blob/main/architecture-decision-record/012-One-cluster-for-dev-staging-prod.md). That includes their dev/staging/prod environments - they are not split off. The key reasoning was:\n- Strong isolation is already required between apps from different teams (via namespaces, network policies), so there is no difference for isolating environments\n- Maintaining clusters for each environment is a cost in effort\n- You risk the clusters diverging. So you might miss problems when testing on the dev/staging clusters, because they aren't the same as prod.\n(We also have clusters for other purposes: a 'management' cluster for Cloud Platform team's CI/CD and ephemeral 'test' clusters for the Cloud Platform team to test changes to the cluster.)\nHowever we have seen some problems with using one cluster, and advantages to moving to multi-cluster:\n- Scaling limits\n- Single point of failure\n- Derisk upgrading of k8s\n- Reduce blast radius for security\n- Reduce blast radius of accidental deletion\n- Pre-prod cluster\n- Cattle not pets\n### Scaling limits\nMulti-cluster helps us we encounter a scale limitation. For example, we've found ourselves unexpectantly hitting an AWS limit during k8s upgrade. In this situation we could off-load some apps to another cluster. It would be advantageous to put each cluster in its own AWS account, to avoid limits, which are imposed per-account.\n### Single point of failure\nRunning everything on a single cluster is a 'single point of failure', which is a growing concern as more services use CP. Multi-cluster would allow us to quickly move apps off a broken cluster to another cluster.\nSeveral elements in the cluster are a single point of failure:\n- ingress (incidents: [1](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-10-06-09-07-intermittent-quot-micro-downtimes-quot-on-various-services-using-dedicated-ingress-controllers) [2](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-04-15-10-58-nginx-tls))\n- external-dns\n- cert manager\n- kiam\n- OPA ([incident](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-02-25-10-58))\n### Derisk upgrading of k8s\nOnce you start a Kubernetes version upgrade, rolling back becomes infeasible (incidents: [1](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#q1-2020-january-march)).\nWith multi-cluster we could do a ""blue-green"" upgrade - spin up an cluster at the newer k8s version and then carefully move the apps across to it.\n### Reduce blast radius for security\nTaking a layered approach to security, extra isolation is beneficial. It resists lateral movement and minimizes the impact of a breach.\nIsolation is added when you split the workloads across multiple clusters, even if they are in the same VPC. And further isolation is gained with separate VPCs or separate AWS accounts.\nMore sensitive apps may require this isolation.\nPre-prod environments are likely to often be running new code that has not have been through all the reviews, quality and security checks yet, so there may be a case for keeping these more isolated from environments with access to production data.\n### Reduce blast radius of accidental deletion\nIn the case of accidental deletion being run by a Cloud Platform team member, the ability to run administrative commands on only one cluster at a time would reduce the impact of this event, such as [this incident](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-09-21-18-27-some-cloud-platform-components-destroyed). Disaster Recovery procedures are in [good shape now](https://runbooks.cloud-platform.service.justice.gov.uk/disaster-recovery-scenarios.html#cloud-platform-disaster-recovery-scenarios), but it's worth minimizing blast radius all the same.\n### Pre-prod cluster\nTest clusters are used by the Cloud Platform team to test changes. These are made as realistic as possible, inheriting IaC and config from the main cluster, and the availability of test apps. However test clusters have a different lifecycle, don't run realistic loads and don't have traffic / loads. Ideally we test CP changes in the most realistic way possible, before changes are deployed to the part of the platform where production workloads are running.\nMulti-cluster will allow us to put pre-prod environments on a separate cluster to prod environments. Then changes to the platform, once tested on a test cluster, we can roll them out to the pre-prod cluster, as a more realistic test, ahead of rolling out to the prod clusters. This is particularly beneficial for more fundamental changes like upgrading k8s, and single points of failure such as ingress or OPA.\n### Challenge of moving apps\nIf we were to create a fresh cluster, and an app is moved onto it, then there are a lot of impacts:\n- **Kubecfg** - a fresh cluster will have a fresh kubernetes key, which invalidates everyone's kubecfg. This means that service teams will need to obtain a fresh token and add it to their app's CI/CD config and every dev will need to refresh their command-line kubecfg for running kubectl.\n- **IP Addresses** - unless the load balancer instance and elastic IPs are reused, it'll have fresh IP addresses. This will particularly affect devices on mobile networks that accessing our CP-hosted apps, because they often cache the DNS longer than the TTL. And if CP-hosted apps access third party systems and have arranged for our egress IP to be allow-listed in their firewall, then they will not work until that's updated.\n\n## Decision\n",TBD\n,"**Decision**: We will move to a multi-cluster architecture. Each cluster will be dedicated to each environment using the same lifecycle as the environment e.g. one cluster for dev/test, one for prod. Environment name will be added to cluster name e.g. 'platform-test-ua'\n\n**Rationale**: As we can see from the Context section, there are several advantages to moving to a multi-cluster architecture, including:\n- Scaling limits\n- Single point of failure\n- Derisk upgrading of k8s\n- Reduce blast radius for security\n- Reduce blast radius of accidental deletion\n- Pre-prod cluster\n\nThe main challenge of moving to a multi-cluster architecture is the need to reconfigure kubecfg and IP addresses for each cluster. However, this is a one-time cost, and the benefits of moving to a multi-cluster architecture outweigh the costs.\n\n**Consequences**: \n- Service teams will need to obtain fresh tokens and add them to their app's CI/CD config.\n- Every dev will need to refresh their command-line kubecfg for running kubectl.\n- CP-hosted apps may experience some downtime while their IP addresses are updated.\n- There is a risk that some apps may not be able to be moved to a new cluster due to technical dependencies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\n\n## Decision\n","After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\n","Enforce a lifecycle policy with a retention period of 180 days for ECR repositories created for use in the Cloud Platform to ensure that the AWS limit of 1000 images is not hit. This ensures that older images are automatically deleted, keeping the registry within the AWS limit and avoiding potential issues with deploying applications. Additionally, teams are encouraged to implement their own cleanup mechanisms to ensure that only necessary images are retained."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMoJ Cloud Platform team has decided to use [kubernetes for container management platform](https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/004-use-kubernetes-for-container-management.md) following the outcome of MOJ Digital's approach to infrastructure management. The team needed the below features for the infrastructure management:\n- An universal authentication mechanism to manage users without depending on the cloud provider\n- Able to customize the control plane of kubernetes for MOJ requirement on Pod security\n- Able to integrate external tools easily\n- Able to manage and configure the control plane for any custom changes\n\n## Decision\n,"There are several leading cloud providers who provide managed production-ready kubernetes cluster:\n- Amazon Elastic Kubernetes Service (Amazon EKS)\n- Azure Kubernetes Service (AKS)\n- Google Kubernetes Engine (GKE)\nWe decided to host our cluster on AWS because our service team has good development experience working with AWS services. This made it easier for teams to migrate to the kubernetes platform\nWe decided to manage the kubernetes cluster ourselves rather than using EKS mainly for the below reasons:\n- When the time MOJ needed to build the kubernetes, Amazon EKS was still in the Alpha stage and was not production ready. Also Amazon EKS require to use IAM for user authentication which will be an overhead for managing users of service teams\n- Kubernetes(k8s) allows to authenticate using OIDC and therefore it was easy to manage the authentication externally using Auth0\n",Use [OpenShift Container Platform](https://www.redhat.com/en/technologies/cloud-computing/openshift) as the container management platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe key proposition of Cloud Platform is to do the ""hosting"" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\nIn agreeing a good interface for service teams, there several concerns:\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\n\n## Decision\n","1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads/resources.\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\n* RBAC - teams can only administer k8s resources in their own namespaces\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\n4. Isolation between AWS resources is achieved using access control.\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\n6. The namespace definition is held in GitHub.\nGitHub provides a mechanism for peer-review, automated checks and versioning.\nOther options considered for configuring a namespace do not come with these advantages, for example:\n* a console / web form, implemented as a custom web app (click ops)\n* commands via a CLI or API\nNamespace definitions are stored in the [environments repo](https://github.com/ministryofjustice/cloud-platform-environments)\n7. Namespace changes are checked by both a bot and a human from the CP team\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\nTo mitigate these concerns:\n* [automated checks](https://github.com/ministryofjustice/cloud-platform-environments/tree/main/.github/workflows) are used to validate against common problems\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\n8. Pipeline to deploy namespace automatically.\nThe ""deploy pipeline"" is a CI/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\n9. Teams have full control within their Kubernetes namespace\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI/CD, for deploying their app.\n","To address multi-tenancy concerns, adopt the concept of _namespaces_ in Kubernetes. These partitions have limited resources and provide isolation between different workspaces. Service teams can have one namespace per environment (dev, test, prod), and one per application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of our [planning principles](https://docs.google.com/document/d/1kHaghp-68ooK-NwxozYkScGZThYJVrdOGWf4_K8Wo6s/edit) we highlighted ""Building in access control"" as a key principle for planning our building our new cloud platform.\nMaking this work for the new cloud platform means implementing ways that our users &mdash; mainly developers &mdash; can access the various bits of the new infrastructure. This is likely to include access to Kubernetes (CLI and API), AWS (things like S3, RDS), GitHub, and any tooling we put on top of Kubernetes that users will access as part of running their apps (e.g. ELK, [Prometheus](https://github.com/ministryofjustice/cloud-platform/blob/main/architecture-decision-record/026-Managed-Prometheus.md#choice-of-prometheus), [Concourse](https://github.com/ministryofjustice/cloud-platform/blob/main/architecture-decision-record/003-Use-Concourse-CI.md)).\nAt the current time there is no consistent access policy for tooling. We use a mixture of the Google domain, GitHub and AWS accounts to access and manage the various parts of our infrastructure. This makes it hard for users to make sure that they have the correct permissions to do what they need to do, resulting in lots of requests for permissions. It also makes it harder to manage the user lifecycle (adding, removing, updating user permissions) and to track exactly who has access to what.\nWe are proposing that we aim for a ""single sign on"" approach where users can use a single logon to access different resources. For this we will need a directory where we can store users and their permissions, including what teams they belong to and what roles they have.\nThe current most complete source of this information for people who will be the first users of the cloud platform is GitHub. So our proposal is to use GitHub as our initial user directory - authentication for the new services that we are building will be through GitHub.\n\n## Decision\n",We will use GitHub as the identify provider for the cloud platform.\nWe will design and build the new cloud platform with the assumption that users will login to all components using a single GitHub id.\n,We will use GitHub as our initial user directory for the new cloud platform. Authentication for the new services that we are building will be through GitHub.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team currently use Docker Registry for storage of docker images. This solution is self-hosted, needs regular patching, and occasionally has downtime.\nExample of an issue [CPT-274](https://dsdmoj.atlassian.net/browse/CPT-274).\nWe want to update the container registry to avoid some of the problems we have been seeing with it. The container registry will also be a key part of our new [Kubernetes based infrastructure](https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/004-use-kubernetes-for-container-management.md).\nThe criteria for selecting a new solution included:\n* Finding a solution that would work with well GitHub based identity which is one of our [major architectural choices](https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/006-Use-github-as-user-directory.md)\n* Decreasing the amount of day to day ops work, perhaps by using a managed service\n* Good value for money\n* Preference for open source tooling\n* Can or is able to run addons i.e. security scanners\n* We can make images that we host public by default\nFrom this process we tentatively chose ECR. Unfortunately none of the SaaS registries in common use support Github authentication, but as ECR uses IAM for authentication, and IAM supports federated identity with Github via Auth0, it will in theory be possible to define ECR access policies that reference Github teams and roles.\n\n## Decision\n",We will replace our self hosted Docker Registry to managed Elastic Container Registry (ECR).\n,"Migrate to Amazon Elastic Container Registry (ECR) for the following reasons:\n\n- ECR is a managed service, which will reduce the amount of day-to-day ops work required to maintain the container registry.\n- ECR supports federated identity with GitHub via Auth0, which meets our requirement for a solution that works with GitHub-based identity.\n- ECR is a good value for money, and it is open source tooling.\n- ECR can run addons, such as security scanners.\n- ECR allows us to make images that we host public by default."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have to send requests to a server that requires unique Request IDs for each message.\nSo, we need to generate uuids.\n\n## Decision\n",We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\nhttps://www.npmjs.com/package/uuid\n,Use the built-in `uuid` module in Python to generate UUIDs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSupporting multiple Kafka versions is something quite trivial due to the way the\nprotocol was designed. With the [`ApiVersions`](https://kafka.apache.org/protocol#The_Messages_ApiVersions)\nmessage, clients are able to retrieve the minimum and maximum supported version\nfor all API messages in a broker.\nThat is critical part in the connection flow and is suggested to be done right\nafter the connection has been established, however such message is only\navailable as of v0.10.\nIn Kafka v0.11, a new record batch format was introduced. This format offers\nbetter metadata control, headers, and support for transactions.\nMore info:\n- https://cwiki.apache.org/confluence/display/KAFKA/KIP-35+-+Retrieving+protocol+version\n- https://kafka.apache.org/documentation/#messages\n\n## Decision\n","This library will only be usable and guaranteed to be fully working with Kafka\nv0.11+, although it's highly recommended always use the latest version\navailable (v2.2.0 at the moment).\n","**Decision:** Migrate Kafka to v0.11 or higher.\n\n**Rationale:**\n\n* **Support for multiple Kafka versions:** Kafka v0.11 introduces the `ApiVersions` message, which allows clients to retrieve the supported API versions from a broker. This makes it possible to support multiple Kafka versions with the same client library.\n* **Improved record batch format:** Kafka v0.11 introduces a new record batch format that provides better metadata control, headers, and support for transactions. This new format improves performance and efficiency.\n* **Backward compatibility:** Kafka v0.11 is backward compatible with v0.10, so existing clients can continue to access the brokers without any changes.\n\n**Consequences:**\n\n* **Migration effort:** Migrating Kafka to v0.11 or higher will require some effort. The effort will depend on the size and complexity of the Kafka deployment.\n* **Client compatibility:** Clients that do not support the `ApiVersions` message will not be able to connect to brokers running v0.11 or higher. These clients will need to be upgraded to a newer version that supports the `ApiVersions` message."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Java implementation of the protocol uses `final static` properties to define\nthe schema for requests and responses. These are defined using objects, making\nit easy to support multiple versions for each API call.\nPHP doesn't have this feature so we need decide how to solve this.\n\n## Decision\n","We're still going to use objects to manipulate data and write/read content\nto/from Kafka, however it would be simpler to use arrays in constants of each\nrequest/response class.\nEach field would be an array item, the key would be the field name and the value\nwould be the field type (or another array for more complex configuration).\nThe mapping would like this:\n```php\nuse Lcobucci\Kafka\Protocol\Type;\nfinal class DoSomethingRequest\n{\nprivate const SCHEMAS = [\n[\n'error_code'   => Type\Int16::class,\n'api_versions' => [\n'type'     => Type\ArrayOf::class,\n'nullable' => false, // optional, default = false\n'items'    => [ // just type name if items don't have complex structure\n'api_key'     => Type\Int16::class,\n'min_version' => Type\Int16::class,\n'max_version' => Type\Int16::class,\n],\n],\n],\n];\n}\n```\n","**Decision:**\n\nUse PHP constants and arrays to define the protocol schema.\n\n**Rationale:**\n\n* Constants provide a statically defined schema that can be easily accessed and modified.\n* Arrays allow for easy versioning and support for multiple API calls.\n* This solution is compatible with the PHP language and follows its coding conventions.\n\n**Consequences:**\n\n* **Ease of use:** The constants and arrays can be easily used throughout the PHP codebase.\n* **Maintainability:** The schema can be easily changed or updated without having to modify the underlying objects.\n* **Performance:** Constants are evaluated at compile time, improving performance compared to dynamic object properties.\n* **Flexibility:** Arrays allow for dynamic schema definition, enabling support for additional API calls and versions in the future."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKafka's protocol defines some primitive types which are used to send requests\nand parse responses: https://kafka.apache.org/protocol#protocol_types\nProviding an easy way to define the schema of the requests and responses is\nquite critical to make this library extensible enough.\n\n## Decision\n,"We've decided to basically port the Java implementation to the PHP world,\nbecause it was very well written and it simplifies things by a lot.\nSome minor things obviously have to be adapted and for now we'll leave some\ntypes to the upcoming releases - just because they aren't need to implement the\nmessages we're planning to provide at the moment.\n",To encode and decode protocol types we will use reflection.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe've previously decided to require PHP 7.2, however many months have passed and\nPHP 7.3 is quite stable nowadays.\n\n## Decision\n",Bump up requirement to latest stable PHP version (7.3 at the moment).\n,"**Decision:** Upgrade the minimum required PHP version to 7.3.\n\n**Rationale:**\n\n* PHP 7.3 is now stable and widely adopted.\n* The upgrade will bring performance improvements, security enhancements, and new features.\n* The cost of supporting both PHP 7.2 and 7.3 is low compared to the benefits of upgrading.\n\n**Consequences:**\n\n* Users with PHP 7.2 will need to upgrade to 7.3 to continue using the application.\n* The application may need to be tested to ensure compatibility with PHP 7.3."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n",The change that we're proposing or have agreed to implement.\n,"The decision itself, in the form of a title and description.\n## Consequences\nThe implications or effects of making this decision, both positive and negative.\n## Rationale\nThe reasoning or judgment that led to this decision. This section should explain the evaluation of alternatives and why the chosen alternative was selected."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Document Architectural Decisions**\n\n**Description:**\nEstablish a process for documenting and tracking architectural decisions throughout the project lifecycle.\n\n**Rationale:**\n* Provides a historical record of the design process and reasoning behind technical choices.\n* Facilitates knowledge sharing and collaboration among team members and stakeholders.\n* Helps identify and address potential architectural risks and challenges early on.\n* Ensures consistency and accountability in decision-making.\n\n**Consequences:**\n\n* **Benefits:**\n    * Improved communication and understanding of architectural intent.\n    * Reduced duplication of effort and wasted time.\n    * Increased traceability and accountability for decisions.\n* **Costs:**\n    * Time and effort required to document and track decisions.\n    * Potential for excessive documentation.\n\n**Alternatives Considered:**\n\n* Informal communication and documentation\n* Using a dedicated tool for architectural decision-making\n* No decision documentation\n\n**Decision:**\n\nImplement a process for documenting architectural decisions using a combination of methods:\n\n* **Online Collaboration Tools:** Use a collaborative platform to capture and share decisions as they are made.\n* **Technical Documentation:** Incorporate decision summaries into project documentation, such as design documents and requirements specifications.\n* **Architectural Decision Logs:** Maintain a central log of all significant architectural decisions, including rationale, alternatives considered, and consequences.\n\n**Owners:**\n\n* Project Architect\n* Lead Developer\n* Technical Team\n\n**Next Steps:**\n\n* Establish guidelines for documenting and reviewing architectural decisions.\n* Train team members on the decision documentation process.\n* Regularly review and update decision logs as the project evolves."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeciding which minimum PHP version to require is quite important because it\nmight restrict people from using the library, at the same time supporting many\nversions increases the cost to maintain the library.\n\n## Decision\n","For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\nonly).\nThe main motivation for only allowing it to be installed in 64-bit systems is\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\ncan't be achieve in 32-bit systems.\nI believe that libraries' maintainers also have the role to help the evolution\nof the PHP ecosystem, which makes me want to require a more up-to-date version\nof the language.\n","Based on the provided context, the best decision would be to require the latest PHP version that is supported by at least 95% of users. This can be determined by using a tool like [PHP Package Dependency Checker](https://github.com/sensiolabs/dependency-checker), which can analyze a project's composer.json file to determine the PHP versions that are compatible with the project's dependencies.\n\nRequiring the latest PHP version ensures that you are using a stable and secure version of the language. It also allows you to take advantage of new features and performance improvements that are available in the latest version.\n\nBy using a tool like PHP Package Dependency Checker, you can determine which PHP version is supported by the majority of your users. This will help you to strike a balance between supporting a wide range of users and minimizing the cost of maintenance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application's scope is not well defined and may cover a lot of concepts around the Menu generation\nfeatures, such as:\n- managing a recipes catalog\n- searching in this catalog for recipes based on multiple criteria\n- collaborating on this catalog\n- handling allergies and tastes\n- generating shopping lists\nAll above concepts may be complex to model and should not be considered as simple data in a CRUD system. Other new\nfeatures may emerge in the future. In addition, all those features are related but should not be treated as a whole in\na big ball of mud architecture.\n\n## Decision\n","[Domain-driven design](https://domainlanguage.com/wp-content/uploads/2016/05/DDD_Reference_2015-03.pdf) as defined by\nEric Evans will help modeling the different concepts managed by the `menu-generation` application, keep the ability to\nadapt the model based on new insights and split the whole application into different bounded contexts to avoid mixing\nthose concepts in one giant messy code base.\n","**Decision:**\n\nThe `menu-generation` application should adopt a **Domain-Driven Design (DDD)** approach to address its complex domain and ensure maintainability and scalability.\n\n**DDD Principles:**\n\n* **Identify bounded contexts:** Define clear boundaries for different functional areas within the application, such as recipe management, collaboration, and shopping list generation.\n* **Develop domain models:** Create rich and expressive domain models for each bounded context, capturing the business rules and constraints of the problem space.\n* **Extract use cases:** Identify and document specific use cases that the application must fulfill.\n* **Define aggregate roots:** Determine the primary entities that represent the core business concepts in each bounded context and serve as the root of a collection of related objects.\n\n**Benefits of DDD:**\n\n* **Improved maintainability:** Decouples different functionalities, making it easier to evolve and update the application over time.\n* **Enhanced comprehension:** Clearly defines the domain concepts and their interactions, making the codebase more understandable.\n* **Scalability:** Supports the growth and complexity of the application by isolating different functional areas.\n* **Increased extensibility:** Facilitates the addition of new features and capabilities without compromising existing functionality."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAdopting the [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.\naggregates and services) from the infrastructure (i.e. application clients and persistence).\n\n## Decision\n,"`menu-generation` application will adopt [hexagonal architecture](https://en.wikipedia.org/wiki/Hexagonal_architecture_(software)),\nas it aims to provide this separation.\n",**Decision:** Isolate domain elements from the infrastructure by:\n- Using a [dependency inversion principle](https://en.wikipedia.org/wiki/Dependency_inversion_principle) via [adapters](https://martinfowler.com/eaaCatalog/adapter.html) to connect domain services to the infrastructure.\n- Using a [repository pattern](https://martinfowler.com/eaaCatalog/repository.html) to abstract the persistence layer from the domain entities.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application will use the Spring framework along with other third party libraries, thus requires a\ndependency management tool.\nEffective development lifecycle requires Continuous Integration, thus a build management tool is necessary.\n[Gradle](https://gradle.org/) is one of the two main build management tools for the Java ecosystem with [Maven](http://maven.apache.org/).\nGradle is considered as more extensive and quicker than Maven. It is also well integrated with many tools, including\nthe Spring framework.\n\n## Decision\n",Gradle will be used to manage project dependencies and build tasks.\n,Use Gradle as the dependency management and build management tool for the `menu-generation` application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application needs to expose data and services in order to be used by multiple different applications,\neither web or mobile front-ends or any other application that will reuse a sub-part of the `menu-generation`\napplication or enhance the provided services.\nWe want the application data and services to be easily consumed through well established communication protocols, such\nas HTTP. It must be intuitive to manipulate the `menu-generation` application as a self-discoverable API, without\nmanaging cumbersome protocols or data formats.\n\n## Decision\n",`menu-generation` application data and services will be exposed through a REST API.\n,"We decided to leverage RESTful APIs as the primary mechanism for exposing data and services from the `menu-generation` application. RESTful APIs are well-established, widely used, and easy to consume by various applications. They provide a simple and straightforward way to access and manipulate data and services through HTTP requests."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEven a self-discoverable API needs to be documented to let consumers understand the underlying application features and\nthe resources attributes significance.\nA good API documentation will provide requests and responses structure, defining each attribute, giving examples and\nmixing functional and technical information so that consumers do not need to traverse different documents to get a\ncomplete overview of the features and technical concerns.\nThe documentation must also provide links between different parts of the document, so that it is easy to go back and\nforth between those parts.\nThe documentation must be kept in sync with the features. Ideally, a documentation is generated from the code.\n\n## Decision\n",`menu-generation` will generate its API documentation with [Spring Rest-Docs](https://spring.io/projects/spring-restdocs).\n,"**Decision:** Use OpenAPI Specification (OAS) to document the self-discoverable API.\n\n**Rationale:**\n\n* OAS is a widely-adopted industry standard for describing RESTful APIs.\n* It provides a standardized way to define request and response structures, attribute definitions, examples, and functional/technical information in a single document.\n* OAS supports hypermedia links, making it easy to navigate between different parts of the documentation.\n* OAS is language-agnostic and can generate documentation from code, keeping it in sync with the underlying features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\neither be a command that performs an action, or a query that returns data to the caller, but not both.\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\nuse queries with much more confidence, and only be careful with commands orchestration.\nCommands and queries terminology is already used in the `menu-generation` application.\n\n## Decision\n","Command-query separation will be enforced in the [core hexagon](./0003-adopt-hexagonal-architecture.md), especially in\napplication services.\n",**Decision:** Adopt the command-query separation principle for the `menu-generation` application.\n\n**Rationale:**\n* Command-query separation promotes a clear separation between methods that change state (commands) and those that do not (queries).\n* This separation improves the reliability of queries and simplifies the orchestration of commands.\n* The existing use of commands and queries terminology in the `menu-generation` application supports the adoption of this principle.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe don't want the development tools or Continuous Integration pipeline to be strongly bound to [Gradle](0006-manage-build-with-gradle.md).\n[Make](https://linux.die.net/man/1/make) is an utility agnostic of any language or build management tools.\n\n## Decision\n,"Make will be used to execute build tasks, abstracting Gradle and potential other tools used during build execution.\n",Use Make as the underlying task runner for development tools and the Continuous Integration pipeline.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\nservices implementations dependencies into the services defined in the core hexagon.\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\nSpring for many years and masters many of the provided features.\n\n## Decision\n",Spring framework will be the backbone for `menu-generation` application.\n,Use Spring as the framework for implementing the incoming adapters in the REST API having Spring as a runtime dependency.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\nDefining new acceptance tests must be easy, through reusable step definitions.\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development)\napproach.\n\n## Decision\n",[Cucumber](https://cucumber.io/) will be used to describe and execute acceptance tests in `menu-generation` application.\n,Use BDD (Behavior-Driven Development) and Gherkin syntax with a sufficient level of expressiveness.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose a programming language to implement the `menu-generation` application.\n`menu-generation` application will be developed initially during the author's free time, thus this time is limited.\nDevelopment may involve other developers in the future, and the chosen language should not restrict participation.\n\n## Decision\n","Java is a broadly used programming language, and the most well mastered one by the author. Thus Java will be the\nprogramming language used to implement the `menu-generation` application.\n",**Python**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance\nscenarii. However, to help maintaining an acceptance tests client library, we need to organize this library to be\nextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.\nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about\nwhat wrong happened during API calls.\n\n## Decision\n",The [Serenity](http://www.thucydides.info/#/) framework will be used to define the acceptance tests library.\n,"**Organize acceptance tests in folders by **feature**.**\n\n**Implement the **Gherkin steps interpreter** in a separate library, with a dedicated DSL.**\n\n**Use a **reporting library** to generate detailed reports in case of error.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application is packaged as a [Spring Boot](0005-use-spring-framework.md) executable JAR file.\nRunning acceptance tests on development machine or during Continuous Integration must be quick, easy and the least\ndependent of the underlying system.\n[Docker](https://www.docker.com/) is a widespread container based solution that can be used during development lifecycle\non most operating systems as well as in well established Cloud solutions such as [Kubernetes](https://kubernetes.io/).\n\n## Decision\n",`menu-generation` application will be packaged as a docker image. A `docker-compose` definition will also be provided\nto help running the application and its dependencies in a consistent and isolated environment.\n,Use Docker to containerize the `menu-generator` application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application will expose its data and services through a [REST API](0004-expose-services-through-rest-api.md).\nOne of the constraints of the REST architectural style implies exposing **Hyperlinks as the Engine of Application State**,\nmeaning any resource should provide links to other accessible resources based on its current state in the application,\nso that clients can discover which actions are available without interpreting the actual resource attributes.\n[Spring HATEOAS](https://spring.io/projects/spring-hateoas) comes by default with [HAL](http://stateless.co/hal_specification.html)\nmedia type support.\n\n## Decision\n",`menu-generation` application will expose its resources using the HAL media type.\n,Use [Spring HATEOAS](https://spring.io/projects/spring-hateoas) as API framework to enable [Hypermedia as the Engine of Application State](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm) in the `menu-generation` application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContract-first approach enforces a definition of the API contract before implementation. This ensures that the API is\nwell designed for specific use-cases, based on consumers point of view.\nAdopting a contract-first approach forces to ensure that contract is not broken during implementation and evolution of\nthe application.\nWe need to ensure that the contract is well defined through a specification and implementation respects this specification.\n\n## Decision\n",[OpenAPI Specification](https://swagger.io/specification/) will be used to enforce contract-first approach.\n,**Decision:** Adopt a contract-first approach through a specification and ensure compliance with it during implementation and evolution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany modern systems require up-front data before a device can be discovered and added to a network. To facilitate this, a Provisioning system is to be added as part of the Discovery services, with the aim to provide the information required for devices to join the network.\nTwo use cases are currently forseen.\n1. Users must provide device information prior to discovery. This information is managed by the binding, however a consistent way is needed to provide this data from the user, to the binding. An example of this is systems may require a cryptographic key to be provided before a device is able to be discovered.\n2. Centrally administered systems may want to provision information about a device from a central repository. Thus a user could be shipped the device, plug it in, and it would automatically be discovered and added with little user interaction.\nIt is proposed to add a `ProvisioningService` and `ProvisioningRegistry` to act as an interface between the provisioning information (from the user, or other system) and the binding. The binding will be able to query the `ProvisioningRegistry` to retrieve information about any provisioned devices when discovery is enabled, and will be able to update the `ProvisioningRegistry` if a device is successfully discovered. Additionally, the `ProvisioningRegistry` can be queryable to allow the status of provisioned devices to be retrieved.\nThe provisioning information provided through the `ProvisioningRegistry` is a set of properties that are interpretted by the binding. These are linked by a `ProvisioningUID` which is used to identify the device. This is akin to a `ThingTypeUID`, however used to identify the device before it is discovered.\n\n## Decision\n",TBD\n,"**Decision:** Implement a `ProvisioningService` and `ProvisioningRegistry` to manage and provide provisioning information for devices before discovery.\n\n**Rationale:**\n\n* **Meets use cases:** The proposed solution addresses both use cases identified in the context:\n    * For use case 1, users can provide device information through the `ProvisioningService` before discovery.\n    * For use case 2, centrally administered systems can provision information from a central repository, enabling automatic discovery and addition of devices.\n* **Consistent interface:** The `ProvisioningRegistry` provides a consistent interface for both users and binding to access and manage provisioning information.\n* **Identification and interpretation:** The `ProvisioningUID` allows for unique identification of devices being provisioned, and the binding can interpret the provided properties accordingly.\n* **Scalability and flexibility:** The proposed solution is scalable and can handle multiple provisioned devices and bindings. It also allows for future extensions to support additional provisioning mechanisms or properties."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany advanced features require additional information to be added to `Type`s - this could include secondary information as part of a `Command` or `State` update, or security context, `Command` source identifier, priority, etc. The type system structure inheritted from Eclipse SmartHome is difficult to extend due to the class hierarchy. A wrapper could be used around the `Command` or `State`, but this would require considerable rework of core functions.\nThere is no base class for a `Type` - the ESH structure defines `Type`, `Command` and `State` as interfaces and many concrete `Type`s are derived from an `enum` (eg `OnOffType`, `UpDownType` etc) while others may be derived from Java classes (eg `DecimalType`). The absence of a clear class hierarchy within the framework makes it difficult or impossible for the framework to control and extend the `Type` system - one of the fundamental building blocks of the framework.\nIt is proposed to rewrite the type system to introduce a clear class hierarchy. `Type`, `Command` and `State` remain as interfaces however an `AbstractBaseType` class is introduced from which all concrete `Type`s are derived. `ComplexType` and `PrimitiveType` are not generally used and are to be removed as they add unnecessary complexity. The type system should be implemented to be compatible at the basic level with the Eclipse SmartHome definitions such that backward compatability with existing bindings is not compromised.\nThe ESH type system derives `DecimalType` and `QuantityType` from `Number` which makes it impossible to extend the `AbstractBaseType`. To resolve this we provide the core `Number` functionality with a new `NumberType` class. This provides the same interface, but can now extend `AbstractBaseType`. This potentially means that any code within the framework that checks `instanceof Number` will now fail and such checks must instead use `instanceof NumberType`.\n\n## Decision\n","This change provides a better separation between Java and OpenSmartHouse types, and most importantly allows further enhancements in functionality so is agreed to be implemented.\n","Implement a new type system based on a clear class hierarchy, with `Type`, `Command` and `State` as interfaces and all concrete `Type`s derived from an `AbstractBaseType` class. Migrate existing `Type`s to the new system and introduce a new `NumberType` class to provide the core `Number` functionality, allowing for future extension of the `AbstractBaseType`. Deprecate `ComplexType` and `PrimitiveType` and ensure backward compatibility with existing bindings."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOften a binding will need to provide notifications to users for binding specific administrative purposes. This is not linked to `Item` data, but is designed to provide users feedback on binding specific functions or alerts. Examples of this could be -:\n* Alerting the user about failed joining, or a trust issue with devices joining\nIn general, these notifications are designed for display on an administration UI rather than a user UI, but there is of course nothing to prevent any UI from subsribing for such events.\nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way. This should provide enough information to allow the UI to present the notification in a standard way - showing the severity, and a notification text.\nThe alerting system may be as simple as real-time notifications direct to the UI. In such case, if the administrative user is not watching the UI, notifications would be lost and not persisted. An enhancement may be to run these through a registry which would allow notifications to be registered and actively dismissed once read by a user with the appropriate access level. At this point, the notification may be removed, or archived to provide an audit trail.\nFurther extensions to the notification system may be considered to provide images, action buttons etc are considered outside of the scope of this initial proposal.\n\n## Decision\n",TBD\n,"**Decision:**\n\nImplement a generic binding notification mechanism that allows bindings to send simple notifications to users.\n\n**Rationale:**\n\n* Provides a consistent mechanism for bindings to send notifications to users.\n* Notifications can be displayed in a standard way by UIs, ensuring users can easily understand and address them.\n* Notifications can be persisted in a registry to ensure they are not lost if the administrative user is not watching the UI.\n* Notifications can be archived to provide an audit trail.\n\n**Consequences:**\n\n* Improves the user experience by providing a consistent and informative way to notify users of binding-specific events.\n* Improves operational efficiency by allowing administrators to quickly identify and resolve binding issues.\n* Provides a foundation for future enhancements to the notification system, such as support for images, action buttons, and more."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEclipse SmartHome defined a static type system. This may not be extended, and uses a static class `TypeParser` to create a `Type`. This limits framework modularity and extensability and is generally against OSGi principals.\nThe architecture is to be changed to introduce a `TypeProvider` service, and `TypeFactory` interfaces. The `TypeProvider` is implemented to provide a `Type`, or set of `Type`s to the system, and `TypeFactory` is used by users to create `Type`s. Between these is a `TypeRegistry` which consumes the `Type`s from multiple `TypeProvider`s, and supplies the `TypeFactory` interface for system users.\nThe standard core types are to be provided by a `CoreTypeProvider`, and the current static `TypeParser` is retained for backward compatability with ESH/OH, but is not to be used by native _OpenSmartHouse_ bundles. Instead, users requiring `TypeParser` functionality should utilise the `TypeRegistry`.\n\n## Decision\n","The introduction of the `TypeRegistry` and `TypeProvider` services, and refactoring of the current types to be provided by a `CoreTypeProvider` is implemented. Existing users of `TypeParser` within the _OpenSmartHouse_ core bundles is to be refactored to use these services, and new bundles should avoid using the static `TypeParser`.\n","The static type system in Eclipse SmartHome should be replaced with a dynamic system that uses a `TypeProvider` service and `TypeFactory` interfaces. The `TypeProvider` is implemented to provide a `Type`, or set of `Type`s to the system, and `TypeFactory` is used by users to create `Type`s. Between these is a `TypeRegistry` which consumes the `Type`s from multiple `TypeProvider`s, and supplies the `TypeFactory` interface for system users. The standard core types are to be provided by a `CoreTypeProvider`, and the current static `TypeParser` is retained for backward compatibility with ESH/OH, but is not to be used by native _OpenSmartHouse_ bundles. Instead, users requiring `TypeParser` functionality should utilise the `TypeRegistry`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOpenHAB uses OSGi as a runtime.\nThis runtime promotes clear separation between implementation and contract (API/implementation/SPI) packages.\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that ""implementation"" packages stay hidden.\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\nSuch approach promotes tight coupling between contract and implementation.\nMore over, it also promotes exposure of specific implementation classes via public API.\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes/excludes mechanism for dependencies.\nIt would work properly, but openHAB core is a single jar which makes things even harder.\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\nopenHAB utilizes Apache Karaf for provisioning of the application.\nKaraf provisioning itself is capable of verifying its ""features"" based on declared modules, bundles, JAR files, etc.\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\nPresent structure of modules / bundles is as follows:\n```\n[openhab thing core] <--- [openhab rest core]\n```\n\n## Decision\n","Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\nWe decided that a clear separation between API and implementation packages should be made.\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\n```\n[openhab thing api] <--- [openhab rest thing]\n^\n|\n[openhab thing core]\n```\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\n",- Split the current `openhab-core` module into `openhab-core-api` and `openhab-core-impl`.\n- Move all API classes from the `org.openhab.core` package to the `org.openhab.core.api` package.\n- Move all implementation classes from the `org.openhab.core` package to the `org.openhab.core.impl` package.\n- Update all dependencies in the `openhab-core` module to only depend on the `openhab-core-api` module.\n- Update all dependencies in the `openhab-rest-core` module to only depend on the `openhab-core-api` module.\n- Update the `openhab-core-base` feature to only depend on the `openhab-core-api` module.\n- Add a new `openhab-core-impl` feature that depends on the `openhab-core-api` module.\n- Update the `openhab-runtime-base` feature to depend on both the `openhab-core-api` and `openhab-core-impl` features.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo allow the system to know if the ThingType definition has changed, a version identifier should be added to the ThingType. This will allow the system to update the thing based on the new Thing Type definition.\nThe `version` should be added to the `ThingType`, and provided by thing type providers such as the XML provider. This should be reflected into the instantiated `Thing` and provided in the `ThingDTO` for use in the user interface.\nThing Type XML definition -:\n```\n<thing-type id=""sample"" version=""2"">\n```\nIt shall then be possible for the user to decide to upgrade to the new definition. This functionality could be provided automatically, however this may result in unexpected changes to a users system as channels could be removed. Such updates are therefore considered to be better if manually initiated by the user.\nThing REST response (part) -:\n```\n{\nUID: ""test:sample:505f8d7b12"",\nthingTypeUID: ""test:sample"",\nthingTypeVersion: 2\n}\n```\nA new REST API is to be added to allow the user to change the thing type via the UI. This uses the `/thing/{{thinguid}}/migrate` call. This migrates the specified thing to the latest version of the ThingType.\n\n## Decision\n","This has minimal impact on the framework, and allows the user and UI to manage the migration of things without the user having to delete and re-add things, which is quite complex as item links also need to be re-added.  Further enhancements could be added to allow this to (optionally) be undertaken automatically.\n","**Decision:** Add a `version` field to the `ThingType` and provide it in the `ThingDTO` and REST API responses.\n\n**Rationale:**\n\n* Adding a `version` field to the `ThingType` allows the system to track changes to the ThingType definition.\n* Providing the `version` in the `ThingDTO` and REST API responses allows users to identify the version of the ThingType associated with a particular Thing.\n* This information enables users to decide whether to upgrade to a new version of the ThingType, providing control over potential changes to their system.\n* The addition of a REST API `/thing/{{thinguid}}/migrate` call allows users to manually initiate ThingType upgrades, promoting user agency and avoiding unexpected system changes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith large bindings with many things, a ThingType and ThingHandler may not be implemented for each Thing. Instead the ThingHandler may implement dynamic definition by interrogating the device, or through a static definition via the Thing XML file. This may be implemented as a single ThingHandler for numerous different devices whose functionality is defined or discovered.  In cases where the Thing is statically defined, there will be some discovery information that is retieved from the device and used to link to the ThingType, and it is desirable for the ThingType definition and the discovery information to be defined in a single place.\nFunctionally, the ThingType definition, and Discovery information should be separate. The proposal here is to introduce a `DiscoveryIndex` service which provides the binding specific thing properties linked to a `ThingThypeUID`. `DiscoveryProperties` are provided by a `DiscoveryIndexProvider` which the binding can query.\nThe `XmlThingTypeProvider` should be modified to also implement `DiscoveryIndexProvider`, and the ThingType XML definition should be updated to introduce a `discovery-properties` section which can include the discovery information to be passed to the binding. The `discovery-properties` section is similar to the existing Thing `properties` definition, but dedicated to the `DiscoveryIndex` function.\n\n## Decision\n",TBD\n,"**Decision:**\n\nIntroduce a `DiscoveryIndex` service and a `DiscoveryIndexProvider` interface to facilitate dynamic ThingType and ThingHandler implementations by providing binding-specific Thing properties linked to a `ThingTypeUID`.\n\n**Rationale:**\n\n* Separates ThingType definition from Discovery information for functional clarity.\n* Allows for dynamic ThingTypdefinition and ThingHandler implementation by leveraging discovery information.\n* Provides a centralized location to define both ThingType definition and Discovery information, reducing redundancy.\n* Leverages existing `XmlThingTypeProvider` infrastructure by extending it to implement `DiscoveryIndexProvider`.\n* Updates the ThingType XML definition to include a `discovery-properties` section for specifying discovery information to be used by the `DiscoveryIndex` service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt has long been stated in ESH that the existing Configuration Parameter API is only meant for Thing Handler configuration and not device configuration. Many bindings use this for device configuration as well as handler configuration. From discussions on ESH, the following distinction was noted -:\n* Handler configuration is merely about configuration of communication parameters, like ip address/host & port, timeouts, access tokens, thing ids, etc.\n* Thing configuration should describe and model the configuration of the physical device. It should not only provide a view but provide an interface to alter the thing configuration. It is the bindings responsibility to implement the read/write access to the corresponding thing.\nIt is proposed to keep things simple - levering off the existing configuration parameter definition that are already being used for device configuration. There should be an indication in the parameter that this is a thing configuration, and there should be a method to define device specific information that the binding can use when sending data to the device.\nThe proposal is to add a new `device-properties` element to the configuration description. The defines a set of properties similar to other sets of properties defines in the XML descriptions. The properties may be used by the binding to specify binding and device specific information that is interpretted by the binding when sending the configuration to the device. This is implemented in `ConfigurationParameter` with the addition of the `ParameterDeviceProperty` class which contains the binding defined properties required to configure the device. If the `ConfigurationParameter` contains more than one `ParameterDeviceProperty` it is considered to be a device parameter rather than a handler parameter.\nDespite the ESH configuration definitions apparently only being for handler configuration, there is a `ConfigStatusProvider` service that allows the binding to provide configuration status via the `ConfigStatusMessage.Type` enum. This allows the binding to set status as `PENDING`, which ""should be used if the transmission of the configuration parameter to the entity is pending"". This should be persisted along with the `ConfigurationParameter` so that the binding is able to establish if configuration still needs to be sent to the device when it starts. Binding users wanting to provide the configuration status should extend the `ConfigStatusThingHandler`.\nTo allow the device property information to be utilised in the binding, a new method `getConfigParameterDescription` is added to the `BaseThingHandler` to allow the binding to retrieve the configuration description for a parameter (and hence the `ParameterDeviceProperty`)\nCurrently within the core, configuration changes are only persisted by the binding in the `ThingHandler`. It is therefore the bindings responsibility to manage the `PENDING` flag if it implements the `ConfigStatusProvider`. In general the binding shall set the status to `PENDING` if it updates any configuration, and update the status appropriately once the configuration in the device is confirmed. The `ConfigStatusService` will persist the state of all parameters since it does not have access to the configuration descriptions, and the binding must manage the status.\nA binding may get the current persisted state of the configuration status by calling `getPersistedConfigStatusInfo`. This will return the persisted state from the previous request to `ConfigStatusProvider` and allows bindings to retrieve the state on startup.\nExample XML configuration -:\n```\n<parameter name=""param-name"">\n...\n<device-properties>\n<property name=""parameter"">12</property>\n<property name=""size"">2</property>\n</device-properties>\n</parameter>\n```\nExample ThingHandler usage during initialisation -:\n```\nConfigStatusInfo configStatusInfo = getPersistedConfigStatusInfo();\nfor (String parameterName : getConfig().getProperties().keySet()) {\nConfigDescriptionParameter configDescription = getConfigParameterDescription(parameterName);\nif (configDescription.getDeviceProperties().isEmpty()) {\n// Parameter is not a remote parameter\ncontinue;\n}\nCollection<ConfigStatusMessage> configStatus = configStatusInfo.getConfigStatusMessages(parameterName);\nif (configStatus.isEmpty() || configStatus.iterator().next().type != Type.PENDING) {\n// Parameter update is not pending\ncontinue;\n}\n// Update the parameter as the update was PENDING when the handler was shut down\n}\n```\nExample ThingHandler usage during `handleConfigurationUpdate` -:\n```\nConfiguration configuration = editConfiguration();\nfor (Entry<String, Object> configurationParameter : configurationParameters.entrySet()) {\nconfiguration.put(configurationParameter.getKey(), configurationParameter.getValue());\nConfigDescriptionParameter configDescription = getConfigParameterDescription(\nconfigurationParameter.getKey());\nif (configDescription.getDeviceProperties().isEmpty()) {\n// Configuration to be sent to remote device\n}\n}\n```\n\n## Decision\n",The addition of the `ParameterDeviceProperty` allows the system to properly differentiate between handler configuration and device configuration. The persisting of `ConfigStatusMessage`s allows the binding to know on startup if `PENDING` configuration still needs to be transferred to the device. These changes have minimal impact on the wider system and allow _OpenSmartHouse_ to better support the device configuration system.\n,"To keep things simple, leverage off the existing configuration parameter definition that are already being used for device configuration. There should be an indication in the parameter that this is a thing configuration, and there should be a method to define device specific information that the binding can use when sending data to the device."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nItems currently have a single value linked to them - eg `ON` or `OFF`, or `OPEN` etc. However often there is the need to provide multiple elements of supporting information along with the `Command` or `State`. Such information may not be required for the system to run, but may be useful for rules, or advanced users where a finer level of system control is desired.\nExamples of this for both `State`s and `Command`s are -:\n* A `Dimmer` command to change a light level may have a ""rate"" associated with it to tell the device how quickly to change to the commanded state.\n* An `OnOff` state for a door lock may have information such as how the door was opened, or who opened it.\nSuch information needs to be considered in an atomic way along side the primary item `State` or `Command` and cannot in itself be provided as a separate `Item` since this will be be correlated with a specific event. For example a dimmer may be commanded to turn on with different rates in different rooms or at different times - providing multiple items - one per dimmer could work, however the asynchonous nature of the event bus does not guarantee that a rate setting will be received by the binding prior to the command itself. Current solutions within openHAB include utilising multiple items with the resulting correlation problems and potentially to massively increase the number of items, or the approach recommended by openHAB maintainers is to use JSON encoded strings which may then not be used directly in the UI.\nThis ADR proposes to add the concept of `Attributes` to all `Command`s and `State`s. An `Attribute` will provide secondary information that is directly linked to the primary event and should not be used to provide data where multiple `Item`s could be used. For example, a sensor providing both temperature and humidity should provide these as separate items, even if they are received from the sensor at exactly the same time.\n`Attribute`s are defined within the `CommandDescription` and `StateDescription` in a `Channel` definition. All attributes that a channel supports must be defined in the channel definition to allow the system to provide a level of consistency checking, and for user interfaces to provide the user with a list of attributes a channel may provide. The UI may elect to present these to the user in a similar way to other `Item`s, or the information may be ommitted.\nExamples of the definition of attributes is provided below. It is proposed to use a similar construct, and similar data types as configuration parameters - although there is no direct re-use of code, reusing the concept will make it easier for users.\n```\n<channel-type id=""door_state"">\n<item-type>Switch</item-type>\n<label>Door Lock State</label>\n<description>Locks and unlocks the door and maintains the lock state</description>\n<category>Door</category>\n<state>\n<options>\n<option value=""0"">Open</option>\n<option value=""1"">Closed</option>\n<options>\n<attributes>\n<attribute id=""unlockmethod"" type=""Number"">\n<name>Unlock Method</name>\n<description>The way in which the lock was opened</description>\n<option value=""0"">Manual</option>\n<option value=""1"">RFID</option>\n<option value=""2"">UserCode</option>\n</attribute>\n<attribute id=""userid"" type=""String"">\n<name>User Id</name>\n<description>The user who opened the lock. Not provided for manual unlocking.</description>\n</attribute>\n<attributes>\n</state>\n</channel-type>\n```\n```\n<channel-type id=""dimmer"">\n<item-type>Dimmer</item-type>\n<label>Dimmer</label>\n<description>Sets the light level</description>\n<category>Light</category>\n<command>\n<attributes>\n<attribute id=""transition"" type=""Number"">\n<name>Transition Duration</name>\n<description>Sets the transition duration in milliseconds</description>\n<maximum>5000</number>\n</attribute>\n<attributes>\n</command>\n</channel-type>\n```\nIdeally, commonly used attributes should be standardised across bindings in the same way as system channels are with channel definitions. This will improve the user experience by ensuring a unified solution no matter what binding is in use.\n\n## Decision\n",TBD\n,"Add the concept of `Attributes` to all `Command`s and `State`s.\n\nAn `Attribute` will provide secondary information that is directly linked to the primary event and should not be used to provide data where multiple `Item`s could be used.\n\nAll attributes that a channel supports must be defined in the channel definition to allow the system to provide a level of consistency checking, and for user interfaces to provide the user with a list of attributes a channel may provide.\n\nIdeally, commonly used attributes should be standardised across bindings in the same way as system channels are with channel definitions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe UQ purple header is included in primo through include files like `assets.library.uq.edu.au/primo-sand-box/reusable-components/`, via the primo BO.\nUp until now we have been hard coding the primo-sand-box bit according to which environment we are in, and having to remember to manually change it depending on which environment we were uploading to.\nManual processes are to be avoided.\n\n## Decision\n","Generate the branch to be used, by looking at the host name and the vid parameter on the url\nhttps://github.com/uqlibrary/uqlibrary-reusable-components/commit/4f1c182\n","**Decision:** Automate the process of updating the primo-sand-box bit in the include files for the UQ purple header.\n\n**Rationale:**\n\n* Automating the process will reduce the risk of errors and inconsistencies, as well as save time and effort.\n* It will make the process more efficient and reliable, ensuring that the correct version of the header is always included in Primo.\n* By removing the need for manual updates, it will free up staff time to focus on other tasks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis needs to work cross platform.\nThe UX will be rendered in browser, which may not appear native.\n\n## Decision\n","To implement Editions as a progressive web app, delivered in a native wrapper. The native wrapper will contain a webview, that renders the PWA.\nThe wrapper will handle required features that are not supported by PWAs such as background fetch, scheduled downloads and push notification handling.\n",Use a cross-platform UI framework such as React Native or Flutter.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe archiver lambda needs to be able to respond to issue published events from tools.\n\n## Decision\n,For the archiver to react to S3 file created events from the fronts tool.\n,"Implement an async publish/subscribe architecture for the pub/sub communication based on Kafka, an event streaming platform."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a consistent and battle tested process for releasing the app on both Android and iOS.\n\n## Decision\n,"### iOS\n#### Internal Beta\nOur internal Beta is managed through testflight on the Guardian developer account. The group which this beta is sent to is labelled `GNM`. This includes the team and internal stakeholders within the organisation. We build this automatically through Fastlane and Github Actions once a day. Occasionally we will set off builds to test things on a number of devices.\nIn github actions we have a [scheduled build](https://github.com/guardian/editions/actions?query=workflow%3Ascheduled-ios-beta) and an [ad-hoc one](https://github.com/guardian/editions/actions?query=workflow%3A%22Upload+ios-beta%22) triggered by a [script](https://github.com/guardian/editions/blob/main/script/upload-ios-build.sh)\nAll builds generate a ['release' in github](https://github.com/guardian/editions/releases) to help us keep track of build numbers against certain commits. This is handled by the [make-release script](https://github.com/guardian/editions/blob/main/script/make-release.js).\n#### External Beta\nBefore every release, we aim to do at least one external beta to gather feedback. We have a number groups within testflight that are prefixed with the name `External Testers...`. These different groups represent the different authentication methods we support. When we decide a build is good enough from an internal test, we add the build to the groups.\n#### Release\nAfter a successful beta period, we release the same build (identified by its build number) through the app store submission process.\n#### Post Release\nWe update the version number in XCode and raise that as a PR. The version number will depend on the goals for the next release. We follow a major and minor number approach with no patch i.e. 5.6\n### Android\n#### Internal Beta\nIn a similar vein as above, the Android internal beta is managed through Google Play. The APK for this is created using Fastlane through TeamCity. The name of this process is `android-beta-deploy`. The list for this is managed within the Google Play console. This process runs once per day. Users will need to update their app through their Google Play store.\n#### External Beta\nWe take a slightly different approach to iOS. Due to not being able to determine within the app whether or not the app is in Beta or release, we have a different build configuration for releasing to production - which hides the 'report bug' button.\nWe build the APK using `android-beta-deploy`. This will release a build to google play to the 'internal beta' group for internal testing by the team. It is then manually promoted within the Google Play console to our external beta testers.\nAs with iOS, releases to the play store can be tracked in [github releases](https://github.com/guardian/editions/releases) - each time the teamcity build is run a new github release is created including the play store version code of that release.\n#### Release\nAfter a successful external beta test, we **DO NOT** promote the external beta. This is because we have code in the app that attempts to determine whether or not the user is in beta. This does not work on Android as there isn't a distinction.\nAs a result, we then use the TeamCity process `android-release-deploy` to then build the APK. As you will only want to release a version that has been beta tested, you can use the [releases](https://github.com/guardian/editions/releases) list to find a release for the build you want to release, and copy the tag for that build. You can then search for this tag in the branch list within teamcity (the `android-release-deploy` config treats github tags as if they were branchs) and run a build on that tag.\nBe warned, this process will automatically release the new version of the app. You will then need to go into the Google Play console to update the release notes.\n","Use the fastlane tool to automate the app release process on both Android and iOS. Fastlane is a popular, open-source tool that provides a streamlined and efficient way to automate tasks related to mobile app development, including release management. By using Fastlane, we can ensure a consistent and battle-tested process for releasing our app on both platforms, reducing the risk of errors and saving time."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\n\n## Decision\n","```sequence\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\nTypescript Layer->App Code: Consume! 🍔\nApp Code-->Ophan Analytics Library: Sends events\n```\n### Key\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it ""React Nativey"" to expose methods to the JS.\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\n**App Code** - Consumes the typescript library where its needed.\n","**Decision:** Implement Ophan using the [Segment](https://segment.com/) analytics library.\n\n## Rationale\n\n**Segment** provides a unified API for sending analytics data to multiple destinations, including Ophan. This will allow us to easily add or remove destinations as needed, and it will provide a consistent interface for sending data from both iOS and Android.\n\n**Segment** is also a well-maintained and widely used library, which means that it is likely to be reliable and performant. It also has a large community of users, which means that we can get help with any issues that we encounter.\n\nOther options that were considered include:\n\n* **Google Analytics:** Google Analytics is a popular analytics platform, but it does not have a native integration with Ophan. This would require us to write our own integration, which would be time-consuming and error-prone.\n* **Facebook Analytics:** Facebook Analytics is another popular analytics platform, but it is not as widely used as Google Analytics. This means that we would have a smaller community of users to help us with any issues that we encounter.\n* **Custom implementation:** We could write our own custom implementation for sending analytics data to Ophan. However, this would be a significant undertaking, and it would be difficult to maintain over time.\n\n## Consequences\n\n**Benefits:**\n\n* **Easy to implement:** Segment is a well-documented and easy-to-use library. This will make it easy to get Ophan up and running quickly.\n* **Reliable and performant:** Segment is a well-maintained and widely used library. This means that it is likely to be reliable and performant.\n* **Flexible:** Segment allows us to send data to multiple destinations, including Ophan. This gives us the flexibility to add or remove destinations as needed.\n\n**Drawbacks:**\n\n* **Cost:** Segment is a paid service. However, the cost is relatively low, and it is outweighed by the benefits of using a reliable and well-maintained library.\n* **Data privacy:** Segment collects and stores user data. This could be a concern for some users. However, Segment has a strong privacy policy, and they are committed to protecting user data."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\n\n## Decision\n,To develop the editions app as a React Native application.\n,"**Decision:** Develop the editions app using React Native.\n\n**Rationale:**\n\n* React Native allows developers to build cross-platform mobile applications using React, which the available developers are familiar with.\n* It enables efficient code reuse between iOS and Android platforms, reducing development time and costs.\n* React Native provides a consistent user experience across different devices and operating systems.\n* The use of React Native's component-based architecture promotes flexibility and maintainability, allowing for seamless updates and feature iterations.\n* React Native supports background functionality, enabling the implementation of background downloads and other asynchronous operations."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe editions lambda needs to be able to identify specific versions of an issue.\n\n## Decision\n,"To have two deployments of the backend, one for previewing, and a second for published issues.\nThe published issues deployment will replace the issueid path parameter with source/issueid.\n`source` will identify which file in the published bucket will be retreived to form the issue on.\n",Store the issue version inside the issue data.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe archiving process is time consuming and consists of many stages.\n\n## Decision\n,For the archiving process to be run as an amazon step function so we can break the process down into component stages.\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\nThis will also allow the publication process to fire the notification at the desired time.\n,**Decision:** Implement incremental archiving to reduce the overall archiving time.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Problem to solve\nProvide unified way to manage translations for translatable entities in API.\n### Possible solutions\nThe translations from collection could be embedded as objects within a `ProductOption`/`ProductOptionValue` resource\n**or** provided by [IRIs](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)\n### Decision and reasoning\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\ndo not provide any value alone.\n\n## Decision\n,Translations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\ndo not provide any value alone.\n,**Translations should be embedded as objects within the `ProductOption`/`ProductOptionValue` resource.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`/`PUT` HTTP methods),\nRead(`GET` HTTP method), Update(`PUT`/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\nworkflow in a state machine) or command execution.\n## Decision Drivers\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\ntwo different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\n\n## Decision\n","* Solution should not be limited by its convention. We may need to support two different actions of the same name,\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\ntwo different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\nThe ""Taking advantage of the `Controller` REST archetype"" should be considered as a recommended solution. All Sylius\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\nor new schema should be introduced for the JSON-LD structure.\nOption 2: ""Defining custom operations in the style of command pattern"" may be useful once async data processing is\ndelivered with vanilla Sylius installation.\n","Use REST verbs (`POST`, `PUT`, `PATCH`, `DELETE`) to indicate the semantics of the operation and add a resource action in the URI.\n\nE.g., for:\n* state machine transition - `POST /users/{user}/states/{state}/transition`\n* command execution - `POST /orders/{order}/execute`"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nInitial implementation of Unified API used `/new-api` prefix, to aggregate all following endpoints. This prefix does not\nclearly state the version of it and is not future-proof. At some moment of time our ""new api"" can become ""old api"". We should\nhave clear guidance how to version our APIs.\n\n## Decision\n","As the underlaying technology, structure and content have changed significantly and taking into account easiness of first solution\nthe decision is to go with the `/api/v2` endpoint path. In the future it does not block us from the usage of the `Accept`\nheader in addition to this path, however it may be misleading for consumers.\n#### References:\n- https://github.com/api-platform/api-platform/issues/290\n- https://github.com/api-platform/core/issues/972\n- https://stackoverflow.com/questions/389169/best-practices-for-api-versioning\n- https://www.troyhunt.com/your-api-versioning-is-wrong-which-is/\n- https://github.com/api-platform/core/issues/45\n- https://github.com/api-platform/docs/issues/451\n- https://github.com/api-platform/docs/pull/452\n- https://api-platform.com/docs/core/content-negotiation/#supporting-custom-formats\n","Migrate the Unified API to use versioned paths instead of the `/new-api` prefix. For example, the new versions of the API should be accessible at `/api/v2` and `/api/v3`. This will allow us to clearly identify the version of the API being used, and will make it easier to maintain and deprecate older versions of the API."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nCustomer should have access only to locales available in their channel\n\n## Decision\n,"Chosen option: Using Data Provider\nShops shouldn't have many locales for each channel, so lack of a pagination is smaller problem than creating overcomplicated\nquery in Doctrine Collection extension\n",**Decision:** Create a localized content distribution service that will only deliver content that is available in the customer's current channel.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhile defining new handlers, we may define a bus to which particular handler should be assigned to. If not, handler will\nbe added to all existing buses. Right now, we have only one bus in Sylius, so decision will not change current behaviour.\nHowever, we may add additional buses in the future and more than one bus is not uncommon thing.\n## Decision Drivers <!-- optional -->\n* Driver 1 - easiness of changing default behaviour and usage of the bus\n* Driver 2 - backward compatible possibility of changing behaviour\n\n## Decision\n","* Driver 1 - easiness of changing default behaviour and usage of the bus\n* Driver 2 - backward compatible possibility of changing behaviour\nChosen option: ""[Explicitly declare, where bus should be registered]"", because this options leaves the most options to end user\nwithout forcing him to use them. What is more, we may easily adjust this option in the future\n","**Decision:** Introduce a concept of default bus and allow handlers to specify a bus they should be assigned to.\n\n**Rationale:** This decision meets the following criteria:\n\n* **Easiness of changing default behaviour and usage of the bus:** By introducing a default bus, it is easy for developers to specify which bus a handler should be assigned to. This can be done by adding a `bus` option to the handler definition. If no bus is specified, the handler will be assigned to the default bus.\n* **Backward compatible possibility of changing behaviour:** This decision is backward compatible with the current behaviour of Sylius. All existing handlers will be assigned to the default bus. However, it is possible to change the default behavior in the future by changing the value of the `default_bus` configuration option.\n\n**Consequences:** The following consequences are expected from this decision:\n\n* Developers will have more control over which bus a handler is assigned to.\n* It will be easier to add new buses to Sylius in the future."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want our API to cover all the functionalities that are currently implemented in the UI.\n## Decision Drivers\n* All the functionalities implemented for API should be tested\n* Tracking whether a feature has been covered in the API or not should be easy\n\n## Decision\n,"* All the functionalities implemented for API should be tested\n* Tracking whether a feature has been covered in the API or not should be easy\nChosen *Using Behat for the feature coverage*, because it's the only option, that meets all the decision drivers criteria.\nWe will gradually add `@api` tag to the scenarios currently tagged with `@ui` and then implement the API contexts.\nAs a consequence, we will have to create a testing tool to use it in Behat contexts.\n","We will create a mapping from UI functionalities to API functionalities. This mapping will be maintained in a spreadsheet or other easily accessible document. As new API functionalities are developed, they will be added to the mapping. This will allow us to easily track which UI functionalities have been covered by the API and which still need to be implemented."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Problem to solve\nCover with API all `ProductOption`s related functionality provided by the Admin panel.\n### Context\nDuring the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle\nProduct Option's values collection, to make it efficient and easy to use.\n### Possible solutions\nValues from collection could be embedded as objects within a `ProductOption` resource **or** provided by\n[IRIs](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)\n### Decision and reasoning\nProduct option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\n(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\nresult in a massive and unmanageable response.\n\n## Decision\n","Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\n(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\nresult in a massive and unmanageable response.\n",Product option values should always be provided with their IRIs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSylius is by design application that will be adjusted to customer needs.\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\n\n## Decision\n","Chosen option: ""Config merging"", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\n","**Decision:** Split the API Platform configurations into smaller, more manageable chunks.\n\nThis will make it easier to customize and replace individual parts of the API Platform configuration, which will make it more flexible and maintainable.\n\n**Benefits:**\n\n* **Increased flexibility:** Smaller configuration chunks will make it easier to tailor the API Platform configuration to specific needs.\n* **Reduced maintenance:** Smaller configuration chunks will be easier to manage and update, reducing the risk of errors.\n* **Improved extensibility:** Smaller configuration chunks will make it easier to extend the API Platform configuration with custom functionality.\n\n**Drawbacks:**\n\n* **Increased complexity:** Splitting the configuration into smaller chunks may increase the overall complexity of the API Platform configuration.\n* **Potential for conflicts:** If the smaller configuration chunks are not carefully managed, there is a potential for conflicts between them.\n\n**Recommendation:**\n\nWe recommend splitting the API Platform configuration into smaller, more manageable chunks. This will provide increased flexibility, reduced maintenance, and improved extensibility. However, it is important to carefully manage the smaller configuration chunks to avoid increased complexity and potential conflicts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTo achieve 100% API coverage, we need to handle emails by API.\n\n## Decision\n","Chosen option: ""Using events"", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport.\n","**Decision:** Introduce an API Gateway with email forwarding functionality.\n\n**Rationale:**\n\n* An API Gateway provides a central point of entry for API requests, allowing for easy integration with email services.\n* Email forwarding allows incoming emails to be automatically relayed to the API Gateway for processing.\n* This solution enables the API to handle emails without direct SMTP connectivity.\n\n**Consequences:**\n\n* **Increased security:** The API Gateway acts as a security layer, controlling access to the API and preventing direct email attacks.\n* **Scalability:** The API Gateway can handle high volumes of email traffic, ensuring consistent API availability.\n* **Reduced complexity:** The API does not need to implement complex email handling logic, simplifying API development.\n* **Potential latency:** Email forwarding can introduce some latency compared to direct SMTP processing.\n* **Additional costs:** The API Gateway may incur additional costs for hosting and maintenance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhile developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\nwe have merged a unification of our endpoints in https://github.com/Sylius/Sylius/pull/11397. This move was dictated by\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\ndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\nand simplify the relations between resources. However, this change rose a lot of new issues to solve:\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\nadmin can see many more links than the regular visitor or shop user.\n* Endpoints like ""Product Show"" are resolved differently depending on the context as well. The shop products are determined\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\nseparation blurs the product identifier, and it is not clear which route should be considered as IRI.\n* the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the\ndecision that we want to be explicit, which user is trying to log in.\nUsing one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able\nto expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin\nresources are channel and locale agnostic, and we should use `codes` instead.\n## Decision Drivers\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\nOne resource in the database may aggregate several API Resources\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\n\n## Decision\n","* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\nOne resource in the database may aggregate several API Resources\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\nChosen option: ""Moving back to prefixed paths for back-office and shop"". This option was selected because it allows us to\neasily leverage the Symfony Security component's benefits like firewalls and ACL. The only second and third option provides\npredictable behavior and clear separation of concerns for admin and shop. Still, suffixes on most of the routes seem like\na worse solution compared to the common prefix. The common prefix will indicate that these endpoints are grouped in the\nsame context.\n","**Decision:** Use a dual-route approach with context-dependent serialization and IRI resolution.\n\n**Rationale:**\n\n* **Different serialization for different users:** Use a centralized serialization library with context-dependent configuration to handle different serialization requirements.\n* **Endpoint resolution:** Use different route prefixes for admin and shop endpoints, and design endpoints to explicitly resolve resources based on context-specific identifiers (e.g., slug for shop products, code for admin products).\n* **IRI determination:** Establish a clear mapping between context and IRI patterns, ensuring that each exposed API Resource has a unique and predictable IRI.\n* **Split log in endpoints:** Maintain separate log in endpoints to explicitly indicate the type of user attempting to log in.\n* **Identifier in shop and admin:** Use 'slug' as the identifier in shop endpoints to facilitate URL-based product exposure, and use 'code' as the identifier in admin endpoints to support channel- and locale-agnostic resource management."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nAPI Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.\nWhile we were designing Sylius new API, we have decided to use commands in many endpoints.\nIn some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`/`id`.\nIn the past, we have been using `code`/`id` instead of IRI, then we have been using both approaches.\nNow we are trying to unify the new API and replace codes and ids with IRI everywhere.\nThe main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`/`code`in its commands and command handlers.\n\n## Decision\n","Chosen option: ""Handling and transforming IRI to `id`/`code`"". Request that is based on command and needed information like `code`/`id` should get it as IRI\n","**Decision:** Use IRI in the request and convert it to `id`/`code` in the command handler.\n\n**Rationale:**\n\n* This approach allows us to use IRI in the request, which provides more clarity and allows for easier resource identification.\n* Converting the IRI to `id`/`code` in the command handler ensures that the commands and command handlers operate on the correct resource.\n* This approach is consistent with the recommendation from API Platform to use IRI as an identifier."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\n\n## Decision\n,Halt implementation on CMS Export and focus on scaling GraphQL.\n,Adopt GraphQL as the new CMS Export solution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAttempts were made to upgrade all usage of Python from an older EOL'd version to Python 3. This presented a number of issues as we also use Ansible and Amazon Linux. The combination of these showed difficulty around how we use Ansible to install packages (`yum` module currently). The Ansible `yum` module is reserved specifically for Python 2. Python 3 requires usage of the `dnf` module. However, according to statements and current evidence, AWS has no intention to support a `dnf` extra for the life of Amazon Linux 2. It was also stated [in this forum post](https://forums.aws.amazon.com/thread.jspa?threadID=317863) ""We are committed to keeping Python 2.7 secure for the life of Amazon Linux 2"".\nAnother attempt was then made to replace our underlying OS with CentOS 8 perhaps to see how heavy of a lift this could be. With a bit of tweaks, we were able to at least run the automation that creates encrypted base AMIs. However, this is only the surface of everything that would have to be tested/changed for this move to happen. The referenced tweaks were as follows (for tracking purposes):\n- Change the default value for the `bake_ami` variable found [here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/applications/config/default.yml#L3) to a CentOS AMI ID (ami-0f5f21f0f86d11d6e was used for testing which is a ""hardened"" CentOS 8 image maintained / distributed by Frontline).\n- Change all usage of the `yum` module to the `dnf` module in this [role / configuration file](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml).\n- Create a task to set symbolic links after installing Python 3 for `/usr/bin/python -> /usr/bin/python3` and `/usr/bin/pip -> /usr/bin/pip3` since Python 3 installations follow the `<package>3` naming convention by default.\n- Comment out / remove the task for [installing, starting, and enabling the NTP daemon](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml#L27-L37) as that package was not found with default repositories on the CentOS image and it also already comes with `chrony` installed and enabled.\n- Comment out / remove Amazon Linux distro based conditional for the install of the `epel-release` package as well as an additional task that currently exists specifically for usage of Amazon Linux 2 image ([both outlined here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml#L44-L48)).\n- Comment out / remove task that [removes yum lock](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml#L53-L56) since `dnf` was the package manager / module used in this scenario.\n- Addition of a task to install the ssm-agent as it would no longer come by default if we stopped leveraging Amazon Linux distributions.\n- Drop the usage of `unicode` type in some of our custom Ansible related Python files within the `filter_plugins` directory [here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/filter_plugins/aws_finders.py#L25) and [here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/filter_plugins/resource_tags.py#L404). More on the ""why"" of that change/dynamic can be found in [this article](https://medium.com/better-programming/strings-unicode-and-bytes-in-python-3-everything-you-always-wanted-to-know-27dc02ff2686).\n\n## Decision\n","After multiple days of testing, analysis, troubleshooting, and presenting these numerous issues to the Operations team as well as a chat with Joe to gather thoughts from a security perspective, it has been decided at this time to ""accept risk"" on the continued usage of Python 2 at this time. At this time, our Python code is not end-user-facing (frontend is Ruby) and the bulk of it exists in internal scripts, lambdas, and our CI/CD pipeline.\nIntentions to revisit this issue would largely hinge on one of the following scenarios:\n- Amazon Linux 3 is released with full Python 3 + dnf support\n- Amazon Linux 2 changes direction to fully support Python 3 + dnf\n- Through the work of our infrastructure recommit, if we can get to a fully containerized environment, we will ensure the images used do not leverage the end of life'd version of Python if at all possible.\n","Due to the difficulties presented by attempting to upgrade Python and the lack of support for `dnf` in Amazon Linux 2, we have decided to explore alternative solutions that will not require a change in our underlying operating system.\n\nWe will investigate using a Docker container to run Python 3 and Ansible within our current Amazon Linux 2 environment. This approach will allow us to isolate the Python 3 environment from the rest of the system and avoid the potential issues caused by the lack of `dnf` support."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, users who log onto va.gov during repeat visits have potentially inconsistent behavior from the site, especially if they switch the credential they login with.\nThis is most stark with aspects of the site that leverage persistent data, such as forms that the user has partially filled out, and which must be matched up to the user.\nThe Accounts table is used today to aid in this, it creates a table entry that links an idme_uuid, icn, sec_id, to different data in our schema, and which we can retrieve\neven after a user session has been ended and a new one started.\nThere are some issues with the current implementation, however, which make the intended experience inconsistent. The Accounts table relies on a matching idme_uuid, first and foremost.\nThen will attempt to match with an ICN or a sec_id, depending on the retrieval method. If a user logs in with different credentials, such as ID.me from va.gov, and then later\nDS Logon from MyHealtheVet (then navigates to VA.gov), they may see different forms and other persistent data depending on the credential.\nMy Proposal involes refactoring the work done for the Accounts table into something more along the lines of how we actually deal with our users. This involves the concept\nof a 'credential_user', and a 'real_user', and tables to represent each of these: `user_verifications` and `user_accounts`.\nA 'credential_user' is our perspective of a user simply as information we get from a specific credential provider. For example, if a user logs in with MHV, we would\nexpect to have an MHV uuid representing their account uuid. When a user authenticates, we would store the 'credential_user' by finding or creating an entry in the `user_verifications`\ntable. This table would store the uuid from whatever credential provider they logged in with, whether or not the user was verified (LOA3/IAL2) and a link to an entry in the `user_accounts`\ntable.\nA 'real_user' is our perspective of an actual human behind the entity logging in, regardless of how they logged in. These are reflected in a `user_accounts` table, and would\nonly have an optional icn value (if icn is set, we know they are a real, verified human. Because we can make an MPI call with only an icn, this value represents all of the 'real'\ndata we have on the 'real_user'. If an icn is not set, we know there is a human behind the entity logging in, but we cannot verify it to any human in our records).\nEvery 'credential_user' will have a link to a single 'real_user', and every `user_verifications` entry will have a link to a `user_accounts` record. If a user was not verified\nand then becomes verified, the `user_accounts` entry may either simply be updated with the new icn, or, if an existing `user_accounts` record exists with that icn, it will be\nreplaced by the existing `user_accounts` entry, and the unverified `user_accounts` entry will be moved into a `deprecated_user_accounts` table.\nEvery persistent table on the backend will point to a `user_accounts` uuid, as we want to correlate these to a 'real_user', not to the specific credential. By organizing\nthings this way, we will have consistent behavior for the end user, regardless of whatever credential combination they often use. This\n\n## Decision\n",1 or more sentences summarizing the decision made.\n,"The decision is to refactor the work done for the Accounts table into something more along the lines of how we actually deal with our users. This involves the concept of a 'credential_user', and a 'real_user', and tables to represent each of these: `user_verifications` and `user_accounts`.\n\nA 'credential_user' is our perspective of a user simply as information we get from a specific credential provider. For example, if a user logs in with MHV, we would expect to have an MHV uuid representing their account uuid. When a user authenticates, we would store the 'credential_user' by finding or creating an entry in the `user_verifications` table. This table would store the uuid from whatever credential provider they logged in with, whether or not the user was verified (LOA3/IAL2) and a link to an entry in the `user_accounts` table.\n\nA 'real_user' is our perspective of an actual human behind the entity logging in, regardless of how they logged in. These are reflected in a `user_accounts` table, and would only have an optional icn value (if icn is set, we know they are a real, verified human. Because we can make an MPI call with only an icn, this value represents all of the 'real' data we have on the 'real_user'. If an icn is not set, we know there is a human behind the entity logging in, but we cannot verify it to any human in our records).\n\nEvery 'credential_user' will have a link to a single 'real_user', and every `user_verifications` entry will have a link to a `user_accounts` record. If a user was not verified and then becomes verified, the `user_accounts` entry may either simply be updated with the new icn, or, if an existing `user_accounts` record exists with that icn, it will be replaced by the existing `user_accounts` entry, and the unverified `user_accounts` entry will be moved into a `deprecated_user_accounts` table.\n\nEvery persistent table on the backend will point to a `user_accounts` uuid, as we want to correlate these to a 'real_user', not to the specific credential. By organizing things this way, we will have consistent behavior for the end user, regardless of whatever credential combination they often use. This will also allow us to more easily track and manage user data, as we will have a single, central location for all user information."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Pact Broker has been on Heroku during development, but its database is running out of rows for the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\n\n## Decision\n",We will move the Pact Broker onto our AWS infrastructure.\n,Move the Pact Broker onto our AWS infrastructure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the larger effort to [refactor the User & Identity on vets-api](https://app.zenhub.com/workspaces/vsp-identity-5f5bab705a94c9001ba33734/issues/department-of-veterans-affairs/va.gov-team/21012), we have decided to more tightly control access to user attributes through specific & improved getter methods. The `va_profile` methods on the User class revealed too much information about a user's MPI profile and has been rendered uneccessary by the updated getter methods.\n\n## Decision\n","The decision made is to transfer the User's `va_profile_status` and `va_profile_error` methods to `mpi_status` and `mpi_error` since that is what they are abstractions of, and to remove the `va_profile` method entirely in favor of relying on getter methods for specific attributes.\n","Deprecate the `va_profile` methods on the User class, replacing them with specific and improved getter methods that provide more granular control over access to user attributes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUsing the Rails 6 zeitwerk file loader requires strict naming conventions of files. So without a Rails inflection, `va_something` class files are expected to have the class name of `VaSomething`. With an inflection, that class name would be `VASomething`. `vets-api` uses a mix of `VA` and `Va` in class names. To make things consistent, as well as support instances of `humanize` outputing `VA` in caps as an acronym, the inflection will be introduced.\nHowever, using this inflection has a downstream affect on json responses transformed into camelCase by the [olive_branch](https://github.com/vigetlabs/olive_branch) gem. Previously keys structured like `the_va_something` were transformed into `theVaSomething`, but with inflection they will now be `theVASomething`.\n\n## Decision\n","Create a patch implemented for the olive_branch middleware that returns keys containing `VA` as if ""VA"" was not an acronym.\nIn the future vets-api should introduce a version that doesn't support olive_branch.\n",**Introduce the inflection.**\n\n**Rationale:**\n\n* Enforces consistent naming conventions in class files.\n* Supports instances of `humanize` outputting `VA` in caps as an acronym.\n\n**Consequences:**\n\n* JSON responses transformed into camelCase by the `olive_branch` gem will now have keys structured like `theVASomething` instead of `theVaSomething`.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described in this Confluence document](https://vfs.atlassian.net/l/c/58vSL6ZP).\n","**Decision:** To use an Architectural Decision Record (ADR) template for documenting and recording architectural decisions.\n\n* **Benefits:**\n    * Provides a consistent and structured approach to documenting decisions.\n    * Ensures that all relevant information is captured, such as the decision, its rationale, and the date it was made.\n    * Facilitates communication and understanding of architectural decisions among stakeholders.\n\n* **Trade-offs:**\n    * Requires discipline to maintain and update ADRs.\n    * May add some overhead to the decision-making process.\n\n**Rationale:**\n\nAn ADR template provides a framework for capturing all essential information about an architectural decision, including its context, options considered, decision made, and implications. By standardizing the format and process for documenting decisions, ADRs improve communication, collaboration, and knowledge sharing within the team.\n\n**Consequences:**\n\n* All architectural decisions will be documented using the approved ADR template.\n* Stakeholders will be responsible for maintaining and updating ADRs as decisions evolve.\n* ADRs will be used as a reference point for design, implementation, and future decision-making."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe class `KnowledgeArtifact` is a helper base class that was not intended to have its own database table (see Issue [#182](https://github.com/UST-QuAntiL/qc-atlas/issues/182)).\nThe current implementation as a joined table generates a database table.\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\n## Decision Drivers <!-- optional -->\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\n\n## Decision\n,"* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\nChosen option: ""[Joined table]"", because it is already implemented, allows for references in both ways and has no significant downside.\n### Positive Consequences <!-- optional -->\n* The current implementation can stay\n",Remove the joined table implementation for `KnowledgeArtifact` and instead store it as a JSON blob in the `DiscussionTopic` table.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince the project started out with both JUnit4 and JUnit5, we only want one unit-testing framework in order to make testing writing consistent.\n\n## Decision\n","Chosen option: ""[JUnit5]"", because it is the newer version and therefore has a higher maturity and a wider feature-set.\n### Positive Consequences <!-- optional -->\n* Uniform tests\n* More and newer features\n","**Decision:**\n\nMigrate all unit tests to JUnit5.\n\n**Justification:**\n\n* JUnit5 is the latest and most actively supported version of JUnit.\n* It offers several advantages over JUnit4, including:\n    * More concise and expressive syntax\n    * Support for parameterized tests\n    * Improved error reporting\n* Migrating to JUnit5 will make our testing code more consistent and easier to maintain.\n\n**Consequences:**\n\n* All unit tests will need to be migrated to JUnit5.\n* This may require some refactoring of existing test code.\n* However, the benefits of migrating to JUnit5 outweigh the costs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n## Decision Drivers\n* readable API documentation\n* effort of manually creating client services\n\n## Decision\n","* readable API documentation\n* effort of manually creating client services\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\n### Positive Consequences\n* Standardized documentation of the API\n* Automatic service generation for clients is possible\n### Negative Consequences <!-- optional -->\n* OpenAPI annotations have to be maintained\n","**Decision:** Use OpenAPI (fka Swagger) to document the API in both human and machine readable form.\n\n**Rationale:**\n\n* OpenAPI is a widely accepted standard for describing REST APIs.\n* OpenAPI specifications can be used to automatically generate client services, reducing manual effort and potential errors.\n* OpenAPI documentation is both human and machine readable, meeting the requirement for both forms of documentation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDue to the use of two different object types for transfering and storing data, the conversion between the types becomes a necessity. Therefore we need means to perform the respective conversion.\n\n## Decision\n","Chosen option: ""[Model Mapper](http://www.modelmapper.org)"", because it fulfils our requirements, is highly configurable and less error prone then manually written converters.\n### Positive Consequences <!-- optional -->\n* Less boilerplate code\n",**Decision:** Implement a converter class that provides methods for converting between the two object types.\n\n**Justification:**\n\n* A converter class provides a centralized and maintainable way to convert between the object types.\n* It removes the responsibility of conversion from the code that uses the objects.\n* It reduces the risk of inconsistencies and errors in the conversion process.\n* It can be used to enhance future flexibility by allowing for different conversion strategies or additional object types.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTo ensure high test coverage within the project, functionalities that interact with the database must be tested as well.\nThere are different approaches to testing these functionalities that require a database running.\n\n## Decision\n","Chosen option: ""Use Production-like [Postgres](http://www.postgresql.org) database"", because it ensures a production-like behavior.\n### Positive Consequences <!-- optional -->\n* Almost same configuration for test and runtime.\n* Less database technologies used.\n* Guaranteed that production and tests behave the same.\n","**Decision:** Use an in-memory database for integration tests.\n\n**Rationale:**\n\n* An in-memory database can be started and stopped quickly, which reduces test execution time.\n* It does not require a separate database server to be set up and maintained.\n* It provides isolation between tests, as each test has its own instance of the database.\n* However, an in-memory database may not be suitable for large datasets or complex database operations. In such cases, a dedicated test database should be used."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\nConstructing such objects, as well as adding the desired links to them is a common operation that\nrequires entity-specific boilerplate code.\nHow can duplicate code in nearly all controller methods be avoided?\n## Decision Drivers <!-- optional -->\n* Avoid duplicate code to create HATEOAS models\n* Decouple link creation from normal entity logic\n\n## Decision\n","* Avoid duplicate code to create HATEOAS models\n* Decouple link creation from normal entity logic\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\nand our DTO classes.\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\n","**Create a factory method to generate HATEOAS models.**\n\n### Benefits\n* Reduces the amount of duplicate code in controller methods.\n* Decouples link creation from normal entity logic, making the code more maintainable.\n* Allows for easier testing of link creation logic.\n\n### Drawbacks\n* Adds a small amount of complexity to the codebase.\n* May not be appropriate for all projects.\n\n### Alternatives\n* Use a third-party library to generate HATEOAS models.\n* Create a custom annotation to generate HATEOAS models.\n\n### Recommendation\nIf you are writing a project that uses Spring HATEOAS and you are concerned about duplicate code in controller methods, then you should consider using a factory method to generate HATEOAS models."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nShould the components in the PlanQK platform be splitted into individual repos?\n\n## Decision\n,"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\n### Positive Consequences <!-- optional -->\n* Recommended approach by [Martin Fowler](https://martinfowler.com/bliki/MonolithFirst.html)\n","**Decision:** Yes, split the components into individual repos.\n\n**Rationale:**\n\n* **Modularity and maintainability:** Individual repos allow for improved modularity and maintainability of the codebase. Each repo can be managed and updated independently, without affecting the other components. This is particularly advantageous in a large and complex platform like PlanQK, where different teams may be responsible for different components.\n* **Reduced dependencies:** Splitting the components into individual repos helps reduce interdependencies between them. This makes it easier to debug and fix issues, as changes to one component will not automatically affect the others.\n* **Improved collaboration:** Individual repos facilitate better collaboration, as different teams can work on specific components without interfering with each other. This also allows for easier code reviews and merging of changes.\n* **Versioning and deployment:** Individual repos provide better control over versioning and deployment. Each component can be versioned and released independently, allowing for more flexibility and easier management of updates.\n* **Scalability:** Splitting the components into individual repos improves the scalability of the platform. It allows for the isolation and horizontal scaling of individual components as needed, without affecting the rest of the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince we use the OpenAPI for frontend client code generations, the HATEOAS links are no longer needed or used.\n## Decision Drivers\n* readable API documentation\n* development effort to test/gather the HATEOAS links\n\n## Decision\n","* readable API documentation\n* development effort to test/gather the HATEOAS links\nChosen option: remove the HATEOAS links from all entities, because this further simplifies the DTO entities\n","**Decision:** Remove HATEOAS links from the API.\n\n**Reasoning:**\n\n* **Reduced development effort:** Removing HATEOAS links eliminates the need for testing and gathering them, reducing development time and effort.\n* **Improved API documentation:** Without HATEOAS links, the API documentation becomes more concise and easier to understand.\n* **Alignment with frontend client code generation:** Since the OpenAPI is used for frontend client generations, the HATEOAS links are no longer necessary, making the API more consistent and efficient."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\n\n## Decision\n,"Chosen option: [Testcontainers](https://www.testcontainers.org/), because it simplifies the test execution process\n### Positive Consequences <!-- optional -->\n* It is not mandatory to ensure postgres is running before starting the tests\n",Use a PostgreSQL Docker container to create an instance that's always up and running for testing purposes.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\nWe need references for the links.\n\n## Decision\n","Chosen option: ""[URIs]"", because UUIDs are generated and thus depend on the underlying database system.\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\n### Positive Consequences <!-- optional -->\n* We follow solid [W3C specification](https://www.w3.org/Addressing/URL/uri-spec.html)\n","The decision is to use ""Hyperlinks"".\n\nSpecifically, we will use web-based hyperlinks from the QC Algorithm to the QC pattern and from the QC pattern to the QC Algorithm.\nThis will allow users to easily navigate between the two entities.\nThe hyperlinks will be implemented using the HTML/CSS syntax."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\n### Who Was Involved in This Decision\n- Alex Ward\n- Chris Weight\n### Relates To\n- N/A\n\n## Decision\n","- Alex Ward\n- Chris Weight\n### Relates To\n- N/A\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\n","**Decision:**\n\nAllow developers to choose the state management library of their choice.\n\n**Rationale:**\n\n* State management libraries are rapidly evolving, with new trends and preferences emerging regularly.\n* Enforcing a particular library could limit the flexibility and adaptability of the codebase.\n* Allowing developers to choose the best tool for the job based on requirements and complexity promotes code ownership and accountability.\n* It encourages knowledge sharing and collaboration among team members.\n* It fosters a culture of learning and exploration, which can lead to the adoption of innovative solutions.\n\n**Consequences:**\n\n* Potential for inconsistent code styles and practices across the codebase.\n* Increased onboarding time for developers who are unfamiliar with the chosen library.\n* Potential conflicts when integrating with existing systems or third-party libraries that support different state management approaches.\n\n**Mitigation Strategies:**\n\n* Provide guidelines and documentation on best practices for state management.\n* Implement a code review process to ensure consistency and quality.\n* Encourage cross-team knowledge sharing and code refactoring to reduce inconsistencies.\n* Allow for experimentation with different libraries and technologies in contained environments, such as sandboxes or dedicated branches."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a convenient system for managing API keys used by the Python client. This system should give the user multiple options for\nproviding an API key to be used when making a request to the API. These options should include:\n* Storing API keys on the users system\n* Reading an API key from the environment\n* Passing an API key directly to the API request methods\nUsers may have multiple valid API keys associated with their account at any given time. The system for storing API keys on the user's\nsystem must accommodate this and provide a clear, deterministic way of resolving an API key for a given project.\nWe anticipate the need to store other data related to Radiant MLHub for uses unrelated to authentication. For instance, we may have a need to\ntrack the progress of downloads so that they can be resumed if interrupted, or we may want to specify a base URL in a config file so that\ndevelopers can test against the staging environment. The method that we choose for storing API keys on the user's system must not preclude\nus from storing this additional information.\n\n## Decision\n","The Python client will resolve the API key to be used in a request in the following order:\n1) Passing an `api_key` argument directly to the method\n2) Setting an `MLHUB_API_KEY` environment variable\n3) Passing a `profile` argument directly to the method. This will read the API key from the given profile (see below for details)\n4) Setting an `MLHUB_PROFILE` environment variable. This will read the API key from the given profile (see below for details)\n5) Using the API from the `default` profile\nProfiles will be stored in a `.mlhub/profiles` file in the user's home directory. This file will be an INI file containing at least a\n`[default]` section with an `api_key` value. The file may contain other sections corresponding to named profiles. Any `profile` argument\npassed to a method must correspond to one of these section names, or it will raise an exception.\n","Implement a keyfile store that reads and writes keyfiles to the user's system in the following format:\n\n```yaml\nkey_1: value_1\nkey_2: value_2\n```\n\nThis file should be stored in a well-known location on the user's system, such as the user's home directory. Users can then provide an API key to the API request methods in any of the following ways:\n\n* Passing the API key directly to the API request method as a parameter.\n* Setting an environment variable with the API key.\n* Specifying the path to a keyfile containing the API key.\n\nThe system will first check for an API key passed to the API request method as a parameter. If no API key is provided as a parameter, the system will then check for an environment variable containing the API key. If no API key is found in the environment, the system will then check for a keyfile at the specified path. If a keyfile is found, the system will read the API key from the keyfile.\n\nThe system will also allow users to store other data related to Radiant MLHub in the keyfile. This data can be accessed by using the `get()` method of the `KeyfileStore` class."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Radiant MLHub API implements the `/items` endpoint as described in the [STAC API - Features](https://github.com/radiantearth/stac-api-spec/tree/master/ogcapi-features)\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\n\n## Decision\n","To avoid a confusing user experience when working with Items, and to avoid inadvertently swamping the API with requests, we will not provide\na method in either the low-level client or on the `Collection` classes to loop over the items in a collection. Preliminary work had adapted\nthe [`Collection.get_items`](https://pystac.readthedocs.io/en/latest/api.html#pystac.Catalog.get_items) method to make paginated requests to\nthe `/items` endpoint. Instead, this method will raise a `NotImplementedError` to indicate that this feature is not available.\n*Work is planned to add an endpoint to the Radiant MLHub API to enable downloading a single archive containing all items associated with a\nCollection. Support for this endpoint in the Python client may be the subject of a separate ADR.*\n",Implement a new `/bulk_items` endpoint that returns all items for a given collection in a single API response.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs/adr` directory within this repository. Instead of using the ""deprecated"" and ""superseded"" status value, we will move ADRs that are no longer applicable into the `docs/adr/archive` directory in this repository.\n","To create an Architectural Decision Record, we should document the following information:\n\n- **Decision:** The decision that was made.\n- **Date:** The date the decision was made.\n- **Reason:** The rationale behind the decision.\n- **Consequences:** The potential consequences of the decision.\n- **Alternatives Considered:** The other options that were considered.\n- **Status:** The current status of the decision.\n\nHere is an example of an Architectural Decision Record:\n\n**Decision:** Use a microservices architecture for the new application.\n**Date:** 2023-03-08\n**Reason:** We need to develop and deploy the application quickly and independently. We expect the application to be used by a large number of users, so we need to be able to scale it easily.\n**Consequences:** The microservices architecture will make the application more complex to develop and maintain. We will need to invest in a good CI/CD pipeline and monitoring system.\n**Alternatives Considered:** Monolithic architecture, serverless architecture\n**Status:** This decision has been approved by the project team and is being implemented."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is the problem where to store different utilities and write every time the same code on all projects.\n\n## Decision\n,"Use [lodash](https://lodash.com/docs/) possibilities as main package for utilities on the JS microservices, in order to spend less time and write less code. And use utils directory as a wrapper for it in order to be able to replace it any time.\n",**Decision:** Create a utility library that can be included in all projects.\n\n**Pros:**\n\n* DRY (Don't Repeat Yourself) principle is followed.\n* Consistent functionality across all projects.\n* Simplified maintenance and updates.\n* Improved code quality and readability.\n* Reduced duplication of effort.\n\n**Cons:**\n\n* Initial effort required to create the library.\n* Potential for versioning conflicts if the library is updated frequently.\n* Additional overhead of maintaining the library separately from projects.\n\n**Implementation:**\n\n* Create a separate repository for the utility library.\n* Include the library as a dependency in all projects.\n* Establish clear versioning and dependency management practices.\n* Regularly review and update the library to ensure compatibility with projects.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA way to document and track my design choices is needed, for myself in order to see how project architecture is changing.\n\n## Decision\n","I will use ADR to document any important architectural decisions I make.\nADRs will be checked into the repository as numbered md files in the folder docs/architecture/adr\nI will follow the template described [here](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\nAll Records will contain, Title, Context, Decision, Status and Consequences\nIf a decision is changed or overruled we don't delete the record, but change the status accordingly (to superseded or deprecated).\nIf a decision is superseded or deprecated we should add a link to the new decision. In the format Superseded by [link]\n","**Decision:** Use an Architectural Decision Record (ADR) template to document design choices.\n\n**Rationale:**\n\n* ADRs provide a structured way to capture design decisions, including the context, considered alternatives, decision, and rationale.\n* They help track the evolution of the project architecture and provide a record of the decision-making process.\n* ADRs facilitate team communication and collaboration by ensuring that design choices are understood and aligned.\n* Using a template streamlines the ADR creation process and ensures consistency across decisions.\n* It allows for easy retrieval and review of design choices throughout the project lifecycle."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn ADR0003, we described the 'Bicycle Network Analysis' task to be run via an asynchronous task queue. Since then, Amazon Web Services (AWS) released a new service simply named 'Batch'. This service provides a managed task queue, with Boto and HTTP API interfaces for creating queues and jobs, and triggering new jobs. Each job in AWS Batch is configured to run a Docker container provided to the job configuration. AWS Batch manages ordering and execution of tasks in the queue. In almost every way, AWS Batch is a superior choice to the strategy outlined in ADR 0003, for a few key reasons:\n- AWS Batch manages the queue and task autoscaling without any management from the parent application. The service can be trivially configured to scale up or down on a few different resource considerations. If there are no jobs in the queue, the pool of workers will automatically scale to zero, saving on hosting costs.\n- AWS Batch, in comparison with a manually managed stack of celery workers + broker + result backend, is easy to configure, as it only requires defining a ""worker"" stack via a JSON cofiguration.\n- Switching from a Celery and ECS task based solution will be easy, as AWS Batch workers are configured with Docker containers in the same way as ECS tasks would be\n- It will be easier to trigger jobs from Django using AWS Batch, since direct calls can be made via Boto, rather than having to write some management layer to trigger ECS tasks or work with the Celery API.\n\n## Decision\n","The team will build the Bicycle Network Analysis task queue on AWS Batch. The reduction in manual task queue management and ease of configuration should vastly outweigh having to learn how to develop applications using an unfamiliar service. While relatively new, AWS Batch has support in both Boto and via HTTP API and manual setup of a Batch stack was relatively straightforward.\n",Use AWS Batch for running asynchronous tasks.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to document major architecture decisions; in the past we have used the [Architecture Decision Record (ADR) format](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). On past projects, we have found the ADR format to be a useful way to write and manage architecture decisions.\nWe have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST / Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\n\n## Decision\n","We have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST / Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\nWe will continue to use the ADR format for writing architecture decisions for this project. We will use Markdown for formatting ADR documents.\n","Based on the given context, the team's preference for Markdown, and the potential use of documentation generators that may limit format choices, we have decided to use the Markdown format for ADRs on this project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\n- Import neighborhood boundary into a PostgreSQL database\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\n- Download related census block and job data for the boundary and import to PostgreSQL\n- Generate a network graph from the imported data\n- Run a series of client-provided analyses on the graph and imported data\n- Export relevant data to an external file store for archival\n- Generate a tile set of the network graph for display on a web map\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\nCelery has multiple options for brokers:\n| Broker | Advantages | Disadvantages |\n| ------ | ---------- | ------------- |\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http://docs.celeryproject.org/en/latest/getting-started/brokers/sqs.html#caveats) |\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\n\n## Decision\n","The team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\n","**Decision:** Use Celery with an SQS broker.\n\n**Rationale:**\nCelery provides a familiar interface for triggering analysis jobs, and SQS is a cheap, easy-to-set-up broker that now provides stable configuration options to isolate environments. While SQS does not provide a result backend, the analysis workflow is already a self-contained process, so a result backend is not necessary. Additionally, the potential issues with result timeouts in SQS can be mitigated by configuring the Celery task to handle retries and timeouts appropriately."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA backend web framework significantly eases the amount of boilerplate necessary for an application to handle web requests. Additionally, it provides scalable, out of the box, well-tested solutions for common features such as user management, authentication, database interaction, and a public API.\nThis project will mostly require only the common components described above, so a backend web framework that we are familiar with will speed development for the project. The only unknown is the integration of the asynchronous 'Bicycle Network Analysis' task mentioned in [ADR 0001](adr-0001-development-environment.md)\nThe team is most familiar with Python, Django and the Django Rest Framework. Due to project constraints and the desired functionality, no other backend frameworks were considered for this project.\n\n## Decision\n","The team will use Django with the Django Rest Framework plugin, written in Python. The team's familiarity with this stack is too much of a positive to pass up. In addition, Django provides many third-party solutions for integrating the asynchronous 'Bicycle Network Analysis' task. This allows the team to be flexible when choosing a solution, without sacrificing development efficiency.\n","Implement the backend with the Django framework, which is familiar to the development team and provides the necessary functionality for this project. Integrate the asynchronous 'Bicycle Network Analysis' task by leveraging Django's built-in support for background tasks to ensure that the main web application loop is not blocked. Explore the use of Django channels for potential performance benefits in handling the asynchronous task."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis application will require a fairly standard application stack - web server, database store and an asynchronous task queue. In the past, the team has used either Vagrant + Ansible or Docker + Docker Compose to build these application stacks.\nIn general, Docker containers improve the CI build and deployment workflows, at the expense of a slightly more complicated development environment, especially for users not using Linux. In the past, the team has wrapped the Docker environment within a barebones Ubuntu VM using Vagrant to streamline the development workflow across different OSes. Recently however, Docker released a new tool, Docker for Mac, which attempts to streamline native use of containers on macOS. This tool may eliminate the need for the wrapper VM, but has some potential pitfalls.\nPrevious projects defaulted to an Ubuntu VM, with the containers using the simplest debian-based OS. Azavea maintains a series of Docker containers that provide the building blocks for the application stack we will be building.\nThis project contains a 'Bicycle Network Analysis' task which is runs on a self-contained PostgreSQL instance. Running this via Vagrant+Ansible or a Docker container should be relatively straightforward either way, since the task has known software dependencies and does not have any external dependencies. When this project begain, this task was configured via Vagrant+Ansible. Some additional work would be necessary to convert this task to a Docker container.\n\n## Decision\n","In order to take advantage of the better deployment and provisioning workflows provided by AWS ECS when using containers, we decided to construct the development environment using a Docker Compose environment wrapped within an Ubuntu VM. While Docker for Mac looks compelling, it has a few downsides:\n- We cannot control the version of Docker installed, which could be problematic as the project ages\n- There are potential incompatibilies for users with the older Docker Toolbox installed\n- It may be difficult to cull outdated container images across projects\n- It may be difficult to isolate various project instances and their dependencies\nUsing the wrapper VM avoids these issues and provides us with a relatively 'known good' experience for a project with somewhat limited budget constraints.\n","Use Docker for Mac with Docker Compose for development and deployment of the application. This will leverage the benefits of Docker containers for improved CI/CD workflows while providing a streamlined development environment for users on macOS. The use of Docker Compose will simplify the management of the application stack. Additionally, the existing Docker containers provided by Azavea will be utilized to build the application stack. The ""Bicycle Network Analysis"" task will be converted to a Docker container to take advantage of the benefits of containerization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\nFirstly, we can control on a per-tenant basis where the child tenant data for that tenant are stored. For example, with two sibling tenants A and B (i.e. tenants that are children of the same parent tenant), the data for the child tenants of A can be stored in a completely different location to that of B. By default, this will be a separate container in the same storage account, but it could be a completely separate storage account.\nSecondly, it can be used to enable better organisation of tenants by using parent tenants to group related tenants together.\nOne of of the functions of tenants is to hold client-specific configuration for the applications that a client is using. An example would be for a client using the Workflow service. Their tenant will contain two pieces of storage information, one for Workflows and one for Workflow instances. This configuration is stored in a collection of key-value pairs attached to the tenant.\nIt is possible for tenants to have child tenants in the hierarchy. If a tenant that uses the Workflow service has children, they may also need to use the Workflow service. In this case we have a choice: we can decide that we will allow the workflow storage configuration from a tenant to be inherited by its children, or we can require each tenant to contain all of its own configuration.\n\n## Decision\n","We have determined that we will not make properties of tenants available to their children by default. Applications which consume this library can implement that functionality for themselves if required - for example, by manually copying properties from parent to children when new tenants are created.\nWhilst property inheritance seems desirable from a development perspective - for example, creating temporary tenants for testing purposes, or setting up tenants for developers - it is likely to be less useful in envisaged production scenarios.\nIn the case when hierarchy is used for organisational purposes, inheritance is not relevant; parent tenants are there solely to group their children and configuration for the parent tenant is irrelevant, as it does not exist to be used as a tenant in its own right.\nIn the case where hierarchy represents a genuine parent-child relationship there are many potential reasons for this, and the goal of the project is not to dictate specific use cases. However, in making the decision not to implement property inheritance it is only necessary to find a use case where it is not desirable.\nOur use case here is a PaaS product providing multiple services - endjin's Marain platform. This platform contains several base services - Tenancy, Workflow, Operations and Claims - which can be licensed by clients.\nA client may choose to use these services to build their own platform, and use Marain's tenancy service to provide their own platform services to their own customers. In this case, the client's customers will be represented by child tenants of its own tenant.\nIn this situation there are two negative outcomes from allowing configuration to inherit from parent to child tenants.\n1. The client may make use of Marain services (e.g. Workflow) to provide services to its customers. Configuration for these services is stored as configuration on the client tenant. Automatic property inheritance would mean that by default, child tenants of the client would also have the ability to access these services, which should not be the case.\n1. The configuration attached to a client's tenant contains various pieces of sensitive information. For example, it may contain storage account details for storage that is not directly owned by the client. For this reason, Marain does not allow clients to view their own configuration data, or that of their parents. However, clients do need to be able to view and modify the configuration of child tenants. If we automatically allowed properties to be inherited by child tenants, it would be possible for a client to create a child tenant and examine those inherited properties to access what is effectively the client's own configuration data.\n",Allow tenants to inherit storage configuration from their parent tenants if they do not override it.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp until now, tenant Ids have always been automatically generated. As we have used Corvus.Tenancy as part of Marain, we have come across scenarios where it is useful to control what a tenant's Id will be. This primarily applies to the tenants we use as top-level containers for Client and Service tenants, as well as the service tenants themselves.\nIn the absence of the ability to know tenant Ids in advance we have fallen back on well known names for these tenants, effectively introducing an alternative well-known Id for them. As well as introducing a potential point of failure due to the need to keep these names unique, it is far less efficient to locate a tenant by name than by Id.\n\n## Decision\n","To avoid the need to effectively introduce an alternative way of identifying tenants, we will allow the Id of tenants to be controlled when they are created.\nThis will be done by adding a new method to the `ITenantProvider` interface: `CreateWellKnownChildTenantAsync`. This allows callers to specify a GUID which is used to generate the new tenant's ID, rather than generating a random GUID internally (which is what the existing `CreateChildTenantAsync` method does).\nThe resulting Id of the new tenant will still be generated by concatenating the parent tenant's Id with the hash of the provided GUID. This means that in order for a tenant to have a well known Id, all of its ancestors must also have well-known Ids.\nIn order to prevent two tenants being created with the same Id, it is necessary to check that the Id is not already in use prior to creating the new tenant.\n","Create a new INameGenerator interface that will replace the current implementation on ITenantStore. This new INameGenerator will be responsible for generating tenant Ids, and will allow us to control the format of the tenant Ids. We will also need to update the ITenantStore interface to include a new method that will allow us to specify the INameGenerator to use."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](./0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\n* a tenant may exist in which only a subset of its storage containers exist\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\n\n## Decision\n","Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\n1. using nothing but v2\n1. using v3 libraries mostly (see below) in v2 mode\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\n1. using v3 libraries in non-transitional mode\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https://github.com/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\n### Phase 2: using v3 libraries, operating in v2 mode\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries—the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\nThe one difference in behaviour (the reason we describe this as ""mostly"" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\n### Configuration migration\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\n",Applications that have already been deployed on `Corvus.Tenancy` v2 will be responsible for creating all necessary storage containers when onboarding a client. This can be done by using the `Corvus.Tenancy` API to create the containers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn `Corvus.Tenancy` v2, the various storage-technology-specific libraries (e.g. `Corvus.Azure.Storage.Tenancy`) could dynamically create new containers for you the first time you asked for them. One of the problems this caused was that the definition types (e.g. `BlobStorageContainerDefinition`) needed to include all of the information required to be able to create a new container on demand. For example, with blob containers, that meant specifying the container's public access type. This was not a great idea, because it muddied the role of the definition types. These were primarily logical names, but they also ended up containing the default configuration settings to use in these auto-container-generation scenarios.\nThe tenant onboarding process (the process of enabling a new tenant to use an application, or some particular piece of application functionality) necessarily includes these steps:\n* determining the storage account (and relevant credentials) to use\n* picking a suitable container name, ensuring proper tenant isolation\n* creating the container\nIn V2, applications were in control of the first of these. But in most cases, the second and third were handled by `Corvus.Tenancy`. These last two were unhelpfully tied together because of an unfortunate comingling of concerns. This happened due to good but misguided intentions. We were aiming to enable applications to have a single configuration serving multiple logical containers. For certain kinds of storage (e.g., Azure blob storage) it's common for an application to split data across multiple containers (e.g., putting all the user profile details in one container, and, e.g., to-do list entries in another container). In a non-tenanted application you'd expect to configure settings such as account name and credentials just once—it wouldn't normally make sense to have per-container configuration settings because you'd expect to use the same account across all the logical containers. When it came to tenanted storage, the v2 libraries tried to support the same approach by offering convention-based mechanisms to enable multiple container 'definitions' (logical names) to refer to the same underlying configuration. However, this was inextricably linked to letting the storage libraries pick the container name.\nThe problem arose because one of the things V2 tried to do for us was to map from the logical container names in the definition types (e.g. `BlobStorageContainerDefinition.ContainerName`) to an actual container name. To enable isolation of data across tenants even when they shared a storage account, this container name mapping would typically incorporate the tenant ID in the real container name. However, this naming scheme was initially an undocumented implementation detail, preventing applications from anticipating what the container would actually be called. If the application doesn't know the container name, it can't create the container itself prior to first use, and so these tenanted storage providers also automatically created the container too.\nThis is how we ended up with the definition types (e.g. `BlobStorageContainerDefinition`), which were meant to be logical identifiers, needing to include all of the information required to be able to create a new container on demand.\nIt was technically possible for application code to take control of all three of the steps listed above itself in V2, but it was problematic. You could disable the tenanted container name generation, giving you control over the container name, making it possible for the application to know the right container name, but an unfortunate side effect of this was that for each new tenant, you ended up needing to create one configuration for every logical container. (I.e., the decision to take control of container creation unavoidably meant using more complex configuration.) We did make some changes enabling applications to predict the names that would be using, so they could get in ahead, but with hindsight, we ended up regretting ever making the tenanted storage libraries create containers in the first place.\nAnother problem with the automatic create-on-first-use behaviour was that any problems that would prevent creation became visible rather late in the day: you might think you'd successfully onboarded a new tenant, only to discover later that not everything is going to work.\n\n## Decision\n","In `Corvus.Tenancy` v3, applications are responsible for creating any containers they need. The new tenanted storage client libraries will never create a container for you.\nAlso, applications determine the strategy for picking tenant-qualified names to ensure isolation in cases where multiple tenants are sharing a storage account. The tenancy libraries will provide mechanisms that do most of the work, so the main change is that the application has to opt in explicitly.\n","The decision is to **deprecate the functionality that dynamically creates new containers in the `Corvus.Tenancy` v3 libraries.**\n\nThe reasoning behind this decision is that this functionality is not a good fit for the role of the definition types, which are primarily logical names. Additionally, it led to problems where applications had to create one configuration for every logical container for each new tenant, and problems with creating containers could become visible late in the day."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context or Problem Statement\nOur handling of user data is far from bullet-proof. We have stumbled upon cases where SQL injection is possible and the current password encryption is only md5. Additionally, we are building a new mobile app in React and Flask and need to implement a way to handle authentication there.\n## Decision Drivers\n1. Security / Risk for us\n2. Ease of Use\n3. Cost\n\n## Decision\n",1. Security / Risk for us\n2. Ease of Use\n3. Cost\nWe are going for Auth0 since\n- We have prior experience with Auth0 in the team\n- a first test in an afternoon coding session were satisfying\n- Auth0 offers a free plan for Open-source projects\n- Auth0 is one of the product leaders in CIAM (Visit Gartner for detail at [https://www.g2.com/categories/customer-identity-and-access-management-ciam#grid] - Read recent Leadership Compass Identity API Platforms By KuppingerCole [https://static.carahsoft.com/concrete/files/6315/6840/5267/kuppingercole-leadership-compass-identity-api-platforms.pdf])\n- Using Auth0 management API and automatic migration and bulk import it can be integrated to PHP app\n- Auth0 offers full featured authentication and authorization solution and it's capable of customized rules and flows.\n- We are not building our own authentication solution to reduce the security risks coming with handling of passwords.\n,"**Decision:** Implement a modern authentication and authorization framework that addresses the identified security concerns and supports the new mobile app development in React and Flask.\n\n**Reasons:**\n\n* **Security:** The framework will address the SQL injection vulnerabilities and strengthen password encryption.\n* **Ease of Use:** The framework will provide a consistent and user-friendly authentication experience across the existing and new mobile app.\n* **Cost:** While implementing a new framework may require initial investment, the long-term benefits of improved security and reduced risk outweigh the costs."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVisual Studio 2017 added support for packaging applications using Docker and running them using Docker Compose.\n\n## Decision\n,We will use Docker for packaging and running the application in a Linux container.\n,Implement Docker Compose support for Visual Studio 2019.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe application needs to be deployed somewhere.\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https://devcenter.heroku.com/articles/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\nHeroku provides a free hosted PostgreSQL option. It will handle setting a ""DATABASE_URL"" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\nWe want the setup process to be as simple as possible.\n\n## Decision\n","We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\n",Use Heroku for application deployment.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the sources for links that are eventually saved to Pocket are email newsletters. However, a user often does not want to put all\narticles from a newsletter in to Pocket. So, newsletters waiting to be reviewed can clutter an email inbox.\nA user could be provided with an email address that they could send emails to. Links would then be extracted from a received email\nand be added to a backlog. A webpage would need to be provided for users to view this backlog, which could be part of the existing\nchrome extension.\nHandling inbound email would require the use of a transactional email service, such as MailGun, that can POST to an API endpoint\nwhen an email is received.\nThe recipient email address, containing a unique identifier associated with a particular user, would be used to determine what list to add\narticles to.\nA protected account ID in the email address is likely to be too long for users to want to use.\nUsing an unprotected account ID in the email address for a user would expose the plaintext and cipher, which could make it easier for an\nattacker to determine the private key material.\nTo prevent both of the above issues, a new UUID would need to be generated for use in the email address for a user.\nEmails would consist of a variety of formats, which may cause issues in identifying links.\nSome links would not be relevant and would need to be removed (i.e. unsubscribe links).\nSome emails contain a link to a webpage containing the same content, in case the user's email client is not able to correctly render the\nemail. This link could be added to Pocket, rather than the links within the email. However, there is not a common format to identifying\nthis email address.\nOther services, such as If This Then That, allow triggers to be setup where links can be added to Pocket when an email is received.\nThe suggested backlog approach may be viewed as unnecessary by users. It may be easier for them to add to Pocket and filter while\nbrowsing through their list of articles. Linked with the difficulties in parsing emails, this feature could be difficult to implement\nand be of little actual use.\n\n## Decision\n",We will not provide users with an email address they can send emails containing articles to.\n,**Decision:** Do not implement a feature to automatically add links from emails to a Pocket backlog.\n\n**Reasoning:**\n\n* The benefits of such a feature are uncertain. Users may not find it useful and may prefer to manually add links to Pocket.\n* Implementing the feature would require significant effort due to the complexity of parsing emails and identifying relevant links.\n* The feature could introduce security risks if user account IDs are exposed in email addresses.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA Controller is responsible for receiving a request, executing it and returning an appropriate response.\nA service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.\n\n## Decision\n",We will use a service layer to ensure that Controllers do not contain business logic.\n,Use a service layer to decouple the Controller from the implementation details of the operation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo access and modify a user's articles stored in Pocket, they have to give us permission. Once permission has been granted, we have to be careful not to leak this privillaged access to malicious parties.\nASP.NET Core has [Data Protection APIs](https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/introduction) for managing the encryption and decryption of data. These APIs requires the private key data to be persisted in a location where it will not be destroyed when the application is restarted.\n\n## Decision\n","We will use the encrypted ID of a PocketAccount entity as an access token, representing the right to access a particular user's articles.\nWe will encrypt the IDs of Pocket articles returned in API responses, using the ASP.NET Core Data Protection APIs.\nWe will store the private key material in the database.\n","**Decision:** Use the ASP.NET Core Data Protection APIs to protect the user's Pocket access token.\n\n**Rationale:**\n\n* **Security:** The Data Protection APIs provide strong encryption and decryption capabilities, making them suitable for protecting sensitive data such as access tokens.\n* **Persistence:** The APIs allow for the private key data to be persisted in a secure location that will not be destroyed upon application restart.\n* **Convenience:** The APIs are provided by the ASP.NET Core framework, making them easy to integrate into the application.\n\n**Consequences:**\n\n* **Increased security:** The user's Pocket access token will be protected from unauthorized access, reducing the risk of data breaches and account takeovers.\n* **Simple implementation:** Integrating the Data Protection APIs into the application will be straightforward, saving time and development effort."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith the introduction of .NET Core, we need to decide whether to use ASP.NET with .NET v4.x or ASP.NET Core.\n\n## Decision\n",We will use ASP.NET Core.\n,Use ASP.NET Core.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA particular database technology needs to be chosen for the storage of data.\nHistorically Microsoft's SQL Server has been the default choice for ASP.NET applications. SQL Server could only be run on a machine running Windows until the release of SQL Server 2017.\nPostgreSQL is a popular choice for use with other web frameworks (i.e. Rails) and is widely used on a range of platforms, including Linux, MacOS and Windows.\nPostgreSQL is open source and free to use for commercial use. SQL Server has a free version for development purposes but require the purchase of a license for commercial use.\n\n## Decision\n",We will use PostgreSQL for the storage of data.\n,"Use PostgreSQL as the database technology for our application. It is open source and free to use for commercial use, and it runs on a variety of platforms."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen releasing some features, we might want to only make them available to a sub set of users initially, to gain feedback and reduce the potential impact of bugs.\nWe also want to be able to continue development of a feature in master over a longer period of time, without it being available in an unfinished state.\n\n## Decision\n",We will associate a set of feature toggles with a Pocket Account.\n,"Use feature flags. Feature flags are a way to enable or disable a feature based on a predefined criteria. They can be used to control the availability of a feature to a specific subset of users, or to enable or disable a feature based on other criteria."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want a mechanism that allows for the tests to be run whenever changes are made to the project.\nWe also want the project to be deployed whenever these tests pass.\nCircleCI allows for builds to run in a Docker container based on a specified ""root"" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests.\nCircleCI provides open source projects with four free linux containers for running builds.\nAll of the configuration for a project built with CircleCI is stored in the project, except for environment variables.\nMicrosoft provides the ""aspnetcore-build"" Docker image, which contains all the dependencies required to build an ASP.NET Core application.\n\n## Decision\n",We will use CircleCI to handle continuous integration builds and deployments.\n,"Use CircleCI along with its Docker integration feature to create a workflow for the project that executes whenever changes are made to the project.\n\nThe workflow should consist of the following steps:\n1. Create a Docker image that contains all the dependencies required to run the tests.\n2. Run the tests in the Docker image.\n3. If the tests pass, deploy the project."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:**\n\nUtilize an Architectural Decision Record (ADR) template to document architectural decisions in a structured and consistent manner.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany ASP.NET applications use [Entity Framework (EF)](https://docs.microsoft.com/en-us/ef/), an Object Relational Mapper (ORM) that helps access data stored in database.\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\nDapper is a library that is commonly referred to as a ""micro-ORM"". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https://samsaffron.com/archive/2011/03/30/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\n\n## Decision\n",We will use Dapper with the [repository pattern](http://blog.mantziaris.eu/blog/2016/10/24/the-repository-and-unit-of-work-pattern/) to access data stored in the database.\n,"**Decision**\n\nDapper is the preferred ORM for new ASP.NET projects.\n\n**Rationale**\n\nDapper offers several advantages over EF for ASP.NET applications:\n\n* **Performance:** Dapper is significantly faster than EF, especially for complex queries.\n* **Simplicity:** Dapper is a lightweight library with a straightforward API, making it easier to learn and use.\n* **Flexibility:** Dapper allows you to write raw SQL queries, giving you more control over the database operations.\n* **No tracking:** Dapper does not track changes to entities, reducing memory overhead and improving performance.\n\nWhile EF provides more features and automatic functionality, these benefits come at a cost of complexity and performance. For most ASP.NET applications, the benefits of Dapper outweigh the advantages of EF.\n\n**Considerations**\n\n* EF may still be a better choice for projects with complex data models or frequent changes to the database schema.\n* It is possible to use both Dapper and EF in the same project, leveraging the strengths of each ORM for specific tasks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide which front-end web framework to use for the Datalabs application.\nThe choices we evaluated were [React.js](https://reactjs.org/) and\n[Angular](https://angular.io/).\n\n## Decision\n,"We have decided to use the React framework for the following reasons:\n* Preferred the ""batteries not included"" approach of React vs the ""batteries\nincluded"" approach of Angular.\n* Better development tooling available for React.\n* Larger community for React.\n* The team had previous experience with React.\n","Based on the evaluation criteria and the team's preference, we decided to use React.js for the Datalabs application.\n\nReact.js is a popular front-end web framework that offers advantages such as:\n\n* **Declarative programming:** React.js uses a declarative programming approach, making it easier to maintain and update the codebase.\n* **Component-based architecture:** React.js promotes a component-based architecture, which allows for code reuse and easier maintenance.\n* **Virtual DOM:** React.js uses a virtual DOM, which improves performance by reducing the number of actual DOM updates.\n* **Strong community support:** React.js has a large and active community, which provides extensive documentation, tutorials, and support."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to minimise our externally exposed footprint. To achieve this we have\nalready decided that access to cluster servers will be via a Bastion server. We do need\nto have some externally accessible services and need to decide how to achieve this.\n\n## Decision\n,We have decided to have external proxy servers for each environment that expose ports 80\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\n,"**Decision:** Expose services through a reverse proxy.\n\n**Rationale:**\n\n* **Minimized external footprint:** Reverse proxies act as a single point of entry, reducing the number of exposed ports and potential attack surfaces.\n* **Centralized control:** Reverse proxies provide a centralized control point for managing access to services, simplifying security management.\n* **Load balancing and failover:** Reverse proxies can distribute traffic across multiple servers, ensuring high availability and performance.\n* **Header manipulation:** Reverse proxies can modify request headers, enabling additional security features such as request filtering or authentication.\n* **Improved security:** Reverse proxies can add an additional layer of security by implementing rate limiting, intrusion detection, or SSL termination."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to store persistent data in a form that is easy to query and need to select the\nappropriate data store. We consider the choice to be between a relational database or a\ndocument database.\n\n## Decision\n,"We have decided to use [MongoDB](https://www.mongodb.com/) as our database for datalabs\ndata. We felt that the data model for the parts of the system known at this time, a\ndocument structure provided more flexibility and easier integration with our Node.js\napplication.\nMongoDB also provides the ability to run in cluster providing the option for greater\nresilience. For the time being we are opting to run a single node storing data to\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\ndoes not give service resilience or time based backups.\n",**Decision:** Implement a **Document Database** as the persistent data store.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to configure access to the servers in the cluster and need to decide between\npassword authentication and SSH key pair authentication.\n\n## Decision\n,We have decided that all server log on authentication will use SSH key pair\nauthentication. The public keys will be provisioned onto the server at server creation\nfor the default deploy user. A password will be required for sudo.\n,Use SSH key pair authentication for secure access to cluster servers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\nfrom notebook environments. However since this decision a number of other options\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\nschedulers.\nWe are now moving to a pattern of users being able to spin up their own clusters\nwhen required.\n\n## Decision\n","We have decided to collapse the centralised Dask & Spark clusters in favour of writing\ndocumentation/working with users to provision their own clusters using projects such\nas [Dask Labextension](https://github.com/dask/dask-labextension), which is now supported\nwithin the Labs environment.\n","**Decision:** Adopt native Kubernetes scheduling for Dask and Spark clusters.\n\n**Rationale:**\n\n* **Improved resource utilization:** Kubernetes scheduling enables more efficient resource utilization, as it can dynamically allocate and scale resources based on cluster demand.\n* **Simplified cluster management:** With Kubernetes scheduling, users can directly manage their clusters without relying on centralized provisioning. This streamlines cluster creation, deletion, and scaling operations.\n* **Cost optimization:** Kubernetes scheduling can help optimize costs by allowing users to only pay for the resources they use, instead of paying for a fixed amount of capacity with centralized clusters.\n* **Enhanced portability:** Kubernetes is a widely adopted platform that can be deployed on various cloud providers and on-premises environments. By adopting native Kubernetes scheduling, we can ensure portability and consistency of our cluster deployment across different platforms."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have reached a point where we need to secure third party web applications that provide\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\nsource version and we haven't had and response from our efforts to contact them.\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\nwas released provides a Kubernetes native route for the same functionality.\n\n## Decision\n,We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\nThis should provide all of the same functionality as Kong and in addition should provide\na mechanism for token and cookie authentication using the `auth_url` annotation that\nwraps the underlying Nginx `auth_request` module.\n,Implement the Nginx Ingress controller as a Kubernetes native solution for securing third party web applications without built-in security.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a testing framework to use for testing node.js applications. There are\nmany choices in this space including [Jasmine](https://jasmine.github.io/) and\n[Jest](https://facebook.github.io/jest/).\n\n## Decision\n,We have decided to use Jest as our testing framework as it provides mocking and\nexpectation functions which would have to be provided separately to Jasmine. Additionally\nthe snapshot testing mechanism can be used to simplify certain types of testing and is\nparticularly beneficial to front end unit testing.\n,"**Decision:** Use Jest as the testing framework for node.js applications.\n\n**Rationale:**\n\nJest is a popular testing framework for JavaScript applications, including node.js. It offers several advantages over Jasmine:\n\n* **Out of the box support for async/await:** Jest natively supports testing asynchronous code using async/await, making it easy to write concise and readable tests.\n* **Snapshot testing:** Jest allows for snapshot testing, which compares the output of a test to a predefined snapshot. This is useful for ensuring UI consistency and preventing regressions.\n* **Built-in test coverage:** Jest has a built-in feature for generating test coverage reports, which helps identify areas that need more testing.\n* **Fast and efficient:** Jest is known for its speed and performance, which can significantly reduce the time spent on testing.\n* **Wide ecosystem and community support:** Jest has a large ecosystem of plugins and tools, making it easy to integrate with other testing frameworks and tools.\n\nBased on these advantages, we believe that Jest is the better choice as the testing framework for our node.js applications. It aligns well with our need for a comprehensive, async-friendly, and efficient testing framework."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\nenvironments as currently we would have to manually update them in order to apply them to\ndifferent environments. The options available are to either use\n[Helm](https://github.com/kubernetes/helm) or to build a custom tool.\n\n## Decision\n,We have decided to build a custom tool called Bara to deploy our templates. This will use\nthe simple mustache rendering engine to allow template YAML files to be rendered and then\napplied using the command line `kubectl` tool.\nThis approach seemed easier than learning and deploying Helm and building Helm charts for\neach independent component given our current requirements are very simple and the tool\nwill only take a few hours to write.\n,"Use Helm, and implement custom tooling that runs on top of Helm to provide a more user-friendly experience and additional functionality."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the aims of the Datalabs project is to provide the climate science community easy\naccess to the compute capabilities of JASMIN. In order to do this we need to find a way\nto distribute computationally expensive calculations across a cluster and need to explore\nthe available options for doing this.\n\n## Decision\n,"We have decided to use [Apache Spark](https://spark.apache.org/) as our first option for\ndistributed compute. It has established itself as the successor to Hadoop and provides\nstrong integration with interactive notebook technologies such as Jupyter and Zeppelin.\nSpark also provides support for multiple languages including Scala, Java, Python and R.\nThis makes is a flexible platform that should appeal to many users.\nAdditionally, Spark provides Streaming and Machine Learning capabilities which may be of\ninterest later in the project.\n",Use the JASMIN Dynamic Queue Manager (DQM) with SLURM workload manager to distribute computationally expensive calculations across the JASMIN cluster.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe originally provisioned a discourse instance alongside DataLab as a user forum, however\nin practise we have found that it is not used as discussion takes place either in\nperson or on Slack, and we can use the documentation page where required.\n\n## Decision\n",Discourse will be removed from the stack.\n,**Decision:** Decommission the discourse instance and use the documentation page as the primary source of information.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a language for our web services. We restricted our options to Java,\nNode.js and Go.\n\n## Decision\n","We have decided to use [Node.js](https://nodejs.org/en/) for our web services. We felt that the existing team\nexperience, combined with the flexibility provided by a dynamic language made this the\nright choice.\nJava would have provided static typing and object orientation but we opted against this\nas it felt a little heavy weight.\nGo would have provided a lighter weight modern statically typed option but given neither\nthe team nor Tessella had existing experience we viewed it as too great a risk for this\nproject.\n",We decided to use Node.js for our web services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDataLabs makes extensive use of reverse proxying, to give users access to resources (such as Minio or JupyterLabs).  These resources need individually from an external URL to an internal service URL.  There are four design options for reverse proxying (<http://sawers.com/blog/reverse-proxying-with-nginx/>):\n1. Subdomain - this allows the external path and internal path to be the same, probably with a default root base path (/).  Different services are identified by the external URL's hostname.  This has some disadvantages - multiple hostnames require a wildcard certificate, or multiple certificates if a wildcard certificate can not be acquired; and it makes the development environment more difficult, because you can not just use localhost.\n2. Port - this also allows the external path and internal path to be the same, probably with a default root base path (/).  Different services are identified by the external URL's port.  This has the disadvantage that some organisational firewalls restrict http traffic to unusual hosts.\n3. Symmetric Path - this allows the external path and internal path to be the same, but with that path configured.  Different services are identified by the path.  This is the best option, but the internal service must allow the path to be configurable.\n4. Asymmetric Path - here the external and internal paths are different.  Different services are identifiable by the external path.  This requires a search-and-replace of the path on the rendered HTML and JavaScript, so unless these are simple, then this is too fragile.\nHistorically DataLabs has used Subdomain proxying.\n\n## Decision\n","Where possible, Symmetric Path or Asymmetric Path proxying should be used.  If this is not possible, a ConfigMap option should determine whether the remaining proxying strategy should be Subdomain or Port proxying.\n","Option 3: Symmetric Path is the best option, as it allows the external path and internal path to be the same, but with that path configured. Different services are identified by the path. This is the most flexible and configurable option, and it does not require any changes to the internal service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContainers running on Kubernetes only provide ephemeral storage. We need to provide\npersistent storage that allows volumes to be mounted into multiple containers. This\nrestricts us to [Kubernetes Volume Types](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) that support ""Read Write Many"". Specifically, we are selecting\nbetween [NFS](https://help.ubuntu.com/lts/serverguide/network-file-system.html),\n[GlusterFS](https://www.gluster.org/) and [Rook](https://rook.io/).\n\n## Decision\n","We have decided to use GlusterFS to provide distributed persistent storage.\nWe have opted not to use Rook as it feels that it isn't yet ready for production usage. Also, while it would be easy to deploy hyper-converged we would need a second Kubernetes\ncluster to run isolated storage as we require.\nWe feel that simple NFS storage isn't sufficient as it won't give any data resilience.\nGiven we have no backups, the data replication will give us limited disaster recover\ncapability.\n",Rook
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a growing number of Docker containers and it is useful to have at a glance\ninformation available about them. [MicroBadger](https://microbadger.com/) provides\na way to inspect and visualise Docker containers.\n\n## Decision\n,We have decided to use MicroBadger for new containers and will update existing containers\nas we make updates to them.\n,#### Decision\nUse MicroBadger to inspect and visualise Docker containers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe JASMIN cloud portal and vCloud director portals give manual options for provisioning\nservers into the JASMIN tennancy. This brings significant effort to rebuild a cluster\nas all servers would need to be manually deleted and recreated. The new OpenStack\ninfrastructure is supposed to be available in a matter of months and we need to decide\nwhether we want to invest effort in automating server creation.\nIf we do decide to automate we need to decide which technology to use between Ansible\nand Terraform. Terraform is known to work with vCloud director but it would be preferable\nto use the same tool for server creation as software provisioning.\n\n## Decision\n,We have decided to use Ansible as our server provisioning tool as there are significant\nbenefits to keeping a single tool for all provisioning steps.\n,"Automate server creation using Ansible, as it is the preferred tool for both server creation and software provisioning."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to run multiple instances of the Datalabs system to allow us to continue to\ndevelop while giving early adopters access to the system. We intend to run both a test\nand production environment and need to decide whether to do this as a completely separate\nKubernetes cluster or to isolate the environments using Kubernetes namespaces.\n\n## Decision\n,We have decided to run both environments on the same Kubernetes cluster but with a\nseparate reverse proxy to allow testing of the proxy configuration. This decision was\ntaken to avoid the maintenance overhead of having two clusters.\n,Use Kubernetes namespaces to isolate the test and production environments.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe core [Material UI React component library](https://material-ui.com/) is used for much of\nthe DataLabs UI. Before new components make it into the core library, they pass through\n[""The Lab""](https://material-ui.com/components/about-the-lab/). This provides a platform\nfor the new components to be tested in real world applications.\nAt the time of writing, the [Autocomplete](https://material-ui.com/components/autocomplete/)\ncomponent would be beneficial to use but it is currently in the lab. Therefore, a decision\nneeds to be made about whether we are happy to use components from the lab. The\nkey considerations are as follows.\n### Pros\n* Gives us access to components that will reduce development effort required to\nimplement functionality.\n* Gives components that have a simpler interface to the components they would be replacing\n(e.g. [Downshift](https://github.com/downshift-js/downshift) in the case of Autocomplete)\nmaking them easier to maintain and adapt.\n### Cons\n* Components in the lab are free to have breaking changes on a more regular basis\ncompared to the ones in the core library.\n* The components being used might not migrate from the lab into the core library.\n\n## Decision\n",We will use the Material UI Lab component library. The ease to develop and maintain the\nuser interface using the components in the lab outweighs the downsides. We don't have to\nupdate the lab package if a new version removes a component that is currently in use\nor introduces breaking changes into the component.\n,"**Decision:** Use components from the Material UI React Lab with caution.\n\n**Rationale:**\n\nWhile components in the Material UI React Lab offer potential benefits such as reduced development effort and simplified interfaces, there are risks associated with using them. Lab components are subject to more frequent breaking changes and may not be guaranteed to migrate to the core library.\n\nTo mitigate these risks, the following guidelines should be followed:\n\n* **Carefully evaluate the stability of components before using them.** Review the component's documentation and track its progress in the Lab. Consider using components that have been stable for a reasonable period of time.\n* **Use version pinning to minimize the impact of breaking changes.**\n* **Be prepared to handle breaking changes.** Monitor the Lab for updates and make necessary adjustments to the application as they arise.\n* **Consider using an abstraction layer.** This will allow for easier swapping of components if they migrate to the core library or need to be replaced due to breaking changes.\n\nBy following these guidelines, it is possible to leverage the benefits of Lab components while minimizing the potential risks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt will be necessary to access the servers that form the Datalabs network but we do not\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\n\n## Decision\n,"We have decided that all access to the cluster will be via a Bastion server over an SSH\nconnection on port 22. We will restrict access through the firewall to known IP address\nranges including the development workstations, the STFC VPN and the Tessella public IP\naddress.\nThis excludes public facing services that should be available over HTTPS on port 443 via\na different route.\n",Establish a private network link between the JASMIN tenancy and the Datalabs tenancy.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe NOC use case requires access to a large (~1TB) NetCDF dataset currently stored on the\nARCHER system. The current usage requires data to be extracted using shell scripts and\nthis process takes a long time. We need to identify a better way to access this\ndataset to allow the Datalabs environment to make best use of it.\n\n## Decision\n,"We have decided to use a [Thredds](http://www.unidata.ucar.edu/software/thredds/current/tds/)\nserver to present a unified view of the dataset as it should provide significant\nperformance improvements over manual scripting.\nIn order to achieve this, we need the data to be moved to a JASMIN Group Workspace (GWS)\nto allow us to provision a Thredds server in the JASMIN managed cloud.\n","Implement a Datalabs library that uses the NetCDF4 library to access the NetCDF dataset on ARCHER. This library should provide a Pythonic interface to the dataset, making it easy to use in Datalabs notebooks. The library should also be able to handle large datasets efficiently."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, creating Dask clusters is not ideal:\n* The service has create-pod permission which is unwise\n* Different projects can currently use each other's clusters, which is poor resource management and could allow algorithm inspection from the Desk scheduler plan\n* There is sometimes a requirement to give Dask clusters storage access, to share data or Conda environments.  This is not self-service, and potentially exposes project data since the Dask clusters are not secured by project.\n* Dask Gateway was investigated, but does not fit in our authentication model (since it does not use OIDC), and it still provides no straight-forward route to storage access.\n\n## Decision\n","After prototyping a working solution, we have decided to handle Dask cluster creation within the infrastructure-api service.\nThis will allow us to securely mount volumes and utilise user-defined Conda environments.\n","Use a Kubernetes Service Account specifically annotated for Dask cluster creation, and set up a Kubernetes Role and RoleBinding to limit permissions within the cluster to the minimum required."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are expecting to have to run a large number of containers across several servers\nand in different environments. Given this, we think that we need a Container\nOrchestration tool and are selecting between Kubernetes, Docker Swarm and Mesos.\n\n## Decision\n",We have selected to use Kubernetes as our container orchestration platform. This is due\nto it being the choice that the JASMIN team have made and also that it has established\nitself as a clear industry favourite.\n,"**Decision:** Adopt Kubernetes as the container orchestration tool.\n\n**Justification:**\n\nBased on a comprehensive evaluation of the available options, Kubernetes emerged as the most suitable solution for our requirements:\n\n* **Scalability:** Kubernetes can effectively handle a large number of containers across multiple servers, meeting our need for scalability.\n* **Flexibility:** Its open-source nature, extensive ecosystem of third-party tools, and support for various cloud platforms provide us with the flexibility to meet our diverse deployment needs.\n* **Maturity:** Kubernetes is a well-established and widely adopted technology, ensuring stability and a large support community.\n* **Features:** Kubernetes offers a comprehensive range of features, including container scheduling, self-healing, load balancing, and storage integration, addressing our functional requirements.\n* **Community Support:** Kubernetes has a robust and active community, providing valuable resources and support for both implementation and maintenance.\n\nConsidering these factors, we believe that Kubernetes is the optimal choice to meet our container orchestration needs."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile Docker containers now provide good root isolation from the host compared to earlier\nversions of Docker there are still security risks. We intend to provide sudo access\nfor users of the Notebook containers and this significantly magnifies the risks.\n\n## Decision\n,We have decided to use [AppArmor](https://wiki.ubuntu.com/AppArmor) to improve our\ncontainer security with the intention to make all policies as restrictive as possible.\nTo make it easier to build AppArmor profiles we have also decided to use\n[Bane](https://github.com/jessfraz/bane).\nTo further secure containers we have also decided to run all of our custom build\ncontainers as a non root user.\n,Enable user namespaces to provide isolation between users and the host.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to dynamically orchestrate the containers running in the Datalab environment we\nneed to interact with the Kubernetes API. There are several choices for this:\n* Use one of the officially [supported clients](https://kubernetes.io/docs/reference/client-libraries/#officially-supported-kubernetes-client-libraries).\n* Use one of the Node.js community clients\n* Directly interact with the Kubernetes REST API.\n\n## Decision\n,We have decided to directly interact with the Kubernetes REST API as this presented the\neasiest option for development.\nWe ruled out using a supported client as we didn't want to have to write a service in a\nlanguage we were not familiar with.\nWe trialled all of the Node.js community clients but didn't feel that they were complete\nenough to meet our needs and were poorly documented.\n,Use the [official client library for Kubernetes](https://github.com/kubernetes-client/javascript) for Node.js.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are using a mono-repository structure for the Datalabs project, this permits\nsharing of components across independent services. In NodeJS sharing of code\nwithout duplication requires linking of modules to the dependant service.\nThere are a few libraries to manages code-sharing:\n* NPM Link\n* Yarn Workspaces\n* Learna\n\n## Decision\n","We have decided to use Yarn Workspaces as is very lightweight and offers\nmanagement of the links required for module share, including auto discovery of\nother services.\nWe have ruled out using straight NPM Links as they are difficult to set-up and\nshare between development team. We have used Learna on other project and found\nit to be very heavyweight and requires that is adds git commits to releasing new\nversions.\n","Use Yarn Workspaces to manage code-sharing within a mono-repository structure.\n\n**Reasoning:**\n\n* **Yarn Workspaces is a simple and lightweight solution.** It does not require the installation of any additional packages or plugins.\n* **Yarn Workspaces is tightly integrated with the Yarn package manager.** This makes it easy to manage dependencies and versions across all workspaces in the mono-repository.\n* **Yarn Workspaces supports multiple workspaces.** This allows us to have multiple independent services in the same mono-repository.\n* **Yarn Workspaces is a widely adopted solution.** It is used by many large organizations, including Google and Facebook.\n\n**Additional Considerations:**\n\n* **NPM Link** is a simple and straightforward solution, but it can be difficult to manage dependencies and versions across multiple workspaces.\n* **Learna** is a more complex solution that offers more features than Yarn Workspaces. However, it can be more difficult to configure and use.\n\nBased on these factors, we believe that Yarn Workspaces is the best solution for managing code-sharing within a mono-repository structure for the Datalabs project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUser Authentication is a complex problem, can be time consuming to implement and errors\nin implementation can lead to security vulnerabilities. We feel that authentication,\nwhile critical, is not a differentiating factor and want to offload the work to a\nmanaged service.\n\n## Decision\n",We have opted to use [Auth0](https://auth0.com/) as our Identify provider. This gives us\na quick way to integrate authentication into our application with minimal effort and as\nan open source project we are able to use the service free of charge.\n,We will use Amazon Cognito for user authentication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR 34](0034-custom-k8s-deployment-tool.md), it was decided that a custom tool would be created instead of using [Helm](https://helm.sh/).\nSince this decision was made, Helm has matured and established itself as _the_ way to share and install applications that run in Kubernetes.\nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.\nWith this in mind, the decision to write and use the custom tool, Bara, needs to be reconsidered.\n\n## Decision\n","We have decided that we should use Helm rather than continuing to use Bara.\nThis will aid with the portability of the system as Helm is the established way of installing applications into Kubernetes meaning it should be familiar to others installing DataLabs.\nAs Helm is well established, cloud providers tend to have support for installing via Helm, simplifying potential deployments to the cloud.\nHelm also provides mechanisms that allow for the installation of complicated applications into Kubernetes, therefore should  provide DataLabs with plenty of room to grow.\n","**Re-adopt Helm as the preferred method of deploying Data Labs:** \n\n- Helm should be used for both local development and production installations\n- The custom tool, Bara, should be retired and replaced with Helm tooling\n- Migration from Bara to Helm should be a high priority"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to dynamically provide secure Notebook containers it will be necessary to\ndynamically generate and securely store secrets. We want to isolate this from the\ndatabase and use a dedicated solution for this problem.\n\n## Decision\n,We have decided to use [Hashicorp Vault](https://www.vaultproject.io/) to store secrets.\nIt provides a dedicated system to securely store and manage access to secrets.\n,Use a dedicated secrets management system.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently we have an Nginx Ingress controller deployed in the same namespace\nas the Datalabs application. This means that when multiple instances of\nDatalabs are deployed to a single cluster (such as prod and test), ingress\ncontrollers are deployed to each of these namespaces.\nAs we are in the process of implementing multiple project functionality within\nDatlabs, there is now a need for an ingress controller than can fulfill ingress\nrules across all namespaces. This also will move control of ingress to be a\nplatform service and not part of the deployment of the application itself,\nwhich will aid to decouple Datalabs into being more of a standalone\napplication.\n\n## Decision\n",We have decided to deploy a single Nginx Ingress Controller into the\nkube-system namespace that will handle the ingress rules for the entire\ncluster.\n,"Deploy a shared Nginx Ingress controller that is not deployed in the same namespace as the Datalabs application. This will allow a single ingress controller to fulfill ingress rules across all namespaces, thereby decoupling Datalabs from the ingress controller deployment and moving control of ingress to a platform service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a front end component framework to style the web application. The\nchoices available are [MaterialUI](https://material-ui-next.com/),\n[SemanticUI](https://react.semantic-ui.com/introduction) and\n[ReactBootstrap](https://react-bootstrap.github.io/).\n\n## Decision\n","We have decided to use MaterialUI for the front end component library. Additionally, we\nhave chosen to use the vNext version that is still in alpha as it provides a\nsignificantly different API and we expect it to be released prior to the end of the\nproject.\n","**Decision:** Implement MaterialUI as the front end component framework.\n\n**Rationale:**\n\nMaterialUI is a popular and well-maintained framework that provides a set of high-quality, customizable components. It is based on Google's Material Design principles, which offer a consistent and user-friendly design system.\n\n**Advantages of MaterialUI:**\n\n* **Widely used and supported:** MaterialUI has a large community and extensive documentation, making it easy to find support and resources.\n* **Material Design compatibility:** As it is based on Material Design, it provides a consistent design experience that aligns well with Google's products and services.\n* **Customization options:** MaterialUI offers a high degree of customization, allowing developers to tailor the components to specific branding and design requirements.\n* **Regular updates and new features:** MaterialUI is actively developed and receives frequent updates with new features and improvements.\n\n**Considerations:**\n\n* **Learning curve:** MaterialUI may have a steeper learning curve than other frameworks for developers who are new to Material Design.\n* **File size:** MaterialUI's bundle size can be larger than some other frameworks, which may need to be optimized for performance on mobile devices.\n\n**Alternatives considered and rejected:**\n\n* **SemanticUI:** SemanticUI is also a well-known framework, but it does not have as strong of a focus on Material Design as MaterialUI.\n* **ReactBootstrap:** ReactBootstrap is a good choice for applications that require a traditional Bootstrap-like design, but it may not be as suitable for modern, Material Design-based applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to use Ansible to provision the servers there needs to be a user with sudo\naccess. The default administrator user configured onto the base VM is not configured\ncorrectly and we are unable to use SSH keypairs with this user. After investigation\nwe are not clear what the issue is with the administrator user and need alternative\noption.\n\n## Decision\n,We have decided to remove the password authentication from the administrator user having\nfirst provisioned a new deploy user with ssh keys for the team in the authorized keys.\nThis user will require a password for sudo which will be stored in the ansible vault to\nallow automated provisioning.\n,Create a new user with sudo access and use that user for Ansible provisioning.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether to run our storage cluster as a standalone cluster or\nhyper-converged by running pods on the Kubernetes cluster.\n\n## Decision\n,We have decided to run a standalone storage cluster. The reason for using a separate\ncluster is that by keeping the persistent data separate we keep flexibility over the\nKubernetes cluster and can drop and recreate it without having to worry about the data.\n,Run the storage cluster as a standalone cluster.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a base operating system to install on all virtual machines that form\nthe Datalabs environment. There are three choices available through the JASIMN portal\nUbuntu 14.04, Ubuntu 16.04 and CentOS 6.9.\n\n## Decision\n",We have selected Ubuntu 16.04 as the base operating system for our servers for several\nreasons:\n* The team are more familiar with Ubuntu over CentOS.\n* Packages are likely to be more easily available on Ubuntu.\n* CentOS 6.9 is no longer being updated (last update 10/5/2017).\n* Ubuntu 16.04 will be supported for far longer. 14.04 end of life is early 2019.\n,"The decision is to use Ubuntu 16.04 as the base operating system for all virtual machines that form the Datalabs environment.\n\n**Reasons for the Decision:**\n\n* Ubuntu 16.04 is the latest LTS (Long Term Support) release of Ubuntu, which provides a stable and secure base for the Datalabs environment.\n* Ubuntu 16.04 is a widely used and well-supported operating system, with a large community of users and developers. This means that there is a wealth of documentation and resources available to help with troubleshooting and maintenance.\n* Ubuntu 16.04 includes a number of features that are well-suited for the Datalabs environment, such as:\n    * A modern kernel and package manager.\n    * A user-friendly desktop environment.\n    * A good selection of pre-installed software packages.\n    * Excellent support for virtualization."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether to configure our web application code base ourselves or make\nuse of the [Create React App](https://github.com/facebookincubator/create-react-app)\nto provide project configuration.\n\n## Decision\n,We have decided to use the Create React App project to provide the base for our React\nproject as this brings best practice configuration and build and is actively being\ndeveloped.\n,"**Decision:** Use Create React App for project configuration.\n\n**Rationale:**\n\n* **Reduced effort:** Create React App provides a pre-configured environment, eliminating the need for manual configuration, saving significant time and effort.\n* **Improved developer experience:** The tool streamlines the setup process, simplifying onboarding for new developers and reducing setup-related issues.\n* **Consistency:** Using Create React App ensures that all developers work with a standard configuration, reducing variations and potential conflicts.\n* **Community support:** Create React App is widely adopted and maintained by a large community, providing access to extensive documentation and support resources.\n* **Compatibility with popular tools:** The tool integrates seamlessly with other popular development tools such as Babel and Webpack, simplifying the integration of additional features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is demand from the users to be able to ""share"" their Notebooks such that other users\nare be able to open the Notebook, change some values and then re-run cells within the\nNotebook.\nThere is also the requirement to be able to ""share"" the results of a notebook in a way\nthat involves less interaction from the viewer. This requirement is fulfilled by Sites,\nbut there needs to be a way to configure who is able to see the Site.\nThere is the need to define whether these are two truly separate concepts and how\n""sharing"" should be implemented in Datalabs.\n\n## Decision\n","We have decided to split these into two different concepts: `sharing` and `visibility`.\n`Sharing` is the means through which a user will allow others access to make changes to\nthe item that is being shared.\n`Visibility` is the means through which a user will allow others to view the output of\ntheir work.\nIt is possible for an item to have both a `sharing` and `visibility` status. For example,\na Site might have a `visibility` status that means anyone is allowed to look at the\ncontent and it might also have a `sharing` status that anyone in the Project the Site\nbelongs to can edit the configuration of the Site.\n","Datalabs will implement two types of sharing:\n\n* **Notebook sharing:** This will allow a user to share a Notebook with other users, allowing them to open the Notebook, change some values and then re-run cells within the Notebook.\n* **Site sharing:** This will allow a user to share the results of a notebook in a way that involves less interaction from the viewer. Sites will be configured to control who is able to see the Site.\n\nThese two types of sharing are separate concepts and will be implemented as follows:\n\n* **Notebook sharing:** Notebook sharing will be implemented using a permission-based system. Users will be able to grant other users permission to view, edit, or run their Notebooks.\n* **Site sharing:** Site sharing will be implemented using a URL-based system. Users will be able to create a URL for a Site and share it with other users. Anyone with the URL will be able to view the Site, but only the user who created the Site will be able to edit it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBuilding a complex web application brings challenges around how to manage state. The\nRedux website provides an excellent [motivation page](http://redux.js.org/docs/introduction/Motivation.html) that discusses the challenges in detail.\n\n## Decision\n,"We have decided to adopt the [Redux](http://redux.js.org/) architecture to provide a clean separation between\nour views, actions and state store.\n","**Adopt Redux for state management.**\n\n**Rationale:**\n\n* Redux provides a predictable, unidirectional data flow model that makes state management easier to reason about and maintain.\n* It promotes a clear separation of concerns between state and UI logic, improving the testability and maintainability of the application.\n* Its middleware capabilities allow for the handling of asynchronous actions and side effects, making it a versatile solution for complex web applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Datalabs project has challenging vision for dynamic creation of scientific analysis\nenvironments. We need to decide what technology will enable us to meet this vision.\n\n## Decision\n,We do not think that regular VM orchestration will meet the vision and have instead\nopted to deploy all services (where possible) as Docker Containers. Alternative container\ntechnologies were not evaluated as the team has previous Docker experience and it is the\nclear leader in this space.\n,Use Docker and Kubernetes to create and manage scientific analysis environments.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide between [Kube Prometheus](https://github.com/coreos/kube-prometheus)\nand [Prometheus Operator](https://github.com/helm/charts/tree/master/stable/prometheus-operator)\nfor use in deploying the prometheus monitoring solution for DataLabs.\n\n## Decision\n,We have decided to use the [Prometheus Operator Helm Chart](https://github.com/helm/charts/tree/master/stable/prometheus-operator)\nas it gives us the option to use an Helm chart over kubectl used in the kube-prometheus\noption. The Prometheus Operator Helm chart provides a similar feature set to the\nkube-prometheus option.\n,"**Decision:** Use the Prometheus Operator.\n\n**Rationale:**\n\n* **Ease of Management:** The Prometheus Operator provides a more user-friendly interface and more extensive support for managing Prometheus deployments. It allows for easy upgrades, configuration management, and monitoring of Prometheus components.\n* **Expanded Functionality:** The Prometheus Operator offers additional features not available in Kube Prometheus, such as automatic service discovery, secure communication, and support for multiple Prometheus deployments.\n* **Maturity and Adoption:** The Prometheus Operator is a well-established and widely adopted solution, with a large community and a proven track record in production environments.\n* **Integration with Helm:** The Prometheus Operator integrates seamlessly with Helm, a popular package manager for Kubernetes. This simplifies deployment and management, enabling the use of Helm charts for consistent and reproducible configurations.\n* **Support for Custom Resources:** The Prometheus Operator allows for the creation of custom resource definitions (CRDs), providing flexibility in customizing and extending Prometheus's functionality."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a proxy server to proxy requests to services running in the private\nKubernetes cluster.\n\n## Decision\n,"We have decided to use [RedBird](https://github.com/OptimalBits/redbird) as a reverse\nproxy. This was selected as it is written in Node.js which we expect our applications\nto be written in and will allow us to extend to support authentication, logging and other\nedge concerns.\n","**Decision:**\n\nImplement and use Envoy Proxy as the proxy server.\n\n**Rationale:**\n\n* Envoy Proxy is a high-performance, open-source proxy built by Lyft.\n* Envoy Proxy supports a wide range of features, including:\n    * Load balancing\n    * TLS termination\n    * Rate limiting\n    * Circuit breaking\n* Envoy Proxy is highly extensible, allowing for the addition of custom filters and protocols.\n* Envoy Proxy is well-supported, with a large community of users and contributors."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose a rendering library to allow testing of React components.\n\n## Decision\n,We have chosen to use [Enzyme](https://github.com/airbnb/enzyme) to provide a library\nfor shallow rendering.\n,Use React Testing Library (RTL).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving selected to run all services and applications in Docker containers we need a\nregistry to store them in.\n\n## Decision\n,We have decided to store the Docker containers in [DockerHub](https://hub.docker.com/u/nerc/dashboard/). Given the project is Open Source this seemed to be the easiest option\nas most tools default to this registry.\n,Use [Docker Hub](https://hub.docker.com/) as a registry to store Docker images.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKubernetes does not provide an overlay network out of the box and we need to choose\nwhich one to use from [here](https://kubernetes.io/docs/concepts/cluster-administration/networking/).\n\n## Decision\n,We have decided to use the [WeaveNet](https://www.weave.works/oss/net/) network as this\nhas already been used by the JASMIN team. It also appears easy to use and there is good\ndocumentation.\n,"We will use [Flannel](https://github.com/coreos/flannel) as our overlay network because:\n\n* It is a lightweight and easy-to-use overlay network that is well-suited for Kubernetes.\n* It supports a variety of network backends, including VXLAN, UDP, and host-gw.\n* It has a large community of users and contributors.\n* It has a simple configuration that can be deployed with a single command."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\nDashboard. We have tested using an Nginx proxy and have found that this provides the\nsupport required but does not provide an API for configuration.\n\n## Decision\n,We have decided to use [Kong](https://getkong.org/) for our reverse proxy as it is a\ncustom build of Nginx that provides an API. We have tested with all of our services and\nit appears to meet our needs.\n,**Decision:** Implement an Nginx proxy with a custom API for configuration.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Redbird proxy does not support WebSockets which are required to support the\ninteractive notebooks. We have also had problems with the reliability\nof the proxy and have found it difficult to configure.\n\n## Decision\n,We have decided to replace the custom Redbird proxy with a [Traefik](https://traefik.io/)\nproxy as this looks easier to configure and claims Web Socket support.\n,"**Decision:** Replace the Redbird proxy with a more reliable and WebSocket-supporting proxy, such as HAProxy or nginx."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a tool to provision servers and software for the datalabs project.\n\n## Decision\n,We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\nteam have experience using it.\n,Use Terraform as the provisioning tool for the datalabs project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, secrets that are dynamically created for notebooks etc. are stored in Hashicorp Vault as decided in [24. Vault for Secret Store](0024-vault-for-secret-store.md).\nVault has been the source of operational challenges such as sealing when it is rescheduled.\nThis has added extra complexity such as requiring a cronjob to periodically check that Vault has not been sealed.\nVault is also non-trivial to initially configure for use; a concern when aiming for portability across different services, especially those that would be self-serve e.g. JASMIN's Cluster as a Service (CaaS).\n\n## Decision\n",It has been decided to move to using [Kubernetes' native secret solution](https://kubernetes.io/docs/concepts/configuration/secret) to make DataLabs simpler to both deploy and maintain.\n,Use the Kubernetes Secrets Store for storing secrets dynamically created for notebooks etc.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFollowing a meeting with the Met Office it is clear that their Python users were seeing\ngreat success using [Dask](https://dask.pydata.org/en/latest/) as their distributed\ncompute environment. Dask appears that it could be easier to use than Spark for users\nwho already know Python and NumPy.\n\n## Decision\n,We have decided to offer Dask in addition to Spark within the Datalabs platform. This\nenables us to appeal to more users at limited cost.\n,"Based on the feedback from the Met Office, Dask will be investigated further as a possible replacement for Spark for Python users. The investigation will focus on the following areas:\n\n* **Ease of use:** Dask should be easier to use than Spark for users who already know Python and NumPy. This is critical for adoption by the Python user community.\n* **Performance:** Dask should be able to provide performance that is comparable to Spark for a variety of workloads. This is essential for meeting the needs of our users.\n* **Scalability:** Dask should be able to scale to large datasets and complex workflows. This is important for supporting the growing data needs of our users.\n\nThe investigation will be conducted by a team of engineers with experience in both Dask and Spark. The team will provide a report on their findings within six weeks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Use an Architectural Decision Record (ADR) template to document and track architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSupporting a new feature, the so called [signal enrichment](https://github.com/eclipse-ditto/ditto/issues/561), raises a few\nquestions towards throughput and scalability impact of that new feature.\nIn the current architecture, Ditto internally publishes events (as part of the applied ""event sourcing"" pattern) for\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\nchanged fields.\nThe ""signal enrichment"" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\nThe following alternatives were considered on how to implement that feature:\n1. Sending along the complete `Thing` state in each event in the cluster\n* upside: ""tell, don't ask"" principle -> would lead to a minimum of required cluster remoting / roundtrips\n* downside: bigger payload sent around\n* downside: a lot of deserialization effort for all event consuming services\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\n* downside: overall a lot of overhead for probably only few consumers\n2. Enriching the data for sessions/connections which selected `extraFields` for each incoming event\n* upside: no additional payload for existing events\n* upside: data is only enriched for sessions/connections really using that feature\n* upside: policy enforcement/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\n* delayed event publishing\n* additional deserialization efforts\n* potentially asking for the same static values each time\n3. Cache based enriching of the data for sessions/connections which selected `extraFields` for each incoming event\n* upsides: all upsides of approach 2 except that policy is always up-to-date\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\n\n## Decision\n","We provide 2 different facade implementations providing approach 2 and 3:\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\n* the overall size of this cache is configured, by default to `20,000` entries\n* there is an additional ""smart-update"" mechanism for cache entries related to enrichment of twin events:\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\n","The following decision was made:\nWe decided to go with **Cache based enriching** of the data for sessions/connections which selected `extraFields` for each incoming event\n* upsides: all upsides of approach 2 except that policy is always up-to-date\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\nThe cache will be implemented in the Event Hub which is anyway responsible for enriching and filtering events. The cache will only be used for ""extraFields` which are not part of the original event payload but have been selected during the initial subscription."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEclipse Ditto project exited the incubation phase with release 1.0.0.\nHenceforth, any change to the Ditto API enters a Ditto release according to [semantic versioning](https://semver.org):\n- Incompatible API changes increment major version number (e.g., 1.7.5 -> 2.0.0);\n- Compatible API changes increment minor version number (e.g., 1.2.3 -> 1.3.0);\n- Changes in the implementation without any API change increment patch version number (e.g., 1.0.0 -> 1.0.1).\nThis document defines what _API compatibility_ means,\nthe modules which are considered API and for which semantic versioning holds,\nand the enforcement of semantic versioning.\n\n## Decision\n","### API compatibility\nFor Eclipse Ditto, API compatibility means _binary compatibility_ defined by\nthe [Java Language Specification, Java SE 8 Edition, chapter 13](https://docs.oracle.com/javase/specs/jls/se8/html/jls-13.html).\nExamples of binary-compatible changes:\n- Adding a top-level interface or class.\n- Making a non-public interface or class public.\n- Adding classes to a class's set of superclasses without introducing circular inheritance.\n- Adding type parameters without changing the signature of existing methods.\n- Renaming type parameters.\n- Deleting private members.\n- Adding enums.\n- Adding abstract methods to interfaces.\n- Adding members to a class that do not collide with any other member in all its subclasses in Ditto.\n- Adding default methods to an interface that do not collide with any other method in all subclasses of the interface\nin Ditto.\nBinary compatibility guarantees that any user code of Ditto does not break on minor version upgrades, provided that\n- the user code does not implement Ditto interfaces, and\n- the user code does not extend Ditto classes.\nInheritance from Ditto classes and interfaces is excluded from API compatibility because Ditto interfaces are often\ndefined to hide implementation details from user code. Compatibility for user-defined subclasses, or source\ncompatibility, is not a part of Ditto's semantic versioning. Inheriting user classes may break after a minor Ditto\nversion upgrade.\n### Modules considered API\nPublic classes, interfaces and their public members of the following modules, and their submodules are considered\nDitto API. Changes to them must enter Ditto release in accord with semantic versioning.\nModules not on this list are not considered API; they may contain incompatible changes for any Ditto version change.\n```\nditto-json\nditto-base-model\nditto-messages-model\nditto-jwt-model\nditto-rql-model\nditto-rql-query\nditto-rql-parser\nditto-rql-search-option-parser\nditto-policies-model\nditto-things-model\nditto-thingsearch-model\nditto-connectivity-model\nditto-placeholders\nditto-protocol\nditto-utils-jsr305\n```\n### Javadoc documentation using `@since`\nWhen adding new public visible API (e.g. new interfaces, classes or methods in existing code) in the defined API modules,\na `@since <version>` javadoc annotation shall be added.\nExample:\n```java\n/**\n* Returns the extra information which enriches the actual value of this change.\n*\n* @return the extra data or an empty Optional.\n* @since 1.1.0\n*/\nOptional<JsonObject> getExtra();\n```\nExisting public API without `@since` can be interpreted as `@since 1.0.0` and can be added when adjusting a class.\n### Enforcement of semantic versioning\nSemantic versioning is enforced through binary compatibility check by `japicmp-maven-plugin`.\n```xml\n<plugin>\n<groupId>com.github.siom79.japicmp</groupId>\n<artifactId>japicmp-maven-plugin</artifactId>\n</plugin>\n```\nDeviations of the behavior of `japicmp-maven-plugin` from binary compatibility defined by the Java language\nspecification are to be corrected through overrides. If `japicmp-maven-plugin` breaks the build for a branch,\nthen a major version increment for the next release is required to merge the branch into Ditto master.\nCheck with the whole Ditto team before adding anything to the exclusion list of `japicmp-maven-plugin`.\n","**Decision:**\nThe Ditto API encompasses all public interfaces and message types accessible to external consumers.\nSpecifically, the following modules are considered API:\n- java.ditto.model\n- java.ditto.api\n- java.ditto.protocol\n- java.ditto.web\n- java.ditto.storage\n\nChanges to these modules will follow semantic versioning as defined in the Context."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSonarQube [RSPEC-2142](https://rules.sonarsource.com/java/tag/multi-threading/RSPEC-2142) complains when we catch\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\ninterruption.\n\n## Decision\n",We will ignore [RSPEC-2142](https://rules.sonarsource.com/java/tag/multi-threading/RSPEC-2142).\n,**Decision:** Suppress SonarQube warning RSPEC-2142 for exceptions caught in JUnit tests.\n\n**Rationale:** Calling `Thread.currentThread().interrupt()` in JUnit tests can have unpredictable consequences due to the way JUnit reuses threads. Suppressing the SonarQube warning ensures that the build will not break due to false positives.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this\n[article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) and also use the proposed\n[template](https://github.com/joelparkerhenderson/architecture_decision_record/blob/master/adr_template_by_michael_nygard.md).\n",Use ArchUnit for Architectural Driven Development in Java to ensure that the system's architecture is implemented correctly and consistently.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to allow partial or merge updates of things with a single request.\n\n## Decision\n,A merge request\n* uses HTTP `PATCH` method.\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https://tools.ietf.org/html/rfc7396).\n* has the request header `content-type` set to `application/merge-patch+json`.\n,Use Patch Semantics
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp to now Ditto used Pekko's distributed publish/subscribe in order to emit e.g. `ThingEvent`s to interested other services:\n* gateway\n* websocket/SSE sessions publishing events\n* connectivity\n* AMQP 1.0 / AMQP 0.9.1 / MQTT / Kafka sessions publishing events\n* things-search\n* ThingUpdater updating the search index\nThat naive approach works, but does not provide *horizontal scalability*:\n* each single service instance generally interested in `ThingEvent`s gets all of them, regardless of whether someone is actually interested in them\n* as a result a lot of avoidable JSON deserialization is done\n* when Ditto needs to scale the event publishing horizontally, adding new gateway or connectivity instances will not help scaling the event publishing\n* still all instances will have to process each `ThingEvent` and discard if not relevant\n\n## Decision\n","We will implement a custom Ditto pub/sub which\n* uses ""authorization subjects"" as topics when subscribing\n* uses ""read subjects"" as topics when publishing\n* manages and distributes the active subscriptions via Pekko Distributed Data (ddata)\n* emits `ThingEvent`s only to service instances where at least one consumer consumes the event\n","Use a message broker that allows horizontal scaling of topics (e.g. RabbitMQ, Kafka, Pulsar) as an event distributor, where each service has its own (durable) subscription. Then the registry component can publish `ThingEvent`s to the message broker with topic `thingEvent`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeprecated design:\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\nThe problem:\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\n- However, this would have made the Practitioner resource more complex.\*\n\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\n\n## Decision\n","- We decided to use the existing OpenMRS model for creating attributes for each resource.\n- There is a one-to-many relationship between a resource and its attributes.\n- Each attribute has an attribute type, which is stored in the attribute_type table.\n- For each new piece of data to be added to a resource:\n- Create a new attribute_type that describes the data type - this should be included in the seed data.\n- Create a new attribute with the attribute_type that you just created and link this new attribute to your resource.\n- Update the translator for the resource (from FHIR to OpenMRS and vice versa) to ensure that the new piece of data is populated.\n- Please see [the addition of the SDS Role Profile Id field to the Practitioner resource](https://github.com/Open-GP/openmrs-module-gpconnect/commit/f0ff30c18f0d386e338d87f22a847b0dc3dff5ff) to see an implementation of the following pattern:\n![Provider Attribute Design](../images/ProviderAttributeDesign2.png)\n","**Use a separate table to represent one-to-many relationships.**\n\nThis decision is made to address the problem of managing one-to-many relationships in the OpenMRS database. The current solution, which involves extending the existing resource table with a new column for each data field, becomes complex and difficult to maintain when there are one-to-many relationships.\n\nBy using a separate table to represent one-to-many relationships, we can simplify the OpenMRS model and make it more flexible. This will allow us to easily add new data fields to the resource without having to make changes to the existing table.\n\nThe separate table will have a foreign key that references the primary key of the resource table. This will allow us to easily retrieve all of the data fields for a given resource.\n\nThis decision is aligned with the principles of data normalization. Normalization is a process of organizing data in a way that reduces redundancy and improves data integrity. By using a separate table to represent one-to-many relationships, we are reducing the redundancy in the OpenMRS database and making it more efficient to query and update data."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPackage versions specified in `package.json` are often never any more precise than the major version.\nBy default, [Yarn](https://yarnpkg.com/lang/en/) records the exact version of the packages. In contrast, [npm](https://www.npmjs.com/) version 4 uses an opt-in feature called [npm-shrinkwrap.json](https://docs.npmjs.com/files/shrinkwrap.json) and in version [5](http://blog.npmjs.org/post/161081169345/v500) this has been superseded by a new file - [package-lock.json](https://docs.npmjs.com/files/package-lock.json). The feature no longer opt-in for version 5 of `npm`.\n\n## Decision\n",We will use `Yarn` as we have more experience of this than version 5 of `npm` and `Yarn` has been proven to work for our use cases.\n,We will use Yarn to manage package versions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe styling of sites developed for NHSUK need to be as consistent as possible.\nCurrently each new site starts by copying what the last developed site had done\nand building on that. This results in the newer sites looking different than\nthe older sites, which would not be a problem if each site was easily updated\nto use the same, most recent, set of styles. Currently there is no central\nstore for the styles which means there is no easy way to find and use them\neither.\nThe ability to review the styles in a development environment before they are\nmade available to a wider audience is important. As is the ability to use well\nknown technologies such as [SASS](http://sass-lang.com/).\n\n## Decision\n",We will use [Brunch](http://brunch.io/) to accomplish a number of tasks\nincluding:\n* Running the site used in the development environment\n* Generating the compiled assets\n,"Use a single styles repository, versioned and maintained on GitHub, to allow for easy reuse and review of styles across sites. The repository will use SASS to allow for easy maintenance and extension of styles."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Employ an Architectural Decision Repository (ADR) to document and track all architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nCurrently, we extract technical metadata per-object and run one extraction job serially per-file. This takes a problematically long time for objects with many files; blocks other objects from accessioning; and complicates restarts which must begin again and process the entire object.\nNOTE: Needs discussion: Fedora 3 does not support concurrent writes on the same datastream so we can either split out filesets as a first-class objects in the F3 data model or use temporary caching to generate a consolidated techMD datastream.\n## Decision Drivers <!-- optional -->\n* Blocker for Google Books project\n* Slows down accessioning process\n\n## Decision\n","* Blocker for Google Books project\n* Slows down accessioning process\nTBD!\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n",Use F3 multiple datastream capability to create a separate datastream for techMD.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\n* Each service must have knowledge about many other services.\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\nIn addition, there are already places within SDR were we have re-invented a message broker and/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\n\n## Decision\n","TBD.\n### Positive Consequences\n* Increased ability to scale number of services.\n* Increased ability to scale under load.\n* Increased resilience to service failures.\n* Provide mechanism for notification of the completion of long-running jobs.\n### Negative Consequences\n* Message broker must be supported by Ops.\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\n* The evolution work to begin using the Message Broker.\n",**Introduce a message broker to support asynchronous communication**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n","[Describe the decision made, including a summary of the alternatives. Elaborate on the decision process if appropriate.]"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nSDR is implemented by way of many different discrete services, some of which make network requests of each other to do their work.  For example, at present:\n* Argo calls Preservation Catalog to retrieve computed checksums for file content, and to retrieve actual preserved file content.\n* Many different services use dor-services-app for read and write operations on our digital repository objects.\nOf course, we want to make sure that not just any client on the network can use these services, since access to them should be limited to authorized callers.\nIn the past, this was accomplished primarily through network access restrictions (e.g., firewall whitelisting IPs of services that should have access, limiting access to clients inside the VPN, etc).  However, sole use of this approach has been deprecated by the industry at large and by Stanford UIT in particular (though it is still an important component of security).\n## Decision Drivers <!-- optional -->\n* We want to secure access to our API endpoints.\n* UIT wants us to secure access to our API endpoints.\n* We would like an approach that's relatively easy to understand and maintain.\n\n## Decision\n","* We want to secure access to our API endpoints.\n* UIT wants us to secure access to our API endpoints.\n* We would like an approach that's relatively easy to understand and maintain.\nThe infrastructure team came to consensus in a weekly planning meeting that, going forward, we should gate access to API endpoints using JWTs (minted by the service, provided with requests by the client).  This ADR is meant to capture and flesh out that decision.\n### Positive Consequences <!-- optional -->\n* More robust and less circumventable than restricting access solely by way of network and firewall configuration.\n* Does not preclude keeping appropriate firewall restrictions, which should remain part of our security practice.\n* Should a network re-configuration result in accidental loosening of firewall restrictions, token based authentication provides a robust additional line of access control.\n* For the ways we've built our applications, token based authentication is at least as easy to implement and maintain as client certificate based authenication, and should provide similarly adequate security.\n* Minting tokens has become a common approach in industry for authenticating client access to APIs, so there is a wealth of current information and advice available online for this practice.\n### Negative Consequences <!-- optional -->\n* More work for developers than solely relying on firewall rules and network configuration (tasks which typically fall to operations and which should happen anyway).\n",Implement OAuth2 as the authentication protocol for our services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nAs described in [ADR-0001](0001-migrate-off-fedora-3.md)], it is necessary to migrate from Fedora 3 as a repository for DOR metadata. Since the drafting of that original ADR significant progress has been made in isolating Fedora 3 from other SDR systems. In addition, COCINA has been adopted as a data model for DOR digital objects and the Fedora data model has been mapped to the COCINA data model.\nThe purpose of this ADR is to propose an alternative datastore.\n## Decision Drivers\n* Support CRUD and querying/indexing of JSON data.\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\n* Support transactions.\n* Support for large number of records.\n* Currently 2 million digital objects.\n* Expect significant growth in number of digital objects.\n* Depending on implementation, may possibly have multiple records per digital object.\n* Ability to migrate JSON model changes.\n* Broad community support for datastore and expected long term viability.\n* High availability deployment configurations.\n* Local Docker deployment configurations.\n* Can be supported by Operations team.\n* Robust export support (e.g., if needed to exit the technology)\n\n## Decision\n","* Support CRUD and querying/indexing of JSON data.\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\n* Support transactions.\n* Support for large number of records.\n* Currently 2 million digital objects.\n* Expect significant growth in number of digital objects.\n* Depending on implementation, may possibly have multiple records per digital object.\n* Ability to migrate JSON model changes.\n* Broad community support for datastore and expected long term viability.\n* High availability deployment configurations.\n* Local Docker deployment configurations.\n* Can be supported by Operations team.\n* Robust export support (e.g., if needed to exit the technology)\nNOT YET.\n",The alternative datastore selected is MongoDB.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nCurrently we are using JHOVE 1.x to generate voluminous technical metadata for every file of every object accessioned in SDR, and we do not use most of this metadata. This is problematic especially for large & many files: we cannot currently accessioning books with many pages because the technical metadata robot consumes all system memory which causes the virtual machine to kill the JHOVE process. We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.  Note: SMPL content ships with its own metadata typically from MediaInfo rather than JHOVE.\n## Decision Drivers <!-- optional -->\n* Cannot accession large files (objects > 1GB or so)\n* Cannot accession objects with many pages, such as books\n* Blocker for Google Books project\n* Causes extreme delays accessioning other content\n\n## Decision\n","* Cannot accession large files (objects > 1GB or so)\n* Cannot accession objects with many pages, such as books\n* Blocker for Google Books project\n* Causes extreme delays accessioning other content\n**Preferred** (by Infrastructure Team) option: option 2, because:\n* Option 1 is preventing us from accessioning books and other large objects, which is unacceptable to SDR customers\n* Option 3 is an unsound preservation strategy and does not meet SDR user needs\n* Option 4 has already been pursued a number of times already, and there's only so much we can toss at the worker machines\n* Option 5 has been rejected as a general deployment strategy for now\nThus, option 2 is the only option that currently meets the department's and its customers' needs.\nAs part of this work, we will move forward with a two-prong strategy in order to resolve the tension between the need to come up with a sound, community-oriented preservation practice and the need to accession large-scale content now.\nIn the short-term, we will come up with a short list of technical metadata attributes that will be extracted from all files and from all files of certain types. We will convene a “technical metadata strike team” in short order that will review attributes being used in Samvera and make recommendations based thereupon. The aim is for this group to finalize their recommendations in advance of the January 2020 Google Books work cycle.\nIn parallel, we will pursue a longer-term effort for determining what an ideal, community-oriented strategy is for doing this work building on best practices (which are currently murky/non-emergent). Along with this longer-term work, we will look into how to support on-demand regeneration of technical metadata so that we can iterate on the short-term work in the prior bullet.\n",Suspend JHOVE ingest until a future time when additional engineering resources are available to investigate the issue and develop an alternative that meets requirements.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n","[Describe your decision, e.g., in free form, following these guidelines, using one to three sentences. The set of braces should be relative to the text they describe, e.g., here, the keyword is **bold**.]\n\n- **Selected Option:** [Restate with **bold** the system or approach chosen from the Considered Options in the context section.]\n- **Rationale:** [Explain why this option was chosen.]\n- **Consequences:** [List the consequences of this decision, both positive and negative.]"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nTo advance SDR evolution towards decoupling from Fedora, we should store workflow state outside of Fedora (in the workflow service's database).\n\n## Decision\n",Remove the datastream.\nThis was done in dor-services v9.0.0 ([commit](https://github.com/sul-dlss/dor-services/commit/8745e7c2e86edbbaa7577af85779c4ea06258dd3)).\n,"**Decision:** Store workflow state outside of Fedora (in the workflow service's database).\n\n**Reasoning:**\n\n* Decoupling workflow state from Fedora will allow for greater flexibility and independence in the evolution of both systems.\n* Storing workflow state in the workflow service's database will provide better performance and reliability compared to storing it in Fedora.\n* The workflow service is already responsible for managing workflow execution, so it makes sense to also store the workflow state in the same place."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n","**Decision:** [Provide a single sentence stating the selected or chosen decision.]\n\n**Rationale:** [Provide a justification for the chosen decision, e.g., in free form using two or more sentences. If applicable, explain how the decision addresses the decision drivers.]"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n",[Describe the decision.]
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nCurrently the way to create new objects in the SDR is to:\n1. Register an object (typically using Argo)\n1. Move files into the `/dor` mount\n1. Run the assembly workflow (pre-assembly?)\nThe Google Books project will grow SDR by millions of new objects. This growth is an opportunity to inject an API into the SDR architecture that serves as the ""face"" of SDR. Creating an external-facing API allows SDR to accept deposits from on-campus stakeholders such as Big Local News and Open Neuro. Fronting SDR with a new API affords us a mechanism to validate the completeness and correctness of deposited data very early in the accessioning process. Furthermore, we might also use such an API to refactor internal projects (such as ETDs) so these projects no longer require intimate knowledge of the internals of SDR, which reduces coupling.\n## Decision Drivers <!-- optional -->\n* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\n\n## Decision\n","* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\nOption 2: we will create a deposit API as part of work on the 2020 Google Books work cycle.\n","**Decision:** Create a new web service API that will be the sole mechanism for creating new objects in SDR.\n\n**Rationale:**\n\n* An API will decouple the deposit process from the persistence layer, making it more flexible and extensible.\n* An API will allow on-campus stakeholders to deposit data to SDR in a self-service mode.\n* An API can be used to validate the completeness and correctness of deposited data early in the accessioning process, reducing the risk of bad data causing problems in the workflow."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management ""repository,"" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\n## Decision Drivers <!-- optional -->\n* Fedora 3 is unsupported and unlikely to be supported\n* Fedora 3 will be harder to install on newer operating systems\n* The Fedora 3 data model is not inherently validatable\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\n* Samvera software that supports Fedora 3 is outdated and maintained/supported only through our own efforts, preventing us from using mainstream Samvera software\n* We have (unverified) concerns about the scalability of Fedora 3\n\n## Decision\n","* Fedora 3 is unsupported and unlikely to be supported\n* Fedora 3 will be harder to install on newer operating systems\n* The Fedora 3 data model is not inherently validatable\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\n* Samvera software that supports Fedora 3 is outdated and maintained/supported only through our own efforts, preventing us from using mainstream Samvera software\n* We have (unverified) concerns about the scalability of Fedora 3\nNo decision made yet. See status field above.\n",Migrate the management repository to a supported version of Fedora.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe would like to generally flatten the directory structure in FDS as well as improving the\nimport statements for client applications. This will be a breaking change for a major\nversion, as clients will need to update imports once we make the change.\n## Decision Drivers\n- Remove unnecessary hierarchy in components dir\n- Simplify documentation\n- Adopt the unofficial CBI standard of organizing by component directories (each component\nhas its own directory).\n- Improve ergonomics and performance of import statements for client applications\n\n## Decision\n","- Remove unnecessary hierarchy in components dir\n- Simplify documentation\n- Adopt the unofficial CBI standard of organizing by component directories (each component\nhas its own directory).\n- Improve ergonomics and performance of import statements for client applications\n### Import statements\nBecause individual imports for each component has a negative performance impact, we decided\nto use destructured imports and rely on a tree shaking strategy:\n```diff\n-import Flex from '@cbinsights/fds/lib/components/layout/Flex';\n-import FlexItem from '@cbinsights/fds/lib/components/layout/FlexItem';\n-import TextInput from '@cbinsights/fds/lib/components/forms/TextInput';\n+import { Flex, FlexItem, TextInput } from '@cbinsights/fds/lib/components';\n```\nThe ""root"" file for each import corresponds to an FDS code module.\n| code module   | Import example                                                                        |\n| ------------- | ------------------------------------------------------------------------------------- |\n| `dictionary`  | `import { COLOR_TIMBERWOLF, COLOR_PURPLE } from '@cbinsights/fds/lib/dictionary';`    |\n| `components`  | `import { Avatar, TextInput } from '@cbinsights/fds/lib/components/';`                |\n| `icons`       | `import { CloudIcon, BellIcon } from '@cbinsights/fds/lib/icons';`                    |\n| `assets`      | Use path `@cbinsights/fds/lib/assets/<file>` where applicable                         |\n| `base-styles` | not applicable; we only publish this CSS as part of the main styles file in `assets/` |\n### `components` file structure\nCategorization directories will be removed.\nEach component will have its own directory under `components/`.\nTests, stories, private components, and helpers will be colocated in component\ndirectories.\n```\ncomponents/\n├── Foo/\n│   ├── FooHelper.jsx\n│   ├── foo.test.jsx\n│   ├── foo.stories.jsx\n│   ├── utils.js\n│   └── index.js  // contains Foo component code\n...\n```\n### Documentation\nThe ""Components"" section in Storybook will no longer have categorization. All components\nwill be documented flat under the section heading.\n","**Decision:** Flatten the directory structure in FDS and improve import statements for client applications.\n\n**Justification:**\n\n* **Remove unnecessary hierarchy in components dir:** The current directory structure contains unnecessary levels of hierarchy, making it difficult to navigate and find components. Flattening the structure will improve organization and ease of use.\n* **Simplify documentation:** The documentation for FDS will be simplified by removing references to the existing directory hierarchy.\n* **Adopt the unofficial CBI standard:** Adopting the unofficial CBI standard of organizing by component directories will make FDS more consistent with other projects and improve compatibility with other tools and libraries.\n* **Improve ergonomics and performance of import statements for client applications:** Flattening the directory structure and adopting the CBI standard will improve the ergonomics and performance of import statements for client applications, making it easier for developers to use FDS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nOur public-facing application is currently suffering from a ""zindex war"". In a ""zindex\nwar"", there are no winners. Engineers must regularly set insanely high and arbitrary\nzindex values to build our front end features.\n\n## Decision\n","We've decided to go with a 2 tier approach:\n- Use pre-defined z-index values & ranges that are defined in FDS.\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\nvalues are not useful)\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\n### Use values between `1` and `9` for fine layering control\nIn cases where one element needs to appear above another, use integers below 10. Rely on\nsurrounding stacking context to do the rest.\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\nvalues `1` through `5` to accomplish this.\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/The_stacking_context).\n### Pre-defined FDS z-index values / ranges\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\n#### Navigation\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to ""frame"" the content on a page.\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\n```\n.mainheader {\nz-index: calc(var(--zindex-navigation) + 9);\n}\n.subheader {\nz-index: var(--zindex-navigation)\n}\n```\n#### Modal\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\n#### Crazy\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\n### For standard values, continue using increments of `10`\nFor base stacking context, standard values are most helpful at increments of 10 because it\ngives engineers flexibility to insert layers between defined stack levels. For example:\n","**Decision:** Use a component library that includes a z-index system.\n\n**Rationale:**\n\n* A component library provides a consistent and structured approach to managing z-index values, preventing arbitrary and excessive usage.\n* It enforces a predefined hierarchy, ensuring that elements are always positioned correctly in relation to each other.\n* By centralizing z-index management, it eliminates the need for engineers to guess or compete for values, reducing the likelihood of z-index wars.\n* Standardizing z-index usage improves code readability, maintainability, and reduces potential conflicts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [material-ui](https://material-ui.com) library is considered ""legacy"". In FDS, we\naim to provide component coverage so that in the future, consumers will no longer need to\nrely on material-ui.\nThe FDS node module however, still provides a `mui-theme` theming object for the\nmaterial-ui library.\n\n## Decision\n",Removed `mui-theme` from FDS.\n### Negative Consequences <!-- optional -->\n- Consumers will need to handle the breaking change. Consumers are now responsible for\ndefining their own material-ui base theme.\n,The FDS node module will gradually deprecate the use of the `mui-theme` object in favor of the `fds-theme` object.\n- All new components in the FDS node module will only support the `fds-theme` object.\n- Existing components in the FDS node module will be updated to support the `fds-theme` object and will deprecate the use of the `mui-theme` object.\n- A timeline for the deprecation and removal of the `mui-theme` object will be provided in a future announcement.\n- Consumers of the FDS node module are encouraged to migrate to the `fds-theme` object as soon as possible.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe take a utility class based approach (aka ""functional CSS"" aka ""expressive CSS"") in FDS.\nWe have a lot of utility classes that do only one thing (e.g. `display--none`).\nAt one point, we discussed adding a `position--relative` class, which led us to a key question\nwe needed to clarify:\n> When does it make sense to add a utility class?\n\n## Decision\n","Chosen option: ""Only add utility classes that are useful on their own""\n### Example of a utility class that is not ""useful on its own""\n```html\n<div class=""position--relative"">\n<div>\nIf this element is absolutely positioned, we need additional ad-hoc CSS rules beyond\nwhat the utility class above provides. `position--relative` is not helpful on its own.\n</div>\n</div>\n```\n### Example of a utility class that is ""useful on its own""\n```html\n<div class=""color--red"">\n<div>\nThis text will be red. No additional CSS rules are needed for the above utility class\nto be useful.\n</div>\n</div>\n```\n","**Decision:** Add a utility class when the need to apply a style is common and the style is simple, specific, and consistent.\n\n**Rationale:**\n\n* **Common need:** Utility classes should be used when the need for a particular style is common across multiple components or use cases.\n* **Simple style:** Utility classes should implement simple, isolated styles that can be easily combined with other classes.\n* **Specific:** Utility classes should target specific elements or behaviors, rather than broad, general styles.\n* **Consistent:** Utility classes should provide consistent styling and avoid introducing edge cases or exceptions.\n\nBy adhering to these principles, we can ensure that utility classes remain a valuable and efficient tool in our design system, allowing us to quickly and consistently apply common styles without the need for custom CSS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe'd like to change the spacing system to something more generic.\n## Decision Drivers <!-- optional -->\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half/half.\n\n## Decision\n","The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half/half.\nGeneric tshirt system was chosen over the other options.\n- Coding a direct ""arbitrary value"" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\n","The new spacing system will use absolute values instead of relative values. This will allow us to add additional spacing values that don't map cleanly to the current words, and it will also make it possible to change the default value in the future.\n\nThe new spacing values will be:\n\n- `default` (16px)\n- `small` (8px)\n- `medium` (12px)\n- `large` (24px)\n- `x-large` (32px)"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\narbitrary style overrides on the component.\nWe want all components in FDS to follow a similar prop interface, so this decision should\napply to _all_ components.\n\n## Decision\n","Chosen option: ""No components accept a `className`"", because...\n- It's too risky to introduce style overrides to low-level layout components.\n- Style customization should happen via props, where the component can be responsible for\nstyle changes and variations.\n### Positive Consequences <!-- optional -->\n- Prop API consistency across all components.\n### Negative Consequences <!-- optional -->\n- Consumers of FDS components must handle the breaking change. Any custom `className`\nprops on FDS components must be moved to a direct parent or direct child of the\ncomponent.\n",The decision is to not allow components in FDS to accept a `classNames` prop.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen should we spread props in components?\n**Example**\n```\nconst Component = ({ foo, bar, ...restProps}) => {\n<element foo={foo} bar={bar} {...restProps} />\n}\n```\nIf we spread rest props on every component, we create an opportunity for developers to\ncustomize components via code instead of explicitly defined props.\nFor components that render attribute-heavy DOM elements (like `button`), it may make\nsense to spread props. For more complex components, spreading props could lead to bugs,\nside effects, and unnecessary customization.\n\n## Decision\n","Chosen option: ""Only components that render attribute-heavy DOM elements spread rest props"", because...\n- Compromise that allows us to create the right prop interface for the job of each component\n- Components that conceptually map to attribute-heavy DOM elements (i.e. html form elements)\nwill allow spread props to keep the explicitly defined props easier to reason about.\n- Components that **do not** conceptually map to attribute-heavy DOM elements (e.g. `Toast`)\nwill be able to continue to define an explicit prop interface instead of spreading props.\n### Negative Consequences <!-- optional -->\n- Not all components will behave the same way.\n- **We must document when a component supports spread props**.\n",**Decision**: Spread props in components only when necessary.\n\n**Reasoning**: \n\n- Spreading props provides flexibility and customization for attribute-heavy DOM elements.\n- It can lead to bugs and side effects in complex components.\n- Explicitly defined props ensure clarity and maintainability.\n\n**Guidelines**: \n\n- **Use props**: For complex components with well-defined behavior and interactions.\n- **Spread props**: For attribute-heavy DOM elements and components that need flexibility and customization.\n- **Consider**: The potential consequences of spreading props and ensure it aligns with the component's purpose and design.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\nAll account types have a set of balances that are composed of `Coins`. The `x/bank` module keeps\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\napplication.\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\noperations in the Cosmos SDK work off of units of 10^6.\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\nas a result the total amount of asset types increases. We propose to have `x/bank` additionally keep\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\nUX and remove the requirement for making any assumptions on the unit of denomination.\n\n## Decision\n","The `x/bank` module will be updated to store and index metadata by `denom`, specifically the ""base"" or\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\nMetadata may also include a non-zero length list of denominations. Each entry contains the name of\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\nand on exchanges. The values in those fields link to an entry in the list of denominations.\nThe list in `denom_units` and the `display` entry may be changed via governance.\nAs a result, we can define the type as follows:\n```protobuf\nmessage DenomUnit {\nstring denom    = 1;\nuint32 exponent = 2;\nrepeated string aliases = 3;\n}\nmessage Metadata {\nstring description = 1;\nrepeated DenomUnit denom_units = 2;\nstring base = 3;\nstring display = 4;\n}\n```\nAs an example, the ATOM's metadata can be defined as follows:\n```json\n{\n""name"": ""atom"",\n""description"": ""The native staking token of the Cosmos Hub."",\n""denom_units"": [\n{\n""denom"": ""uatom"",\n""exponent"": 0,\n""aliases"": [\n""microatom""\n],\n},\n{\n""denom"": ""matom"",\n""exponent"": 3,\n""aliases"": [\n""milliatom""\n]\n},\n{\n""denom"": ""atom"",\n""exponent"": 6,\n}\n],\n""base"": ""uatom"",\n""display"": ""atom"",\n}\n```\nGiven the above metadata, a client may infer the following things:\n* 4.3atom = 4.3 * (10^6) = 4,300,000uatom\n* The string ""atom"" can be used as a display name in a list of tokens.\n* The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\nan explicit decision to choose a different representation.\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\nas the base framework for this already exists in the Cosmos SDK.\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x/bank` module which is also\nindexed by the base `denom`.\n```go\ntype GenesisState struct {\nSendEnabled   bool        `json:""send_enabled"" yaml:""send_enabled""`\nBalances      []Balance   `json:""balances"" yaml:""balances""`\nSupply        sdk.Coins   `json:""supply"" yaml:""supply""`\nDenomMetadata []Metadata  `json:""denom_metadata"" yaml:""denom_metadata""`\n}\n```\n","The Cosmos SDK should introduce a new field to the `x/bank` module's `Balance` struct to store metadata about each denomination. This metadata should include the following information:\n\n- decimal_units: The number of decimal units that the denomination has. For example, if the denomination is ""uatom"", then the decimal_units would be 6, because 1 ATOM = 10^6 uatom.\n- display: The user-friendly name of the denomination. For example, if the denomination is ""uatom"", then the display would be ""microatom"".\n\nThis change will allow clients, wallet providers, and explorers to improve their UX by providing more information about each denomination. It will also remove the requirement for clients to make any assumptions about the unit of denomination."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis idea was first conceived of in order to fulfill the use case of the\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\nmembers would be elected by a governing community and would fulfill the role of\ncoordinating the community under emergency situations. This thinking\ncan be further abstracted into the concept of ""blockchain specialization\ngroups"".\nThe creation of these groups are the beginning of specialization capabilities\nwithin a wider blockchain community which could be used to enable a certain\nlevel of delegated responsibilities. Examples of specialization which could be\nbeneficial to a blockchain community include: code auditing, emergency response,\ncode development etc. This type of community organization paves the way for\nindividual stakeholders to delegate votes by issue type, if in the future\ngovernance proposals include a field for issue type.\n\n## Decision\n","A specialization group can be broadly broken down into the following functions\n(herein containing examples):\n* Membership Admittance\n* Membership Acceptance\n* Membership Revocation\n* (probably) Without Penalty\n* member steps down (self-Revocation)\n* replaced by new member from governance\n* (probably) With Penalty\n* due to breach of soft-agreement (determined through governance)\n* due to breach of hard-agreement (determined by code)\n* Execution of Duties\n* Special transactions which only execute for members of a specialization\ngroup (for example, dCERT members voting to turn off transaction routes in\nan emergency scenario)\n* Compensation\n* Group compensation (further distribution decided by the specialization group)\n* Individual compensation for all constituents of a group from the\ngreater community\nMembership admission to a specialization group could take place over a wide\nvariety of mechanisms. The most obvious example is through a general vote among\nthe entire community, however in certain systems a community may want to allow\nthe members already in a specialization group to internally elect new members,\nor maybe the community may assign a permission to a particular specialization\ngroup to appoint members to other 3rd party groups. The sky is really the limit\nas to how membership admittance can be structured. We attempt to capture\nsome of these possibilities in a common interface dubbed the `Electionator`. For\nits initial implementation as a part of this ADR we recommend that the general\nelection abstraction (`Electionator`) is provided as well as a basic\nimplementation of that abstraction which allows for a continuous election of\nmembers of a specialization group.\n``` golang\n// The Electionator abstraction covers the concept space for\n// a wide variety of election kinds.\ntype Electionator interface {\n// is the election object accepting votes.\nActive() bool\n// functionality to execute for when a vote is cast in this election, here\n// the vote field is anticipated to be marshalled into a vote type used\n// by an election.\n//\n// NOTE There are no explicit ids here. Just votes which pertain specifically\n// to one electionator. Anyone can create and send a vote to the electionator item\n// which will presumably attempt to marshal those bytes into a particular struct\n// and apply the vote information in some arbitrary way. There can be multiple\n// Electionators within the Cosmos-Hub for multiple specialization groups, votes\n// would need to be routed to the Electionator upstream of here.\nVote(addr sdk.AccAddress, vote []byte)\n// here lies all functionality to authenticate and execute changes for\n// when a member accepts being elected\nAcceptElection(sdk.AccAddress)\n// Register a revoker object\nRegisterRevoker(Revoker)\n// No more revokers may be registered after this function is called\nSealRevokers()\n// register hooks to call when an election actions occur\nRegisterHooks(ElectionatorHooks)\n// query for the current winner(s) of this election based on arbitrary\n// election ruleset\nQueryElected() []sdk.AccAddress\n// query metadata for an address in the election this\n// could include for example position that an address\n// is being elected for within a group\n//\n// this metadata may be directly related to\n// voting information and/or privileges enabled\n// to members within a group.\nQueryMetadata(sdk.AccAddress) []byte\n}\n// ElectionatorHooks, once registered with an Electionator,\n// trigger execution of relevant interface functions when\n// Electionator events occur.\ntype ElectionatorHooks interface {\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\nAfterMemberAccepted(addr sdk.AccAddress)\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\n}\n// Revoker defines the function required for a membership revocation rule-set\n// used by a specialization group. This could be used to create self revoking,\n// and evidence based revoking, etc. Revokers types may be created and\n// reused for different election types.\n//\n// When revoking the ""cause"" bytes may be arbitrarily marshalled into evidence,\n// memos, etc.\ntype Revoker interface {\nRevokeName() string      // identifier for this revoker type\nRevokeMember(addr sdk.AccAddress, cause []byte) error\n}\n```\nCertain level of commonality likely exists between the existing code within\n`x/governance` and required functionality of elections. This common\nfunctionality should be abstracted during implementation. Similarly for each\nvote implementation client CLI/REST functionality should be abstracted\nto be reused for multiple elections.\nThe specialization group abstraction firstly extends the `Electionator`\nbut also further defines traits of the group.\n``` golang\ntype SpecializationGroup interface {\nElectionator\nGetName() string\nGetDescription() string\n// general soft contract the group is expected\n// to fulfill with the greater community\nGetContract() string\n// messages which can be executed by the members of the group\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\n// logic to be executed at endblock, this may for instance\n// include payment of a stipend to the group members\n// for participation in the security group.\nEndBlocker(ctx sdk.Context)\n}\n```\n","Utilize blockchain technology to establish specialized groups within the blockchain community to enable delegated responsibilities, such as code auditing, emergency response, and code development."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user’s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user/computer.\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don’t provide a native secret store.\n\n## Decision\n",We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https://github.com/99designs/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\nThis appears to fulfill the requirement of protecting both key material and metadata from rogue software on a user’s machine.\n,Implement a secrets manager using a secure enclave.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations:\n* Key rotation is not possible, although this can be solved with [account rekeying](adr-034-account-rekeying.md).\n* Thresholds can't be changed.\n* UX is cumbersome for non-technical users ([#5661](https://github.com/cosmos/cosmos-sdk/issues/5661)).\n* It requires `legacy_amino` sign mode ([#8141](https://github.com/cosmos/cosmos-sdk/issues/8141)).\nWhile the group module is not meant to be a total replacement for the current multi-signature accounts, it provides a solution to the limitations described above, with a more flexible key management system where keys can be added, updated or removed, as well as configurable thresholds.\nIt's meant to be used with other access control modules such as [`x/feegrant`](./adr-029-fee-grant-module.md) and [`x/authz`](adr-030-authz-module.md) to simplify key management for individuals and organizations.\n\n## Decision\n","We propose merging the `x/group` module with its supporting ORM/Table Store package ([#7098](https://github.com/cosmos/cosmos-sdk/issues/7098)) into the Cosmos SDK and continuing development here. There will be a dedicated ADR for the ORM package.\n### Group\nA group is a composition of accounts with associated weights. It is not\nan account and doesn't have a balance. It doesn't in and of itself have any\nsort of voting or decision weight.\nGroup members can create proposals and vote on them through group accounts using different decision policies.\nIt has an `admin` account which can manage members in the group, update the group\nmetadata and set a new admin.\n```protobuf\nmessage GroupInfo {\n// group_id is the unique ID of this group.\nuint64 group_id = 1;\n// admin is the account address of the group's admin.\nstring admin = 2;\n// metadata is any arbitrary metadata to attached to the group.\nbytes metadata = 3;\n// version is used to track changes to a group's membership structure that\n// would break existing proposals. Whenever a member weight has changed,\n// or any member is added or removed, the version is incremented and will\n// invalidate all proposals from older versions.\nuint64 version = 4;\n// total_weight is the sum of the group members' weights.\nstring total_weight = 5;\n}\n```\n```protobuf\nmessage GroupMember {\n// group_id is the unique ID of the group.\nuint64 group_id = 1;\n// member is the member data.\nMember member = 2;\n}\n// Member represents a group member with an account address,\n// non-zero weight and metadata.\nmessage Member {\n// address is the member's account address.\nstring address = 1;\n// weight is the member's voting weight that should be greater than 0.\nstring weight = 2;\n// metadata is any arbitrary metadata to attached to the member.\nbytes metadata = 3;\n}\n```\n### Group Account\nA group account is an account associated with a group and a decision policy.\nA group account does have a balance.\nGroup accounts are abstracted from groups because a single group may have\nmultiple decision policies for different types of actions. Managing group\nmembership separately from decision policies results in the least overhead\nand keeps membership consistent across different policies. The pattern that\nis recommended is to have a single master group account for a given group,\nand then to create separate group accounts with different decision policies\nand delegate the desired permissions from the master account to\nthose ""sub-accounts"" using the [`x/authz` module](adr-030-authz-module.md).\n```protobuf\nmessage GroupAccountInfo {\n// address is the group account address.\nstring address = 1;\n// group_id is the ID of the Group the GroupAccount belongs to.\nuint64 group_id = 2;\n// admin is the account address of the group admin.\nstring admin = 3;\n// metadata is any arbitrary metadata of this group account.\nbytes metadata = 4;\n// version is used to track changes to a group's GroupAccountInfo structure that\n// invalidates active proposal from old versions.\nuint64 version = 5;\n// decision_policy specifies the group account's decision policy.\ngoogle.protobuf.Any decision_policy = 6 [(cosmos_proto.accepts_interface) = ""cosmos.group.v1.DecisionPolicy""];\n}\n```\nSimilarly to a group admin, a group account admin can update its metadata, decision policy or set a new group account admin.\nA group account can also be an admin or a member of a group.\nFor instance, a group admin could be another group account which could ""elects"" the members or it could be the same group that elects itself.\n### Decision Policy\nA decision policy is the mechanism by which members of a group can vote on\nproposals.\nAll decision policies should have a minimum and maximum voting window.\nThe minimum voting window is the minimum duration that must pass in order\nfor a proposal to potentially pass, and it may be set to 0. The maximum voting\nwindow is the maximum time that a proposal may be voted on and executed if\nit reached enough support before it is closed.\nBoth of these values must be less than a chain-wide max voting window parameter.\nWe define the `DecisionPolicy` interface that all decision policies must implement:\n```go\ntype DecisionPolicy interface {\ncodec.ProtoMarshaler\nValidateBasic() error\nGetTimeout() types.Duration\nAllow(tally Tally, totalPower string, votingDuration time.Duration) (DecisionPolicyResult, error)\nValidate(g GroupInfo) error\n}\ntype DecisionPolicyResult struct {\nAllow bool\nFinal bool\n}\n```\n#### Threshold decision policy\nA threshold decision policy defines a minimum support votes (_yes_), based on a tally\nof voter weights, for a proposal to pass. For\nthis decision policy, abstain and veto are treated as no support (_no_).\n```protobuf\nmessage ThresholdDecisionPolicy {\n// threshold is the minimum weighted sum of support votes for a proposal to succeed.\nstring threshold = 1;\n// voting_period is the duration from submission of a proposal to the end of voting period\n// Within this period, votes and exec messages can be submitted.\ngoogle.protobuf.Duration voting_period = 2 [(gogoproto.nullable) = false];\n}\n```\n### Proposal\nAny member of a group can submit a proposal for a group account to decide upon.\nA proposal consists of a set of `sdk.Msg`s that will be executed if the proposal\npasses as well as any metadata associated with the proposal. These `sdk.Msg`s get validated as part of the `Msg/CreateProposal` request validation. They should also have their signer set as the group account.\nInternally, a proposal also tracks:\n* its current `Status`: submitted, closed or aborted\n* its `Result`: unfinalized, accepted or rejected\n* its `VoteState` in the form of a `Tally`, which is calculated on new votes and when executing the proposal.\n```protobuf\n// Tally represents the sum of weighted votes.\nmessage Tally {\noption (gogoproto.goproto_getters) = false;\n// yes_count is the weighted sum of yes votes.\nstring yes_count = 1;\n// no_count is the weighted sum of no votes.\nstring no_count = 2;\n// abstain_count is the weighted sum of abstainers.\nstring abstain_count = 3;\n// veto_count is the weighted sum of vetoes.\nstring veto_count = 4;\n}\n```\n### Voting\nMembers of a group can vote on proposals. There are four choices to choose while voting - yes, no, abstain and veto. Not\nall decision policies will support them. Votes can contain some optional metadata.\nIn the current implementation, the voting window begins as soon as a proposal\nis submitted.\nVoting internally updates the proposal `VoteState` as well as `Status` and `Result` if needed.\n### Executing Proposals\nProposals will not be automatically executed by the chain in this current design,\nbut rather a user must submit a `Msg/Exec` transaction to attempt to execute the\nproposal based on the current votes and decision policy. A future upgrade could\nautomate this and have the group account (or a fee granter) pay.\n#### Changing Group Membership\nIn the current implementation, updating a group or a group account after submitting a proposal will make it invalid. It will simply fail if someone calls `Msg/Exec` and will eventually be garbage collected.\n### Notes on current implementation\nThis section outlines the current implementation used in the proof of concept of the group module but this could be subject to changes and iterated on.\n#### ORM\nThe [ORM package](https://github.com/cosmos/cosmos-sdk/discussions/9156) defines tables, sequences and secondary indexes which are used in the group module.\nGroups are stored in state as part of a `groupTable`, the `group_id` being an auto-increment integer. Group members are stored in a `groupMemberTable`.\nGroup accounts are stored in a `groupAccountTable`. The group account address is generated based on an auto-increment integer which is used to derive the group module `RootModuleKey` into a `DerivedModuleKey`, as stated in [ADR-033](adr-033-protobuf-inter-module-comm.md#modulekeys-and-moduleids). The group account is added as a new `ModuleAccount` through `x/auth`.\nProposals are stored as part of the `proposalTable` using the `Proposal` type. The `proposal_id` is an auto-increment integer.\nVotes are stored in the `voteTable`. The primary key is based on the vote's `proposal_id` and `voter` account address.\n#### ADR-033 to route proposal messages\nInter-module communication introduced by [ADR-033](adr-033-protobuf-inter-module-comm.md) can be used to route a proposal's messages using the `DerivedModuleKey` corresponding to the proposal's group account.\n",The Cosmos SDK should adopt the new Group Module as the primary multi-signature solution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https://github.com/cosmos/cosmos-sdk/issues/9406) and [refunding unused gas](https://github.com/cosmos/cosmos-sdk/issues/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\nAn naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https://github.com/cosmos/cosmos-sdk/discussions/9182)), which includes making BaseApp more lightweight and modular.\n\n## Decision\n","We decide to transform Baseapp's implementation of ABCI `{Check,Deliver}Tx` and its own `Simulate` methods to use a middleware-based design.\nThe two following interfaces are the base of the middleware design, and are defined in `types/tx`:\n```go\ntype Handler interface {\nCheckTx(ctx context.Context, req Request, checkReq RequestCheckTx) (Response, ResponseCheckTx, error)\nDeliverTx(ctx context.Context, req Request) (Response, error)\nSimulateTx(ctx context.Context, req Request (Response, error)\n}\ntype Middleware func(Handler) Handler\n```\nwhere we define the following arguments and return types:\n```go\ntype Request struct {\nTx      sdk.Tx\nTxBytes []byte\n}\ntype Response struct {\nGasWanted uint64\nGasUsed   uint64\n// MsgResponses is an array containing each Msg service handler's response\n// type, packed in an Any. This will get proto-serialized into the `Data` field\n// in the ABCI Check/DeliverTx responses.\nMsgResponses []*codectypes.Any\nLog          string\nEvents       []abci.Event\n}\ntype RequestCheckTx struct {\nType abci.CheckTxType\n}\ntype ResponseCheckTx struct {\nPriority int64\n}\n```\nPlease note that because CheckTx handles separate logic related to mempool priotization, its signature is different than DeliverTx and SimulateTx.\nBaseApp holds a reference to a `tx.Handler`:\n```go\ntype BaseApp  struct {\n// other fields\ntxHandler tx.Handler\n}\n```\nBaseapp's ABCI `{Check,Deliver}Tx()` and `Simulate()` methods simply call `app.txHandler.{Check,Deliver,Simulate}Tx()` with the relevant arguments. For example, for `DeliverTx`:\n```go\nfunc (app *BaseApp) DeliverTx(req abci.RequestDeliverTx) abci.ResponseDeliverTx {\nvar abciRes abci.ResponseDeliverTx\nctx := app.getContextForTx(runTxModeDeliver, req.Tx)\nres, err := app.txHandler.DeliverTx(ctx, tx.Request{TxBytes: req.Tx})\nif err != nil {\nabciRes = sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\nreturn abciRes\n}\nabciRes, err = convertTxResponseToDeliverTx(res)\nif err != nil {\nreturn sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\n}\nreturn abciRes\n}\n// convertTxResponseToDeliverTx converts a tx.Response into a abci.ResponseDeliverTx.\nfunc convertTxResponseToDeliverTx(txRes tx.Response) (abci.ResponseDeliverTx, error) {\ndata, err := makeABCIData(txRes)\nif err != nil {\nreturn abci.ResponseDeliverTx{}, nil\n}\nreturn abci.ResponseDeliverTx{\nData:   data,\nLog:    txRes.Log,\nEvents: txRes.Events,\n}, nil\n}\n// makeABCIData generates the Data field to be sent to ABCI Check/DeliverTx.\nfunc makeABCIData(txRes tx.Response) ([]byte, error) {\nreturn proto.Marshal(&sdk.TxMsgData{MsgResponses: txRes.MsgResponses})\n}\n```\nThe implementations are similar for `BaseApp.CheckTx` and `BaseApp.Simulate`.\n`baseapp.txHandler`'s three methods' implementations can obviously be monolithic functions, but for modularity we propose a middleware composition design, where a middleware is simply a function that takes a `tx.Handler`, and returns another `tx.Handler` wrapped around the previous one.\n### Implementing a Middleware\nIn practice, middlewares are created by Go function that takes as arguments some parameters needed for the middleware, and returns a `tx.Middleware`.\nFor example, for creating an arbitrary `MyMiddleware`, we can implement:\n```go\n// myTxHandler is the tx.Handler of this middleware. Note that it holds a\n// reference to the next tx.Handler in the stack.\ntype myTxHandler struct {\n// next is the next tx.Handler in the middleware stack.\nnext tx.Handler\n// some other fields that are relevant to the middleware can be added here\n}\n// NewMyMiddleware returns a middleware that does this and that.\nfunc NewMyMiddleware(arg1, arg2) tx.Middleware {\nreturn func (txh tx.Handler) tx.Handler {\nreturn myTxHandler{\nnext: txh,\n// optionally, set arg1, arg2... if they are needed in the middleware\n}\n}\n}\n// Assert myTxHandler is a tx.Handler.\nvar _ tx.Handler = myTxHandler{}\nfunc (h myTxHandler) CheckTx(ctx context.Context, req Request, checkReq RequestcheckTx) (Response, ResponseCheckTx, error) {\n// CheckTx specific pre-processing logic\n// run the next middleware\nres, checkRes, err := txh.next.CheckTx(ctx, req, checkReq)\n// CheckTx specific post-processing logic\nreturn res, checkRes, err\n}\nfunc (h myTxHandler) DeliverTx(ctx context.Context, req Request) (Response, error) {\n// DeliverTx specific pre-processing logic\n// run the next middleware\nres, err := txh.next.DeliverTx(ctx, tx, req)\n// DeliverTx specific post-processing logic\nreturn res, err\n}\nfunc (h myTxHandler) SimulateTx(ctx context.Context, req Request) (Response, error) {\n// SimulateTx specific pre-processing logic\n// run the next middleware\nres, err := txh.next.SimulateTx(ctx, tx, req)\n// SimulateTx specific post-processing logic\nreturn res, err\n}\n```\n### Composing Middlewares\nWhile BaseApp simply holds a reference to a `tx.Handler`, this `tx.Handler` itself is defined using a middleware stack. The Cosmos SDK exposes a base (i.e. innermost) `tx.Handler` called `RunMsgsTxHandler`, which executes messages.\nThen, the app developer can compose multiple middlewares on top on the base `tx.Handler`. Each middleware can run pre-and-post-processing logic around its next middleware, as described in the section above. Conceptually, as an example, given the middlewares `A`, `B`, and `C` and the base `tx.Handler` `H` the stack looks like:\n```text\nA.pre\nB.pre\nC.pre\nH # The base tx.handler, for example `RunMsgsTxHandler`\nC.post\nB.post\nA.post\n```\nWe define a `ComposeMiddlewares` function for composing middlewares. It takes the base handler as first argument, and middlewares in the ""outer to inner"" order. For the above stack, the final `tx.Handler` is:\n```go\ntxHandler := middleware.ComposeMiddlewares(H, A, B, C)\n```\nThe middleware is set in BaseApp via its `SetTxHandler` setter:\n```go\n// simapp/app.go\ntxHandler := middleware.ComposeMiddlewares(...)\napp.SetTxHandler(txHandler)\n```\nThe app developer can define their own middlewares, or use the Cosmos SDK's pre-defined middlewares from `middleware.NewDefaultTxHandler()`.\n### Middlewares Maintained by the Cosmos SDK\nWhile the app developer can define and compose the middlewares of their choice, the Cosmos SDK provides a set of middlewares that caters for the ecosystem's most common use cases. These middlewares are:\n| Middleware              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| RunMsgsTxHandler        | This is the base `tx.Handler`. It replaces the old baseapp's `runMsgs`, and executes a transaction's `Msg`s.                                                                                                                                                                                                                                                                                                                                                                             |\n| TxDecoderMiddleware     | This middleware takes in transaction raw bytes, and decodes them into a `sdk.Tx`. It replaces the `baseapp.txDecoder` field, so that BaseApp stays as thin as possible. Since most middlewares read the contents of the `sdk.Tx`, the TxDecoderMiddleware should be run first in the middleware stack.                                                                                                                                                                                   |\n| {Antehandlers}          | Each antehandler is converted to its own middleware. These middlewares perform signature verification, fee deductions and other validations on the incoming transaction.                                                                                                                                                                                                                                                                                                                 |\n| IndexEventsTxMiddleware | This is a simple middleware that chooses which events to index in Tendermint. Replaces `baseapp.indexEvents` (which unfortunately still exists in baseapp too, because it's used to index Begin/EndBlock events)                                                                                                                                                                                                                                                                         |\n| RecoveryTxMiddleware    | This index recovers from panics. It replaces baseapp.runTx's panic recovery described in [ADR-022](./adr-022-custom-panic-handling.md).                                                                                                                                                                                                                                                                                                                                                  |\n| GasTxMiddleware         | This replaces the [`Setup`](https://github.com/cosmos/cosmos-sdk/blob/v0.43.0/x/auth/ante/setup.go) Antehandler. It sets a GasMeter on sdk.Context. Note that before, GasMeter was set on sdk.Context inside the antehandlers, and there was some mess around the fact that antehandlers had their own panic recovery system so that the GasMeter could be read by baseapp's recovery system. Now, this mess is all removed: one middleware sets GasMeter, another one handles recovery. |\n### Similarities and Differences between Antehandlers and Middlewares\nThe middleware-based design builds upon the existing antehandlers design described in [ADR-010](./adr-010-modular-antehandler.md). Even though the final decision of ADR-010 was to go with the ""Simple Decorators"" approach, the middleware design is actually very similar to the other [Decorator Pattern](./adr-010-modular-antehandler.md#decorator-pattern) proposal, also used in [weave](https://github.com/iov-one/weave).\n#### Similarities with Antehandlers\n* Designed as chaining/composing small modular pieces.\n* Allow code reuse for `{Check,Deliver}Tx` and for `Simulate`.\n* Set up in `app.go`, and easily customizable by app developers.\n* Order is important.\n#### Differences with Antehandlers\n* The Antehandlers are run before `Msg` execution, whereas middlewares can run before and after.\n* The middleware approach uses separate methods for `{Check,Deliver,Simulate}Tx`, whereas the antehandlers pass a `simulate bool` flag and uses the `sdkCtx.Is{Check,Recheck}Tx()` flags to determine in which transaction mode we are.\n* The middleware design lets each middleware hold a reference to the next middleware, whereas the antehandlers pass a `next` argument in the `AnteHandle` method.\n* The middleware design use Go's standard `context.Context`, whereas the antehandlers use `sdk.Context`.\n","**Introduce a new `runTxWithCtx` method in BaseApp**\n\nThis method would take a `context.Context` as an argument and would be called instead of `runTx` by `ABCI {Check,Deliver}Tx()` and `Simulate()`. The `context.Context` would be passed to all antehandlers and `Msg`s, allowing them to access custom data and perform custom logic after the `Msg`s have been executed.\n\nThis solution is more flexible and modular than adding post-`Msg` hooks to BaseApp, and it is in line with the Cosmos SDK team's vision of making BaseApp more lightweight and modular."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIssue [\#3685](https://github.com/cosmos/cosmos-sdk/issues/3685) identified that public key\naddress spaces are currently overlapping. We confirmed that it significantly decreases security of Cosmos SDK.\n### Problem\nAn attacker can control an input for an address generation function. This leads to a birthday attack, which significantly decreases the security space.\nTo overcome this, we need to separate the inputs for different kind of account types:\na security break of one account type shouldn't impact the security of other account types.\n### Initial proposals\nOne initial proposal was extending the address length and\nadding prefixes for different types of addresses.\n@ethanfrey explained an alternate approach originally used in https://github.com/iov-one/weave:\n> I spent quite a bit of time thinking about this issue while building weave... The other cosmos Sdk.\n> Basically I define a condition to be a type and format as human readable string with some binary data appended. This condition is hashed into an Address (again at 20 bytes). The use of this prefix makes it impossible to find a preimage for a given address with a different condition (eg ed25519 vs secp256k1).\n> This is explained in depth here https://weave.readthedocs.io/en/latest/design/permissions.html\n> And the code is here, look mainly at the top where we process conditions. https://github.com/iov-one/weave/blob/master/conditions.go\nAnd explained how this approach should be sufficiently collision resistant:\n> Yeah, AFAIK, 20 bytes should be collision resistance when the preimages are unique and not malleable. A space of 2^160 would expect some collision to be likely around 2^80 elements (birthday paradox). And if you want to find a collision for some existing element in the database, it is still 2^160. 2^80 only is if all these elements are written to state.\n> The good example you brought up was eg. a public key bytes being a valid public key on two algorithms supported by the codec. Meaning if either was broken, you would break accounts even if they were secured with the safer variant. This is only as the issue when no differentiating type info is present in the preimage (before hashing into an address).\n> I would like to hear an argument if the 20 bytes space is an actual issue for security, as I would be happy to increase my address sizes in weave. I just figured cosmos and ethereum and bitcoin all use 20 bytes, it should be good enough. And the arguments above which made me feel it was secure. But I have not done a deeper analysis.\nThis led to the first proposal (which we proved to be not good enough):\nwe concatenate a key type with a public key, hash it and take the first 20 bytes of that hash, summarized as `sha256(keyTypePrefix || keybytes)[:20]`.\n### Review and Discussions\nIn [\#5694](https://github.com/cosmos/cosmos-sdk/issues/5694) we discussed various solutions.\nWe agreed that 20 bytes it's not future proof, and extending the address length is the only way to allow addresses of different types, various signature types, etc.\nThis disqualifies the initial proposal.\nIn the issue we discussed various modifications:\n* Choice of the hash function.\n* Move the prefix out of the hash function: `keyTypePrefix + sha256(keybytes)[:20]` [post-hash-prefix-proposal].\n* Use double hashing: `sha256(keyTypePrefix + sha256(keybytes)[:20])`.\n* Increase to keybytes hash slice from 20 byte to 32 or 40 bytes. We concluded that 32 bytes, produced by a good hash functions is future secure.\n### Requirements\n* Support currently used tools - we don't want to break an ecosystem, or add a long adaptation period. Ref: https://github.com/cosmos/cosmos-sdk/issues/8041\n* Try to keep the address length small - addresses are widely used in state, both as part of a key and object value.\n### Scope\nThis ADR only defines a process for the generation of address bytes. For end-user interactions with addresses (through the API, or CLI, etc.), we still use bech32 to format these addresses as strings. This ADR doesn't change that.\nUsing Bech32 for string encoding gives us support for checksum error codes and handling of user typos.\n\n## Decision\n","We define the following account types, for which we define the address function:\n1. simple accounts: represented by a regular public key (ie: secp256k1, sr25519)\n2. naive multisig: accounts composed by other addressable objects (ie: naive multisig)\n3. composed accounts with a native address key (ie: bls, group module accounts)\n4. module accounts: basically any accounts which cannot sign transactions and which are managed internally by modules\n### Legacy Public Key Addresses Don't Change\nCurrently (Jan 2021), the only officially supported Cosmos SDK user accounts are `secp256k1` basic accounts and legacy amino multisig.\nThey are used in existing Cosmos SDK zones. They use the following address formats:\n* secp256k1: `ripemd160(sha256(pk_bytes))[:20]`\n* legacy amino multisig: `sha256(aminoCdc.Marshal(pk))[:20]`\nWe don't want to change existing addresses. So the addresses for these two key types will remain the same.\nThe current multisig public keys use amino serialization to generate the address. We will retain\nthose public keys and their address formatting, and call them ""legacy amino"" multisig public keys\nin protobuf. We will also create multisig public keys without amino addresses to be described below.\n### Hash Function Choice\nAs in other parts of the Cosmos SDK, we will use `sha256`.\n### Basic Address\nWe start with defining a base algorithm for generating addresses which we will call `Hash`. Notably, it's used for accounts represented by a single key pair. For each public key schema we have to have an associated `typ` string, explained in the next section. `hash` is the cryptographic hash function defined in the previous section.\n```go\nconst A_LEN = 32\nfunc Hash(typ string, key []byte) []byte {\nreturn hash(hash(typ) + key)[:A_LEN]\n}\n```\nThe `+` is bytes concatenation, which doesn't use any separator.\nThis algorithm is the outcome of a consultation session with a professional cryptographer.\nMotivation: this algorithm keeps the address relatively small (length of the `typ` doesn't impact the length of the final address)\nand it's more secure than [post-hash-prefix-proposal] (which uses the first 20 bytes of a pubkey hash, significantly reducing the address space).\nMoreover the cryptographer motivated the choice of adding `typ` in the hash to protect against a switch table attack.\n`address.Hash` is a low level function to generate _base_ addresses for new key types. Example:\n* BLS: `address.Hash(""bls"", pubkey)`\n### Composed Addresses\nFor simple composed accounts (like a new naive multisig) we generalize the `address.Hash`. The address is constructed by recursively creating addresses for the sub accounts, sorting the addresses and composing them into a single address. It ensures that the ordering of keys doesn't impact the resulting address.\n```go\n// We don't need a PubKey interface - we need anything which is addressable.\ntype Addressable interface {\nAddress() []byte\n}\nfunc Composed(typ string, subaccounts []Addressable) []byte {\naddresses = map(subaccounts, \a -> LengthPrefix(a.Address()))\naddresses = sort(addresses)\nreturn address.Hash(typ, addresses[0] + ... + addresses[n])\n}\n```\nThe `typ` parameter should be a schema descriptor, containing all significant attributes with deterministic serialization (eg: utf8 string).\n`LengthPrefix` is a function which prepends 1 byte to the address. The value of that byte is the length of the address bits before prepending. The address must be at most 255 bits long.\nWe are using `LengthPrefix` to eliminate conflicts - it assures, that for 2 lists of addresses: `as = {a1, a2, ..., an}` and `bs = {b1, b2, ..., bm}` such that every `bi` and `ai` is at most 255 long, `concatenate(map(as, (a) => LengthPrefix(a))) = map(bs, (b) => LengthPrefix(b))` if `as = bs`.\nImplementation Tip: account implementations should cache addresses.\n#### Multisig Addresses\nFor a new multisig public keys, we define the `typ` parameter not based on any encoding scheme (amino or protobuf). This avoids issues with non-determinism in the encoding scheme.\nExample:\n```protobuf\npackage cosmos.crypto.multisig;\nmessage PubKey {\nuint32 threshold = 1;\nrepeated google.protobuf.Any pubkeys = 2;\n}\n```\n```go\nfunc (multisig PubKey) Address() {\n// first gather all nested pub keys\nvar keys []address.Addressable  // cryptotypes.PubKey implements Addressable\nfor _, _key := range multisig.Pubkeys {\nkeys = append(keys, key.GetCachedValue().(cryptotypes.PubKey))\n}\n// form the type from the message name (cosmos.crypto.multisig.PubKey) and the threshold joined together\nprefix := fmt.Sprintf(""%s/%d"", proto.MessageName(multisig), multisig.Threshold)\n// use the Composed function defined above\nreturn address.Composed(prefix, keys)\n}\n```\n### Derived Addresses\nWe must be able to cryptographically derive one address from another one. The derivation process must guarantee hash properties, hence we use the already defined `Hash` function:\n```go\nfunc Derive(address, derivationKey []byte) []byte {\nreturn Hash(address, derivationKey)\n}\n```\n### Module Account Addresses\nA module account will have `""module""` type. Module accounts can have sub accounts. The submodule account will be created based on module name, and sequence of derivation keys. Typically, the first derivation key should be a class of the derived accounts. The derivation process has a defined order: module name, submodule key, subsubmodule key... An example module account is created using:\n```go\naddress.Module(moduleName, key)\n```\nAn example sub-module account is created using:\n```go\ngroupPolicyAddresses := []byte{1}\naddress.Module(moduleName, groupPolicyAddresses, policyID)\n```\nThe `address.Module` function is using `address.Hash` with `""module""` as the type argument, and byte representation of the module name concatenated with submodule key. The two last component must be uniquely separated to avoid potential clashes (example: modulename=""ab"" & submodulekey=""bc"" will have the same derivation key as modulename=""a"" & submodulekey=""bbc"").\nWe use a null byte (`'\x00'`) to separate module name from the submodule key. This works, because null byte is not a part of a valid module name. Finally, the sub-submodule accounts are created by applying the `Derive` function recursively.\nWe could use `Derive` function also in the first step (rather than concatenating module name with zero byte and the submodule key). We decided to do concatenation to avoid one level of derivation and speed up computation.\nFor backward compatibility with the existing `authtypes.NewModuleAddress`, we add a special case in `Module` function: when no derivation key is provided, we fallback to the ""legacy"" implementation.\n```go\nfunc Module(moduleName string, derivationKeys ...[]byte) []byte{\nif len(derivationKeys) == 0 {\nreturn authtypes.NewModuleAddress(moduleName)  // legacy case\n}\nsubmoduleAddress := Hash(""module"", []byte(moduleName) + 0 + key)\nreturn fold((a, k) => Derive(a, k), subsubKeys, submoduleAddress)\n}\n```\n**Example 1**  A lending BTC pool address would be:\n```go\nbtcPool := address.Module(""lending"", btc.Address()})\n```\nIf we want to create an address for a module account depending on more than one key, we can concatenate them:\n```go\nbtcAtomAMM := address.Module(""amm"", btc.Address() + atom.Address()})\n```\n**Example 2**  a smart-contract address could be constructed by:\n```go\nsmartContractAddr = Module(""mySmartContractVM"", smartContractsNamespace, smartContractKey})\n// which equals to:\nsmartContractAddr = Derived(\nModule(""mySmartContractVM"", smartContractsNamespace),\n[]{smartContractKey})\n```\n### Schema Types\nA `typ` parameter used in `Hash` function SHOULD be unique for each account type.\nSince all Cosmos SDK account types are serialized in the state, we propose to use the protobuf message name string.\nExample: all public key types have a unique protobuf message type similar to:\n```protobuf\npackage cosmos.crypto.sr25519;\nmessage PubKey {\nbytes key = 1;\n}\n```\nAll protobuf messages have unique fully qualified names, in this example `cosmos.crypto.sr25519.PubKey`.\nThese names are derived directly from .proto files in a standardized way and used\nin other places such as the type URL in `Any`s. We can easily obtain the name using\n`proto.MessageName(msg)`.\n","**Decision:** Adopt a double-hashing approach with a 32-byte output size, prefixed by a 1-byte key type. This results in a 33-byte address.\n\n**Rationale:**\n\n* This approach provides a collision-resistant and future-proof solution, as the hash function output is 32 bytes long.\n* The use of double hashing further increases the security of the address generation process.\n* The addition of a 1-byte key type prefix ensures that addresses of different types are easily distinguishable, reducing the risk of birthday attacks.\n* This approach is compatible with existing tools and ecosystems, as it maintains the 20-byte address size for legacy addresses.\n* The 33-byte address length is still relatively small and manageable for use in state."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md) and\n[ADR 020](./adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nThis ADR continues from [ADD 020](./adr-020-protobuf-transaction-encoding.md)\nto specify the encoding of queries.\n\n## Decision\n","### Custom Query Definition\nModules define custom queries through a protocol buffers `service` definition.\nThese `service` definitions are generally associated with and used by the\nGRPC protocol. However, the protocol buffers specification indicates that\nthey can be used more generically by any request/response protocol that uses\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\nEach module with custom queries should define a service canonically named `Query`:\n```protobuf\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\n}\n```\n#### Handling of Interface Types\nModules that use interface types and need true polymorphism generally force a\n`oneof` up to the app-level that provides the set of concrete implementations of\nthat interface that the app supports. While app's are welcome to do the same for\nqueries and implement an app-level query service, it is recommended that modules\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\nThere is a concern on the transaction level that the overhead of `Any` is too\nhigh to justify its usage. However for queries this is not a concern, and\nproviding generic module-level queries that use `Any` does not preclude apps\nfrom also providing app-level queries that return use the app-level `oneof`s.\nA hypothetical example for the `gov` module would look something like:\n```protobuf\n// x/gov/types/types.proto\nimport ""google/protobuf/any.proto"";\nservice Query {\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\n}\nmessage AnyProposal {\nProposalBase base = 1;\ngoogle.protobuf.Any content = 2;\n}\n```\n### Custom Query Implementation\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https://github.com/cosmos/gogoproto)\ngrpc plugin, which for a service named `Query` generates an interface named\n`QueryServer` as below:\n```go\ntype QueryServer interface {\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\n}\n```\nThe custom queries for our module are implemented by implementing this interface.\nThe first parameter in this generated interface is a generic `context.Context`,\nwhereas querier methods generally need an instance of `sdk.Context` to read\nfrom the store. Since arbitrary values can be attached to `context.Context`\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\n`context.Context`.\nAn example implementation of `QueryBalance` for the bank module as above would\nlook something like:\n```go\ntype Querier struct {\nKeeper\n}\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\nreturn &balance, nil\n}\n```\n### Custom Query Registration and Routing\nQuery server implementations as above would be registered with `AppModule`s using\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\nas below:\n```go\n// x/bank/module.go\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\n}\n```\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\nquery routing table (with the routing method being described below).\nThe signature for this method matches the existing\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\nquery server implementation described above.\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\nand method name (ex. `QueryBalance`) combined with `/`s to form a full\nmethod name (ex. `/cosmos_sdk.x.bank.v1.Query/QueryBalance`). This gets translated\ninto an ABCI query as `custom/cosmos_sdk.x.bank.v1.Query/QueryBalance`. Service handlers\nregistered with `QueryRouter.RegisterService` will be routed this way.\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\n`sdk.Query` and `QueryRouter` infrastructure.\nThis basic specification allows us to reuse protocol buffer `service` definitions\nfor ABCI custom queries substantially reducing the need for manual decoding and\nencoding in query methods.\n### GRPC Protocol Support\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\nproxy server that routes requests in the GRPC protocol to ABCI query requests\nunder the hood. In this way, clients could use their host languages' existing\nGRPC implementations to make direct queries against Cosmos SDK app's using\nthese `service` definitions. In order for this server to work, the `QueryRouter`\non `BaseApp` will need to expose the service handlers registered with\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\nlaunch the proxy server on a separate port in the same process as the ABCI app\nwith a command-line flag.\n### REST Queries and Swagger Generation\n[grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway) is a project that\ntranslates REST calls into GRPC calls using special annotations on service\nmethods. Modules that want to expose REST queries should add `google.api.http`\nannotations to their `rpc` methods as in this example below.\n```protobuf\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balance/{address}/{denom}""\n};\n}\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balances/{address}""\n};\n}\n}\n```\ngrpc-gateway will work directly against the GRPC proxy described above which will\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\ngenerate Swagger definitions automatically.\nIn the current implementation of REST queries, each module needs to implement\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\napproach, there will be no need to generate separate REST query handlers, just\nquery servers as described above as grpc-gateway handles the translation of protobuf\nto REST as well as Swagger definitions.\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\na separate process or the same process as the ABCI app, as well as provide a\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\nfile.\n### Client Usage\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\ninterface like:\n```go\ntype QueryClient interface {\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\n}\n```\nVia a small patch to gogo protobuf ([gogo/protobuf#675](https://github.com/gogo/protobuf/pull/675))\nwe have tweaked the grpc codegen to use an interface rather than concrete type\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\nfor ABCI client queries.\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\nthat routes calls to ABCI queries\nClients (such as CLI methods) will then be able to call query methods like this:\n```go\nclientCtx := client.NewContext()\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\nparams := &types.QueryBalanceParams{addr, denom}\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\n```\n### Testing\nTests would be able to create a query client directly from keeper and `sdk.Context`\nreferences using a `QueryServerTestHelper` as below:\n```go\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\nqueryClient := types.NewQueryClient(queryHelper)\n```\n","To encode queries in Protocol Buffers, we will leverage the existing `QueryRequest` and `QueryResponse` types from the `google.cloud.grpc` protocol buffers package. These types provide a common interface for all queries, regardless of their specific type, and allow for the inclusion of arbitrary data in the query and response."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, in the Cosmos SDK, modules that require the use of parameters use the\n`x/params` module. The `x/params` works by having modules define parameters,\ntypically via a simple `Params` structure, and registering that structure in\nthe `x/params` module via a unique `Subspace` that belongs to the respective\nregistering module. The registering module then has unique access to its respective\n`Subspace`. Through this `Subspace`, the module can get and set its `Params`\nstructure.\nIn addition, the Cosmos SDK's `x/gov` module has direct support for changing\nparameters on-chain via a `ParamChangeProposal` governance proposal type, where\nstakeholders can vote on suggested parameter changes.\nThere are various tradeoffs to using the `x/params` module to manage individual\nmodule parameters. Namely, managing parameters essentially comes for ""free"" in\nthat developers only need to define the `Params` struct, the `Subspace`, and the\nvarious auxiliary functions, e.g. `ParamSetPairs`, on the `Params` type. However,\nthere are some notable drawbacks. These drawbacks include the fact that parameters\nare serialized in state via JSON which is extremely slow. In addition, parameter\nchanges via `ParamChangeProposal` governance proposals have no way of reading from\nor writing to state. In other words, it is currently not possible to have any\nstate transitions in the application during an attempt to change param(s).\n\n## Decision\n","We will build off of the alignment of `x/gov` and `x/authz` work per\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810). Namely, module developers\nwill create one or more unique parameter data structures that must be serialized\nto state. The Param data structures must implement `sdk.Msg` interface with respective\nProtobuf Msg service method which will validate and update the parameters with all\nnecessary changes. The `x/gov` module via the work done in\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810), will dispatch Param\nmessages, which will be handled by Protobuf Msg services.\nNote, it is up to developers to decide how to structure their parameters and\nthe respective `sdk.Msg` messages. Consider the parameters currently defined in\n`x/auth` using the `x/params` module for parameter management:\n```protobuf\nmessage Params {\nuint64 max_memo_characters       = 1;\nuint64 tx_sig_limit              = 2;\nuint64 tx_size_cost_per_byte     = 3;\nuint64 sig_verify_cost_ed25519   = 4;\nuint64 sig_verify_cost_secp256k1 = 5;\n}\n```\nDevelopers can choose to either create a unique data structure for every field in\n`Params` or they can create a single `Params` structure as outlined above in the\ncase of `x/auth`.\nIn the former, `x/params`, approach, a `sdk.Msg` would need to be created for every single\nfield along with a handler. This can become burdensome if there are a lot of\nparameter fields. In the latter case, there is only a single data structure and\nthus only a single message handler, however, the message handler might have to be\nmore sophisticated in that it might need to understand what parameters are being\nchanged vs what parameters are untouched.\nParams change proposals are made using the `x/gov` module. Execution is done through\n`x/authz` authorization to the root `x/gov` module's account.\nContinuing to use `x/auth`, we demonstrate a more complete example:\n```go\ntype Params struct {\nMaxMemoCharacters      uint64\nTxSigLimit             uint64\nTxSizeCostPerByte      uint64\nSigVerifyCostED25519   uint64\nSigVerifyCostSecp256k1 uint64\n}\ntype MsgUpdateParams struct {\nMaxMemoCharacters      uint64\nTxSigLimit             uint64\nTxSizeCostPerByte      uint64\nSigVerifyCostED25519   uint64\nSigVerifyCostSecp256k1 uint64\n}\ntype MsgUpdateParamsResponse struct {}\nfunc (ms msgServer) UpdateParams(goCtx context.Context, msg *types.MsgUpdateParams) (*types.MsgUpdateParamsResponse, error) {\nctx := sdk.UnwrapSDKContext(goCtx)\n// verification logic...\n// persist params\nparams := ParamsFromMsg(msg)\nms.SaveParams(ctx, params)\nreturn &types.MsgUpdateParamsResponse{}, nil\n}\nfunc ParamsFromMsg(msg *types.MsgUpdateParams) Params {\n// ...\n}\n```\nA gRPC `Service` query should also be provided, for example:\n```protobuf\nservice Query {\n// ...\nrpc Params(QueryParamsRequest) returns (QueryParamsResponse) {\noption (google.api.http).get = ""/cosmos/<module>/v1beta1/params"";\n}\n}\nmessage QueryParamsResponse {\nParams params = 1 [(gogoproto.nullable) = false];\n}\n```\n","**Decision:**\n\nReplace the `x/params` module with a more efficient and extensible parameter management system that addresses the drawbacks of the current approach, while preserving the benefits of parameter management in the Cosmos SDK.\n\n**Rationale:**\n\n* **Improved Performance:** The current JSON serialization of parameters in state is inefficient and can lead to performance bottlenecks. A more efficient binary serialization mechanism would significantly improve performance.\n* **State Transitions in Governance Proposals:** The current `ParamChangeProposal` governance proposals cannot read from or write to state. This limitation prevents more complex state transitions during parameter changes. A new system should allow for such state transitions to enable a wider range of parameter management use cases.\n* **Parameter Dependency and Validation:** The current system does not provide a mechanism to define dependencies or perform validation on parameter changes. A more robust parameter management system should allow for these capabilities to ensure the integrity of parameter changes.\n* **Extensibility:** The current `x/params` module is not easily extensible for modules that require additional parameter management functionality. A more extensible system would allow modules to customize parameter handling based on their specific needs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https://docs.cosmos.network/main/learn/advanced/ocap#ocaps-in-practice), it is stated that:\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\n### `x/bank` Case Study\nCurrently the `x/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module’s\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\nHowever, these permissions don’t really do much. They control what modules can be referenced in the `MintCoins`,\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access —\njust a simple string. So the `x/upgrade` module could mint tokens for the `x/staking` module simple by calling\n`MintCoins(“staking”)`. Furthermore, all modules which have access to these keeper methods, also have access to\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\n\n## Decision\n","Based on [ADR-021](./adr-021-protobuf-query-encoding.md) and [ADR-031](./adr-031-msg-service.md), we introduce the\nInter-Module Communication framework for secure module authorization and OCAPs.\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\nOf particular note — the decision is to _enable_ this functionality for modules to adopt at their own discretion.\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\naddressed as amendments to this ADR.\n### New ""Keeper"" Paradigm\nIn [ADR 021](./adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\nwas introduced and in [ADR 31](./adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg/Send` message type:\n```go\npackage bank\ntype MsgClient interface {\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\n}\ntype MsgServer interface {\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\n}\n```\n[ADR 021](./adr-021-protobuf-query-encoding.md) and [ADR 31](./adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\nbased service interfaces already used by clients for inter-module communication.\nUsing this `QueryClient`/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\n1. Protobuf types are checked for breaking changes using [buf](https://buf.build/docs/breaking-overview) and because of\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\nevolution.\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\nobject capability system (see below).\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\nenabling atomicy of operations ([currently a problem](https://github.com/cosmos/cosmos-sdk/issues/8030)). Any failure within a module-to-module call would result in a failure of the entire\ntransaction\nThis mechanism has the added benefits of:\n* reducing boilerplate through code generation, and\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\n### Inter-module Communication\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https://github.com/grpc/grpc-go/blob/v1.49.x/clientconn.go#L441-L450)\nimplementation. For this we introduce\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the ""private\nkey"" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\ndescribed in more detail below.\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole ""signer"". It's worth to note\nthat we don't use any cryptographic signature in this case.\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `/cosmos.bank.Msg/Send` transaction. `MsgSend` validation\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\nHere's an example of a hypothetical module `foo` interacting with `x/bank`:\n```go\npackage foo\ntype FooMsgServer {\n// ...\nbankQuery bank.QueryClient\nbankMsg   bank.MsgClient\n}\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\n// ...\nreturn FooMsgServer {\n// ...\nmodouleKey: moduleKey,\nbankQuery: bank.NewQueryClient(moduleKey),\nbankMsg: bank.NewMsgClient(moduleKey),\n}\n}\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: ""foo""})\n...\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\n...\n}\n```\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\ndenom prefix being restricted to certain modules (as discussed in\n[#7459](https://github.com/cosmos/cosmos-sdk/pull/7459#discussion_r529545528)).\n### `ModuleKey`s and `ModuleID`s\nA `ModuleKey` can be thought of as a ""private key"" for a module account and a `ModuleID` can be thought of as the\ncorresponding ""public key"". From the [ADR 028](./adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\nderivation path. `ModuleID` is a simple struct which contains the module name and optional ""derivation"" path,\nand forms its address based on the `AddressHash` method from [the ADR-028](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-028-public-key-addresses.md):\n```go\ntype ModuleID struct {\nModuleName string\nPath []byte\n}\nfunc (key ModuleID) Address() []byte {\nreturn AddressHash(key.ModuleName, key.Path)\n}\n```\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\nthe `ModuleKey` security.\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\n```go\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\ntype CallInfo {\nMethod string\nCaller ModuleID\n}\ntype RootModuleKey struct {\nmoduleName string\ninvoker Invoker\n}\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { /* ... */}\ntype DerivedModuleKey struct {\nmoduleName string\npath []byte\ninvoker Invoker\n}\n```\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\n```go\npackage foo\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\nbankMsgClient := bank.NewMsgClient(derivedKey)\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\n...\n}\n```\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\nfrom either the root or a derived module account to pass through.\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\nchecking permissions.\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\nname impersonation.\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\n```go\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\nreturn f(ctx, args, reply)\n}\n```\n### `AppModule` Wiring and Requirements\nIn [ADR 031](./adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\nspecify their dependencies on other modules using `RequireServer()`:\n```go\ntype Configurator interface {\nMsgServer() grpc.Server\nQueryServer() grpc.Server\nModuleKey() ModuleKey\nRequireServer(msgServer interface{})\n}\n```\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\nmodule `foo` could declare it's dependency on `x/bank` like this:\n```go\npackage foo\nfunc (am AppModule) RegisterServices(cfg Configurator) {\ncfg.RequireServer((*bank.QueryServer)(nil))\ncfg.RequireServer((*bank.MsgServer)(nil))\n}\n```\n### Security Considerations\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\nthe underlying router infrastructure.\n#### Recursion and Re-entry\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\ncalls Module B and Module B calls module A again in the same call.\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\nin the router could be used to perform this security check.\n#### Queries\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\n### Internal Methods\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\npurpose, we add the `InternalServer` method to `Configurator`:\n```go\ntype Configurator interface {\nMsgServer() grpc.Server\nQueryServer() grpc.Server\nInternalServer() grpc.Server\n}\n```\nAs an example, x/slashing's Slash must call x/staking's Slash, but we don't want to expose x/staking's Slash to end users\nand clients.\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\nproto package.\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\nAn alternative solution to internal-only methods could involve hooks / plugins as discussed [here](https://github.com/cosmos/cosmos-sdk/pull/7459#issuecomment-733807753).\nA more detailed evaluation of a hooks / plugin system will be addressed later in follow-ups to this ADR or as a separate\nADR.\n### Authorization\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-030-authz-module.md).\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\nAuthorization middleware should take into account the need to grant certain modules effectively ""admin"" privileges to\nother modules. This will be addressed in separate ADRs or updates to this ADR.\n### Future Work\nOther future improvements may include:\n* custom code generation that:\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\n* code generation which makes inter-module communication more performant\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\n* inter-module hooks and plugins\n","Implement a proper object-capability system for the Cosmos SDK. This system should include the following features:\n- A unique object capability token for each object.\n- A set of permissions associated with each object capability token.\n- A mechanism for enforcing the permissions associated with each object capability token.\n\nThis system will provide a more secure and flexible way to manage access to objects in the Cosmos SDK. It will also make it easier to build and compose modules, and it will help to promote the development of a thriving ecosystem of Cosmos SDK modules."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to support building highly secure, robust and interoperable blockchain\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\nevidence can be submitted, evaluated and verified resulting in some agreed upon\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\nFurthermore, such a mechanism is paramount for any\n[IBC](https://github.com/cosmos/ibc) or\ncross-chain validation protocol implementation in order to support the ability\nfor any misbehavior to be relayed back from a collateralized chain to a primary\nchain so that the equivocating validator(s) can be slashed.\n\n## Decision\n","We will implement an evidence module in the Cosmos SDK supporting the following\nfunctionality:\n* Provide developers with the abstractions and interfaces necessary to define\ncustom evidence messages, message handlers, and methods to slash and penalize\naccordingly for misbehavior.\n* Support the ability to route evidence messages to handlers in any module to\ndetermine the validity of submitted misbehavior.\n* Support the ability, through governance, to modify slashing penalties of any\nevidence type.\n* Querier implementation to support querying params, evidence types, params, and\nall submitted valid misbehavior.\n### Types\nFirst, we define the `Evidence` interface type. The `x/evidence` module may implement\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\nIn addition, other modules may implement their own `Evidence` types in a similar\nmanner in which governance is extensible. It is important to note any concrete\ntype implementing the `Evidence` interface may include arbitrary fields such as\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\nWhen submitting evidence to the `x/evidence` module, the concrete type must provide\nthe validator's consensus address, which should be known by the `x/slashing`\nmodule (assuming the infraction is valid), the height at which the infraction\noccurred and the validator's power at same height in which the infraction occurred.\n```go\ntype Evidence interface {\nRoute() string\nType() string\nString() string\nHash() HexBytes\nValidateBasic() error\n// The consensus address of the malicious validator at time of infraction\nGetConsensusAddress() ConsAddress\n// Height at which the infraction occurred\nGetHeight() int64\n// The total power of the malicious validator at time of infraction\nGetValidatorPower() int64\n// The total validator set power at time of infraction\nGetTotalPower() int64\n}\n```\n### Routing & Handling\nEach `Evidence` type must map to a specific unique route and be registered with\nthe `x/evidence` module. It accomplishes this through the `Router` implementation.\n```go\ntype Router interface {\nAddRoute(r string, h Handler) Router\nHasRoute(r string) bool\nGetRoute(path string) Handler\nSeal()\n}\n```\nUpon successful routing through the `x/evidence` module, the `Evidence` type\nis passed through a `Handler`. This `Handler` is responsible for executing all\ncorresponding business logic necessary for verifying the evidence as valid. In\naddition, the `Handler` may execute any necessary slashing and potential jailing.\nSince slashing fractions will typically result from some form of static functions,\nallow the `Handler` to do this provides the greatest flexibility. An example could\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\nby governance. The `Evidence` type should provide all the external information\nnecessary in order for the `Handler` to make the necessary state transitions.\nIf no error is returned, the `Evidence` is considered valid.\n```go\ntype Handler func(Context, Evidence) error\n```\n### Submission\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\nhandled by the `x/evidence` module's `SubmitEvidence`.\n```go\ntype MsgSubmitEvidence struct {\nEvidence\n}\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\nreturn err.Result()\n}\n// emit events...\nreturn Result{\n// ...\n}\n}\n```\nThe `x/evidence` module's keeper is responsible for matching the `Evidence` against\nthe module's router and invoking the corresponding `Handler` which may include\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\n```go\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\nhandler := keeper.router.GetRoute(evidence.Route())\nif err := handler(ctx, evidence); err != nil {\nreturn ErrInvalidEvidence(keeper.codespace, err)\n}\nkeeper.setEvidence(ctx, evidence)\nreturn nil\n}\n```\n### Genesis\nFinally, we need to represent the genesis state of the `x/evidence` module. The\nmodule only needs a list of all submitted valid infractions and any necessary params\nfor which the module needs in order to handle submitted evidence. The `x/evidence`\nmodule will naturally define and route native evidence types for which it'll most\nlikely need slashing penalty constants for.\n```go\ntype GenesisState struct {\nParams       Params\nInfractions  []Evidence\n}\n```\n","Implement a fully decentralized slashing protocol, encapsulating the functionality into a separate module and define a corresponding message set and client service interface. The slashing module should provide the following functionality:\n\n- A mechanism for proposing and storing evidence of validator misbehavior.\n- A mechanism for evaluating and verifying evidence, resulting in some agreed upon penalty for any misbehavior committed by a validator.\n- A mechanism for updating the validator set to reflect the penalty (e.g. slashing the validator's stake)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving the ability to sign messages off-chain has proven to be a fundamental aspect of nearly any blockchain. The notion of signing messages off-chain has many added benefits such as saving on computational costs and reducing transaction throughput and overhead. Within the context of the Cosmos, some of the major applications of signing such data includes, but is not limited to, providing a cryptographic secure and verifiable means of proving validator identity and possibly associating it with some other framework or organization. In addition, having the ability to sign Cosmos messages with a Ledger or similar HSM device.\nFurther context and use cases can be found in the references links.\n\n## Decision\n","The aim is being able to sign arbitrary messages, even using Ledger or similar HSM devices.\nAs a result signed messages should look roughly like Cosmos SDK messages but **must not** be a valid on-chain transaction. `chain-id`, `account_number` and `sequence` can all be assigned invalid values.\nCosmos SDK 0.40 also introduces a concept of “auth_info” this can specify SIGN_MODES.\nA spec should include an `auth_info` that supports SIGN_MODE_DIRECT and SIGN_MODE_LEGACY_AMINO.\nCreate the `offchain` proto definitions, we extend the auth module with `offchain` package to offer functionalities to verify and sign offline messages.\nAn offchain transaction follows these rules:\n* the memo must be empty\n* nonce, sequence number must be equal to 0\n* chain-id must be equal to “”\n* fee gas must be equal to 0\n* fee amount must be an empty array\nVerification of an offchain transaction follows the same rules as an onchain one, except for the spec differences highlighted above.\nThe first message added to the `offchain` package is `MsgSignData`.\n`MsgSignData` allows developers to sign arbitrary bytes valid offchain only. Where `Signer` is the account address of the signer. `Data` is arbitrary bytes which can represent `text`, `files`, `object`s. It's applications developers decision how `Data` should be deserialized, serialized and the object it can represent in their context.\nIt's applications developers decision how `Data` should be treated, by treated we mean the serialization and deserialization process and the Object `Data` should represent.\nProto definition:\n```protobuf\n// MsgSignData defines an arbitrary, general-purpose, off-chain message\nmessage MsgSignData {\n// Signer is the sdk.AccAddress of the message signer\nbytes Signer = 1 [(gogoproto.jsontag) = ""signer"", (gogoproto.casttype) = ""github.com/cosmos/cosmos-sdk/types.AccAddress""];\n// Data represents the raw bytes of the content that is signed (text, json, etc)\nbytes Data = 2 [(gogoproto.jsontag) = ""data""];\n}\n```\nSigned MsgSignData json example:\n```json\n{\n""type"": ""cosmos-sdk/StdTx"",\n""value"": {\n""msg"": [\n{\n""type"": ""sign/MsgSignData"",\n""value"": {\n""signer"": ""cosmos1hftz5ugqmpg9243xeegsqqav62f8hnywsjr4xr"",\n""data"": ""cmFuZG9t""\n}\n}\n],\n""fee"": {\n""amount"": [],\n""gas"": ""0""\n},\n""signatures"": [\n{\n""pub_key"": {\n""type"": ""tendermint/PubKeySecp256k1"",\n""value"": ""AqnDSiRoFmTPfq97xxEb2VkQ/Hm28cPsqsZm9jEVsYK9""\n},\n""signature"": ""8y8i34qJakkjse9pOD2De+dnlc4KvFgh0wQpes4eydN66D9kv7cmCEouRrkka9tlW9cAkIL52ErB+6ye7X5aEg==""\n}\n],\n""memo"": """"\n}\n}\n```\n","**Decision:** Implement an off-chain message signing capability.\n\n**Rationale:**\n\n* Off-chain message signing provides significant benefits, including:\n    * Reduced computational costs\n    * Reduced transaction throughput and overhead\n* Key applications within the Cosmos context include:\n    * Cryptographically secure and verifiable validator identity\n    * Association with other frameworks or organizations\n    * Signing messages with Ledger or HSM devices\n\n**Implications:**\n\n* New API endpoints or functionality will need to be developed to support off-chain message signing.\n* Security considerations must be addressed, including key management and authorization.\n* Interoperability with other Cosmos applications and protocols must be ensured.\n\n**Additional Considerations:**\n\n* The specific implementation approach should be carefully considered, balancing security, performance, and usability.\n* The off-chain message signing capability should be well-documented and easily accessible to developers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\nBy default, when telemetry is enabled, the application should track and expose metrics that are\nstored in-memory. The operator may choose to enable additional sinks, where we support only\n[Prometheus](https://prometheus.io/) for now, as it's battle-tested, simple to setup, open source,\nand is rich with ecosystem tooling.\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\nmetrics may be added or removed at will and without much friction. To do this, we will use the\n[go-metrics](https://github.com/hashicorp/go-metrics) library.\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\nwill be exposed via `/metrics?format={text|prometheus}` via the API server.\n\n## Decision\n","We will add an additional configuration block to `app.toml` that defines telemetry settings:\n```toml\n###############################################################################\n###                         Telemetry Configuration                         ###\n###############################################################################\n[telemetry]\n# Prefixed with keys to separate services\nservice-name = {{ .Telemetry.ServiceName }}\n# Enabled enables the application telemetry functionality. When enabled,\n# an in-memory sink is also enabled by default. Operators may also enabled\n# other sinks such as Prometheus.\nenabled = {{ .Telemetry.Enabled }}\n# Enable prefixing gauge values with hostname\nenable-hostname = {{ .Telemetry.EnableHostname }}\n# Enable adding hostname to labels\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\n# Enable adding service to labels\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\n```\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\n```go\n// Metrics defines a wrapper around application telemetry functionality. It allows\n// metrics to be gathered at any point in time. When creating a Metrics object,\n// internally, a global metrics is registered with a set of sinks as configured\n// by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\n// dump of formatted recent metrics will be sent to STDERR.\ntype Metrics struct {\nmemSink           *metrics.InmemSink\nprometheusEnabled bool\n}\n// Gather collects all registered metrics and returns a GatherResponse where the\n// metrics are encoded depending on the type. Metrics are either encoded via\n// Prometheus or JSON if in-memory.\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\nswitch format {\ncase FormatPrometheus:\nreturn m.gatherPrometheus()\ncase FormatText:\nreturn m.gatherGeneric()\ncase FormatDefault:\nreturn m.gatherGeneric()\ndefault:\nreturn GatherResponse{}, fmt.Errorf(""unsupported metrics format: %s"", format)\n}\n}\n```\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\nhandler accordingly.\n```go\nfunc (s *Server) Start(cfg config.Config) error {\n// ...\nif cfg.Telemetry.Enabled {\nm, err := telemetry.New(cfg.Telemetry)\nif err != nil {\nreturn err\n}\ns.metrics = m\ns.registerMetrics()\n}\n// ...\n}\nfunc (s *Server) registerMetrics() {\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\nformat := strings.TrimSpace(r.FormValue(""format""))\ngr, err := s.metrics.Gather(format)\nif err != nil {\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(""failed to gather metrics: %s"", err))\nreturn\n}\nw.Header().Set(""Content-Type"", gr.ContentType)\n_, _ = w.Write(gr.Metrics)\n}\ns.Router.HandleFunc(""/metrics"", metricsHandler).Methods(""GET"")\n}\n```\nApplication developers may track counters, gauges, summaries, and key/value metrics. There is no\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\n```go\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\ndefer metrics.MeasureSince(time.Now(), ""MintCoins"")\n// ...\n}\n```\n","Enable telemetry by default, exposing metrics in-memory. Support multiple configurable sinks, including Prometheus. Use the go-metrics library for seamless metrics integration within the Cosmos SDK. Allow operators to enable telemetry and configure it through specific configuration options. Expose metrics via `/metrics?format={text|prometheus}` if telemetry is enabled."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\nIdeally, we would have:\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\n\n## Decision\n","Re-structure the `/docs` folder of the Cosmos SDK github repo as follows:\n```text\ndocs/\n├── README\n├── intro/\n├── concepts/\n│   ├── baseapp\n│   ├── types\n│   ├── store\n│   ├── server\n│   ├── modules/\n│   │   ├── keeper\n│   │   ├── handler\n│   │   ├── cli\n│   ├── gas\n│   └── commands\n├── clients/\n│   ├── lite/\n│   ├── service-providers\n├── modules/\n├── spec/\n├── translations/\n└── architecture/\n```\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\n* `README`: Landing page of the docs.\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https://github.com/cosmos/sdk-application-tutorial/) will be highlighted, as well as the `godocs`.\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\n* `spec`: Contains specs of modules, and others.\n* `modules`: Contains links to `godocs` and the spec of the modules.\n* `architecture`: Contains architecture-related docs like the present one.\n* `translations`: Contains different translations of the documentation.\nWebsite docs sidebar will only include the following sections:\n* `README`\n* `intro`\n* `concepts`\n* `clients`\n`architecture` need not be displayed on the website.\n","All documentation related to development frameworks or tools should be moved to their respective GitHub repositories. This includes the SDK repository, the Hub repository, the Lotion repository, and so on. All other documentation, such as FAQs, whitepapers, and high-level material about Cosmos, should be moved to the Cosmos website."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFull implementation of the [IBC specification](https://github.com/cosmos/ibc) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\nas described in [ICS 5](https://github.com/cosmos/ibc/tree/master/spec/core/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\nobject-capability keys at this time.\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L132))\nand passed to Keepers as fixed arguments ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L160)). Keepers cannot create or store capability keys during transaction execution — although they could call `NewKVStoreKey` and take the memory address\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional — were this not the case, the keys would be predictable and couldn't serve as object capabilities).\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\nThis ADR proposes such an interface & mechanism.\n\n## Decision\n","The Cosmos SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\ninvoked.\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\nchain is running.\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\nThe persistent `KVStore` tracks which capability is owned by which modules.\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\na reverse mapping that map from module name, capability name to the capability index.\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\ngo-map to retrieve the capability at the original memory location.\nThe `CapabilityKeeper` will define the following types & functions:\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\na name. A `String()` method is provided for debugging.\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\n```go\ntype Capability struct {\nindex uint64\n}\n```\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\n```go\ntype CapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleNames   map[string]interface{}\nsealed        bool\n}\n```\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\npassed by other modules.\n```go\ntype ScopedCapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleName    string\n}\n```\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\nIt MUST be called before `InitialiseAndSeal`.\n```go\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\nif ck.sealed {\npanic(""cannot scope to module via a sealed capability keeper"")\n}\nif _, ok := ck.scopedModules[moduleName]; ok {\npanic(fmt.Sprintf(""cannot create multiple scoped keepers for the same module name: %s"", moduleName))\n}\nck.scopedModules[moduleName] = struct{}{}\nreturn ScopedKeeper{\ncdc:      ck.cdc,\nstoreKey: ck.storeKey,\nmemKey:   ck.memKey,\ncapMap:   ck.capMap,\nmodule:   moduleName,\n}\n}\n```\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\ncreation of any new `ScopedCapabilityKeeper`s.\n```go\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\nif ck.sealed {\npanic(""capability keeper is sealed"")\n}\npersistentStore := ctx.KVStore(ck.persistentKey)\nmap := ctx.KVStore(ck.memKey)\n// initialise memory store for all names in persistent store\nfor index, value := range persistentStore.Iter() {\ncapability = &CapabilityKey{index: index}\nfor moduleAndCapability := range value {\nmoduleName, capabilityName := moduleAndCapability.Split(""/"")\nmemStore.Set(moduleName + ""/fwd/"" + capability, capabilityName)\nmemStore.Set(moduleName + ""/rev/"" + capabilityName, index)\nck.capMap[index] = capability\n}\n}\nck.sealed = true\n}\n```\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\nreference. The newly created capability is automatically persisted; the calling module need not\ncall `ClaimCapability`.\n```go\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\n// check name not taken in memory store\nif capStore.Get(""rev/"" + name) != nil {\nreturn nil, errors.New(""name already taken"")\n}\n// fetch the current index\nindex := persistentStore.Get(""index"")\n// create a new capability\ncapability := &CapabilityKey{index: index}\n// set persistent store\npersistentStore.Set(index, Set.singleton(sck.moduleName + ""/"" + name))\n// update the index\nindex++\npersistentStore.Set(""index"", index)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to index\nmemStore.Set(sck.moduleName + ""/rev/"" + name, index)\n// set the in-memory mapping from index to capability pointer\ncapMap[index] = capability\n// return the newly created capability\nreturn capability\n}\n```\n`AuthenticateCapability` can be called by any module to check that a capability\ndoes in fact correspond to a particular name (the name can be untrusted user input)\nwith which the calling module previously associated it.\n```go\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\n// return whether forward mapping in memory store matches name\nreturn memStore.Get(sck.moduleName + ""/fwd/"" + capability) === name\n}\n```\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\nso that future `GetCapability` calls will succeed.\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\nthey will all own it.\n```go\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\npersistentStore := ctx.KVStore(sck.persistentKey)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to capability\nmemStore.Set(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.add(sck.moduleName + ""/"" + name)\npersistentStore.Set(capability.Index(), owners)\n}\n```\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\nThe module is not allowed to retrieve capabilities which it does not own.\n```go\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\n// fetch the index of capability using reverse mapping in memstore\nindex := memStore.Get(sck.moduleName + ""/rev/"" + name)\n// fetch capability from go-map using index\ncapability := capMap[index]\n// return the capability\nreturn capability\n}\n```\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\nmore owners exist, the capability will be deleted globally.\n```go\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\npersistentStore := ctx.KVStore(sck.persistentKey)\nname := capStore.Get(sck.moduleName + ""/fwd/"" + capability)\nif name == nil {\nreturn error(""capability not owned by module"")\n}\n// delete forward mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/fwd/"" + capability, name)\n// delete reverse mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.remove(sck.moduleName + ""/"" + name)\nif owners.size() > 0 {\n// there are still other owners, keep the capability around\npersistentStore.Set(capability.Index(), owners)\n} else {\n// no more owners, delete the capability\npersistentStore.Delete(capability.Index())\ndelete(capMap[capability.Index()])\n}\n}\n```\n### Usage patterns\n#### Initialisation\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\n```go\nck := NewCapabilityKeeper(persistentKey, memoryKey)\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(""mod1""), ....)\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(""mod2""), ....)\n// other initialisation logic ...\n// load initial state...\nck.InitialiseAndSeal(initialContext)\n```\n#### Creating, passing, claiming and using capabilities\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\nModule 1 would have the following code:\n```go\ncapability := scopedCapabilityKeeper.NewCapability(ctx, ""resourceABC"")\nmod2Keeper.SomeFunction(ctx, capability, args...)\n```\n`SomeFunction`, running in module 2, could then claim the capability:\n```go\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\nk.sck.ClaimCapability(ctx, capability, ""resourceABC"")\n// other logic...\n}\n```\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\n```go\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\ncapability := k.sck.GetCapability(ctx, name)\nmod1.UseResource(ctx, capability, ""resourceABC"")\n}\n```\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\n```go\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\nif !k.sck.AuthenticateCapability(name, capability) {\nreturn errors.New(""unauthenticated"")\n}\n// do something with the resource\n}\n```\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\n","**Decision:** Implement an interface and mechanism for Keepers to create and store object-capability keys during transaction execution. This interface should include methods for creating new keys, storing keys, and regenerating keys on application start or restart. The mechanism should be able to revert capability creation on tx failure."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nSpecifically, the client-side migration path primarily includes tx generation and\nsigning, message construction and routing, in addition to CLI & REST handlers and\nbusiness logic (i.e. queriers).\nWith this in mind, we will tackle the migration path via two main areas, txs and\nquerying. However, this ADR solely focuses on transactions. Querying should be\naddressed in a future ADR, but it should build off of these proposals.\nBased on detailed discussions ([\#6030](https://github.com/cosmos/cosmos-sdk/issues/6030)\nand [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078)), the original\ndesign for transactions was changed substantially from an `oneof` /JSON-signing\napproach to the approach described below.\n\n## Decision\n","### Transactions\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\n`sdk.Msg`s are encoding with `Any` in transactions.\nOne of the main goals of using `Any` to encode interface values is to have a\ncore set of types which is reused by apps so that\nclients can safely be compatible with as many chains as possible.\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\nformat that can serve a wide variety of use cases without breaking client\ncompatibility.\nIn order to facilitate signing, transactions are separated into `TxBody`,\nwhich will be re-used by `SignDoc` below, and `signatures`:\n```protobuf\n// types/types.proto\npackage cosmos_sdk.v1;\nmessage Tx {\nTxBody body = 1;\nAuthInfo auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\n// A variant of Tx that pins the signer's exact binary representation of body and\n// auth_info. This is used for signing, broadcasting and verification. The binary\n// `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\n// becomes the ""txhash"", commonly used as the transaction ID.\nmessage TxRaw {\n// A protobuf serialization of a TxBody that matches the representation in SignDoc.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\nbytes auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\nmessage TxBody {\n// A list of messages to be executed. The required signers of those messages define\n// the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\n// Each required signer address is added to the list only the first time it occurs.\n//\n// By convention, the first required signer (usually from the first message) is referred\n// to as the primary signer and pays the fee for the whole transaction.\nrepeated google.protobuf.Any messages = 1;\nstring memo = 2;\nint64 timeout_height = 3;\nrepeated google.protobuf.Any extension_options = 1023;\n}\nmessage AuthInfo {\n// This list defines the signing modes for the required signers. The number\n// and order of elements must match the required signers from TxBody's messages.\n// The first element is the primary signer and the one which pays the fee.\nrepeated SignerInfo signer_infos = 1;\n// The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\nFee fee = 2;\n}\nmessage SignerInfo {\n// The public key is optional for accounts that already exist in state. If unset, the\n// verifier can use the required signer address for this position and lookup the public key.\ngoogle.protobuf.Any public_key = 1;\n// ModeInfo describes the signing mode of the signer and is a nested\n// structure to support nested multisig pubkey's\nModeInfo mode_info = 2;\n// sequence is the sequence of the account, which describes the\n// number of committed transactions signed by a given address. It is used to prevent\n// replay attacks.\nuint64 sequence = 3;\n}\nmessage ModeInfo {\noneof sum {\nSingle single = 1;\nMulti multi = 2;\n}\n// Single is the mode info for a single signer. It is structured as a message\n// to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\nmessage Single {\nSignMode mode = 1;\n}\n// Multi is the mode info for a multisig public key\nmessage Multi {\n// bitarray specifies which keys within the multisig are signing\nCompactBitArray bitarray = 1;\n// mode_infos is the corresponding modes of the signers of the multisig\n// which could include nested multisig public keys\nrepeated ModeInfo mode_infos = 2;\n}\n}\nenum SignMode {\nSIGN_MODE_UNSPECIFIED = 0;\nSIGN_MODE_DIRECT = 1;\nSIGN_MODE_TEXTUAL = 2;\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\n}\n```\nAs will be discussed below, in order to include as much of the `Tx` as possible\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\nraw signatures themselves live outside of what is signed over.\nBecause we are aiming for a flexible, extensible cross-chain transaction\nformat, new transaction processing options should be added to `TxBody` as soon\nthose use cases are discovered, even if they can't be implemented yet.\nBecause there is coordination overhead in this, `TxBody` includes an\n`extension_options` field which can be used for any transaction processing\noptions that are not already covered. App developers should, nevertheless,\nattempt to upstream important improvements to `Tx`.\n### Signing\nAll of the signing modes below aim to provide the following guarantees:\n* **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\nis signed\n* **Predictable Gas**: if I am signing a transaction where I am paying a fee,\nthe final gas is fully dependent on what I am signing\nThese guarantees give the maximum amount confidence to message signers that\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\n#### `SIGN_MODE_DIRECT`\nThe ""direct"" signing behavior is to sign the raw `TxBody` bytes as broadcast over\nthe wire. This has the advantages of:\n* requiring the minimum additional client capabilities beyond a standard protocol\nbuffers implementation\n* leaving effectively zero holes for transaction malleability (i.e. there are no\nsubtle differences between the signing and encoding formats which could\npotentially be exploited by an attacker)\nSignatures are structured using the `SignDoc` below which reuses the serialization of\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\n```protobuf\n// types/types.proto\nmessage SignDoc {\n// A protobuf serialization of a TxBody that matches the representation in TxRaw.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\nbytes auth_info = 2;\nstring chain_id = 3;\nuint64 account_number = 4;\n}\n```\nIn order to sign in the default mode, clients take the following steps:\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\n2. Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n3. Sign the encoded `SignDoc` bytes.\n4. Build a `TxRaw` and serialize it for broadcasting.\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\nbytes encoded in `TxRaw` not based on any [""canonicalization""](https://github.com/regen-network/canonical-proto3)\nalgorithm which creates added complexity for clients in addition to preventing\nsome forms of upgradeability (to be addressed later in this document).\nSignature verifiers do:\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\n2. Create a list of required signer addresses from the messages.\n3. For each required signer:\n* Pull account number and sequence from the state.\n* Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\n* Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n* Verify the signature at the same list position against the serialized `SignDoc`.\n#### `SIGN_MODE_LEGACY_AMINO`\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\nsupported transaction signing. Once wallets and exchanges have had a\nchance to upgrade to protobuf based signing, this option will be disabled. In\nthe meantime, it is foreseen that disabling the current Amino signing would cause\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\nCosmos Hub and other chains may choose to disable Amino signing immediately.\nLegacy clients will be able to sign a transaction using the current Amino\nJSON format and have it encoded to protobuf using the REST `/tx/encode`\nendpoint before broadcasting.\n#### `SIGN_MODE_TEXTUAL`\nAs was discussed extensively in [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078),\nthere is a desire for a human-readable signing encoding, especially for hardware\nwallets like the [Ledger](https://www.ledger.com) which display\ntransaction contents to users before signing. JSON was an attempt at this but\nfalls short of the ideal.\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\nencoding which will replace Amino JSON. This new encoding should be even more\nfocused on readability than JSON, possibly based on formatting strings like\n[MessageFormat](http://userguide.icu-project.org/formatparse/messages).\nIn order to ensure that the new human-readable format does not suffer from\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\nto generate sign bytes.\nMultiple human-readable formats (maybe even localized messages) may be supported\nby `SIGN_MODE_TEXTUAL` when it is implemented.\n### Unknown Field Filtering\nUnknown fields in protobuf messages should generally be rejected by transaction\nprocessors because:\n* important data may be present in the unknown fields, that if ignored, will\ncause unexpected behavior for clients\n* they present a malleability vulnerability where attackers can bloat tx size\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\nnot `TxBody`)\nThere are also scenarios where we may choose to safely ignore unknown fields\n(https://github.com/cosmos/cosmos-sdk/issues/6078#issuecomment-624400188) to\nprovide graceful forwards compatibility with newer clients.\nWe propose that field numbers with bit 11 set (for most use cases this is\nthe range of 1024-2047) be considered non-critical fields that can safely be\nignored if unknown.\nTo handle this we will need an unknown field filter that:\n* always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\nunsigned parts of `AuthInfo` if present based on the signing mode)\n* rejects unknown fields in all messages (including nested `Any`s) other than\nfields with bit 11 set\nThis will likely need to be a custom protobuf parser pass that takes message bytes\nand `FileDescriptor`s and returns a boolean result.\n### Public Key Encoding\nPublic keys in the Cosmos SDK implement the `cryptotypes.PubKey` interface.\nWe propose to use `Any` for protobuf encoding as we are doing with other interfaces (for example, in `BaseAccount.PubKey` and `SignerInfo.PublicKey`).\nThe following public keys are implemented: secp256k1, secp256r1, ed25519 and legacy-multisignature.\nEx:\n```protobuf\nmessage PubKey {\nbytes key = 1;\n}\n```\n`multisig.LegacyAminoPubKey` has an array of `Any`'s member to support any\nprotobuf public key type.\nApps should only attempt to handle a registered set of public keys that they\nhave tested. The provided signature verification ante handler decorators will\nenforce this.\n### CLI & REST\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\nin the client can be interfaces, similar to how we described in [ADR 019](./adr-019-protobuf-state-encoding.md),\nthe client logic will now need to take a codec interface that knows not only how\nto handle all the types, but also knows how to generate transactions, signatures,\nand messages.\nIf the account is sending its first transaction, the account number must be set to 0. This is due to the account not being created yet.\n```go\ntype AccountRetriever interface {\nGetAccount(clientCtx Context, addr sdk.AccAddress) (client.Account, error)\nGetAccountWithHeight(clientCtx Context, addr sdk.AccAddress) (client.Account, int64, error)\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\n}\ntype Generator interface {\nNewTx() TxBuilder\nNewFee() ClientFee\nNewSignature() ClientSignature\nMarshalTx(tx types.Tx) ([]byte, error)\n}\ntype TxBuilder interface {\nGetTx() sdk.Tx\nSetMsgs(...sdk.Msg) error\nGetSignatures() []sdk.Signature\nSetSignatures(...sdk.Signature)\nGetFee() sdk.Fee\nSetFee(sdk.Fee)\nGetMemo() string\nSetMemo(string)\n}\n```\nWe then update `Context` to have new fields: `Codec`, `TxGenerator`,\nand `AccountRetriever`, and we update `AppModule.GetTxCmd` to take\na `Context` which should have all of these fields pre-populated.\nEach client method should then use one of the `Init` methods to re-initialize\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\ngenerate or broadcast a transaction. For example:\n```go\nimport ""github.com/spf13/cobra""\nimport ""github.com/cosmos/cosmos-sdk/client""\nimport ""github.com/cosmos/cosmos-sdk/client/tx""\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\nreturn &cobra.Command{\nRunE: func(cmd *cobra.Command, args []string) error {\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\nmsg := NewSomeMsg{...}\ntx.GenerateOrBroadcastTx(clientCtx, msg)\n},\n}\n}\n```\n",The decision for this ADR is not provided in the given context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, in the Cosmos SDK, the address of an auth `BaseAccount` is based on the hash of the public key.  Once an account is created, the public key for the account is set in stone, and cannot be changed.  This can be a problem for users, as key rotation is a useful security practice, but is not possible currently.  Furthermore, as multisigs are a type of pubkey, once a multisig for an account is set, it can not be updated.  This is problematic, as multisigs are often used by organizations or companies, who may need to change their set of multisig signers for internal reasons.\nTransferring all the assets of an account to a new account with the updated pubkey is not sufficient, because some ""engagements"" of an account are not easily transferable.  For example, in staking, to transfer bonded Atoms, an account would have to unbond all delegations and wait the three week unbonding period.  Even more significantly, for validator operators, ownership over a validator is not transferrable at all, meaning that the operator key for a validator can never be updated, leading to poor operational security for validators.\n\n## Decision\n","We propose the addition of a new feature to `x/auth` that allows accounts to update the public key associated with their account, while keeping the address the same.\nThis is possible because the Cosmos SDK `BaseAccount` stores the public key for an account in state, instead of making the assumption that the public key is included in the transaction (whether explicitly or implicitly through the signature) as in other blockchains such as Bitcoin and Ethereum.  Because the public key is stored on chain, it is okay for the public key to not hash to the address of an account, as the address is not pertinent to the signature checking process.\nTo build this system, we design a new Msg type as follows:\n```protobuf\nservice Msg {\nrpc ChangePubKey(MsgChangePubKey) returns (MsgChangePubKeyResponse);\n}\nmessage MsgChangePubKey {\nstring address = 1;\ngoogle.protobuf.Any pub_key = 2;\n}\nmessage MsgChangePubKeyResponse {}\n```\nThe MsgChangePubKey transaction needs to be signed by the existing pubkey in state.\nOnce, approved, the handler for this message type, which takes in the AccountKeeper, will update the in-state pubkey for the account and replace it with the pubkey from the Msg.\nAn account that has had its pubkey changed cannot be automatically pruned from state.  This is because if pruned, the original pubkey of the account would be needed to recreate the same address, but the owner of the address may not have the original pubkey anymore.  Currently, we do not automatically prune any accounts anyways, but we would like to keep this option open the road (this is the purpose of account numbers).  To resolve this, we charge an additional gas fee for this operation to compensate for this this externality (this bound gas amount is configured as parameter `PubKeyChangeCost`). The bonus gas is charged inside the handler, using the `ConsumeGas` function.  Furthermore, in the future, we can allow accounts that have rekeyed manually prune themselves using a new Msg type such as `MsgDeleteAccount`.  Manually pruning accounts can give a gas refund as an incentive for performing the action.\n```go\namount := ak.GetParams(ctx).PubKeyChangeCost\nctx.GasMeter().ConsumeGas(amount, ""pubkey change fee"")\n```\nEvery time a key for an address is changed, we will store a log of this change in the state of the chain, thus creating a stack of all previous keys for an address and the time intervals for which they were active.  This allows dapps and clients to easily query past keys for an account which may be useful for features such as verifying timestamped off-chain signed messages.\n","Do not implement key rotation in BaseAccount. Instead, introduce a concept of the 'primary account' and 'subaccounts' that share a common root key."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficiently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion/consideration.\n\n## Decision\n","We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\nThus, we propose the following mechanism:\n### Params\n* The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the default voting period that all governance proposal voting periods start with.\n* There is a new gov param called `MaxVotingPeriodExtension`.\n### Mechanism\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the gauge the conentiousness/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\n### Dealing with Unbonding/Redelegation\nThere is one thing that needs to be addressed.  How to deal with redelegation/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\n### Delegators\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and ""unextend"" as that would contradict the ""voting power length can only be ratcheted up"" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\n",Extend the proposal submission form to allow for different voting periods on a proposal-by-proposal basis.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order for the Cosmos SDK to implement the [IBC specification](https://github.com/cosmos/ics), modules within the Cosmos SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\n\n## Decision\n","The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\n```go\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper) error {\ninfo := HistoricalInfo{\napphash: ctx.HeaderInfo().AppHash,\nTime: ctx.HeaderInfo().Time,\nNextValidatorsHash: ctx.CometInfo().NextValidatorsHash,\n}\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\nn := keeper.GetParamRecentHeadersToStore()\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\n// continue handling request\n}\n```\nAlternatively, the application MAY store only the hash of the validator set.\nThe application MUST make these past `n` committed headers available for querying by Cosmos SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x/staking` or `x/ibc`).\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\n",The Cosmos SDK should leverage the Tendermint gRPC APIs to retrieve consensus states from the Tendermint core.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the Cosmos SDK utilizes [go-amino](https://github.com/tendermint/go-amino/) for binary\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\nFrom the Amino docs:\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\n> support. See the [Proto3 spec](https://developers.google.com/protocol-buffers/docs/proto3) for more\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\n>\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\nAmino also aims to have the following goals (not a complete list):\n* Binary bytes must be decode-able with a schema.\n* Schema must be upgradeable.\n* The encoder and decoder logic must be reasonably simple.\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\nclients written in various languages while providing virtually little in the way of true backwards\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1</sup>. This is\nlargely reflected in the performance of simulations and application transaction throughput.\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\n* Language agnostic\n* Platform agnostic\n* Rich client support and thriving ecosystem\n* High performance\n* Minimal encoded message size\n* Codegen-based over reflection-based\n* Supports backward and forward compatibility\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\nmade to address client-side encoding.\n\n## Decision\n","We will adopt [Protocol Buffers](https://developers.google.com/protocol-buffers) for serializing\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\nwill provide two concrete implementations of the `Marshaler` interface: `AminoCodec` and `ProtoCodec`.\n* `AminoCodec`: Uses Amino for both binary and JSON encoding.\n* `ProtoCodec`: Uses Protobuf for both binary and JSON encoding.\nModules will use whichever codec that is instantiated in the app. By default, the Cosmos SDK's `simapp`\ninstantiates a `ProtoCodec` as the concrete implementation of `Marshaler`, inside the `MakeTestEncodingConfig`\nfunction. This can be easily overwritten by app developers if they so desire.\nThe ultimate goal will be to replace Amino JSON encoding with Protobuf encoding and thus have\nmodules accept and/or extend `ProtoCodec`. Until then, Amino JSON is still provided for legacy use-cases.\nA handful of places in the Cosmos SDK still have Amino JSON hardcoded, such as the Legacy API REST endpoints\nand the `x/params` store. They are planned to be converted to Protobuf in a gradual manner.\n### Module Codecs\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\n`Marshaler` that will be a `ProtoCodec`. This migration is simple as things will just work as-is.\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\n[gogoprotobuf](https://github.com/cosmos/gogoproto) Value types.\nExample:\n```go\nts, err := gogotypes.TimestampProto(completionTime)\nif err != nil {\n// ...\n}\nbz := cdc.MustMarshal(ts)\n```\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\nto the module and will contain method contracts that know how to serialize the needed interfaces.\nExample:\n```go\n// x/auth/types/codec.go\ntype Codec interface {\ncodec.Codec\nMarshalAccount(acc exported.Account) ([]byte, error)\nUnmarshalAccount(bz []byte) (exported.Account, error)\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\n}\n```\n### Usage of `Any` to encode interfaces\nIn general, module-level .proto files should define messages which encode interfaces\nusing [`google.protobuf.Any`](https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/any.proto).\nAfter [extension discussion](https://github.com/cosmos/cosmos-sdk/issues/6030),\nthis was chosen as the preferred alternative to application-level `oneof`s\nas in our original protobuf design. The arguments in favor of `Any` can be\nsummarized as follows:\n* `Any` provides a simpler, more consistent client UX for dealing with\ninterfaces than app-level `oneof`s that will need to be coordinated more\ncarefully across applications. Creating a generic transaction\nsigning library using `oneof`s may be cumbersome and critical logic may need\nto be reimplemented for each chain\n* `Any` provides more resistance against human error than `oneof`\n* `Any` is generally simpler to implement for both modules and apps\nThe main counter-argument to using `Any` centers around its additional space\nand possibly performance overhead. The space overhead could be dealt with using\ncompression at the persistence layer in the future and the performance impact\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\nwith user experience as the higher order concern.\nNote, that given the Cosmos SDK's decision to adopt the `Codec` interfaces described\nabove, apps can still choose to use `oneof` to encode state and transactions\nbut it is not the recommended approach. If apps do choose to use `oneof`s\ninstead of `Any` they will likely lose compatibility with client apps that\nsupport multiple chains. Thus developers should think carefully about whether\nthey care more about what is possibly a pre-mature optimization or end-user\nand client developer UX.\n### Safe usage of `Any`\nBy default, the [gogo protobuf implementation of `Any`](https://pkg.go.dev/github.com/cosmos/gogoproto/types)\nuses [global type registration]( https://github.com/cosmos/gogoproto/blob/master/proto/properties.go#L540)\nto decode values packed in `Any` into concrete\ngo types. This introduces a vulnerability where any malicious module\nin the dependency tree could register a type with the global protobuf registry\nand cause it to be loaded and unmarshaled by a transaction that referenced\nit in the `type_url` field.\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\nvalues into concrete types through the `InterfaceRegistry` interface which\nbears some similarity to type registration with Amino:\n```go\ntype InterfaceRegistry interface {\n// RegisterInterface associates protoName as the public name for the\n// interface passed in as iface\n// Ex:\n//   registry.RegisterInterface(""cosmos_sdk.Msg"", (*sdk.Msg)(nil))\nRegisterInterface(protoName string, iface interface{})\n// RegisterImplementations registers impls as a concrete implementations of\n// the interface iface\n// Ex:\n//  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\nRegisterImplementations(iface interface{}, impls ...proto.Message)\n}\n```\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\nto communicate the list of concrete types that satisfy an interface to clients.\nIn .proto files:\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\nmay be used via code generation, reflection &/or static linting.\nThe same struct that implements `InterfaceRegistry` will also implement an\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\n```go\ntype InterfaceUnpacker interface {\n// UnpackAny unpacks the value in any to the interface pointer passed in as\n// iface. Note that the type in any must have been registered with\n// RegisterImplementations as a concrete type for that interface\n// Ex:\n//    var msg sdk.Msg\n//    err := ctx.UnpackAny(any, &msg)\n//    ...\nUnpackAny(any *Any, iface interface{}) error\n}\n```\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\nusage of `Any`, it just introduces a security and introspection layer for\ngolang usage.\n`InterfaceRegistry` will be a member of `ProtoCodec`\ndescribed above. In order for modules to register interface types, app modules\ncan optionally implement the following interface:\n```go\ntype InterfaceModule interface {\nRegisterInterfaceTypes(InterfaceRegistry)\n}\n```\nThe module manager will include a method to call `RegisterInterfaceTypes` on\nevery module that implements it in order to populate the `InterfaceRegistry`.\n### Using `Any` to encode state\nThe Cosmos SDK will provide support methods `MarshalInterface` and `UnmarshalInterface` to hide a complexity of wrapping interface types into `Any` and allow easy serialization.\n```go\nimport ""github.com/cosmos/cosmos-sdk/codec""\n// note: eviexported.Evidence is an interface type\nfunc MarshalEvidence(cdc codec.BinaryCodec, e eviexported.Evidence) ([]byte, error) {\nreturn cdc.MarshalInterface(e)\n}\nfunc UnmarshalEvidence(cdc codec.BinaryCodec, bz []byte) (eviexported.Evidence, error) {\nvar evi eviexported.Evidence\nerr := cdc.UnmarshalInterface(&evi, bz)\nreturn err, nil\n}\n```\n### Using `Any` in `sdk.Msg`s\nA similar concept is to be applied for messages that contain interfaces fields.\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\nan interface:\n```protobuf\n// x/evidence/types/types.proto\nmessage MsgSubmitEvidence {\nbytes submitter = 1\n[\n(gogoproto.casttype) = ""github.com/cosmos/cosmos-sdk/types.AccAddress""\n];\ngoogle.protobuf.Any evidence = 2;\n}\n```\nNote that in order to unpack the evidence from `Any` we do need a reference to\n`InterfaceRegistry`. In order to reference evidence in methods like\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\ninterfaces before they're needed.\n### Unpacking Interfaces\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\ninterfaces wrapped in `Any` before they're needed, we create an interface\nthat `sdk.Msg`s and other types can implement:\n```go\ntype UnpackInterfacesMessage interface {\nUnpackInterfaces(InterfaceUnpacker) error\n}\n```\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\nstruct itself with a public getter `GetCachedValue() interface{}`.\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\nand stored in `cachedValue` for reference later.\nThen unpacked interface values can safely be used in any code afterwards\nwithout knowledge of the `InterfaceRegistry`\nand messages can introduce a simple getter to cast the cached value to the\ncorrect interface type.\nThis has the added benefit that unmarshaling of `Any` values only happens once\nduring initial deserialization rather than every time the value is read. Also,\nwhen `Any` values are first packed (for instance in a call to\n`NewMsgSubmitEvidence`), the original interface value is cached so that\nunmarshaling isn't needed to read it again.\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\n`GetEvidence` as follows:\n```go\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\nvar evi eviexported.Evidence\nreturn ctx.UnpackAny(msg.Evidence, *evi)\n}\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\n}\n```\n### Amino Compatibility\nOur custom implementation of `Any` can be used transparently with Amino if used\nwith the proper codec instance. What this means is that interfaces packed within\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\nhave been registered properly with Amino).\nIn order for this functionality to work:\n* **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\nnow a wrapper which properly handles `Any`**\n* **all new code should use `Marshaler` which is compatible with both amino and\nprotobuf**\n* Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\n### Why Wasn't X Chosen Instead\nFor a more complete comparison to alternative protocols, see [here](https://codeburst.io/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\n### Cap'n Proto\nWhile [Cap’n Proto](https://capnproto.org/) does seem like an advantageous alternative to Protobuf\ndue to it's native support for interfaces/generics and built in canonicalization, it does lack the\nrich client ecosystem compared to Protobuf and is a bit less mature.\n### FlatBuffers\n[FlatBuffers](https://google.github.io/flatbuffers/) is also a potentially viable alternative, with the\nprimary difference being that FlatBuffers does not need a parsing/unpacking step to a secondary\nrepresentation before you can access data, often coupled with per-object memory allocation.\nHowever, it would require great efforts into research and full understanding the scope of the migration\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\nuntrusted inputs.\n","**Migrate State Serialization to Protobuf**\n\n**Rationale:**\n\nProtobuf meets all the criteria specified in the context:\n\n* **Language agnostic:** Protobuf supports multiple languages, including Go, Python, Java, and more.\n* **Platform agnostic:** Protobuf can be used on various platforms, including Linux, Windows, and macOS.\n* **Rich client support and thriving ecosystem:** Protobuf has extensive client support and a thriving ecosystem with numerous tools and libraries.\n* **High performance:** Protobuf is highly performant for serialization and deserialization.\n* **Minimal encoded message size:** Protobuf generates compact encoded messages.\n* **Codegen-based over reflection-based:** Protobuf uses code generation, which improves performance and is more developer-friendly.\n* **Supports backward and forward compatibility:** Protobuf provides mechanisms for backward and forward compatibility through message versioning and field presence.\n\nFurthermore, adopting Protobuf for state serialization aligns with the Cosmos SDK's goal of providing a flexible and extensible ecosystem. It will also improve interoperability with other blockchains that use Protobuf for state serialization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, Cosmos SDK uses IAVL for both state [commitments](https://cryptography.fandom.com/wiki/Commitment_scheme) and data storage.\nIAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkelized key/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems:\n* Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level.\n* Each edge traversal requires a DB query.\n* Creating snapshots is [expensive](https://github.com/cosmos/cosmos-sdk/issues/7215#issuecomment-684804950). It takes about 30 seconds to export less than 100 MB of state (as of March 2020).\n* Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck.\n* The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, [ref](https://github.com/cosmos/iavl/blob/master/docs/node/node.md\n).\nMoreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\n\n## Decision\n","We propose to separate the concerns of state commitment (**SC**), needed for consensus, and state storage (**SS**), needed for state machine. Finally we replace IAVL with [Celestia's SMT](https://github.com/lazyledger/smt). Celestia SMT is based on Diem (called jellyfish) design [*] - it uses a compute-optimised SMT by replacing subtrees with only default values with a single node (same approach is used by Ethereum2) and implements compact proofs.\nThe storage model presented here doesn't deal with data structure nor serialization. It's a Key-Value database, where both key and value are binaries. The storage user is responsible for data serialization.\n### Decouple state commitment from storage\nSeparation of storage and commitment (by the SMT) will allow the optimization of different components according to their usage and access patterns.\n`SC` (SMT) is used to commit to a data and compute Merkle proofs. `SS` is used to directly access data. To avoid collisions, both `SS` and `SC` will use a separate storage namespace (they could use the same database underneath). `SS` will store each record directly (mapping `(key, value)` as `key → value`).\nSMT is a merkle tree structure: we don't store keys directly. For every `(key, value)` pair, `hash(key)` is used as leaf path (we hash a key to uniformly distribute leaves in the tree) and `hash(value)` as the leaf contents. The tree structure is specified in more depth [below](#smt-for-state-commitment).\nFor data access we propose 2 additional KV buckets (implemented as namespaces for the key-value pairs, sometimes called [column family](https://github.com/facebook/rocksdb/wiki/Terminology)):\n1. B1: `key → value`: the principal object storage, used by a state machine, behind the Cosmos SDK `KVStore` interface: provides direct access by key and allows prefix iteration (KV DB backend must support it).\n2. B2: `hash(key) → key`: a reverse index to get a key from an SMT path. Internally the SMT will store `(key, value)` as `prefix || hash(key) || hash(value)`. So, we can get an object value by composing `hash(key) → B2 → B1`.\n3. We could use more buckets to optimize the app usage if needed.\nWe propose to use a KV database for both `SS` and `SC`. The store interface will allow to use the same physical DB backend for both `SS` and `SC` as well two separate DBs. The latter option allows for the separation of `SS` and `SC` into different hardware units, providing support for more complex setup scenarios and improving overall performance: one can use different backends (eg RocksDB and Badger) as well as independently tuning the underlying DB configuration.\n### Requirements\nState Storage requirements:\n* range queries\n* quick (key, value) access\n* creating a snapshot\n* historical versioning\n* pruning (garbage collection)\nState Commitment requirements:\n* fast updates\n* tree path should be short\n* query historical commitment proofs using ICS-23 standard\n* pruning (garbage collection)\n### SMT for State Commitment\nA Sparse Merkle tree is based on the idea of a complete Merkle tree of an intractable size. The assumption here is that as the size of the tree is intractable, there would only be a few leaf nodes with valid data blocks relative to the tree size, rendering a sparse tree.\nThe full specification can be found at [Celestia](https://github.com/celestiaorg/celestia-specs/blob/ec98170398dfc6394423ee79b00b71038879e211/src/specs/data_structures.md#sparse-merkle-tree). In summary:\n* The SMT consists of a binary Merkle tree, constructed in the same fashion as described in [Certificate Transparency (RFC-6962)](https://tools.ietf.org/html/rfc6962), but using as the hashing function SHA-2-256 as defined in [FIPS 180-4](https://doi.org/10.6028/NIST.FIPS.180-4).\n* Leaves and internal nodes are hashed differently: the one-byte `0x00` is prepended for leaf nodes while `0x01` is prepended for internal nodes.\n* Default values are given to leaf nodes with empty leaves.\n* While the above rule is sufficient to pre-compute the values of intermediate nodes that are roots of empty subtrees, a further simplification is to extend this default value to all nodes that are roots of empty subtrees. The 32-byte zero is used as the default value. This rule takes precedence over the above one.\n* An internal node that is the root of a subtree that contains exactly one non-empty leaf is replaced by that leaf's leaf node.\n### Snapshots for storage sync and state versioning\nBelow, with simple _snapshot_ we refer to a database snapshot mechanism, not to a _ABCI snapshot sync_. The latter will be referred as _snapshot sync_ (which will directly use DB snapshot as described below).\nDatabase snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a _copy on write_ and it allows DB state to be efficiently delivered at a certain stage.\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs.\nOne of the Stargate core features is a _snapshot sync_ delivered in the `/snapshot` package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a `SS` at a certain version together with a header chain.\nA new database snapshot will be created in every `EndBlocker` and identified by a block height. The `root` store keeps track of the available snapshots to offer `SS` at a certain version. The `root` store implements the `RootStore` interface described below. In essence, `RootStore` encapsulates a `Committer` interface. `Committer` has a `Commit`, `SetPruning`, `GetPruning` functions which will be used for creating and removing snapshots. The `rootStore.Commit` function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the `Committer` interface.\nNOTE: `Commit` must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into `Committer` and `PruningCommitter` - only the multiroot should implement `PruningCommitter` (cache and prefix store don't need pruning).\nNumber of historical versions for `abci.RequestQuery` and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions.\nPruning old snapshots is effectively done by a database. Whenever we update a record in `SC`, SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions.\nTo manage the active snapshots we will either use a DB _max number of snapshots_ option (if available), or we will remove DB snapshots in the `EndBlocker`. The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\n#### Accessing old state versions\nOne of the functional requirements is to access old state. This is done through `abci.RequestQuery` structure.  The version is specified by a block height (so we query for an object by a key `K` at block height `H`). The number of old versions supported for `abci.RequestQuery` is configurable. Accessing an old state is done by using available snapshots.\n`abci.RequestQuery` doesn't need old state of `SC` unless the `prove=true` parameter is set. The SMT merkle proof must be included in the `abci.ResponseQuery` only if both `SC` and `SS` have a snapshot for requested version.\nMoreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution.\nWe positively [validated](https://github.com/cosmos/cosmos-sdk/discussions/8297) a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\n### State Proofs\nFor any object stored in State Store (SS), we have corresponding object in `SC`. A proof for object `V` identified by a key `K` is a branch of `SC`, where the path corresponds to the key `hash(K)`, and the leaf is `hash(K, V)`.\n### Rollbacks\nWe need to be able to process transactions and roll-back state updates if a transaction fails. This can be done in the following way: during transaction processing, we keep all state change requests (writes) in a `CacheWrapper` abstraction (as it's done today). Once we finish the block processing, in the `Endblocker`,  we commit a root store - at that time, all changes are written to the SMT and to the `SS` and a snapshot is created.\n### Committing to an object without saving it\nWe identified use-cases, where modules will need to save an object commitment without storing an object itself. Sometimes clients are receiving complex objects, and they have no way to prove a correctness of that object without knowing the storage layout. For those use cases it would be easier to commit to the object without storing it directly.\n### Refactor MultiStore\nThe Stargate `/store` implementation (store/v1) adds an additional layer in the SDK store construction - the `MultiStore` structure. The multistore exists to support the modularity of the Cosmos SDK - each module is using its own instance of IAVL, but in the current implementation, all instances share the same database. The latter indicates, however, that the implementation doesn't provide true modularity. Instead it causes problems related to race condition and atomic DB commits (see: [\#6370](https://github.com/cosmos/cosmos-sdk/issues/6370) and [discussion](https://github.com/cosmos/cosmos-sdk/discussions/8297#discussioncomment-757043)).\nWe propose to reduce the multistore concept from the SDK, and to use a single instance of `SC` and `SS` in a `RootStore` object. To avoid confusion, we should rename the `MultiStore` interface to `RootStore`. The `RootStore` will have the following interface; the methods for configuring tracing and listeners are omitted for brevity.\n```go\n// Used where read-only access to versions is needed.\ntype BasicRootStore interface {\nStore\nGetKVStore(StoreKey) KVStore\nCacheRootStore() CacheRootStore\n}\n// Used as the main app state, replacing CommitMultiStore.\ntype CommitRootStore interface {\nBasicRootStore\nCommitter\nSnapshotter\nGetVersion(uint64) (BasicRootStore, error)\nSetInitialVersion(uint64) error\n... // Trace and Listen methods\n}\n// Replaces CacheMultiStore for branched state.\ntype CacheRootStore interface {\nBasicRootStore\nWrite()\n... // Trace and Listen methods\n}\n// Example of constructor parameters for the concrete type.\ntype RootStoreConfig struct {\nUpgrades        *StoreUpgrades\nInitialVersion  uint64\nReservePrefix(StoreKey, StoreType)\n}\n```\n<!-- TODO: Review whether these types can be further reduced or simplified -->\n<!-- TODO: RootStorePersistentCache type -->\nIn contrast to `MultiStore`, `RootStore` doesn't allow to dynamically mount sub-stores or provide an arbitrary backing DB for individual sub-stores.\nNOTE: modules will be able to use a special commitment and their own DBs. For example: a module which will use ZK proofs for state can store and commit this proof in the `RootStore` (usually as a single record) and manage the specialized store privately or using the `SC` low level interface.\n#### Compatibility support\nTo ease the transition to this new interface for users, we can create a shim which wraps a `CommitMultiStore` but provides a `CommitRootStore` interface, and expose functions to safely create and access the underlying `CommitMultiStore`.\nThe new `RootStore` and supporting types can be implemented in a `store/v2alpha1` package to avoid breaking existing code.\n#### Merkle Proofs and IBC\nCurrently, an IBC (v1.0) Merkle proof path consists of two elements (`[""<store-key>"", ""<record-key>""]`), with each key corresponding to a separate proof. These are each verified according to individual [ICS-23 specs](https://github.com/cosmos/ibc-go/blob/f7051429e1cf833a6f65d51e6c3df1609290a549/modules/core/23-commitment/types/merkle.go#L17), and the result hash of each step is used as the committed value of the next step, until a root commitment hash is obtained.\nThe root hash of the proof for `""<record-key>""` is hashed with the `""<store-key>""` to validate against the App Hash.\nThis is not compatible with the `RootStore`, which stores all records in a single Merkle tree structure, and won't produce separate proofs for the store- and record-key. Ideally, the store-key component of the proof could just be omitted, and updated to use a ""no-op"" spec, so only the record-key is used. However, because the IBC verification code hardcodes the `""ibc""` prefix and applies it to the SDK proof as a separate element of the proof path, this isn't possible without a breaking change. Breaking this behavior would severely impact the Cosmos ecosystem which already widely adopts the IBC module. Requesting an update of the IBC module across the chains is a time consuming effort and not easily feasible.\nAs a workaround, the `RootStore` will have to use two separate SMTs (they could use the same underlying DB): one for IBC state and one for everything else. A simple Merkle map that reference these SMTs will act as a Merkle Tree to create a final App hash. The Merkle map is not stored in a DBs - it's constructed in the runtime. The IBC substore key must be `""ibc""`.\nThe workaround can still guarantee atomic syncs: the [proposed DB backends](#evaluated-kv-databases) support atomic transactions and efficient rollbacks, which will be used in the commit phase.\nThe presented workaround can be used until the IBC module is fully upgraded to supports single-element commitment proofs.\n### Optimization: compress module key prefixes\nWe consider a compression of prefix keys by creating a mapping from module key to an integer, and serializing the integer using varint coding. Varint coding assures that different values don't have common byte prefix. For Merkle Proofs we can't use prefix compression - so it should only apply for the `SS` keys. Moreover, the prefix compression should be only applied for the module namespace. More precisely:\n* each module has it's own namespace;\n* when accessing a module namespace we create a KVStore with embedded prefix;\n* that prefix will be compressed only when accessing and managing `SS`.\nWe need to assure that the codes won't change. We can fix the mapping in a static variable (provided by an app) or SS state under a special key.\nTODO: need to make decision about the key compression.\n","We should move to Poseidon Tree for state commitments and BadgerDB for data storage.\n\n**Benefits:**\n\n* Poseidon Tree is a state-of-the-art Merkle Tree for state commitments that is more efficient, secure, and easier to maintain than IAVL.\n* BadgerDB is a high-performance, embedded NoSQL database that is well-suited for storing the large amounts of data that are typically found in a blockchain system.\n\n**Risks:**\n\n* Moving to a new state commitment data structure and storage engine will require significant development effort.\n* There is a risk that the new solutions will not perform as well as expected.\n* There is a risk that the new solutions will introduce new security vulnerabilities.\n\n**Mitigation:**\n\n* The development effort will be carefully planned and executed.\n* The new solutions will be thoroughly tested before being deployed in production.\n* The new solutions will be audited by a security expert."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the Cosmos SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\n\n## Decision\n","In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`’s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\nDetailed changes:\n### 1) (Un)Marshal accounts directly using amino\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\n```go\n// GenesisState - all auth state that must be provided at genesis\ntype GenesisState struct {\nParams   Params           `json:""params"" yaml:""params""`\nAccounts []GenesisAccount `json:""accounts"" yaml:""accounts""`\n}\n```\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\n```go\n// InitGenesis - Init store state from genesis data\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\nak.SetParams(ctx, data.Params)\n// load the accounts\nfor _, a := range data.Accounts {\nacc := ak.NewAccount(ctx, a) // set account number\nak.SetAccount(ctx, acc)\n}\n}\n// ExportGenesis returns a GenesisState for a given context and keeper\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\nparams := ak.GetParams(ctx)\nvar genAccounts []exported.GenesisAccount\nak.IterateAccounts(ctx, func(account exported.Account) bool {\ngenAccount := account.(exported.GenesisAccount)\ngenAccounts = append(genAccounts, genAccount)\nreturn false\n})\nreturn NewGenesisState(params, genAccounts)\n}\n```\n### 2) Register custom account types on the `auth` codec\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\nAn example custom account definition:\n```go\nimport authtypes ""cosmossdk.io/x/auth/types""\n// Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\nfunc init() {\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, ""cosmos-sdk/ModuleAccount"")\n}\ntype ModuleAccount struct {\n...\n```\nThe `auth` codec definition:\n```go\nvar ModuleCdc *codec.LegacyAmino\nfunc init() {\nModuleCdc = codec.NewLegacyAmino()\n// register module msg's and Account interface\n...\n// leave the codec unsealed\n}\n// RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\nModuleCdc.RegisterConcrete(o, name, nil)\n}\n```\n### 3) Genesis validation for custom account types\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\n```go\ntype GenesisAccount interface {\nexported.Account\nValidate() error\n}\n```\nThen the `auth` `ValidateGenesis` function becomes:\n```go\n// ValidateGenesis performs basic validation of auth genesis data returning an\n// error for any failed validation criteria.\nfunc ValidateGenesis(data GenesisState) error {\n// Validate params\n...\n// Validate accounts\naddrMap := make(map[string]bool, len(data.Accounts))\nfor _, acc := range data.Accounts {\n// check for duplicated accounts\naddrStr := acc.GetAddress().String()\nif _, ok := addrMap[addrStr]; ok {\nreturn fmt.Errorf(""duplicate account found in genesis state; address: %s"", addrStr)\n}\naddrMap[addrStr] = true\n// check account specific validation\nif err := acc.Validate(); err != nil {\nreturn fmt.Errorf(""invalid account found in genesis state; address: %s, error: %s"", addrStr, err.Error())\n}\n}\nreturn nil\n}\n```\n### 4) Move add-genesis-account cli to `auth`\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\n### 5) Update module and vesting accounts\nUnder the new scheme, module and vesting account types need some minor updates:\n* Type registration on `auth`'s codec (shown above)\n* A `Validate` method for each `Account` concrete type\n",Modify `genaccounts` to support any type of account supported by `auth` by introducing an `AccountFactory` interface that can be registered in `auth` and is responsible for loading and exporting accounts that use that factory.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Cosmos SDK maintains a set of [Protobuf definitions](https://github.com/cosmos/cosmos-sdk/tree/main/proto/cosmos). It is important to correctly design Protobuf definitions to avoid any breaking changes within the same version. The reasons are to not break tooling (including indexers and explorers), wallets and other third-party integrations.\nWhen making changes to these Protobuf definitions, the Cosmos SDK currently only follows [Buf's](https://docs.buf.build/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. For example:\n* Adding fields to `Msg`s. Adding fields is a not a Protobuf spec-breaking operation. However, when adding new fields to `Msg`s, the unknown field rejection will throw an error when sending the new `Msg` to an older node.\n* Marking fields as `reserved`. Protobuf proposes the `reserved` keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn't generate anything for `reserved` fields. See [#9446](https://github.com/cosmos/cosmos-sdk/issues/9446) for more details on this issue.\nMoreover, module developers often face other questions around Protobuf definitions such as ""Can I rename a field?"" or ""Can I deprecate a field?"" This ADR aims to answer all these questions by providing clear guidelines about allowed updates for Protobuf definitions.\n\n## Decision\n","We decide to keep [Buf's](https://docs.buf.build/) recommendations with the following exceptions:\n* `UNARY_RPC`: the Cosmos SDK currently does not support streaming RPCs.\n* `COMMENT_FIELD`: the Cosmos SDK allows fields with no comments.\n* `SERVICE_SUFFIX`: we use the `Query` and `Msg` service naming convention, which doesn't use the `-Service` suffix.\n* `PACKAGE_VERSION_SUFFIX`: some packages, such as `cosmos.crypto.ed25519`, don't use a version suffix.\n* `RPC_REQUEST_STANDARD_NAME`: Requests for the `Msg` service don't have the `-Request` suffix to keep backwards compatibility.\nOn top of Buf's recommendations we add the following guidelines that are specific to the Cosmos SDK.\n### Updating Protobuf Definition Without Bumping Version\n#### 1. Module developers MAY add new Protobuf definitions\nModule developers MAY add new `message`s, new `Service`s, new `rpc` endpoints, and new fields to existing messages. This recommendation follows the Protobuf specification, but is added in this document for clarity, as the SDK requires one additional change.\nThe SDK requires the Protobuf comment of the new addition to contain one line with the following format:\n```protobuf\n// Since: cosmos-sdk <version>{, <version>...}\n```\nWhere each `version` denotes a minor (""0.45"") or patch (""0.44.5"") version from which the field is available. This will greatly help client libraries, who can optionally use reflection or custom code generation to show/hide these fields depending on the targeted node version.\nAs examples, the following comments are valid:\n```protobuf\n// Since: cosmos-sdk 0.44\n// Since: cosmos-sdk 0.42.11, 0.44.5\n```\nand the following ones are NOT valid:\n```protobuf\n// Since cosmos-sdk v0.44\n// since: cosmos-sdk 0.44\n// Since: cosmos-sdk 0.42.11 0.44.5\n// Since: Cosmos SDK 0.42.11, 0.44.5\n```\n#### 2. Fields MAY be marked as `deprecated`, and nodes MAY implement a protocol-breaking change for handling these fields\nProtobuf supports the [`deprecated` field option](https://developers.google.com/protocol-buffers/docs/proto#options), and this option MAY be used on any field, including `Msg` fields. If a node handles a Protobuf message with a non-empty deprecated field, the node MAY change its behavior upon processing it, even in a protocol-breaking way. When possible, the node MUST handle backwards compatibility without breaking the consensus (unless we increment the proto version).\nAs an example, the Cosmos SDK v0.42 to v0.43 update contained two Protobuf-breaking changes, listed below. Instead of bumping the package versions from `v1beta1` to `v1`, the SDK team decided to follow this guideline, by reverting the breaking changes, marking those changes as deprecated, and modifying the node implementation when processing messages with deprecated fields. More specifically:\n* The Cosmos SDK recently removed support for [time-based software upgrades](https://github.com/cosmos/cosmos-sdk/pull/8849). As such, the `time` field has been marked as deprecated in `cosmos.upgrade.v1beta1.Plan`. Moreover, the node will reject any proposal containing an upgrade Plan whose `time` field is non-empty.\n* The Cosmos SDK now supports [governance split votes](./adr-037-gov-split-vote.md). When querying for votes, the returned `cosmos.gov.v1beta1.Vote` message has its `option` field (used for 1 vote option) deprecated in favor of its `options` field (allowing multiple vote options). Whenever possible, the SDK still populates the deprecated `option` field, that is, if and only if the `len(options) == 1` and `options[0].Weight == 1.0`.\n#### 3. Fields MUST NOT be renamed\nWhereas the official Protobuf recommendations do not prohibit renaming fields, as it does not break the Protobuf binary representation, the SDK explicitly forbids renaming fields in Protobuf structs. The main reason for this choice is to avoid introducing breaking changes for clients, which often rely on hard-coded fields from generated types. Moreover, renaming fields will lead to client-breaking JSON representations of Protobuf definitions, used in REST endpoints and in the CLI.\n### Incrementing Protobuf Package Version\nTODO, needs architecture review. Some topics:\n* Bumping versions frequency\n* When bumping versions, should the Cosmos SDK support both versions?\n* i.e. v1beta1 -> v1, should we have two folders in the Cosmos SDK, and handlers for both versions?\n* mention ADR-023 Protobuf naming\n","To avoid breaking changes within the same version, the Cosmos SDK will follow a set of Protobuf definition update guidelines. These guidelines will be based on the Protobuf specification and will take into account the specific needs of the Cosmos SDK.\n\n**Allowed Updates:**\n\n* Adding new fields to messages\n* Adding new enum values\n* Changing the order of fields or enum values\n* Deprecating fields or enum values\n\n**Disallowed Updates:**\n\n* Removing fields or enum values\n* Renaming fields or enum values\n* Changing the type of fields or enum values\n* Making fields required or optional\n\n**Guidelines:**\n\n* Deprecating deprecated fields or enum values for two major versions to give developers time to update their code\n* Only making breaking changes in major versions"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Rosetta API](https://www.rosetta-api.org/) is an open-source specification and set of tools developed by Coinbase to\nstandardise blockchain interactions.\nThrough the use of a standard API for integrating blockchain applications it will\n* Be easier for a user to interact with a given blockchain\n* Allow exchanges to integrate new blockchains quickly and easily\n* Enable application developers to build cross-blockchain applications such as block explorers, wallets and dApps at\nconsiderably lower cost and effort.\n\n## Decision\n","It is clear that adding Rosetta API support to the Cosmos SDK will bring value to all the developers and\nCosmos SDK based chains in the ecosystem. How it is implemented is key.\nThe driving principles of the proposed design are:\n1. **Extensibility:** it must be as riskless and painless as possible for application developers to set-up network\nconfigurations to expose Rosetta API-compliant services.\n2. **Long term support:** This proposal aims to provide support for all the supported Cosmos SDK release series.\n3. **Cost-efficiency:** Backporting changes to Rosetta API specifications from `master` to the various stable\nbranches of Cosmos SDK is a cost that needs to be reduced.\nWe will achieve these delivering on these principles by the following:\n1. There will be a package `rosetta/lib`\nfor the implementation of the core Rosetta API features, particularly:\na. The types and interfaces (`Client`, `OfflineClient`...), this separates design from implementation detail.\nb. The `Server` functionality as this is independent of the Cosmos SDK version.\nc. The `Online/OfflineNetwork`, which is not exported, and implements the rosetta API using the `Client` interface to query the node, build tx and so on.\nd. The `errors` package to extend rosetta errors.\n2. Due to differences between the Cosmos release series, each series will have its own specific implementation of `Client` interface.\n3. There will be two options for starting an API service in applications:\na. API shares the application process\nb. API-specific process.\n","**Decision:** Adopt Rosetta API as the standard for blockchain interactions.\n\n**Rationale:**\n\n* **Standardization:** Rosetta API provides a standardized interface for interacting with different blockchains, making it easier for users, exchanges, and application developers to integrate with and utilize various blockchain platforms.\n* **Ease of Integration:** Using Rosetta API, exchanges can quickly and easily integrate with new blockchains, reducing the time and effort required for onboarding.\n* **Cross-Blockchain Applications:** Rosetta API enables developers to build cross-blockchain applications, such as block explorers, wallets, and dApps, with reduced costs and effort compared to developing separate applications for each blockchain.\n* **Enhanced User Experience:** By simplifying the interaction with blockchains through a standardized API, Rosetta API improves the user experience for blockchain users and developers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https://github.com/cosmos/cosmos-sdk/discussions/9065.\nAs discussed in [#9065](https://github.com/cosmos/cosmos-sdk/discussions/9065), several potential solutions can be considered:\n* irismod/nft and modules/incubator/nft\n* CW721\n* DID NFTs\n* interNFT\nSince functions/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\nThe current design is based on the work done by [IRISnet team](https://github.com/irisnet/irismod/tree/master/modules/nft) and an older implementation in the [Cosmos repository](https://github.com/cosmos/modules/tree/master/incubator/nft).\n\n## Decision\n","We create a `x/nft` module, which contains the following functionality:\n* Store NFTs and track their ownership.\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\n* Query NFTs and their supply information.\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\n### Types\nWe propose two main types:\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\n#### Class\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\n```protobuf\nmessage Class {\nstring id          = 1;\nstring name        = 2;\nstring symbol      = 3;\nstring description = 4;\nstring uri         = 5;\nstring uri_hash    = 6;\ngoogle.protobuf.Any data = 7;\n}\n```\n* `id` is used as the primary index for storing the class; _required_\n* `name` is a descriptive name of the NFT class; _optional_\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\n* `description` is a detailed description of the NFT class; _optional_\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https://docs.opensea.io/docs/contract-level-metadata)); _optional_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is app specific metadata of the class; _optional_\n#### NFT\nWe define a general model for `NFT` as follows.\n```protobuf\nmessage NFT {\nstring class_id           = 1;\nstring id                 = 2;\nstring uri                = 3;\nstring uri_hash           = 4;\ngoogle.protobuf.Any data  = 10;\n}\n```\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\n```text\n{class_id}/{id} --> NFT (bytes)\n```\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https://docs.opensea.io/docs/metadata-standards)); _required_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational/UI functionality.\n### `Keeper` Interface\n```go\ntype Keeper interface {\nNewClass(ctx sdk.Context,class Class)\nUpdateClass(ctx sdk.Context,class Class)\nMint(ctx sdk.Context,nft NFT，receiver sdk.AccAddress)   // updates totalSupply\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\nBurn(ctx sdk.Context, classId string, nftId string)    // updates totalSupply\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\nUpdate(ctx sdk.Context, nft NFT)\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\nGetClass(ctx sdk.Context, classId string) Class\nGetClasses(ctx sdk.Context) []Class\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\nGetTotalSupply(ctx sdk.Context, classId string) uint64\n}\n```\nOther business logic implementations should be defined in composing modules that import `x/nft` and use its `Keeper`.\n### `Msg` Service\n```protobuf\nservice Msg {\nrpc Send(MsgSend)         returns (MsgSendResponse);\n}\nmessage MsgSend {\nstring class_id = 1;\nstring id       = 2;\nstring sender   = 3;\nstring reveiver = 4;\n}\nmessage MsgSendResponse {}\n```\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\nThe implementation outline of the server is as follows:\n```go\ntype msgServer struct{\nk Keeper\n}\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\n// check current ownership\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\n// transfer ownership\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\nreturn &types.MsgSendResponse{}, nil\n}\n```\nThe query service methods for the `x/nft` module are:\n```protobuf\nservice Query {\n// Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/balance/{owner}/{class_id}"";\n}\n// Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/owner/{class_id}/{id}"";\n}\n// Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/supply/{class_id}"";\n}\n// NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/nfts"";\n}\n// NFT queries an NFT based on its class and id.\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/nfts/{class_id}/{id}"";\n}\n// Class queries an NFT class based on its id\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/classes/{class_id}"";\n}\n// Classes queries all NFT classes\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/classes"";\n}\n}\n// QueryBalanceRequest is the request type for the Query/Balance RPC method\nmessage QueryBalanceRequest {\nstring class_id = 1;\nstring owner    = 2;\n}\n// QueryBalanceResponse is the response type for the Query/Balance RPC method\nmessage QueryBalanceResponse {\nuint64 amount = 1;\n}\n// QueryOwnerRequest is the request type for the Query/Owner RPC method\nmessage QueryOwnerRequest {\nstring class_id = 1;\nstring id       = 2;\n}\n// QueryOwnerResponse is the response type for the Query/Owner RPC method\nmessage QueryOwnerResponse {\nstring owner = 1;\n}\n// QuerySupplyRequest is the request type for the Query/Supply RPC method\nmessage QuerySupplyRequest {\nstring class_id = 1;\n}\n// QuerySupplyResponse is the response type for the Query/Supply RPC method\nmessage QuerySupplyResponse {\nuint64 amount = 1;\n}\n// QueryNFTstRequest is the request type for the Query/NFTs RPC method\nmessage QueryNFTsRequest {\nstring                                class_id   = 1;\nstring                                owner      = 2;\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\n}\n// QueryNFTsResponse is the response type for the Query/NFTs RPC methods\nmessage QueryNFTsResponse {\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n// QueryNFTRequest is the request type for the Query/NFT RPC method\nmessage QueryNFTRequest {\nstring class_id = 1;\nstring id       = 2;\n}\n// QueryNFTResponse is the response type for the Query/NFT RPC method\nmessage QueryNFTResponse {\ncosmos.nft.v1beta1.NFT nft = 1;\n}\n// QueryClassRequest is the request type for the Query/Class RPC method\nmessage QueryClassRequest {\nstring class_id = 1;\n}\n// QueryClassResponse is the response type for the Query/Class RPC method\nmessage QueryClassResponse {\ncosmos.nft.v1beta1.Class class = 1;\n}\n// QueryClassesRequest is the request type for the Query/Classes RPC method\nmessage QueryClassesRequest {\n// pagination defines an optional pagination for the request.\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\n}\n// QueryClassesResponse is the response type for the Query/Classes RPC method\nmessage QueryClassesResponse {\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n```\n### Interoperability\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x/nft. This requires creation of a new IBC standard and implementation of it.\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x/nft interoperability, custom NFT implementations (example: x/cryptokitty) should use the canonical x/nft module and proxy all NFT balance keeping functionality to x/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x/nft becomes the standard NFT registry for all Cosmos NFTs (example: x/cryptokitty will register a kitty NFT in x/nft and use x/nft for book keeping). This was [discussed](https://github.com/cosmos/cosmos-sdk/discussions/9065#discussioncomment-873206) in the context of using x/bank as a general asset balance book. Not using x/nft will require implementing another module for IBC.\n","The decision is to implement a generic NFT module in Cosmos Hub, based on the work done by the IRISnet team and the previous implementation in the Cosmos repository. This module will handle the generic NFT logic, while application-specific functions can be managed by other modules on Cosmos Hub or other Zones by importing the NFT module. This design will allow for composability and support the generic use cases and compatibility of interchain protocols such as IBC and Gravity Bridge."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current proof of stake module takes the design decision to apply staking weight changes to the consensus engine immediately. This means that delegations and unbonds get applied immediately to the validator set. This decision was primarily done as it was implementationally simplest, and because we at the time believed that this would lead to better UX for clients.\nAn alternative design choice is to allow buffering staking updates (delegations, unbonds, validators joining) for a number of blocks. This 'epoch'd proof of stake consensus provides the guarantee that the consensus weights for validators will not change mid-epoch, except in the event of a slash condition.\nAdditionally, the UX hurdle may not be as significant as was previously thought. This is because it is possible to provide users immediate acknowledgement that their bond was recorded and will be executed.\nFurthermore, it has become clearer over time that immediate execution of staking events comes with limitations, such as:\n* Threshold based cryptography. One of the main limitations is that because the validator set can change so regularly, it makes the running of multiparty computation by a fixed validator set difficult. Many threshold-based cryptographic features for blockchains such as randomness beacons and threshold decryption require a computationally-expensive DKG process (will take much longer than 1 block to create). To productively use these, we need to guarantee that the result of the DKG will be used for a reasonably long time. It wouldn't be feasible to rerun the DKG every block. By epoching staking, it guarantees we'll only need to run a new DKG once every epoch.\n* Light client efficiency. This would lessen the overhead for IBC when there is high churn in the validator set. In the Tendermint light client bisection algorithm, the number of headers you need to verify is related to bounding the difference in validator sets between a trusted header and the latest header. If the difference is too great, you verify more header in between the two. By limiting the frequency of validator set changes, we can reduce the worst case size of IBC lite client proofs, which occurs when a validator set has high churn.\n* Fairness of deterministic leader election. Currently we have no ways of reasoning of fairness of deterministic leader election in the presence of staking changes without epochs (tendermint/spec#217). Breaking fairness of leader election is profitable for validators, as they earn additional rewards from being the proposer. Adding epochs at least makes it easier for our deterministic leader election to match something we can prove secure. (Albeit, we still haven’t proven if our current algorithm is fair with > 2 validators in the presence of stake changes)\n* Staking derivative design. Currently, reward distribution is done lazily using the F1 fee distribution. While saving computational complexity, lazy accounting requires a more stateful staking implementation. Right now, each delegation entry has to track the time of last withdrawal. Handling this can be a challenge for some staking derivatives designs that seek to provide fungibility for all tokens staked to a single validator. Force-withdrawing rewards to users can help solve this, however it is infeasible to force-withdraw rewards to users on a per block basis. With epochs, a chain could more easily alter the design to have rewards be forcefully withdrawn (iterating over delegator accounts only once per-epoch), and can thus remove delegation timing from state. This may be useful for certain staking derivative designs.\n\n## Decision\n","**Step-1**:  Implement buffering of all staking and slashing messages.\nFirst we create a pool for storing tokens that are being bonded, but should be applied at the epoch boundary called the `EpochDelegationPool`. Then, we have two separate queues, one for staking, one for slashing. We describe what happens on each message being delivered below:\n### Staking messages\n* **MsgCreateValidator**: Move user's self-bond to `EpochDelegationPool` immediately. Queue a message for the epoch boundary to handle the self-bond, taking the funds from the `EpochDelegationPool`. If Epoch execution fail, return back funds from `EpochDelegationPool` to user's account.\n* **MsgEditValidator**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **MsgDelegate**: Move user's funds to `EpochDelegationPool` immediately. Queue a message for the epoch boundary to handle the delegation, taking the funds from the `EpochDelegationPool`. If Epoch execution fail, return back funds from `EpochDelegationPool` to user's account.\n* **MsgBeginRedelegate**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **MsgUndelegate**: Validate message and if valid queue the message for execution at the end of the Epoch.\n### Slashing messages\n* **MsgUnjail**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **Slash Event**: Whenever a slash event is created, it gets queued in the slashing module to apply at the end of the epoch. The queues should be setup such that this slash applies immediately.\n### Evidence Messages\n* **MsgSubmitEvidence**: This gets executed immediately, and the validator gets jailed immediately. However in slashing, the actual slash event gets queued.\nThen we add methods to the end blockers, to ensure that at the epoch boundary the queues are cleared and delegation updates are applied.\n**Step-2**: Implement querying of queued staking txs.\nWhen querying the staking activity of a given address, the status should return not only the amount of tokens staked, but also if there are any queued stake events for that address. This will require more work to be done in the querying logic, to trace the queued upcoming staking events.\nAs an initial implementation, this can be implemented as a linear search over all queued staking events. However, for chains that need long epochs, they should eventually build additional support for nodes that support querying to be able to produce results in constant time. (This is do-able by maintaining an auxiliary hashmap for indexing upcoming staking events by address)\n**Step-3**: Adjust gas\nCurrently gas represents the cost of executing a transaction when its done immediately. (Merging together costs of p2p overhead, state access overhead, and computational overhead) However, now a transaction can cause computation in a future block, namely at the epoch boundary.\nTo handle this, we should initially include parameters for estimating the amount of future computation (denominated in gas), and add that as a flat charge needed for the message.\nWe leave it as out of scope for how to weight future computation versus current computation in gas pricing, and have it set such that the are weighted equally for now.\n",Use epochs for proof of stake updates.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to reduce the number of parties involved with handling sensitive\ninformation in an emergency scenario, we propose the creation of a\nspecialization group named The Decentralized Computer Emergency Response Team\n(dCERT).  Initially this group's role is intended to serve as coordinators\nbetween various actors within a blockchain community such as validators,\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\naggregate and relay input from a variety of stakeholders to the developers who\nare actively devising a patch to the software, this way sensitive information\ndoes not need to be publicly disclosed while some input from the community can\nstill be gained.\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\nto ""circuit-break"" (aka. temporarily disable)  a particular message path. Note\nthat this privilege should be enabled/disabled globally with a governance\nparameter such that this privilege could start disabled and later be enabled\nthrough a parameter change proposal, once a dCERT group has been established.\nIn the future it is foreseeable that the community may wish to expand the roles\nof dCERT with further responsibilities such as the capacity to ""pre-approve"" a\nsecurity update on behalf of the community prior to a full community\nwide vote whereby the sensitive information would be revealed prior to a\nvulnerability being patched on the live network.\n\n## Decision\n","The dCERT group is proposed to include an implementation of a `SpecializationGroup`\nas defined in [ADR 007](./adr-007-specialization-groups.md). This will include the\nimplementation of:\n* continuous voting\n* slashing due to breach of soft contract\n* revoking a member due to breach of soft contract\n* emergency disband of the entire dCERT group (ex. for colluding maliciously)\n* compensation stipend from the community pool or other means decided by\ngovernance\nThis system necessitates the following new parameters:\n* blockly stipend allowance per dCERT member\n* maximum number of dCERT members\n* required staked slashable tokens for each dCERT member\n* quorum for suspending a particular member\n* proposal wager for disbanding the dCERT group\n* stabilization period for dCERT member transition\n* circuit break dCERT privileges enabled\nThese parameters are expected to be implemented through the param keeper such\nthat governance may change them at any given point.\n### Continuous Voting Electionator\nAn `Electionator` object is to be implemented as continuous voting and with the\nfollowing specifications:\n* All delegation addresses may submit votes at any point which updates their\npreferred representation on the dCERT group.\n* Preferred representation may be arbitrarily split between addresses (ex. 50%\nto John, 25% to Sally, 25% to Carol)\n* In order for a new member to be added to the dCERT group they must\nsend a transaction accepting their admission at which point the validity of\ntheir admission is to be confirmed.\n* A sequence number is assigned when a member is added to dCERT group.\nIf a member leaves the dCERT group and then enters back, a new sequence number\nis assigned.\n* Addresses which control the greatest amount of preferred-representation are\neligible to join the dCERT group (up the _maximum number of dCERT members_).\nIf the dCERT group is already full and new member is admitted, the existing\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\n* In the split situation where the dCERT group is full but a vying candidate\nhas the same amount of vote as an existing dCERT member, the existing\nmember should maintain its position.\n* In the split situation where somebody must be kicked out but the two\naddresses with the smallest number of votes have the same number of votes,\nthe address with the smallest sequence number maintains its position.\n* A stabilization period can be optionally included to reduce the\n""flip-flopping"" of the dCERT membership tail members. If a stabilization\nperiod is provided which is greater than 0, when members are kicked due to\ninsufficient support, a queue entry is created which documents which member is\nto replace which other member. While this entry is in the queue, no new entries\nto kick that same dCERT member can be made. When the entry matures at the\nduration of the  stabilization period, the new member is instantiated, and old\nmember kicked.\n### Staking/Slashing\nAll members of the dCERT group must stake tokens _specifically_ to maintain\neligibility as a dCERT member. These tokens can be staked directly by the vying\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\nbenefits for doing so). This staking mechanism should use the existing global\nunbonding time of tokens staked for network validator security. A dCERT member\ncan _only be_ a member if it has the required tokens staked under this\nmechanism. If those tokens are unbonded then the dCERT member must be\nautomatically kicked from the group.\nSlashing of a particular dCERT member due to soft-contract breach should be\nperformed by governance on a per member basis based on the magnitude of the\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\nby the dCERT group prior to being slashed by governance.\nMembership suspension by the dCERT group takes place through a voting procedure\nby the dCERT group members. After this suspension has taken place, a governance\nproposal to slash the dCERT member must be submitted, if the proposal is not\napproved by the time the rescinding member has completed unbonding their\ntokens, then the tokens are no longer staked and unable to be slashed.\nAdditionally in the case of an emergency situation of a colluding and malicious\ndCERT group, the community needs the capability to disband the entire dCERT\ngroup and likely fully slash them. This could be achieved though a special new\nproposal type (implemented as a general governance proposal) which would halt\nthe functionality of the dCERT group until the proposal was concluded. This\nspecial proposal type would likely need to also have a fairly large wager which\ncould be slashed if the proposal creator was malicious. The reason a large\nwager should be required is because as soon as the proposal is made, the\ncapability of the dCERT group to halt message routes is put on temporarily\nsuspended, meaning that a malicious actor who created such a proposal could\nthen potentially exploit a bug during this period of time, with no dCERT group\ncapable of shutting down the exploitable message routes.\n### dCERT membership transactions\nActive dCERT members\n* change of the description of the dCERT group\n* circuit break a message route\n* vote to suspend a dCERT member.\nHere circuit-breaking refers to the capability to disable a groups of messages,\nThis could for instance mean: ""disable all staking-delegation messages"", or\n""disable all distribution messages"". This could be accomplished by verifying\nthat the message route has not been ""circuit-broken"" at CheckTx time (in\n`baseapp/baseapp.go`).\n""unbreaking"" a circuit is anticipated only to occur during a hard fork upgrade\nmeaning that no capability to unbreak a message route on a live chain is\nrequired.\nNote also, that if there was a problem with governance voting (for instance a\ncapability to vote many times) then governance would be broken and should be\nhalted with this mechanism, it would be then up to the validator set to\ncoordinate and hard-fork upgrade to a patched version of the software where\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\nthey should all be severely slashed.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\n\n## Decision\n","### Design\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\n```text\nslash_amount = k * power // power is the faulting validator's voting power and k is some on-chain constant\n```\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts (sybil attack), so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\n```text\nslash_amount = k * (power_1 + power_2 + ... + power_n) // where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\n```\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will get slashed at the sum 10% amount.\nHowever in practice, we likely don't want a linear relation between amount of stake at fault, and the percentage of stake to slash. In particular, solely 5% of stake double signing effectively did nothing to majorly threaten security, whereas 30% of stake being at fault clearly merits a large slashing factor, due to being very close to the point at which Tendermint security is threatened. A linear relation would require a factor of 6 gap between these two, whereas the difference in risk posed to the network is much larger. We propose using S-curves (formally [logistic functions](https://en.wikipedia.org/wiki/Logistic_function) to solve this). S-Curves capture the desired criterion quite well. They allow the slashing factor to be minimal for small values, and then grow very rapidly near some threshold point where the risk posed becomes notable.\n#### Parameterization\nThis requires parameterizing a logistic function. It is very well understood how to parameterize this. It has four parameters:\n1) A minimum slashing factor\n2) A maximum slashing factor\n3) The inflection point of the S-curve (essentially where do you want to center the S)\n4) The rate of growth of the S-curve (How elongated is the S)\n#### Correlation across non-sybil validators\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\n#### Griefing\nGriefing, the act of intentionally getting oneself slashed in order to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker also gets equally impacted by the grief as the victim, so it would not provide much benefit to the griefer.\n### Implementation\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define ""recent slashes"" as ones that have occurred within the last `unbonding period`.  For liveness faults, we will define ""recent slashes"" as ones that have occurred within the last `jail period`.\n```go\ntype SlashEvent struct {\nAddress                     sdk.ValAddress\nValidatorVotingPercent      sdk.Dec\nSlashedSoFar                sdk.Dec\n}\n```\nThese slash events will be pruned from the queue once they are older than their respective ""recent slash period"".\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\nWe then will iterate over all the SlashEvents in the queue, adding their `ValidatorVotingPercent` to calculate the new percent to slash all the validators in the queue at, using the ""Square of Sum of Roots"" formula introduced above.\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occurred, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\n","Implement a progressive fee for delegation on validators based on the share of consensus power they hold.  The fee would be collected by the protocol and used to fund public goods in the ecosystem.  The fee would be tiered, with higher fees for validators that hold a larger share of consensus power.  This would incentivize delegators to spread their delegations across a wider range of validators, reducing the risk of centralization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen a chain upgrade introduces state-breaking changes inside modules, the current procedure consists of exporting the whole state into a JSON file (via the `simd genesis export` command), running migration scripts on the JSON file (`simd genesis migrate` command), clearing the stores (`simd unsafe-reset-all` command), and starting a new chain with the migrated JSON file as new genesis (optionally with a custom initial block height). An example of such a procedure can be seen [in the Cosmos Hub 3->4 migration guide](https://github.com/cosmos/gaia/blob/v4.0.3/docs/migration/cosmoshub-3.md#upgrade-procedure).\nThis procedure is cumbersome for multiple reasons:\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https://github.com/cosmos/cosmos-sdk/issues/6936)).\n\n## Decision\n","We propose a migration procedure based on modifying the KV store in-place without involving the JSON export-process-import flow described above.\n### Module `ConsensusVersion`\nWe introduce a new method on the `AppModule` interface:\n```go\ntype AppModule interface {\n// --snip--\nConsensusVersion() uint64\n}\n```\nThis methods returns an `uint64` which serves as state-breaking version of the module. It MUST be incremented on each consensus-breaking change introduced by the module. To avoid potential errors with default values, the initial version of a module MUST be set to 1. In the Cosmos SDK, version 1 corresponds to the modules in the v0.41 series.\n### Module-Specific Migration Functions\nFor each consensus-breaking change introduced by the module, a migration script from ConsensusVersion `N` to version `N+1` MUST be registered in the `Configurator` using its newly-added `RegisterMigration` method. All modules receive a reference to the configurator in their `RegisterServices` method on `AppModule`, and this is where the migration functions should be registered. The migration functions should be registered in increasing order.\n```go\nfunc (am AppModule) RegisterServices(cfg module.Configurator) {\n// --snip--\ncfg.RegisterMigration(types.ModuleName, 1, func(ctx sdk.Context) error {\n// Perform in-place store migrations from ConsensusVersion 1 to 2.\n})\ncfg.RegisterMigration(types.ModuleName, 2, func(ctx sdk.Context) error {\n// Perform in-place store migrations from ConsensusVersion 2 to 3.\n})\n// etc.\n}\n```\nFor example, if the new ConsensusVersion of a module is `N` , then `N-1` migration functions MUST be registered in the configurator.\nIn the Cosmos SDK, the migration functions are handled by each module's keeper, because the keeper holds the `sdk.StoreKey` used to perform in-place store migrations. To not overload the keeper, a `Migrator` wrapper is used by each module to handle the migration functions:\n```go\n// Migrator is a struct for handling in-place store migrations.\ntype Migrator struct {\nBaseKeeper\n}\n```\nMigration functions should live inside the `migrations/` folder of each module, and be called by the Migrator's methods. We propose the format `Migrate{M}to{N}` for method names.\n```go\n// Migrate1to2 migrates from version 1 to 2.\nfunc (m Migrator) Migrate1to2(ctx sdk.Context) error {\nreturn v2bank.MigrateStore(ctx, m.keeper.storeKey) // v043bank is package `x/bank/migrations/v2`.\n}\n```\nEach module's migration functions are specific to the module's store evolutions, and are not described in this ADR. An example of x/bank store key migrations after the introduction of ADR-028 length-prefixed addresses can be seen in this [store.go code](https://github.com/cosmos/cosmos-sdk/blob/36f68eb9e041e20a5bb47e216ac5eb8b91f95471/x/bank/legacy/v043/store.go#L41-L62).\n### Tracking Module Versions in `x/upgrade`\nWe introduce a new prefix store in `x/upgrade`'s store. This store will track each module's current version, it can be modelized as a `map[string]uint64` of module name to module ConsensusVersion, and will be used when running the migrations (see next section for details). The key prefix used is `0x1`, and the key/value format is:\n```text\n0x2 | {bytes(module_name)} => BigEndian(module_consensus_version)\n```\nThe initial state of the store is set from `app.go`'s `InitChainer` method.\nThe UpgradeHandler signature needs to be updated to take a `VersionMap`, as well as return an upgraded `VersionMap` and an error:\n```diff\n- type UpgradeHandler func(ctx sdk.Context, plan Plan)\n+ type UpgradeHandler func(ctx sdk.Context, plan Plan, versionMap VersionMap) (VersionMap, error)\n```\nTo apply an upgrade, we query the `VersionMap` from the `x/upgrade` store and pass it into the handler. The handler runs the actual migration functions (see next section), and if successful, returns an updated `VersionMap` to be stored in state.\n```diff\nfunc (k UpgradeKeeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {\n// --snip--\n-   handler(ctx, plan)\n+   updatedVM, err := handler(ctx, plan, k.GetModuleVersionMap(ctx)) // k.GetModuleVersionMap() fetches the VersionMap stored in state.\n+   if err != nil {\n+       return err\n+   }\n+\n+   // Set the updated consensus versions to state\n+   k.SetModuleVersionMap(ctx, updatedVM)\n}\n```\nA gRPC query endpoint to query the `VersionMap` stored in `x/upgrade`'s state will also be added, so that app developers can double-check the `VersionMap` before the upgrade handler runs.\n### Running Migrations\nOnce all the migration handlers are registered inside the configurator (which happens at startup), running migrations can happen by calling the `RunMigrations` method on `module.Manager`. This function will loop through all modules, and for each module:\n* Get the old ConsensusVersion of the module from its `VersionMap` argument (let's call it `M`).\n* Fetch the new ConsensusVersion of the module from the `ConsensusVersion()` method on `AppModule` (call it `N`).\n* If `N>M`, run all registered migrations for the module sequentially `M -> M+1 -> M+2...` until `N`.\n* There is a special case where there is no ConsensusVersion for the module, as this means that the module has been newly added during the upgrade. In this case, no migration function is run, and the module's current ConsensusVersion is saved to `x/upgrade`'s store.\nIf a required migration is missing (e.g. if it has not been registered in the `Configurator`), then the `RunMigrations` function will error.\nIn practice, the `RunMigrations` method should be called from inside an `UpgradeHandler`.\n```go\napp.UpgradeKeeper.SetUpgradeHandler(""my-plan"", func(ctx sdk.Context, plan upgradetypes.Plan, vm module.VersionMap)  (module.VersionMap, error) {\nreturn app.mm.RunMigrations(ctx, vm)\n})\n```\nAssuming a chain upgrades at block `n`, the procedure should run as follows:\n* the old binary will halt in `BeginBlock` when starting block `N`. In its store, the ConsensusVersions of the old binary's modules are stored.\n* the new binary will start at block `N`. The UpgradeHandler is set in the new binary, so will run at `BeginBlock` of the new binary. Inside `x/upgrade`'s `ApplyUpgrade`, the `VersionMap` will be retrieved from the (old binary's) store, and passed into the `RunMigrations` function, migrating all module stores in-place before the modules' own `BeginBlock`s.\n","**Introduce a new `migrate` command to the Cosmos SDK which would take a genesis JSON file and a list of migration scripts as inputs and output a new genesis file with the applied migrations.**\n\nThis new `migrate` command would simplify the migration process by automating the steps of exporting the state, running the migration scripts, and updating the genesis file. It would also make the migration process more efficient by reducing the time and resources required to complete the migration.\n\nThe `migrate` command would be implemented as a new subcommand of the `simd` tool. It would take the following arguments:\n\n* `genesis-file`: The path to the genesis JSON file to be migrated.\n* `migration-scripts`: A list of paths to the migration scripts to be applied.\n* `output-file`: The path to the output genesis JSON file.\n\nThe `migrate` command would work by first loading the genesis JSON file into memory. It would then iterate over the list of migration scripts, applying each script to the in-memory genesis state. Once all of the migration scripts have been applied, the `migrate` command would write the updated genesis state to the output file.\n\nThe `migrate` command would provide a number of benefits over the existing migration procedure. First, it would be more efficient, reducing the time and resources required to complete a migration. Second, it would be more user-friendly, providing a simple and straightforward way to apply migrations to a genesis file. Third, it would be more extensible, allowing for the development of new migration scripts to support future state-breaking changes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCosmos SDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\nmediocre.\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\nwhich are repetitive and hard to maintain.\nSecond, this makes it harder to align with the object capability theorem: the right to access the\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\nFinally, because the getter/setter functions are defined as methods of a module's `Keeper`, the reviewers\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\nThere is no static way to know which part of the state that the function is accessing (and which is not).\n\n## Decision\n","We will define a type named `Value`:\n```go\ntype Value struct {\nm   Mapping\nkey []byte\n}\n```\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\nspace it will access and `Value.key` defines the exact key for the reference.\nWe will define a type named `Mapping`:\n```go\ntype Mapping struct {\nstoreKey sdk.StoreKey\ncdc      *codec.LegacyAmino\nprefix   []byte\n}\n```\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\nWe will define the following core methods for the `Value` type:\n```go\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Value) Get(ctx Context, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\n// Get stored data as raw byte slice\nfunc (Value) GetRaw(ctx Context) []byte {}\n// Marshal and set a raw value\nfunc (Value) Set(ctx Context, o interface{}) {}\n// Check if a raw value exists\nfunc (Value) Exists(ctx Context) bool {}\n// Delete a raw value value\nfunc (Value) Delete(ctx Context) {}\n```\nWe will define the following core methods for the `Mapping` type:\n```go\n// Constructs key-value pair reference corresponding to the key argument in the Mapping space\nfunc (Mapping) Value(key []byte) Value {}\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\n// Get stored data as raw byte slice\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\n// Marshal and set a raw value\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\n// Check if a raw value exists\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\n// Delete a raw value value\nfunc (Mapping) Delete(ctx Context, key []byte) {}\n```\nEach method of the `Mapping` type that is passed the arguments `ctx`, `key`, and `args...` will proxy\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\nIn addition, we will define and provide a common set of types derived from the `Value` type:\n```go\ntype Boolean struct { Value }\ntype Enum struct { Value }\ntype Integer struct { Value; enc IntEncoding }\ntype String struct { Value }\n// ...\n```\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\nin core methods are replaced by explicit return types.\nFinally, we will define a family of types derived from the `Mapping` type:\n```go\ntype Indexer struct {\nm   Mapping\nenc IntEncoding\n}\n```\nWhere the `key` argument in core method is typed.\nSome of the properties of the accessor types are:\n* State access happens only when a function which takes a `Context` as an argument is invoked\n* Accessor type structs give rights to access the state only that the struct is referring, no other\n* Marshalling/Unmarshalling happens implicitly within the core methods\n","The module state should be modeled as Rust structs, which are stored in a KV store as serialized bytes. The module structs should implement the `Codable` trait to allow for easy serialization/deserialization. Additionally, a module should only have access to the keyspace that it needs, and should not be able to access other modules' keyspaces. This can be enforced using the object capability theorem. Finally, the getter/setter functions should be defined as static functions, so that the reviewers can easily see which part of the state that the function is accessing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nProtocol Buffers provide a basic [style guide](https://developers.google.com/protocol-buffers/docs/style)\nand [Buf](https://buf.build/docs/style-guide) builds upon that. To the\nextent possible, we want to follow industry accepted guidelines and wisdom for\nthe effective usage of protobuf, deviating from those only when there is clear\nrationale for our use case.\n### Adoption of `Any`\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\ninterface types (as opposed to `oneof`) makes package naming a central part\nof the encoding as fully-qualified message names now appear in encoded\nmessages.\n### Current Directory Organization\nThus far we have mostly followed [Buf's](https://buf.build) [DEFAULT](https://buf.build/docs/lint-checkers#default)\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout)\nwhich although being convenient for developing code comes with the warning\nfrom Buf that:\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\n### Adoption of gRPC Queries\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\npath. In the future, gRPC queries may be allowed from within persistent scripts\nby technologies such as CosmWasm and these query routes would be stored within\nscript binaries.\n\n## Decision\n","The goal of this ADR is to provide thoughtful naming conventions that:\n* encourage a good user experience for when users interact directly with\n.proto files and fully-qualified protobuf names\n* balance conciseness against the possibility of either over-optimizing (making\nnames too short and cryptic) or under-optimizing (just accepting bloated names\nwith lots of redundant information)\nThese guidelines are meant to act as a style guide for both the Cosmos SDK and\nthird-party modules.\nAs a starting point, we should adopt all of the [DEFAULT](https://buf.build/docs/lint-checkers#default)\ncheckers in [Buf's](https://buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout),\nexcept:\n* [PACKAGE_VERSION_SUFFIX](https://buf.build/docs/lint-checkers#package_version_suffix)\n* [SERVICE_SUFFIX](https://buf.build/docs/lint-checkers#service_suffix)\nFurther guidelines to be described below.\n### Principles\n#### Concise and Descriptive Names\nNames should be descriptive enough to convey their meaning and distinguish\nthem from other names.\nGiven that we are using fully-qualifed names within\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\nkeep names concise, without going overboard. The general rule of thumb should\nbe if a shorter name would convey more or else the same thing, pick the shorter\nname.\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\nSuch conciseness makes names both more pleasant to work with and take up less\nspace within transactions and on the wire.\nWe should also resist the temptation to over-optimize, by making names\ncryptically short with abbreviations. For instance, we shouldn't try to\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\nThe goal is to make names **_concise but not cryptic_**.\n#### Names are for Clients First\nPackage and type names should be chosen for the benefit of users, not\nnecessarily because of legacy concerns related to the go code-base.\n#### Plan for Longevity\nIn the interests of long-term support, we should plan on the names we do\nchoose to be in usage for a long time, so now is the opportunity to make\nthe best choices for the future.\n### Versioning\n#### Guidelines on Stable Package Versions\nIn general, schema evolution is the way to update protobuf schemas. That means that new fields,\nmessages, and RPC methods are _added_ to existing schemas and old fields, messages and RPC methods\nare maintained as long as possible.\nBreaking things is often unacceptable in a blockchain scenario. For instance, immutable smart contracts\nmay depend on certain data schemas on the host chain. If the host chain breaks those schemas, the smart\ncontract may be irreparably broken. Even when things can be fixed (for instance in client software),\nthis often comes at a high cost.\nInstead of breaking things, we should make every effort to evolve schemas rather than just breaking them.\n[Buf](https://buf.build) breaking change detection should be used on all stable (non-alpha or beta) packages\nto prevent such breakage.\nWith that in mind, different stable versions (i.e. `v1` or `v2`) of a package should more or less be considered\ndifferent packages and this should be last resort approach for upgrading protobuf schemas. Scenarios where creating\na `v2` may make sense are:\n* we want to create a new module with similar functionality to an existing module and adding `v2` is the most natural\nway to do this. In that case, there are really just two different, but similar modules with different APIs.\n* we want to add a new revamped API for an existing module and it's just too cumbersome to add it to the existing package,\nso putting it in `v2` is cleaner for users. In this case, care should be made to not deprecate support for\n`v1` if it is actively used in immutable smart contracts.\n#### Guidelines on unstable (alpha and beta) package versions\nThe following guidelines are recommended for marking packages as alpha or beta:\n* marking something as `alpha` or `beta` should be a last resort and just putting something in the\nstable package (i.e. `v1` or `v2`) should be preferred\n* a package _should_ be marked as `alpha` _if and only if_ there are active discussions to remove\nor significantly alter the package in the near future\n* a package _should_ be marked as `beta` _if and only if_ there is an active discussion to\nsignificantly refactor/rework the functionality in the near future but not remove it\n* modules _can and should_ have types in both stable (i.e. `v1` or `v2`) and unstable (`alpha` or `beta`) packages.\n_`alpha` and `beta` should not be used to avoid responsibility for maintaining compatibility._\nWhenever code is released into the wild, especially on a blockchain, there is a high cost to changing things. In some\ncases, for instance with immutable smart contracts, a breaking change may be impossible to fix.\nWhen marking something as `alpha` or `beta`, maintainers should ask the questions:\n* what is the cost of asking others to change their code vs the benefit of us maintaining the optionality to change it?\n* what is the plan for moving this to `v1` and how will that affect users?\n`alpha` or `beta` should really be used to communicate ""changes are planned"".\nAs a case study, gRPC reflection is in the package `grpc.reflection.v1alpha`. It hasn't been changed since\n2017 and it is now used in other widely used software like gRPCurl. Some folks probably use it in production services\nand so if they actually went and changed the package to `grpc.reflection.v1`, some software would break and\nthey probably don't want to do that... So now the `v1alpha` package is more or less the de-facto `v1`. Let's not do that.\nThe following are guidelines for working with non-stable packages:\n* [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix)\n(ex. `v1alpha1`) _should_ be used for non-stable packages\n* non-stable packages should generally be excluded from breaking change detection\n* immutable smart contract modules (i.e. CosmWasm) _should_ block smart contracts/persistent\nscripts from interacting with `alpha`/`beta` packages\n#### Omit v1 suffix\nInstead of using [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix),\nwe can omit `v1` for packages that don't actually have a second version. This\nallows for more concise names for common use cases like `cosmos.bank.Send`.\nPackages that do have a second or third version can indicate that with `.v2`\nor `.v3`.\n### Package Naming\n#### Adopt a short, unique top-level package name\nTop-level packages should adopt a short name that is known to not collide with\nother names in common usage within the Cosmos ecosystem. In the near future, a\nregistry should be created to reserve and index top-level package names used\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\n[ICS](https://github.com/cosmos/ics) specifications could consider a\nshort top-level package like `ics23` based upon the standard number.\n#### Limit sub-package depth\nSub-package depth should be increased with caution. Generally a single\nsub-package is needed for a module or a library. Even though `x` or `modules`\nis used in source code to denote modules, this is often unnecessary for .proto\nfiles as modules are the primary thing sub-packages are used for. Only items which\nare known to be used infrequently should have deep sub-package depths.\nFor the Cosmos SDK, it is recommended that we simply write `cosmos.bank`,\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\ntypes can go straight in the `cosmos` package or we can introduce a\n`cosmos.base` package if needed. Note that this naming _will not_ change\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\n`x/bank`.\n### Message Naming\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\ntypes which are used in transactions will retain the `Msg` prefix as that provides\nhelpful context.\n### Service and RPC Naming\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\nimplement a gRPC query service. We should consider the principle of conciseness\nfor query service and RPC names as these may be called from persistent script\nmodules such as CosmWasm. Also, users may use these query paths from tools like\n[gRPCurl](https://github.com/fullstorydev/grpcurl). As an example, we can shorten\n`/cosmos_sdk.x.bank.v1.QueryService/QueryBalance` to\n`/cosmos.bank.Query/Balance` without losing much useful information.\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`/\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\non the `Query` service, the request and response types would be `QueryBalanceRequest`\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\nand `BalanceResponse`.\n#### Use just `Query` for the query service\nInstead of [Buf's default service suffix recommendation](https://github.com/cosmos/cosmos-sdk/pull/6033),\nwe should simply use the shorter `Query` for query services.\nFor other types of gRPC services, we should consider sticking with Buf's\ndefault recommendation.\n#### Omit `Get` and `Query` from query service RPC names\n`Get` and `Query` should be omitted from `Query` service names because they are\nredundant in the fully-qualified name. For instance, `/cosmos.bank.Query/QueryBalance`\njust says `Query` twice without any new information.\n","**Decision:**\n\nWe will continue to follow the style guide for Protobuf and Buf to the extent feasible for our use case. However, we will deviate from these guidelines in certain situations, where there is clear rationale for doing so.\n\n**Rationale:**\n\nFollowing industry-accepted guidelines and wisdom for the effective usage of protobuf is important for ensuring that our code is readable, maintainable, and extensible. However, it is also important to deviate from these guidelines when necessary to accommodate our specific use case.\n\nIn the case of Protobuf Any, we will adopt it as the recommended approach for encoding interface types. This is because package naming is a central part of the encoding, and fully-qualified message names now appear in encoded messages.\n\nIn terms of directory organization, we will continue to follow Buf's DEFAULT recommendations, but we will deviate in one minor way. We will disable the `PACKAGE_DIRECTORY_MATCH` lint checker. This is because it is convenient for developing code, and the warning from Buf about potential issues with protobuf plugins is not a major concern for our use case.\n\nFinally, we will continue to support gRPC queries in Protobuf. This is because gRPC is a powerful RPC framework that provides a number of benefits over other approaches. Additionally, gRPC queries may be allowed from within persistent scripts by technologies such as CosmWasm in the future, and it is important to be prepared for this possibility."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, KVStore data can be remotely accessed through [Queries](https://github.com/cosmos/cosmos-sdk/blob/main/docs/build/building-modules/02-messages-and-queries.md#queries)\nwhich proceed either through Tendermint and the ABCI, or through the gRPC server.\nIn addition to these request/response queries, it would be beneficial to have a means of listening to state changes as they occur in real time.\n\n## Decision\n","We will modify the `CommitMultiStore` interface and its concrete (`rootmulti`) implementations and introduce a new `listenkv.Store` to allow listening to state changes in underlying KVStores. We don't need to listen to cache stores, because we can't be sure that the writes will be committed eventually, and the writes are duplicated in `rootmulti.Store` eventually, so we should only listen to `rootmulti.Store`.\nWe will introduce a plugin system for configuring and running streaming services that write these state changes and their surrounding ABCI message context to different destinations.\n### Listening\nIn a new file, `store/types/listening.go`, we will create a `MemoryListener` struct for streaming out protobuf encoded KV pairs state changes from a KVStore.\nThe `MemoryListener` will be used internally by the concrete `rootmulti` implementation to collect state changes from KVStores.\n```go\n// MemoryListener listens to the state writes and accumulate the records in memory.\ntype MemoryListener struct {\nstateCache []StoreKVPair\n}\n// NewMemoryListener creates a listener that accumulate the state writes in memory.\nfunc NewMemoryListener() *MemoryListener {\nreturn &MemoryListener{}\n}\n// OnWrite writes state change events to the internal cache\nfunc (fl *MemoryListener) OnWrite(storeKey StoreKey, key []byte, value []byte, delete bool) {\nfl.stateCache = append(fl.stateCache, StoreKVPair{\nStoreKey: storeKey.Name(),\nDelete:   delete,\nKey:      key,\nValue:    value,\n})\n}\n// PopStateCache returns the current state caches and set to nil\nfunc (fl *MemoryListener) PopStateCache() []StoreKVPair {\nres := fl.stateCache\nfl.stateCache = nil\nreturn res\n}\n```\nWe will also define a protobuf type for the KV pairs. In addition to the key and value fields this message\nwill include the StoreKey for the originating KVStore so that we can collect information from separate KVStores and determine the source of each KV pair.\n```protobuf\nmessage StoreKVPair {\noptional string store_key = 1; // the store key for the KVStore this pair originates from\nrequired bool set = 2; // true indicates a set operation, false indicates a delete operation\nrequired bytes key = 3;\nrequired bytes value = 4;\n}\n```\n### ListenKVStore\nWe will create a new `Store` type `listenkv.Store` that the `rootmulti` store will use to wrap a `KVStore` to enable state listening.\nWe will configure the `Store` with a `MemoryListener` which will collect state changes for output to specific destinations.\n```go\n// Store implements the KVStore interface with listening enabled.\n// Operations are traced on each core KVStore call and written to any of the\n// underlying listeners with the proper key and operation permissions\ntype Store struct {\nparent    types.KVStore\nlistener  *types.MemoryListener\nparentStoreKey types.StoreKey\n}\n// NewStore returns a reference to a new traceKVStore given a parent\n// KVStore implementation and a buffered writer.\nfunc NewStore(parent types.KVStore, psk types.StoreKey, listener *types.MemoryListener) *Store {\nreturn &Store{parent: parent, listener: listener, parentStoreKey: psk}\n}\n// Set implements the KVStore interface. It traces a write operation and\n// delegates the Set call to the parent KVStore.\nfunc (s *Store) Set(key []byte, value []byte) {\ntypes.AssertValidKey(key)\ns.parent.Set(key, value)\ns.listener.OnWrite(s.parentStoreKey, key, value, false)\n}\n// Delete implements the KVStore interface. It traces a write operation and\n// delegates the Delete call to the parent KVStore.\nfunc (s *Store) Delete(key []byte) {\ns.parent.Delete(key)\ns.listener.OnWrite(s.parentStoreKey, key, nil, true)\n}\n```\n### MultiStore interface updates\nWe will update the `CommitMultiStore` interface to allow us to wrap a `Memorylistener` to a specific `KVStore`.\nNote that the `MemoryListener` will be attached internally by the concrete `rootmulti` implementation.\n```go\ntype CommitMultiStore interface {\n...\n// AddListeners adds a listener for the KVStore belonging to the provided StoreKey\nAddListeners(keys []StoreKey)\n// PopStateCache returns the accumulated state change messages from MemoryListener\nPopStateCache() []StoreKVPair\n}\n```\n### MultiStore implementation updates\nWe will adjust the `rootmulti` `GetKVStore` method to wrap the returned `KVStore` with a `listenkv.Store` if listening is turned on for that `Store`.\n```go\nfunc (rs *Store) GetKVStore(key types.StoreKey) types.KVStore {\nstore := rs.stores[key].(types.KVStore)\nif rs.TracingEnabled() {\nstore = tracekv.NewStore(store, rs.traceWriter, rs.traceContext)\n}\nif rs.ListeningEnabled(key) {\nstore = listenkv.NewStore(store, key, rs.listeners[key])\n}\nreturn store\n}\n```\nWe will implement `AddListeners` to manage KVStore listeners internally and implement `PopStateCache`\nfor a means of retrieving the current state.\n```go\n// AddListeners adds state change listener for a specific KVStore\nfunc (rs *Store) AddListeners(keys []types.StoreKey) {\nlistener := types.NewMemoryListener()\nfor i := range keys {\nrs.listeners[keys[i]] = listener\n}\n}\n```\n```go\nfunc (rs *Store) PopStateCache() []types.StoreKVPair {\nvar cache []types.StoreKVPair\nfor _, ls := range rs.listeners {\ncache = append(cache, ls.PopStateCache()...)\n}\nsort.SliceStable(cache, func(i, j int) bool {\nreturn cache[i].StoreKey < cache[j].StoreKey\n})\nreturn cache\n}\n```\nWe will also adjust the `rootmulti` `CacheMultiStore` and `CacheMultiStoreWithVersion` methods to enable listening in\nthe cache layer.\n```go\nfunc (rs *Store) CacheMultiStore() types.CacheMultiStore {\nstores := make(map[types.StoreKey]types.CacheWrapper)\nfor k, v := range rs.stores {\nstore := v.(types.KVStore)\n// Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\n// set same listeners on cache store will observe duplicated writes.\nif rs.ListeningEnabled(k) {\nstore = listenkv.NewStore(store, k, rs.listeners[k])\n}\nstores[k] = store\n}\nreturn cachemulti.NewStore(rs.db, stores, rs.keysByName, rs.traceWriter, rs.getTracingContext())\n}\n```\n```go\nfunc (rs *Store) CacheMultiStoreWithVersion(version int64) (types.CacheMultiStore, error) {\n// ...\n// Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\n// set same listeners on cache store will observe duplicated writes.\nif rs.ListeningEnabled(key) {\ncacheStore = listenkv.NewStore(cacheStore, key, rs.listeners[key])\n}\ncachedStores[key] = cacheStore\n}\nreturn cachemulti.NewStore(rs.db, cachedStores, rs.keysByName, rs.traceWriter, rs.getTracingContext()), nil\n}\n```\n### Exposing the data\n#### Streaming Service\nWe will introduce a new `ABCIListener` interface that plugs into the BaseApp and relays ABCI requests and responses\nso that the service can group the state changes with the ABCI requests.\n```go\n// baseapp/streaming.go\n// ABCIListener is the interface that we're exposing as a streaming service.\ntype ABCIListener interface {\n// ListenFinalizeBlock updates the streaming service with the latest FinalizeBlock messages\nListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error\n// ListenCommit updates the steaming service with the latest Commit messages and state changes\nListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []*StoreKVPair) error\n}\n```\n#### BaseApp Registration\nWe will add a new method to the `BaseApp` to enable the registration of `StreamingService`s:\n```go\n// SetStreamingService is used to set a streaming service into the BaseApp hooks and load the listeners into the multistore\nfunc (app *BaseApp) SetStreamingService(s ABCIListener) {\n// register the StreamingService within the BaseApp\n// BaseApp will pass BeginBlock, DeliverTx, and EndBlock requests and responses to the streaming services to update their ABCI context\napp.abciListeners = append(app.abciListeners, s)\n}\n```\nWe will add two new fields to the `BaseApp` struct:\n```go\ntype BaseApp struct {\n...\n// abciListenersAsync for determining if abciListeners will run asynchronously.\n// When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\n// When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\nabciListenersAsync bool\n// stopNodeOnABCIListenerErr halts the node when ABCI streaming service listening results in an error.\n// stopNodeOnABCIListenerErr=true must be paired with abciListenersAsync=false.\nstopNodeOnABCIListenerErr bool\n}\n```\n#### ABCI Event Hooks\nWe will modify the `FinalizeBlock` and `Commit` methods to pass ABCI requests and responses\nto any streaming service hooks registered with the `BaseApp`.\n```go\nfunc (app *BaseApp) FinalizeBlock(req abci.RequestFinalizeBlock) abci.ResponseFinalizeBlock {\nvar abciRes abci.ResponseFinalizeBlock\ndefer func() {\n// call the streaming service hook with the FinalizeBlock messages\nfor _, abciListener := range app.abciListeners {\nctx := app.finalizeState.ctx\nblockHeight := ctx.BlockHeight()\nif app.abciListenersAsync {\ngo func(req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) {\nif err := app.abciListener.FinalizeBlock(blockHeight, req, res); err != nil {\napp.logger.Error(""FinalizeBlock listening hook failed"", ""height"", blockHeight, ""err"", err)\n}\n}(req, abciRes)\n} else {\nif err := app.abciListener.ListenFinalizeBlock(blockHeight, req, res); err != nil {\napp.logger.Error(""FinalizeBlock listening hook failed"", ""height"", blockHeight, ""err"", err)\nif app.stopNodeOnABCIListenerErr {\nos.Exit(1)\n}\n}\n}\n}\n}()\n...\nreturn abciRes\n}\n```\n```go\nfunc (app *BaseApp) Commit() abci.ResponseCommit {\n...\nres := abci.ResponseCommit{\nData:         commitID.Hash,\nRetainHeight: retainHeight,\n}\n// call the streaming service hook with the Commit messages\nfor _, abciListener := range app.abciListeners {\nctx := app.deliverState.ctx\nblockHeight := ctx.BlockHeight()\nchangeSet := app.cms.PopStateCache()\nif app.abciListenersAsync {\ngo func(res abci.ResponseCommit, changeSet []store.StoreKVPair) {\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\napp.logger.Error(""ListenCommit listening hook failed"", ""height"", blockHeight, ""err"", err)\n}\n}(res, changeSet)\n} else {\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\napp.logger.Error(""ListenCommit listening hook failed"", ""height"", blockHeight, ""err"", err)\nif app.stopNodeOnABCIListenerErr {\nos.Exit(1)\n}\n}\n}\n}\n...\nreturn res\n}\n```\n#### Go Plugin System\nWe propose a plugin architecture to load and run `Streaming` plugins and other types of implementations. We will introduce a plugin\nsystem over gRPC that is used to load and run Cosmos-SDK plugins. The plugin system uses [hashicorp/go-plugin](https://github.com/hashicorp/go-plugin).\nEach plugin must have a struct that implements the `plugin.Plugin` interface and an `Impl` interface for processing messages over gRPC.\nEach plugin must also have a message protocol defined for the gRPC service:\n```go\n// streaming/plugins/abci/{plugin_version}/interface.go\n// Handshake is a common handshake that is shared by streaming and host.\n// This prevents users from executing bad plugins or executing a plugin\n// directory. It is a UX feature, not a security feature.\nvar Handshake = plugin.HandshakeConfig{\nProtocolVersion:  1,\nMagicCookieKey:   ""ABCI_LISTENER_PLUGIN"",\nMagicCookieValue: ""ef78114d-7bdf-411c-868f-347c99a78345"",\n}\n// ListenerPlugin is the base struct for all kinds of go-plugin implementations\n// It will be included in interfaces of different Plugins\ntype ABCIListenerPlugin struct {\n// GRPCPlugin must still implement the Plugin interface\nplugin.Plugin\n// Concrete implementation, written in Go. This is only used for plugins\n// that are written in Go.\nImpl baseapp.ABCIListener\n}\nfunc (p *ListenerGRPCPlugin) GRPCServer(_ *plugin.GRPCBroker, s *grpc.Server) error {\nRegisterABCIListenerServiceServer(s, &GRPCServer{Impl: p.Impl})\nreturn nil\n}\nfunc (p *ListenerGRPCPlugin) GRPCClient(\n_ context.Context,\n_ *plugin.GRPCBroker,\nc *grpc.ClientConn,\n) (interface{}, error) {\nreturn &GRPCClient{client: NewABCIListenerServiceClient(c)}, nil\n}\n```\nThe `plugin.Plugin` interface has two methods `Client` and `Server`. For our GRPC service these are `GRPCClient` and `GRPCServer`\nThe `Impl` field holds the concrete implementation of our `baseapp.ABCIListener` interface written in Go.\nNote: this is only used for plugin implementations written in Go.\nThe advantage of having such a plugin system is that within each plugin authors can define the message protocol in a way that fits their use case.\nFor example, when state change listening is desired, the `ABCIListener` message protocol can be defined as below (*for illustrative purposes only*).\nWhen state change listening is not desired than `ListenCommit` can be omitted from the protocol.\n```protobuf\nsyntax = ""proto3"";\n...\nmessage Empty {}\nmessage ListenFinalizeBlockRequest {\nRequestFinalizeBlock  req = 1;\nResponseFinalizeBlock res = 2;\n}\nmessage ListenCommitRequest {\nint64                block_height = 1;\nResponseCommit       res          = 2;\nrepeated StoreKVPair changeSet    = 3;\n}\n// plugin that listens to state changes\nservice ABCIListenerService {\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\n}\n```\n```protobuf\n...\n// plugin that doesn't listen to state changes\nservice ABCIListenerService {\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\n}\n```\nImplementing the service above:\n```go\n// streaming/plugins/abci/{plugin_version}/grpc.go\nvar (\n_ baseapp.ABCIListener = (*GRPCClient)(nil)\n)\n// GRPCClient is an implementation of the ABCIListener and ABCIListenerPlugin interfaces that talks over RPC.\ntype GRPCClient struct {\nclient ABCIListenerServiceClient\n}\nfunc (m *GRPCClient) ListenFinalizeBlock(goCtx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\nctx := sdk.UnwrapSDKContext(goCtx)\n_, err := m.client.ListenDeliverTx(ctx, &ListenDeliverTxRequest{BlockHeight: ctx.BlockHeight(), Req: req, Res: res})\nreturn err\n}\nfunc (m *GRPCClient) ListenCommit(goCtx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\nctx := sdk.UnwrapSDKContext(goCtx)\n_, err := m.client.ListenCommit(ctx, &ListenCommitRequest{BlockHeight: ctx.BlockHeight(), Res: res, ChangeSet: changeSet})\nreturn err\n}\n// GRPCServer is the gRPC server that GRPCClient talks to.\ntype GRPCServer struct {\n// This is the real implementation\nImpl baseapp.ABCIListener\n}\nfunc (m *GRPCServer) ListenFinalizeBlock(ctx context.Context, req *ListenFinalizeBlockRequest) (*Empty, error) {\nreturn &Empty{}, m.Impl.ListenFinalizeBlock(ctx, req.Req, req.Res)\n}\nfunc (m *GRPCServer) ListenCommit(ctx context.Context, req *ListenCommitRequest) (*Empty, error) {\nreturn &Empty{}, m.Impl.ListenCommit(ctx, req.Res, req.ChangeSet)\n}\n```\nAnd the pre-compiled Go plugin `Impl`(*this is only used for plugins that are written in Go*):\n```go\n// streaming/plugins/abci/{plugin_version}/impl/plugin.go\n// Plugins are pre-compiled and loaded by the plugin system\n// ABCIListener is the implementation of the baseapp.ABCIListener interface\ntype ABCIListener struct{}\nfunc (m *ABCIListenerPlugin) ListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\n// send data to external system\n}\nfunc (m *ABCIListenerPlugin) ListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\n// send data to external system\n}\nfunc main() {\nplugin.Serve(&plugin.ServeConfig{\nHandshakeConfig: grpc_abci_v1.Handshake,\nPlugins: map[string]plugin.Plugin{\n""grpc_plugin_v1"": &grpc_abci_v1.ABCIListenerGRPCPlugin{Impl: &ABCIListenerPlugin{}},\n},\n// A non-nil value here enables gRPC serving for this streaming...\nGRPCServer: plugin.DefaultGRPCServer,\n})\n}\n```\nWe will introduce a plugin loading system that will return `(interface{}, error)`.\nThis provides the advantage of using versioned plugins where the plugin interface and gRPC protocol change over time.\nIn addition, it allows for building independent plugin that can expose different parts of the system over gRPC.\n```go\nfunc NewStreamingPlugin(name string, logLevel string) (interface{}, error) {\nlogger := hclog.New(&hclog.LoggerOptions{\nOutput: hclog.DefaultOutput,\nLevel:  toHclogLevel(logLevel),\nName:   fmt.Sprintf(""plugin.%s"", name),\n})\n// We're a host. Start by launching the streaming process.\nenv := os.Getenv(GetPluginEnvKey(name))\nclient := plugin.NewClient(&plugin.ClientConfig{\nHandshakeConfig: HandshakeMap[name],\nPlugins:         PluginMap,\nCmd:             exec.Command(""sh"", ""-c"", env),\nLogger:          logger,\nAllowedProtocols: []plugin.Protocol{\nplugin.ProtocolNetRPC, plugin.ProtocolGRPC},\n})\n// Connect via RPC\nrpcClient, err := client.Client()\nif err != nil {\nreturn nil, err\n}\n// Request streaming plugin\nreturn rpcClient.Dispense(name)\n}\n```\nWe propose a `RegisterStreamingPlugin` function for the App to register `NewStreamingPlugin`s with the App's BaseApp.\nStreaming plugins can be of `Any` type; therefore, the function takes in an interface vs a concrete type.\nFor example, we could have plugins of `ABCIListener`, `WasmListener` or `IBCListener`. Note that `RegisterStreamingPluing` function\nis helper function and not a requirement. Plugin registration can easily be moved from the App to the BaseApp directly.\n```go\n// baseapp/streaming.go\n// RegisterStreamingPlugin registers streaming plugins with the App.\n// This method returns an error if a plugin is not supported.\nfunc RegisterStreamingPlugin(\nbApp *BaseApp,\nappOpts servertypes.AppOptions,\nkeys map[string]*types.KVStoreKey,\nstreamingPlugin interface{},\n) error {\nswitch t := streamingPlugin.(type) {\ncase ABCIListener:\nregisterABCIListenerPlugin(bApp, appOpts, keys, t)\ndefault:\nreturn fmt.Errorf(""unexpected plugin type %T"", t)\n}\nreturn nil\n}\n```\n```go\nfunc registerABCIListenerPlugin(\nbApp *BaseApp,\nappOpts servertypes.AppOptions,\nkeys map[string]*store.KVStoreKey,\nabciListener ABCIListener,\n) {\nasyncKey := fmt.Sprintf(""%s.%s.%s"", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIAsync)\nasync := cast.ToBool(appOpts.Get(asyncKey))\nstopNodeOnErrKey := fmt.Sprintf(""%s.%s.%s"", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIStopNodeOnErrTomlKey)\nstopNodeOnErr := cast.ToBool(appOpts.Get(stopNodeOnErrKey))\nkeysKey := fmt.Sprintf(""%s.%s.%s"", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIKeysTomlKey)\nexposeKeysStr := cast.ToStringSlice(appOpts.Get(keysKey))\nexposedKeys := exposeStoreKeysSorted(exposeKeysStr, keys)\nbApp.cms.AddListeners(exposedKeys)\napp.SetStreamingManager(\nstoretypes.StreamingManager{\nABCIListeners: []storetypes.ABCIListener{abciListener},\nStopNodeOnErr: stopNodeOnErr,\n},\n)\n}\n```\n```go\nfunc exposeAll(list []string) bool {\nfor _, ele := range list {\nif ele == ""*"" {\nreturn true\n}\n}\nreturn false\n}\nfunc exposeStoreKeys(keysStr []string, keys map[string]*types.KVStoreKey) []types.StoreKey {\nvar exposeStoreKeys []types.StoreKey\nif exposeAll(keysStr) {\nexposeStoreKeys = make([]types.StoreKey, 0, len(keys))\nfor _, storeKey := range keys {\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\n}\n} else {\nexposeStoreKeys = make([]types.StoreKey, 0, len(keysStr))\nfor _, keyStr := range keysStr {\nif storeKey, ok := keys[keyStr]; ok {\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\n}\n}\n}\n// sort storeKeys for deterministic output\nsort.SliceStable(exposeStoreKeys, func(i, j int) bool {\nreturn exposeStoreKeys[i].Name() < exposeStoreKeys[j].Name()\n})\nreturn exposeStoreKeys\n}\n```\nThe `NewStreamingPlugin` and `RegisterStreamingPlugin` functions are used to register a plugin with the App's BaseApp.\ne.g. in `NewSimApp`:\n```go\nfunc NewSimApp(\nlogger log.Logger,\ndb dbm.DB,\ntraceStore io.Writer,\nloadLatest bool,\nappOpts servertypes.AppOptions,\nbaseAppOptions ...func(*baseapp.BaseApp),\n) *SimApp {\n...\nkeys := sdk.NewKVStoreKeys(\nauthtypes.StoreKey, banktypes.StoreKey, stakingtypes.StoreKey,\nminttypes.StoreKey, distrtypes.StoreKey, slashingtypes.StoreKey,\ngovtypes.StoreKey, paramstypes.StoreKey, ibchost.StoreKey, upgradetypes.StoreKey,\nevidencetypes.StoreKey, ibctransfertypes.StoreKey, capabilitytypes.StoreKey,\n)\n...\n// register streaming services\nstreamingCfg := cast.ToStringMap(appOpts.Get(baseapp.StreamingTomlKey))\nfor service := range streamingCfg {\npluginKey := fmt.Sprintf(""%s.%s.%s"", baseapp.StreamingTomlKey, service, baseapp.StreamingPluginTomlKey)\npluginName := strings.TrimSpace(cast.ToString(appOpts.Get(pluginKey)))\nif len(pluginName) > 0 {\nlogLevel := cast.ToString(appOpts.Get(flags.FlagLogLevel))\nplugin, err := streaming.NewStreamingPlugin(pluginName, logLevel)\nif err != nil {\ntmos.Exit(err.Error())\n}\nif err := baseapp.RegisterStreamingPlugin(bApp, appOpts, keys, plugin); err != nil {\ntmos.Exit(err.Error())\n}\n}\n}\nreturn app\n```\n#### Configuration\nThe plugin system will be configured within an App's TOML configuration files.\n```toml\n# gRPC streaming\n[streaming]\n# ABCI streaming service\n[streaming.abci]\n# The plugin version to use for ABCI listening\nplugin = ""abci_v1""\n# List of kv store keys to listen to for state changes.\n# Set to [""*""] to expose all keys.\nkeys = [""*""]\n# Enable abciListeners to run asynchronously.\n# When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\n# When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\nasync = false\n# Whether to stop the node on message deliver error.\nstop-node-on-err = true\n```\nThere will be four parameters for configuring `ABCIListener` plugin: `streaming.abci.plugin`, `streaming.abci.keys`, `streaming.abci.async` and `streaming.abci.stop-node-on-err`.\n`streaming.abci.plugin` is the name of the plugin we want to use for streaming, `streaming.abci.keys` is a set of store keys for stores it listens to,\n`streaming.abci.async` is bool enabling asynchronous listening and `streaming.abci.stop-node-on-err` is a bool that stops the node when true and when operating\non synchronized mode `streaming.abci.async=false`. Note that `streaming.abci.stop-node-on-err=true` will be ignored if `streaming.abci.async=true`.\nThe configuration above support additional streaming plugins by adding the plugin to the `[streaming]` configuration section\nand registering the plugin with `RegisterStreamingPlugin` helper function.\nNote the that each plugin must include `streaming.{service}.plugin` property as it is a requirement for doing the lookup and registration of the plugin\nwith the App. All other properties are unique to the individual services.\n#### Encoding and decoding streams\nADR-038 introduces the interfaces and types for streaming state changes out from KVStores, associating this\ndata with their related ABCI requests and responses, and registering a service for consuming this data and streaming it to some destination in a final format.\nInstead of prescribing a final data format in this ADR, it is left to a specific plugin implementation to define and document this format.\nWe take this approach because flexibility in the final format is necessary to support a wide range of streaming service plugins. For example,\nthe data format for a streaming service that writes the data out to a set of files will differ from the data format that is written to a Kafka topic.\n",Create a new KVStore subscription API that allows clients to subscribe to changes made to a KVStore and receive notifications when those changes occur.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nFor signature verification in Cosmos SDK, the signer and verifier need to agree on\nthe same serialization of a `SignDoc` as defined in\n[ADR-020](./adr-020-protobuf-transaction-encoding.md) without transmitting the\nserialization.\nCurrently, for block signatures we are using a workaround: we create a new [TxRaw](https://github.com/cosmos/cosmos-sdk/blob/9e85e81e0e8140067dd893421290c191529c148c/proto/cosmos/tx/v1beta1/tx.proto#L30)\ninstance (as defined in [adr-020-protobuf-transaction-encoding](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-020-protobuf-transaction-encoding.md#transactions))\nby converting all [Tx](https://github.com/cosmos/cosmos-sdk/blob/9e85e81e0e8140067dd893421290c191529c148c/proto/cosmos/tx/v1beta1/tx.proto#L13)\nfields to bytes on the client side. This adds an additional manual\nstep when sending and signing transactions.\n### Decision\nThe following encoding scheme is to be used by other ADRs,\nand in particular for `SignDoc` serialization.\n\n## Decision\n","The following encoding scheme is to be used by other ADRs,\nand in particular for `SignDoc` serialization.\n","The following encoding scheme is to be used by other ADRs, and in particular for `SignDoc` serialization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently `Msg` handlers in the Cosmos SDK do have return values that are placed in the `data` field of the response.\nThese return values, however, are not specified anywhere except in the golang handler code.\nIn early conversations [it was proposed](https://docs.google.com/document/d/1eEgYgvgZqLE45vETjhwIw4VOqK-5hwQtZtjVbiXnIGc/edit)\nthat `Msg` return types be captured using a protobuf extension field, ex:\n```protobuf\npackage cosmos.gov;\nmessage MsgSubmitProposal\noption (cosmos_proto.msg_return) = “uint64”;\nstring delegator_address = 1;\nstring validator_address = 2;\nrepeated sdk.Coin amount = 3;\n}\n```\nThis was never adopted, however.\nHaving a well-specified return value for `Msg`s would improve client UX. For instance,\nin `x/gov`,  `MsgSubmitProposal` returns the proposal ID as a big-endian `uint64`.\nThis isn’t really documented anywhere and clients would need to know the internals\nof the Cosmos SDK to parse that value and return it to users.\nAlso, there may be cases where we want to use these return values programmatically.\nFor instance, https://github.com/cosmos/cosmos-sdk/issues/7093 proposes a method for\ndoing inter-module Ocaps using the `Msg` router. A well-defined return type would\nimprove the developer UX for this approach.\nIn addition, handler registration of `Msg` types tends to add a bit of\nboilerplate on top of keepers and is usually done through manual type switches.\nThis isn't necessarily bad, but it does add overhead to creating modules.\n\n## Decision\n","We decide to use protobuf `service` definitions for defining `Msg`s as well as\nthe code generated by them as a replacement for `Msg` handlers.\nBelow we define how this will look for the `SubmitProposal` message from `x/gov` module.\nWe start with a `Msg` `service` definition:\n```protobuf\npackage cosmos.gov;\nservice Msg {\nrpc SubmitProposal(MsgSubmitProposal) returns (MsgSubmitProposalResponse);\n}\n// Note that for backwards compatibility this uses MsgSubmitProposal as the request\n// type instead of the more canonical MsgSubmitProposalRequest\nmessage MsgSubmitProposal {\ngoogle.protobuf.Any content = 1;\nstring proposer = 2;\n}\nmessage MsgSubmitProposalResponse {\nuint64 proposal_id;\n}\n```\nWhile this is most commonly used for gRPC, overloading protobuf `service` definitions like this does not violate\nthe intent of the [protobuf spec](https://developers.google.com/protocol-buffers/docs/proto3#services) which says:\n> If you don’t want to use gRPC, it’s also possible to use protocol buffers with your own RPC implementation.\nWith this approach, we would get an auto-generated `MsgServer` interface:\nIn addition to clearly specifying return types, this has the benefit of generating client and server code. On the server\nside, this is almost like an automatically generated keeper method and could maybe be used instead of keepers eventually\n(see [\#7093](https://github.com/cosmos/cosmos-sdk/issues/7093)):\n```go\npackage gov\ntype MsgServer interface {\nSubmitProposal(context.Context, *MsgSubmitProposal) (*MsgSubmitProposalResponse, error)\n}\n```\nOn the client side, developers could take advantage of this by creating RPC implementations that encapsulate transaction\nlogic. Protobuf libraries that use asynchronous callbacks, like [protobuf.js](https://github.com/protobufjs/protobuf.js#using-services)\ncould use this to register callbacks for specific messages even for transactions that include multiple `Msg`s.\nEach `Msg` service method should have exactly one request parameter: its corresponding `Msg` type. For example, the `Msg` service method `/cosmos.gov.v1beta1.Msg/SubmitProposal` above has exactly one request parameter, namely the `Msg` type `/cosmos.gov.v1beta1.MsgSubmitProposal`. It is important the reader understands clearly the nomenclature difference between a `Msg` service (a Protobuf service) and a `Msg` type (a Protobuf message), and the differences in their fully-qualified name.\nThis convention has been decided over the more canonical `Msg...Request` names mainly for backwards compatibility, but also for better readability in `TxBody.messages` (see [Encoding section](#encoding) below): transactions containing `/cosmos.gov.MsgSubmitProposal` read better than those containing `/cosmos.gov.v1beta1.MsgSubmitProposalRequest`.\nOne consequence of this convention is that each `Msg` type can be the request parameter of only one `Msg` service method. However, we consider this limitation a good practice in explicitness.\n### Encoding\nEncoding of transactions generated with `Msg` services do not differ from current Protobuf transaction encoding as defined in [ADR-020](./adr-020-protobuf-transaction-encoding.md). We are encoding `Msg` types (which are exactly `Msg` service methods' request parameters) as `Any` in `Tx`s which involves packing the\nbinary-encoded `Msg` with its type URL.\n### Decoding\nSince `Msg` types are packed into `Any`, decoding transactions messages are done by unpacking `Any`s into `Msg` types. For more information, please refer to [ADR-020](./adr-020-protobuf-transaction-encoding.md#transactions).\n### Routing\nWe propose to add a `msg_service_router` in BaseApp. This router is a key/value map which maps `Msg` types' `type_url`s to their corresponding `Msg` service method handler. Since there is a 1-to-1 mapping between `Msg` types and `Msg` service method, the `msg_service_router` has exactly one entry per `Msg` service method.\nWhen a transaction is processed by BaseApp (in CheckTx or in DeliverTx), its `TxBody.messages` are decoded as `Msg`s. Each `Msg`'s `type_url` is matched against an entry in the `msg_service_router`, and the respective `Msg` service method handler is called.\nFor backward compatibility, the old handlers are not removed yet. If BaseApp receives a legacy `Msg` with no corresponding entry in the `msg_service_router`, it will be routed via its legacy `Route()` method into the legacy handler.\n### Module Configuration\nIn [ADR 021](./adr-021-protobuf-query-encoding.md), we introduced a method `RegisterQueryService`\nto `AppModule` which allows for modules to register gRPC queriers.\nTo register `Msg` services, we attempt a more extensible approach by converting `RegisterQueryService`\nto a more generic `RegisterServices` method:\n```go\ntype AppModule interface {\nRegisterServices(Configurator)\n...\n}\ntype Configurator interface {\nQueryServer() grpc.Server\nMsgServer() grpc.Server\n}\n// example module:\nfunc (am AppModule) RegisterServices(cfg Configurator) {\ntypes.RegisterQueryServer(cfg.QueryServer(), keeper)\ntypes.RegisterMsgServer(cfg.MsgServer(), keeper)\n}\n```\nThe `RegisterServices` method and the `Configurator` interface are intended to\nevolve to satisfy the use cases discussed in [\#7093](https://github.com/cosmos/cosmos-sdk/issues/7093)\nand [\#7122](https://github.com/cosmos/cosmos-sdk/issues/7421).\nWhen `Msg` services are registered, the framework _should_ verify that all `Msg` types\nimplement the `sdk.Msg` interface and throw an error during initialization rather\nthan later when transactions are processed.\n### `Msg` Service Implementation\nJust like query services, `Msg` service methods can retrieve the `sdk.Context`\nfrom the `context.Context` parameter method using the `sdk.UnwrapSDKContext`\nmethod:\n```go\npackage gov\nfunc (k Keeper) SubmitProposal(goCtx context.Context, params *types.MsgSubmitProposal) (*MsgSubmitProposalResponse, error) {\nctx := sdk.UnwrapSDKContext(goCtx)\n...\n}\n```\nThe `sdk.Context` should have an `EventManager` already attached by BaseApp's `msg_service_router`.\nSeparate handler definition is no longer needed with this approach.\n","**Adopt a protobuf extension field to specify `Msg` return types.**\n\n**Rationale:**\n\n* **Consistency with protobuf best practices:** Protobuf extensions are a well-supported mechanism for extending protobuf messages. By using an extension field, we can specify return types in a consistent and structured manner.\n* **Improved client UX:** A well-defined return value for `Msg`s will make it easier for clients to parse and return the value to users. This will enhance the overall user experience.\n* **Programmability:** A well-defined return type will enable programmatic use of return values. This will be useful for features such as inter-module Ocaps.\n* **Reduced boilerplate:** Using a protobuf extension field for return types will reduce the boilerplate code for handler registration, simplifying the creation of modules."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to make blockchain transactions, the signing account must possess a sufficient balance of the right denomination\nin order to pay fees. There are classes of transactions where needing to maintain a wallet with sufficient fees is a\nbarrier to adoption.\nFor instance, when proper permissions are setup, someone may temporarily delegate the ability to vote on proposals to\na ""burner"" account that is stored on a mobile phone with only minimal security.\nOther use cases include workers tracking items in a supply chain or farmers submitting field data for analytics\nor compliance purposes.\nFor all of these use cases, UX would be significantly enhanced by obviating the need for these accounts to always\nmaintain the appropriate fee balance. This is especially true if we wanted to achieve enterprise adoption for something\nlike supply chain tracking.\nWhile one solution would be to have a service that fills up these accounts automatically with the appropriate fees, a better UX\nwould be provided by allowing these accounts to pull from a common fee pool account with proper spending limits.\nA single pool would reduce the churn of making lots of small ""fill up"" transactions and also more effectively leverages\nthe resources of the organization setting up the pool.\n\n## Decision\n","As a solution we propose a module, `x/feegrant` which allows one account, the ""granter"" to grant another account, the ""grantee""\nan allowance to spend the granter's account balance for fees within certain well-defined limits.\nFee allowances are defined by the extensible `FeeAllowanceI` interface:\n```go\ntype FeeAllowanceI {\n// Accept can use fee payment requested as well as timestamp of the current block\n// to determine whether or not to process this. This is checked in\n// Keeper.UseGrantedFees and the return values should match how it is handled there.\n//\n// If it returns an error, the fee payment is rejected, otherwise it is accepted.\n// The FeeAllowance implementation is expected to update it's internal state\n// and will be saved again after an acceptance.\n//\n// If remove is true (regardless of the error), the FeeAllowance will be deleted from storage\n// (eg. when it is used up). (See call to RevokeFeeAllowance in Keeper.UseGrantedFees)\nAccept(ctx sdk.Context, fee sdk.Coins, msgs []sdk.Msg) (remove bool, err error)\n// ValidateBasic should evaluate this FeeAllowance for internal consistency.\n// Don't allow negative amounts, or negative periods for example.\nValidateBasic() error\n}\n```\nTwo basic fee allowance types, `BasicAllowance` and `PeriodicAllowance` are defined to support known use cases:\n```protobuf\n// BasicAllowance implements FeeAllowanceI with a one-time grant of tokens\n// that optionally expires. The delegatee can use up to SpendLimit to cover fees.\nmessage BasicAllowance {\n// spend_limit specifies the maximum amount of tokens that can be spent\n// by this allowance and will be updated as tokens are spent. If it is\n// empty, there is no spend limit and any amount of coins can be spent.\nrepeated cosmos_sdk.v1.Coin spend_limit = 1;\n// expiration specifies an optional time when this allowance expires\ngoogle.protobuf.Timestamp expiration = 2;\n}\n// PeriodicAllowance extends FeeAllowanceI to allow for both a maximum cap,\n// as well as a limit per time period.\nmessage PeriodicAllowance {\nBasicAllowance basic = 1;\n// period specifies the time duration in which period_spend_limit coins can\n// be spent before that allowance is reset\ngoogle.protobuf.Duration period = 2;\n// period_spend_limit specifies the maximum number of coins that can be spent\n// in the period\nrepeated cosmos_sdk.v1.Coin period_spend_limit = 3;\n// period_can_spend is the number of coins left to be spent before the period_reset time\nrepeated cosmos_sdk.v1.Coin period_can_spend = 4;\n// period_reset is the time at which this period resets and a new one begins,\n// it is calculated from the start time of the first transaction after the\n// last period ended\ngoogle.protobuf.Timestamp period_reset = 5;\n}\n```\nAllowances can be granted and revoked using `MsgGrantAllowance` and `MsgRevokeAllowance`:\n```protobuf\n// MsgGrantAllowance adds permission for Grantee to spend up to Allowance\n// of fees from the account of Granter.\nmessage MsgGrantAllowance {\nstring granter = 1;\nstring grantee = 2;\ngoogle.protobuf.Any allowance = 3;\n}\n// MsgRevokeAllowance removes any existing FeeAllowance from Granter to Grantee.\nmessage MsgRevokeAllowance {\nstring granter = 1;\nstring grantee = 2;\n}\n```\nIn order to use allowances in transactions, we add a new field `granter` to the transaction `Fee` type:\n```protobuf\npackage cosmos.tx.v1beta1;\nmessage Fee {\nrepeated cosmos.base.v1beta1.Coin amount = 1;\nuint64 gas_limit = 2;\nstring payer = 3;\nstring granter = 4;\n}\n```\n`granter` must either be left empty or must correspond to an account which has granted\na fee allowance to fee payer (either the first signer or the value of the `payer` field).\nA new `AnteDecorator` named `DeductGrantedFeeDecorator` will be created in order to process transactions with `fee_payer`\nset and correctly deduct fees based on fee allowances.\n",**Decision:** Implement a common fee pool account with proper spending limits to allow signing accounts to pull from a shared resource for transaction fees.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https://github.com/cosmos/cosmos-sdk/issues/5467) and [4982](https://github.com/cosmos/cosmos-sdk/issues/4982) for additional context.\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\n\n## Decision\n","Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\n### Account interface (x/auth)\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\nnow be stored in & managed by the bank module.\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\naccount balance.\nVesting accounts will continue to store original vesting, delegated free, and delegated\nvesting coins (which is safe since these cannot contain arbitrary denominations).\n### Bank keeper (x/bank)\nThe following APIs will be added to the `x/bank` keeper:\n* `GetAllBalances(ctx Context, addr AccAddress) Coins`\n* `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\n* `SetBalance(ctx Context, addr AccAddress, coin Coin)`\n* `LockedCoins(ctx Context, addr AccAddress) Coins`\n* `SpendableCoins(ctx Context, addr AccAddress) Coins`\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\ncore functionality or persistence.\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\nbut retrieval of all balances for a single account is presumed to be more frequent):\n```go\nvar BalancesPrefix = []byte(""balances"")\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\nif !balance.IsValid() {\nreturn err\n}\nstore := ctx.KVStore(k.storeKey)\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\nbz := Marshal(balance)\naccountStore.Set([]byte(balance.Denom), bz)\nreturn nil\n}\n```\nThis will result in the balances being indexed by the byte representation of\n`balances/{address}/{denom}`.\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\naccount balance by denomination found in the (un)delegation amount. As a result,\nany mutations to the account balance by will made by denomination.\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\ndirectly instead of calling `GetCoins()` / `SetCoins()` (which no longer exist).\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\naccount balances.\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\npossible as to not load the entire account balance.\n### Supply module\nThe supply module, in order to implement the total supply invariant, will now need\nto scan all accounts & call `GetAllBalances` using the `x/bank` Keeper, then sum\nthe balances and check that they match the expected total supply.\n","Regarding the potential denial-of-service (DoS) concern resulting from a high number of denominations within the `sdk.Coins` structure, the decision is to:\n\n**Introduce a soft cap on the number of distinct denominations an account can hold.**\n\n**Rationale:**\n\n* Implementing a hard limit could still cause griefing vectors where a malicious actor could fill an account with a large number of different denominations to prevent the account holder from using the account.\n* A soft cap allows the modification of account balances using a bounded number of denominations, allowing the system to handle normal operations while mitigating the DoS risk.\n* This approach allows for flexibility and customization, as the soft cap can be adjusted if the system's capabilities evolve."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe concrete use cases which motivated this module include:\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\ndelegated stake\n* ""sub-keys"" functionality, as originally proposed in [\#4480](https://github.com/cosmos/cosmos-sdk/issues/4480) which\nis a term used to describe the functionality provided by this module together with\nthe `fee_grant` module from [ADR 029](./adr-029-fee-grant-module.md) and the [group module](https://github.com/cosmos/cosmos-sdk/tree/main/x/group).\nThe ""sub-keys"" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\nkeys.\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https://github.com/cosmos-gaians/cosmos-sdk/tree/hackatom/x/delegation).\n\n## Decision\n","We will create a module named `authz` which provides functionality for\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\nmust be granted for a particular `Msg` service methods one by one using an implementation\nof `Authorization` interface.\n### Types\nAuthorizations determine exactly what privileges are granted. They are extensible\nand can be defined for any `Msg` service method even outside of the module where\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\n#### Authorization\n```go\ntype Authorization interface {\nproto.Message\n// MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\n// which will process and accept or reject a request.\nMsgTypeURL() string\n// Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\n// so provides an upgraded authorization instance.\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\n// ValidateBasic does a simple validation check that\n// doesn't require access to any other information.\nValidateBasic() error\n}\n// AcceptResponse instruments the controller of an authz message if the request is accepted\n// and if it should be updated or deleted.\ntype AcceptResponse struct {\n// If Accept=true, the controller can accept and authorization and handle the update.\nAccept bool\n// If Delete=true, the controller must delete the authorization object and release\n// storage resources.\nDelete bool\n// Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\n// it must use the updated version and handle the update on the storage level.\nUpdated Authorization\n}\n```\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\na `SpendLimit` and updates it down to zero:\n```go\ntype SendAuthorization struct {\n// SpendLimit specifies the maximum amount of tokens that can be spent\n// by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\n// can be used with bank msg type url to create limit less bank authorization).\nSpendLimit sdk.Coins\n}\nfunc (a SendAuthorization) MsgTypeURL() string {\nreturn sdk.MsgTypeURL(&MsgSend{})\n}\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\nmSend, ok := msg.(*MsgSend)\nif !ok {\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(""type mismatch"")\n}\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\nif isNegative {\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(""requested amount is more than spend limit"")\n}\nif limitLeft.IsZero() {\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\n}\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\n}\n```\nA different type of capability for `MsgSend` could be implemented\nusing the `Authorization` interface with no need to change the underlying\n`bank` module.\n##### Small notes on `AcceptResponse`\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\nthe field will be `nil`.\n### `Msg` Service\n```protobuf\nservice Msg {\n// Grant grants the provided authorization to the grantee on the granter's\n// account with the provided expiration time.\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\n// Exec attempts to execute the provided messages using\n// authorizations granted to the grantee. Each message should have only\n// one signer corresponding to the granter of the authorization.\nrpc Exec(MsgExec) returns (MsgExecResponse);\n// Revoke revokes any authorization corresponding to the provided method name on the\n// granter's account that has been granted to the grantee.\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\n}\n// Grant gives permissions to execute\n// the provided method with expiration time.\nmessage Grant {\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = ""cosmos.authz.v1beta1.Authorization""];\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\n}\nmessage MsgGrant {\nstring granter = 1;\nstring grantee = 2;\nGrant grant = 3 [(gogoproto.nullable) = false];\n}\nmessage MsgExecResponse {\ncosmos.base.abci.v1beta1.Result result = 1;\n}\nmessage MsgExec {\nstring   grantee                  = 1;\n// Authorization Msg requests to execute. Each msg must implement Authorization interface\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = ""cosmos.base.v1beta1.Msg""];;\n}\n```\n### Router Middleware\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\nto the router based on `Authorization` grants:\n```go\ntype Keeper interface {\n// DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\n// to send those messages by the first (and only) signer of each msg.\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\n}\n```\n### CLI\n#### `tx exec` Method\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\nwould send a transaction like this:\n```go\nMsgExec {\nGrantee: mykey,\nMsgs: []sdk.Msg{\nMsgVote {\nProposalID: 1,\nVoter: cosmos3thsdgh983egh823\nOption: Yes\n}\n}\n}\n```\n#### `tx grant <grantee> <authorization> --from <granter>`\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\nJSON on the CLI.\n#### `tx revoke <grantee> <method-name> --from <granter>`\nThis CLI command will send a `MsgRevoke` transaction.\n### Built-in Authorizations\n#### `SendAuthorization`\n```protobuf\n// SendAuthorization allows the grantee to spend up to spend_limit coins from\n// the granter's account.\nmessage SendAuthorization {\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\n}\n```\n#### `GenericAuthorization`\n```protobuf\n// GenericAuthorization gives the grantee unrestricted permissions to execute\n// the provided method on behalf of the granter's account.\nmessage GenericAuthorization {\noption (cosmos_proto.implements_interface) = ""Authorization"";\n// Msg, identified by it's type URL, to grant unrestricted permissions to execute\nstring msg = 1;\n}\n```\n","The decision is to implement a delegation module that allows users to delegate their voting power to other accounts. This will enable users to cast votes on proposals without having to delegate their stake to those accounts. The module will also allow users to create ""sub-keys"" that have limited permissions, such as the ability to spend small amounts of funds or vote on proposals. This will allow users to grant specific permissions to other accounts without giving them full access to their accounts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\n[runTx()](https://github.com/cosmos/cosmos-sdk/blob/bad4ca75f58b182f600396ca350ad844c18fc80b/baseapp/baseapp.go#L539)\nmethod. We think that this method can be more flexible and can give Cosmos SDK users more options for customizations without\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\nmight be handled in a ""standard"" way (middleware) alongside the others.\nWe propose middleware-solution, which could help developers implement the following cases:\n* add external logging (let's say sending reports to external services like [Sentry](https://sentry.io));\n* call panic for specific error cases;\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#recovery-middleware)).\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\nin order to customize it.\n\n## Decision\n","### Design\n#### Overview\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\ncan be found [here](https://github.com/cosmos/cosmos-sdk/pull/6053).\n#### Implementation details\n##### Recovery handler\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\n`recover()` from the `builtin` package.\n```go\ntype RecoveryHandler func(recoveryObj interface{}) error\n```\nHandler should type assert (or other methods) an object to define if object should be handled.\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\nAn example:\n```go\nfunc exampleErrHandler(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(error)\nif !ok { return nil }\nif someSpecificError.Is(err) {\npanic(customPanicMsg)\n} else {\nreturn nil\n}\n}\n```\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\n##### Recovery middleware\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\nchain processing.\n```go\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\nif err := handler(recoveryObj); err != nil {\nreturn nil, err\n}\nreturn next, nil\n}\n}\n```\nFunction receives a `recoveryObj` object and returns:\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\n`OutOfGas` middleware example:\n```go\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\nif !ok { return nil }\nreturn errorsmod.Wrap(\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\n""out of gas in location: %v; gasWanted: %d, gasUsed: %d"", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\n),\n)\n}\nreturn newRecoveryMiddleware(handler, next)\n}\n```\n`Default` middleware example:\n```go\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nreturn errorsmod.Wrap(\nsdkerrors.ErrPanic, fmt.Sprintf(""recovered: %v\nstack:\n%v"", recoveryObj, string(debug.Stack())),\n)\n}\nreturn newRecoveryMiddleware(handler, nil)\n}\n```\n##### Recovery processing\nBasic chain of middlewares processing would look like:\n```go\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\nif middleware == nil { return nil }\nnext, err := middleware(recoveryObj)\nif err != nil { return err }\nif next == nil { return nil }\nreturn processRecovery(recoveryObj, next)\n}\n```\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\n`default` handler which must return an `error`.\n##### BaseApp changes\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\n```go\ntype BaseApp struct {\n// ...\nrunTxRecoveryMiddleware recoveryMiddleware\n}\nfunc NewBaseApp(...) {\n// ...\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\n}\nfunc (app *BaseApp) runTx(...) {\n// ...\ndefer func() {\nif r := recover(); r != nil {\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\nerr, result = processRecovery(r, recoveryMW), nil\n}\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\n}()\n// ...\n}\n```\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\n```go\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\nfor _, h := range handlers {\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\n}\n}\n```\nThis method would prepend handlers to an existing chain.\n","**Implement a middleware-based approach to error handling in BaseApp.**\n\n**Benefits:**\n\n* Allows developers to write custom error handlers for panic recovery.\n* Provides a more flexible and customizable error handling mechanism.\n* Enables specific cases like logging and panicking to be handled in a standard way.\n* Addresses the specific need for node crashing when node <-> sidecar communication breaks, ensuring deterministic state machine execution.\n\n**Implementation Details:**\n\n* Introduce a new `RecoveryHandler` interface with a `Handle` method, which takes a `Handler` function as an argument.\n* Add a new `Middleware` field to the `BaseApp` struct, which is a slice of `RecoveryHandler` middleware.\n* Update the `runTx` method to iterate over the `Middleware` slice and call the `Handle` method for each middleware.\n* Provide a default `RecoveryHandler` middleware that wraps the recovery object as an error and logs it.\n* Allow developers to write and register custom `RecoveryHandler` middleware to handle specific error cases.\n\n**Example Middleware Implementation:**\n\n```go\ntype outOfGasMiddleware struct{}\n\nfunc (oom outOfGasMiddleware) Handle(next sdk.Handler) sdk.Handler {\n    return func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n        res, err := next(ctx, msg)\n        if err == sdk.ErrorOutOfGas {\n            // Handle OutOfGas error\n        }\n        return res, err\n    }\n}\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https://github.com/tendermint/tendermint/issues/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\n\n## Decision\n","### Pseudo procedure for consensus key rotation\n* create new random consensus key.\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\n* start validating with new consensus key.\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\n### Considerations\n* consensus key mapping information management strategy\n* store history of each key mapping changes in the kvstore.\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\n* the state machine does not need any historical mapping information which is past more than unbonding period.\n* key rotation costs related to LCD and IBC\n* LCD and IBC will have traffic/computation burden when there exists frequent power changes\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\n* limits\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\n* parameters can be decided by governance and stored in genesis file.\n* key rotation fee\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\n* evidence module\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\n* abci.ValidatorUpdate\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\n* new genesis parameters in `staking` module\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\n### Workflow\n1. The validator generates a new consensus keypair.\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\n```go\ntype MsgRotateConsPubKey struct {\nValidatorAddress  sdk.ValAddress\nNewPubKey         crypto.PubKey\n}\n```\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\n4. `RotateConsPubKey`\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\n* checks if the signing account has enough balance to pay `KeyRotationFee`\n* pays `KeyRotationFee` to community fund\n* overwrites `NewPubKey` in `validator.ConsPubKey`\n* deletes old `ValidatorByConsAddr`\n* `SetValidatorByConsAddr` for `NewPubKey`\n* Add `ConsPubKeyRotationHistory` for tracking rotation\n```go\ntype ConsPubKeyRotationHistory struct {\nOperatorAddress         sdk.ValAddress\nOldConsPubKey           crypto.PubKey\nNewConsPubKey           crypto.PubKey\nRotatedHeight           int64\n}\n```\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\n```go\nabci.ValidatorUpdate{\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\nPower:  0,\n}\nabci.ValidatorUpdate{\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\nPower:  v.ConsensusPower(),\n}\n```\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\n* Note : All above features shall be implemented in `staking` module.\n","**Decision:** Implement a simple form of validator consensus key rotation feature in Cosmos SDK.\n\n**Rationale:**\n\n* The requested feature has been requested for a long time and is considered important for safer validator key management.\n* The implementation is relatively simple and does not require changes to Tendermint's consensus logic.\n* This proposal does not include the more complex concept of multiple consensus keys, which will be considered as a long-term goal."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\n[Our platform](http://github.com/ovrclk/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https://github.com/ovrclk/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\n\n## Decision\n","**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\n```go\n// types/events.go\n// EmitTypedEvent takes typed event and emits converting it into sdk.Event\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\nevtType := proto.MessageName(event)\nevtJSON, err := codec.ProtoMarshalJSON(event)\nif err != nil {\nreturn err\n}\nvar attrMap map[string]json.RawMessage\nerr = json.Unmarshal(evtJSON, &attrMap)\nif err != nil {\nreturn err\n}\nvar attrs []abci.EventAttribute\nfor k, v := range attrMap {\nattrs = append(attrs, abci.EventAttribute{\nKey:   []byte(k),\nValue: v,\n})\n}\nem.EmitEvent(Event{\nType:       evtType,\nAttributes: attrs,\n})\nreturn nil\n}\n// ParseTypedEvent converts abci.Event back to typed event\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\nconcreteGoType := proto.MessageType(event.Type)\nif concreteGoType == nil {\nreturn nil, fmt.Errorf(""failed to retrieve the message of type %q"", event.Type)\n}\nvar value reflect.Value\nif concreteGoType.Kind() == reflect.Ptr {\nvalue = reflect.New(concreteGoType.Elem())\n} else {\nvalue = reflect.Zero(concreteGoType)\n}\nprotoMsg, ok := value.Interface().(proto.Message)\nif !ok {\nreturn nil, fmt.Errorf(""%q does not implement proto.Message"", event.Type)\n}\nattrMap := make(map[string]json.RawMessage)\nfor _, attr := range event.Attributes {\nattrMap[string(attr.Key)] = attr.Value\n}\nattrBytes, err := json.Marshal(attrMap)\nif err != nil {\nreturn nil, err\n}\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\nif err != nil {\nreturn nil, err\n}\nreturn protoMsg, nil\n}\n```\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\n**Step-2**: Add proto definitions for typed events for msgs in each module:\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\n```protobuf\n// proto/cosmos/gov/v1beta1/gov.proto\n// Add typed event definition\npackage cosmos.gov.v1beta1;\nmessage EventSubmitProposal {\nstring from_address   = 1;\nuint64 proposal_id    = 2;\nTextProposal proposal = 3;\n}\n```\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\n```go\n// x/gov/handler.go\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\n...\ntypes.Context.EventManager().EmitTypedEvent(\n&EventSubmitProposal{\nFromAddress: fromAddress,\nProposalId: id,\nProposal: proposal,\n},\n)\n...\n}\n```\n### How to subscribe to these typed events in `Client`\n> NOTE: Full code example below\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\nAkash Network has built a simple [`pubsub`](https://github.com/ovrclk/akash/blob/90d258caeb933b611d575355b8df281208a214f8/pubsub/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https://github.com/ovrclk/akash/blob/90d258caeb933b611d575355b8df281208a214f8/events/publish.go#L21) them as typed events.\nPlease see the below code sample for more detail on this flow looks for clients.\n","The proposal to implement typed events in the Cosmos SDK is accepted. This will enable Cosmos SDK applications to quickly and easily build event-driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, an address can cast a vote with only one options (Yes/No/Abstain/NoWithVeto) and use their full voting power behind that choice.\nHowever, often times the entity owning that address might not be a single individual.  For example, a company might have different stakeholders who want to vote differently, and so it makes sense to allow them to split their voting power.  Another example use case is exchanges.  Many centralized exchanges often stake a portion of their users' tokens in their custody.  Currently, it is not possible for them to do ""passthrough voting"" and giving their users voting rights over their tokens.  However, with this system, exchanges can poll their users for voting preferences, and then vote on-chain proportionally to the results of the poll.\n\n## Decision\n","We modify the vote structs to be\n```go\ntype WeightedVoteOption struct {\nOption string\nWeight sdk.Dec\n}\ntype Vote struct {\nProposalID int64\nVoter      sdk.Address\nOptions    []WeightedVoteOption\n}\n```\nAnd for backwards compatibility, we introduce `MsgVoteWeighted` while keeping `MsgVote`.\n```go\ntype MsgVote struct {\nProposalID int64\nVoter      sdk.Address\nOption     Option\n}\ntype MsgVoteWeighted struct {\nProposalID int64\nVoter      sdk.Address\nOptions    []WeightedVoteOption\n}\n```\nThe `ValidateBasic` of a `MsgVoteWeighted` struct would require that\n1. The sum of all the Rates is equal to 1.0\n2. No Option is repeated\nThe governance tally function will iterate over all the options in a vote and add to the tally the result of the voter's voting power * the rate for that option.\n```go\ntally() {\nresults := map[types.VoteOption]sdk.Dec\nfor _, vote := range votes {\nfor i, weightedOption := range vote.Options {\nresults[weightedOption.Option] += getVotingPower(vote.voter) * weightedOption.Weight\n}\n}\n}\n```\nThe CLI command for creating a multi-option vote would be as such:\n```shell\nsimd tx gov vote 1 ""yes=0.6,no=0.3,abstain=0.05,no_with_veto=0.05"" --from mykey\n```\nTo create a single-option vote a user can do either\n```shell\nsimd tx gov vote 1 ""yes=1"" --from mykey\n```\nor\n```shell\nsimd tx gov vote 1 yes --from mykey\n```\nto maintain backwards compatibility.\n","**Decision:**\n\n**Implement a voting system that allows addresses to split their voting power across multiple options.**\n\n**Justification:**\n\n* **Supports diverse stakeholder interests:** Allows entities with multiple stakeholders to cast proportional votes based on their preferences.\n* **Enables passthrough voting:** Facilitates exchanges and other custodians to aggregate user preferences and vote proportionally on their behalf.\n* **Promotes flexibility and granularity:** Provides fine-grained control over vote allocation, allowing users to express nuanced preferences.\n* **Improves representation and inclusivity:** Ensures that all stakeholders have a fair opportunity to participate in decision-making.\n\n**Consequences:**\n\n* **Increased complexity:** Introducing splitting of voting power requires careful design and implementation to avoid potential confusion or abuse.\n* **Potential for fragmentation:** Allowing multiple options per address could lead to dispersed voting power and difficulty in reaching consensus.\n* **Security considerations:** Implementation must ensure proper validation of vote splits to prevent double-counting or fraudulent voting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nPreviously as part of the xpub project we used `Formik` as our form framework of choice. This relied on using Wrapper components and HOC's for form/ input control. Between then and now React has introduced [Hooks](https://reactjs.org/blog/2019/02/06/react-v16.8.0.html) which many libraries, including `Formik` have implemented to provide easier to use APIs allowing for more readable and concise components. A number of new form frameworks built entirely around hooks have also emerged which we felt we should also consider before settling on a library.\n#### formik\nhttps://github.com/libero/reviewer-client/issues/83\nFormik seems to have improved a lot since we last worked with it. The introduction of hooks has made implementation simpler and more readable and some of the performance issues we had encountered relating to unnecessary re-renders when using the `Field` component have been fixed.\n`Formik` is quite a mature library with its initial release &gt; 3 years ago. The community is very active and there are plenty of resources / examples out there of different use cases.\n#### unform\nThis is a relatively new library first being published 9 months ago. We didn't do to much investigating here as the issues backlog and tagged release dates hint at the community not being widely active and updates not being too frequent (last updated 4 months ago as of writing this).\nUnform offers hooks however to register your own form components you need to create your own refs and use `useEffect` to register fields. This makes implementation code more verbose and complicated to follow.\nUnform doesn't include any extra dependencies and is quite light weight but this is probably down to it missing some key features which the other libraries do offer (see [RFC](https://github.com/Rocketseat/unform/issues/118))\n#### react-hook-form\nhttps://github.com/libero/reviewer-client/issues/84\nDesigned entirely around hooks and with performance in mind `react-hook-form` uses refs to the DOM elements to maintain form state which makes registering form fields straight forward and is intended to limit any unnecessary re-renders. This may offer us more control over what is rendered in the DOM so we can be more accessibility conscious when developing components.\n`react-hook-form` is very light weight and ships with no dependencies. By default it only requires the use of a single `useForm` hook and then can be used with `form` and `input` elements by passing `ref` props. This helps to make form code clean and readable.\nThe library is relatively new first published 10 months ago. The community seems responsive and updates are released frequently.\n### Decision\nWe have decided to go with `react-hook-form` based on the following:\n- little difference in functionality between `formik` and `react-hook-form` but `react-hook-form` api made code easier to follow and slightly less verbose in most use-cases\n- `react-hook-form` has 0 dependencies and a slightly smaller bundle size.\n- seems to have a greater focus on implementation through hooks rather than `Input` components meaning we should find it easier to control what is rendered to the DOM (good for accessibility)\n\n## Decision\n",We have decided to go with `react-hook-form` based on the following:\n- little difference in functionality between `formik` and `react-hook-form` but `react-hook-form` api made code easier to follow and slightly less verbose in most use-cases\n- `react-hook-form` has 0 dependencies and a slightly smaller bundle size.\n- seems to have a greater focus on implementation through hooks rather than `Input` components meaning we should find it easier to control what is rendered to the DOM (good for accessibility)\n,Use `react-hook-form` as the form framework for the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA microservices architecture is complex and we'll need to make many decisions.\nWe'll need a way to keep track of the important decision we make, so that we can revisit and re-evalute them in the future.\nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.\n\n## Decision\n","We've decided to use [Michael Nygard's lightweight architectural decision record (LADR)](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) format.\nLADR is text based and is lightweight enough to meet our needs.\nWe'll keep each LADR record in its own text file and manage the files like code.\nWe also considered the following alternative solutions:\n* Project management tooling (not selected, because we didn't want to install tools)\n* Informal or ""word of mouth"" record keeping (not reliable)\n",**Use an Architectural Decision Record (ADR) template to document important architectural decisions.**\n\n**Benefits:**\n\n* Provides a standardized format for documenting decisions\n* Facilitates easy revisions and tracking of changes\n* Ensures consistency and transparency in decision-making\n* Promotes knowledge sharing and understanding across the team\n* Allows for future review and re-evaluation of decisions without requiring additional software tools
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe resources, database, storage and API endpoints needs to be managed as a unit.\nWhen build our application with the Serverless framework we get CloudFormation templates that manages our infrastructure in AWS.\n\n## Decision\n",To help package the individual AWS resource into a serverless application we will use [Serverless](https://www.serverless.com/) framework.\nWe also considered the following alternative solutions:\n* Manual creation of resource in AWS throurgh the AWS Console.\n* AWS Serverless Application Model (AWS SAM)\n,"Use the Serverless framework to manage the infrastructure for the application. This will provide a consistent experience for managing the infrastructure, and will ensure that all resources are created and managed in a consistent way."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\n\n## Decision\n,"The frontend will be build with [React](https://reactjs.org/) and [Snowpack](https://www.snowpack.dev) will be used as build tool.\nThe application will be styled with [Tailwind CSS](https://tailwindcss.com)\nWe also considered the following alternative solutions:\n* Vanilla HTML would work for the scope of this application.\n* Vanilla CSS would work, but take more time to generate nice layouts.\n","**Decision:** Use a Progressive Web App (PWA)\n\n**Rationale:**\n\n* PWAs offer a responsive and device-agnostic experience, making them suitable for use on various devices.\n* They provide a native-like experience while being easy to install and update.\n* PWAs can be deployed and distributed through web browsers, eliminating the need for app stores or software installations.\n* They support offline caching, enabling users to access the API even without an internet connection.\n* PWAs integrate with the device's operating system, allowing for notifications, push messages, and other native features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nComponents and Projects pages display entire contents of the database in one table. Reading data, generating HTML\npage, and rendering the table (which is paginated by datatables on the client) all take a long time with thousands of\nentities.\nDatatables supports server-side processing for displaying and paginating data. For that, the server must be able to\ntell datatables how many rows are in the table in total and to load a page of data starting from given index.\nHowever, CouchDB cookbook strongly discourages loading of data starting from some index because of performance\nconcerns. Instead, loading data starting from a specific key should be used. This is incompatible with what datatables\nrequires and also makes going to previous pages highly complicated.\nTo support sorting of the table by multiple columns would require creating a CouchDB view per column.\nIn addition, projects are filtered by visibility in backend after loading from CouchDB. This filtering cannot be\nimplemented in CouchDB. This complicates the matters even further with regards to pagination of projects table in\nbackend.\n### Decision\nWe will not use datatables' server-side processing as it's not worth the effort.\nWe will load only some number of latest components by default and let the user increase that number up to all\navailable components. We will make this choice sticky between sessions.\nWe will not change the projects table for now as the users have the option of loading only the projects from their\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\n### Status\nAccepted\n### Consequences\nUsers will not see some components that are already created in the system and may try to create the ""missing""\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\nthe total number of components in the system.\nLoading time of unfiltered projects table will still be slow with thousands of projects.\n\n## Decision\n","We will not use datatables' server-side processing as it's not worth the effort.\nWe will load only some number of latest components by default and let the user increase that number up to all\navailable components. We will make this choice sticky between sessions.\nWe will not change the projects table for now as the users have the option of loading only the projects from their\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\n### Status\nAccepted\n### Consequences\nUsers will not see some components that are already created in the system and may try to create the ""missing""\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\nthe total number of components in the system.\nLoading time of unfiltered projects table will still be slow with thousands of projects.\n",We will not use datatables' server-side processing as it's not worth the effort. We will load only some number of latest components by default and let the user increase that number up to all available components. We will make this choice sticky between sessions. We will not change the projects table for now as the users have the option of loading only the projects from their group and are not disturbed much by the performance of the page when all projects are displayed.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\nRecently, we added a ""companion"" Docker image which is used by `cx-server` to run scripts.\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\n\n## Decision\n","We move the bash script inside the `s4sdk/cxserver-companion` Docker image.\nThe old `cx-server` script just delegates the command to the script inside the companion container.\nA new `cx-server.bat` script is added, doing the same for Windows.\nWe don't use PowerShell to increase compatibility with Windows.\n","Move `cx-server` into the ""companion"" Docker image and provide a Windows compatible script file as a wrapper."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe provide a CD Pipeline for SAP Cloud Platform applications, which adheres to the ""build once"" principle.\nStill, the pipeline does each build step, like building the artifact, running tests or static code checks in separate stages.\nWe use Maven for building the application, using Gradle or other build tools is not an option without much rework.\nFor this document, the term ""release"" (noun) refers to a uniquely identifiable version of software.\nThis includes the source code version (commit or tag) from which the artifacts are built, and the build artifacts themselves.\nThe verb ""to release"" refers to the process of creating a new release.\nPart of this process is to determine the version number of the release candidate.\nThe release candidate becomes a release, when its build pipeline succeeded, and the build artifact is deployed to the Cloud Platform and the artifact repository.\nWith Maven, this is usually facilitated with the [Maven Release Plugin](http://maven.apache.org/maven-release/maven-release-plugin/).\nUsing this plugin does not satisfy our requirements as described below.\nThe pipeline automatically uploads build artifacts to an artifact repository and deploys the app to the Cloud Platform.\nArtifact uploads and deployments happen only for commits on the so co called ""productive branch"" (`master` by default).\nMaven's versioning schema appends `SNAPSHOT` suffix to versions which are not released.\nA version like `1.0.2-SNAPSHOT` does not say from which commit this was built.\nArtifact repositories might delete `SNAPSHOT` versions after some time, because those are not releases.\n\n## Decision\n","We implement an automated versioning schema, in which each commit to the productive branch is equivalent to a new release.\nThis feature is enabled by default, but can be disabled.\nThe version number shall contain a human readable _build_ time stamp (ISO 8601, without colons for file-name compatibility on Windows, always `UTC`) and the git commit id of the most recent commit to `master`, for example `2.7.3-2018-03-02T114757UTC_ff46bb0f00a663018f3efea697b2fb5e86fe6d41`.\nAn auto-created release does not imply creating a tag in the repository.\nCreating tags may be done manually to mark noteworthy versions by the developer.\n### Reasoning\n* Each commit on `master` is a new release: We assume the work happens in feature branches, which are merged once they implement a feature and meet the team's definition of done.\nMerging to `master` is implicitly approval for release.\n* Feature can be disabled: You might still have builds which don't follow this release approach.\nFor those, it must be possible to disable automatic versioning.\n* _Build_ instead of _commit_ time stamp: This implies that multiple builds of the same commit have a different version number.\nThis avoids conflicts, when uploading a second build of a commit to a artifact repository.\n* Always ISO 8601 date-time format: Can be sorted in lexical order which results in a chronological list.\n* Always `UTC`: Most simple solution, avoids daylight saving time issues and is unambiguous for teams working distributed in multiple time zones.\n* Don't create git tags: The version number contains the commit id, which is sufficient to check out this particular version.\nIf we created tags automatically for each version, tags would be cluttered very quickly.\nTags still can be used to mark a version on purpose, with semantic versioning if desired.\n","We build and test everything on the productive branch (`master` by default).\nThe artifact produced by the build step doesn't have a `SNAPSHOT` suffix in its version.\nEach commit on the productive branch produces a new release candidate.\nAt most one release candidate exists for any point in time.\nIf the build pipeline for a release candidate fails, the release candidate is discarded.\nIf the build pipeline for a release candidate succeeds, the artifact is deployed to the Cloud Platform and uploaded to the artifact repository.\nThe release candidate becomes a release.\nWe use version tags to identify releases.\nThe version tag is created as part of the build pipeline.\nIt contains the version of the release candidate and the Git hash of the commit from which the artifact was built.\nA version tag might look like `1.0.2-commit-hash`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeploying artifacts to nexus was done with this plugin before, but a bug was reported.\nWhen reading a child pom without `version`, for example, it does not retrieve the information inherited from the parent pom.\nWe have to investigate alternatives.\n### Alternatives\n* [Apache Maven Deploy Plugin](http://maven.apache.org/plugins/maven-deploy-plugin/)\n* Maven lifecycle phase : deploy\n* [Nexus Artifact Uploader](https://wiki.jenkins.io/display/JENKINS/Nexus+Artifact+Uploader)\n### Pros and Cons\n#### Apache Maven Deploy Plugin (deploy:deploy-file)\nFor this option, we only consider the goal `deploy:deploy-file`.\n##### :+1:\n- Official maven plugin for deployment, which is perfect for any maven projects if you only care whether your artifacts are deployed correctly.\n##### :-1:\n- A list of parameters has to be generated before using the plugin, including `artifactId` and `version`, which is the same case as the `Nexus Artifact Uploader`.\n- Credential info has to be stored in the `settings.xml`, which introduces additional implementation.\nLet's assume users have saved all the credentials in the Jenkins server.\nWe may inject a list of `server` tags under the `servers` tag with credentials info into the global `settings.xml`.\nTo make it as secrets, `mvn --encrypt-master-password <password>` has to be executed afterwards.\n#### Maven lifecycle phase: deploy\nBy default, the maven lifecycle phase `deploy` binds to the goal `deploy:deploy` of the `Apache Maven Deploy Plugin`.\n##### :+1:\n- Same as the `Apache Maven Deploy Plugin`\n- You don't have to pass the parameters as `Apache Maven Deploy Plugin` and `Nexus Artifact Uploader`,\nbecause `package` phase is executed implicitly and makes the parameters ready before `deploy` phase.\n##### :-1:\n- Same case as the `Apache Maven Deploy Plugin` for handling credentials.\n- As a maven phase, a list of phases is triggered implicitly before this phase, including `compile`, `test` and `package`.\nTo follow the build-once principle, all these phases have to be skipped.\nHowever, it's not possible to skip some of the maven goals binding to certain phases.\nFor example, if the `<packaging>` tag of the `pom.xml` is set to `jar`, then the `jar:jar` goal of the [`Apache Maven JAR Plugin`](https://maven.apache.org/plugins/maven-jar-plugin/) is bound to `package` phase.\nUnfortunately, however, `Apache Maven JAR Plugin` does not provide an option to skip the the `jar:jar` goal.\n**This is the main reason why we cannot use this option.**\n#### Nexus Artifact Uploader\n##### :+1:\n- Without the pain of handling the credentials, which was mentioned above in `Apache Maven Deploy Plugin` section.\n- It's promising, when the plugin is used properly\n##### :-1:\n- Same as the `Apache Maven Deploy Plugin`. A list of parameters has to be prepared.\n### Decision\n`Nexus Artifact Uploader` is chosen, because:\n- `Maven lifecycle phase: deploy` does not meet our build-once principle.\n- `Nexus Artifact Uploader` has the same situation regarding parameters as `Apache Maven Deploy Plugin`, but can handle credentials as a Jenkins plugin.\n\n## Decision\n","`Nexus Artifact Uploader` is chosen, because:\n- `Maven lifecycle phase: deploy` does not meet our build-once principle.\n- `Nexus Artifact Uploader` has the same situation regarding parameters as `Apache Maven Deploy Plugin`, but can handle credentials as a Jenkins plugin.\n",Use `Nexus Artifact Uploader` to deploy artifacts to Nexus.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen deciding on a naming scheme for unit test methods, the following criteria were most important to us:\n* **Readability** - The test method names should read like English sentences, with clear word boundaries\n* **Flexibility** - We should be able to follow different patterns, like ""Given ... When .. Then"", but also like ""Does X"".\n\n## Decision\n","We use `snake_case` for method names in unit tests. We adapt the coding style settings to ignore the deviation from our usual `camelCase` convention.\nIf it makes sense, we use sentences containing the words `given`, `when` and `then`. To give each section a clear boundary, when we use `given` or `when`, then we also use `then`.\nGood:\ntest_given_first_time_visitor_then_return_main_banner\nBad:\ntest_given_first_time_visitor_return_main_banner\nWe keep in mind that the sentences always refer to the system-under-test (SUT) and don't unnecessarily repeat its class name.\n","The team decided to adopt a naming scheme for unit test methods that prioritizes readability and flexibility. Specifically, the following guidelines will be followed:\n\n- Test method names should be written in sentence format, using clear word boundaries.\n- The ""Given ... When ... Then"" pattern is preferred for tests that follow a specific scenario.\n- Other patterns, such as ""Does X"", are acceptable as long as they are clear and concise.\n\nThis naming scheme will help ensure that our unit tests are easy to read and understand, regardless of the pattern that is being followed."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur current dns naming follows a couple of very similar patterns. Sometimes using 'gcp', 'frankfurt', 'oregon-b' as ways to separate different environments.  We should have one pattern and stick to it the best we can.\nThings the pattern needs to solve for:\n* Should be 'the same' for all of the meao services (for example: nucleus/bedrock/snippets).\n* should allow for multiple 'environments' of a service to be deployed in the same region ('prod'|'stg'|'dev')\n* should allow for multiple regions/deployments of the same service + environment ('or', 'fr', 'ia')\n* should also have a good 'user facing' pattern, that is not the same as the above pattern. (www.mozilla.org -> 'bedrock' 'prod' 'or' && 'bedrock' 'prod' 'fr' with some mechanism for choosing between the two deployments.)\n\n## Decision\n","For the backend deployments follow this pattern: 'service'.'environment'.'region'.'domain'. An incomplete list of each of examples of values for those variables:\n| Service  |\n|----------|\n| bedrock  |\n| nucleus  |\n| snippets |\n| prom     |\n| Environments |\n|--------------|\n| dev          |\n| stg          |\n| prod         |\n| demo1        |\n| Region | Description           |\n|--------|-----------------------|\n| or     | oregon eks cluster    |\n| fr     | frankfurt eks cluster |\n| ia     | iowa gcp cluster      |\n| Domain     |\n|------------|\n| moz.works  |\n| mozmar.org |\n| ramzom.org |\nThis leads to a few examples:\n| Examples                 |\n|--------------------------|\n| bedrock.dev.or.moz.works |\n| prom.prod.fr.mozmar.org  |\n| nucleus.stg.ia.moz.works |\nNote that these are for 'internal' use primarily.  The user facing domains will stay as they are.  A few examples, nucleus.mozilla.org (prod) and nucleus.allizom.org (stg), www.mozilla.org (bedrock prod) www.allizom.org (bedrock stg).  The connection between the new dns entries and the user facing will stay the same. (If we're using a r53 traffic policy now, we will continue to after this change, if we're just using cname/alias records we will again after this change,etc, including cloudflare vs cloudfront etc.)\n","Use a combination of a naming pattern and a user-facing pattern to provide a consistent and easy-to-use naming scheme for our DNS records.\n\n**Naming Pattern**\n\nThe naming pattern will be as follows:\n\n```\n{service}-{environment}-{region}\n```\n\nFor example, the production deployment of our nucleus service in the Oregon-B region would be named:\n\n```\nnucleus-prod-or-b\n```\n\nThis pattern meets all of the requirements outlined in the context:\n\n* It is the same for all of the meao services.\n* It allows for multiple environments of a service to be deployed in the same region.\n* It allows for multiple regions/deployments of the same service + environment.\n\n**User-Facing Pattern**\n\nThe user-facing pattern will be as follows:\n\n```\n{service}.{environment}.mozilla.org\n```\n\nFor example, the production deployment of our nucleus service would be accessible at:\n\n```\nnucleus.prod.mozilla.org\n```\n\nThis pattern is simple and easy to remember, and it provides a clear way to identify the service, environment, and region of a deployment.\n\n**Choosing Between Deployments**\n\nWhen multiple deployments of the same service and environment are available, users can choose between them using a variety of methods, such as:\n\n* DNS round-robin\n* Load balancing\n* Manual selection\n\nThe specific method used will depend on the specific needs of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nChanging networking can be hard.  It usually requires a full redeploy of all services and all infrastructure to make it 'real'.  Our current network has overlaps, which makes it more difficult to setup vpns, usually requring syncing of live IP addresses to their desired state.\n\n## Decision\n","Netops will reserve a /16 block of ips for mozmeao. For each VPC make a block of /20 ipv4 addresses.  Where VPC maps to a region within a cloud provider we use.  Divide that into /24 subnets, where we'll have just one subnet per AZ.\nOur /16 is - 10.154.0.0/16\nFor example, in oregon, the network would look like:\n| Label           | CIDR          | Range Start  | Range End      | Description                     |\n|-----------------|---------------|--------------|----------------|---------------------------------|\n| Oregon VPC      | 10.154.0.0/20 | 10.154.0.1   | 10.154.15.254  | A large block for the whole VPC |\n| Oregon Subnet A | 10.154.0.0/24 | 10.154.0.1   | 10.154.0.254   | Subnet for oregon-a az          |\n| Oregon Subnet B | 10.154.1.0/24 | 10.154.1.1   | 10.154.1.254   | Subnet for oregon-b az          |\n| Oregon Subnet C | 10.154.2.0/24 | 10.154.2.1   | 10.154.3.254   | Subnet for oregon-c az          |\nand Frankfurt would be:\n| Label              | CIDR           | Range Start   | Range End       | Description                        |\n|--------------------|----------------|---------------|-----------------|------------------------------------|\n| Frankfurt VPC      | 10.154.16.0/20 | 10.154.31.1   | 10.154.255.254  | A large block for the whole VPC    |\n| Frankfurt Subnet A | 10.154.16.0/24 | 10.154.16.1   | 10.154.16.254   | Subnet for frankfurt-a az          |\n| Frankfurt Subnet B | 10.154.17.0/24 | 10.154.17.1   | 10.154.17.254   | Subnet for frankfurt-b az          |\n| Frankfurt Subnet C | 10.154.18.0/24 | 10.154.18.1   | 10.154.18.254   | Subnet for frankfurt-c az          |\nThe next few vpc blocks would be 10.154.32.0/20, 19.154.48.0/20, 19.154.128.0/20\nIn oregon we could continue with 10.154.3 and 10.154.4 until 15 for the subnets.  Essentially the same for frankfurt 10.154.19, 10.154.20.\n",Use a cloud-native networking platform to manage networking infrastructure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to have load balancers between our CDNs and our services, in order to deal with k8s node failures (and for handing traffic, and as a place to log stuff, and as a way to block bad actors if needed).  We'd prefer to only pay AWS/GCP for a single load balancer thing, while still having the ability to generate unique dns addresses for each of our services.  They must be able to host certs correctly.\n\n## Decision\n","We're currently managing our ELBs in aws with some out of band terraform that connects the nodes of each k8s cluster to a load balancer. Since this is out of band, upgrading clusters or services implies doing a bunch of k8s stuff, and then also running terraform.  We could possibly simplify the whole experience by moving the full definition of the load balancer, and dns, and certs to objects inside k8s.  Voyager + External Ingress seem like the most common way to do this.  Deploying one 'ingress' object per group of services you want to have behind an ALB, and listing all the DNS to point at those services solves the problem outlined above.\nThe primary advantage of doing this work, is that it allows dynamic things (deployments created in response to events, such as pull requests and ephemeral 'demo' branches) to be created simply by writing the yaml and deploying it to kubernetes.\n","Utilize a managed load balancer service, such as AWS Elastic Load Balancing or Google Cloud Load Balancing, to provide the required functionality while optimizing cost and flexibility. This approach allows for a single load balancer to handle traffic for multiple services, while enabling the generation of unique DNS addresses for each service. Additionally, the managed service will handle certificate management, traffic logging, and the ability to block malicious traffic."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\n\n## Decision\n","Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev/staging/prod versions of the service.\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\n","The decision is to use Helm charts to manage Kubernetes deployments.\n\nHelm charts provide a way to package and manage Kubernetes applications. They define the resources that need to be created or updated in a Kubernetes cluster, and can be used to deploy applications in a consistent way across multiple environments.\n\nBy using Helm charts, we can reduce the amount of drift between different environments by ensuring that the same deployment configuration is used in each environment. We can also improve code reuse by defining our deployments in a single place, rather than having to create separate sets of yml files for each environment."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to have a flexible, easy to run, but not too expensive alerting and monitoring solution.  Since we are primarily kubernetes based, something built around that seems ideal.  We are in a little bit of a rush given that our current monitoring solution has yearly renewals, and that renewal would be coming up at the end of the month.\nPossible options include, influx cloud (v1, v2), influx we host (rejected mostly because v2 is not yet ready for primetime, and team has no influx familiarity).  New Relic, DataDog, HoneyComb are all great products, but are a bit expensive for us (saas prices for our data load seem to be too much).  Tools like Nagios are hard to run in the cloud because they assume mostly assume 100% network reliability.  Given all that, and our team's familiarity with prometheus, it seems like the best choice.\n\n## Decision\n","Run prometheus and alertmanager, on our clusters.  Run one prom per k8s cluster to collect metrics. Run one prom/grafana/alertmanager deployment to collect and display the information all in one place.  Send alerts to slack and/or pagerduty depending on severity.  Monitor this stack with dead man's snitch or other negative alerting services.\n",Use Prometheus as the primary alerting and monitoring solution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMozilla IT SREs have built a VPC module at https://github.com/mozilla-it/itsre-deploy/tree/master/modules/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public/private subnets. That difference will be described in this document.\n\n## Decision\n","We will create two subnets per az. One 'public' and one 'private'. In general this is mostly by convention, rather than something that is strictly enforced.  We may later validate instances in private are not accessible to the general public with automated auditing solution. But, for now, we'll just do our best to only assign public things to the public subnet, and everything else can go in private.\nThe one functional item for the subnets, 'public ips' will be assigned by default in the public subnets, but not the private subnets.\n",The Mozilla IT SRE VPC module will provide the following functionality:\n\n\n- **Create a VPC with the specified CIDR block.**\n- **Create a public subnet with the specified CIDR block and availability zone.**\n- **Create a private subnet with the specified CIDR block and availability zone.**\n- **Associate the specified public subnet with the VPC.**\n- **Associate the specified private subnet with the VPC.**\n- **Create a NAT Gateway in the specified public subnet.**\n- **Create a route table for the specified VPC.**\n- **Add a route to the specified route table for the specified destination CIDR block and next hop.**\n- **Associate the specified route table with the specified subnet.**\n- **Delete the specified VPC.**\n- **Delete the specified subnet.**\n- **Delete the specified NAT Gateway.**\n- **Delete the specified route table.**\n\nThe module will not provide the following functionality:\n\n\n- **Create a VPC with a CIDR block that overlaps with an existing VPC.**\n- **Create a subnet with a CIDR block that overlaps with an existing subnet in the same VPC.**\n- **Create a NAT Gateway in a private subnet.**\n- **Create a route table for a VPC that is not associated with the specified VPC.**\n- **Add a route to a route table for a destination CIDR block that is not within the VPC's CIDR block.**\n- **Associate a route table with a subnet that is not associated with the same VPC.**\n
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",We will use an Architectural Decision Record (ADR) template to document our architectural decisions. The template will include the following sections:\n\n\n1. Title: A brief title for the decision.\n2. Date: The date the decision was made.\n3. Context: A brief description of the context in which the decision was made.\n4. Decision: A clear statement of the decision that was made.\n5. Rationale: The reasons for making the decision.\n6. Consequences: The potential consequences of the decision.\n7. Trade-offs: The trade-offs that were considered before making the decision.\n8. Alternatives: The alternative options that were considered before making the decision.\n9. Related decisions: Any other architectural decisions that are related to this decision.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Open Data Hub itself consists of many components all having their own release cycles. ODH users might decide to\nupdate individual components such as container images used by JupyterHub. Nevertheless it is required to create\nreleases of ODH as a whole.\n\n## Decision\n,"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\nproject/product level while maintianing freedom of others to update.\n### Positive Consequences <!-- optional -->\n* Operators of ODH have a clear base line of versions, these versions have been tested with each other and have\nundergone ODH integration testing.\n* Operators of ODH can update individual components, they could maintain a repository analog to odh-manifests declaring\nthe exact versions (container image tags, git repository tags) of components they want to deploy.\n* Operators can mix in their builds of container images following the method mentioned above.\n### Negative Consequences <!-- optional -->\n* An ODH release (a tag in the odh-manifests repository) might not contain the latest versions of components, for example\nsecurity updates might have forced the build of a S2I image used with JupyterHub component of ODH.\n<!-- markdownlint-disable-file MD013 -->\n",**Decision:** ODH will create releases for the platform as a whole as well as for individual components.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\nAnother option was Rust, which could be a useful learning exercise.\nPython is used in FIAAS, so we have some experience using it for an operator.\n\n## Decision\n",We will use Python for Kafkarator.\n,"We will use Python for this project, as we have some experience using it for an operator in FIAAS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur application needs to supply some web endpoints for healthchecks, so we need to select a web framework.\nThere are a multitude to choose from, but we have some preferences:\n- We want something that is well maintained and has a community\n- We want something that has decent performance\n- As a learning experience, we want to use one of the many async frameworks\nThe front contenders seems to be Sanic, FastAPI, Tornado, Vibora and Quart.\nTornado seems to be somewhat dated, as it was created before Python had async built in.\nVibora is a relatively new project, with uncertain community adoption.\nQuart is API compatible with Flask, which is the most common non-async web framework.\nSanic seems to be the most popular of the async frameworks.\nFastAPI is by far the most performant of them all.\n\n## Decision\n","We will use FastAPI, because of the performance, and some of the interesting features.\n","We have decided to use **FastAPI** for our web framework.\n\n**Reasons:**\n\n* **High performance:** FastAPI is the most performant of the async frameworks we considered, which is important for our application's performance requirements.\n* **Well-maintained and active community:** FastAPI is a well-maintained project with an active community, ensuring that we will have support and resources available as we use it.\n* **Learning experience:** FastAPI is an async framework, which aligns with our goal of using this project as a learning experience.\n\n**Additional Considerations:**\n\n* **API compatibility with Flask:** While Quart offers API compatibility with Flask, we did not find this to be a strong enough reason to choose it over FastAPI given the other advantages it offers.\n* **Popularity of Sanic:** Sanic's popularity was a factor in our decision, but we ultimately prioritized performance and community support over popularity.\n* **Maturity of Vibora:** While Vibora is a promising project, its relatively new status and uncertain community adoption made us hesitant to choose it for our application.\n* **Age of Tornado:** Tornado's age and lack of built-in async support in Python led us to exclude it from our final decision."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n\n## Decision\n","Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\n",NAV will move Kafka to Aiven.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\nspace, and the Python community has not landed on a single standard. Each project needs to select which\ntools to use, usually a selection of tools that integrate well.\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\nclaims to support the use case well. It is also following the new pyproject.toml standard.\n\n## Decision\n",We will use Poetry for this project. (https://python-poetry.org/)\n,Use Poetry to manage Python dependencies in virtualenvs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile CRDs are a good API for Kubernetes Operators, they are not in common use for most developers, and a better\nuser experience is wanted.\nUsers today are used to creating an Application object to deploy their application. For users who are not owning a\ntopic, it would be easier if they can declare their topics as part of the Application resources. However, if we design\nour CRDs to be simple enough, this might not be a real problem.\nThere are other use cases where parts of the Application object are translated into a specific CRD handled by a separate\noperator. This model could also be used here. This will move some of the responsibility from the users to Naiserator,\nbut at the same time create an unnecessary coupling to Naiserator.\nBy making sure our flow is built around a set of CRDs, details of the UI can be worked on iteratively, refining the\nexperience as we get more experience.\nIn order to do this, we will need more than the two CRDs detailed in [6. Kafkarator API is focused around dedicated CRDs](0006-kafkarator-api-is-focused-around-dedicated-crds.md).\nThe flow can be separated into three distinct parts:\n- Topic creation\n- Getting topic access\n- Managing topic access\n### A suggested flow\n![Suggested sequence diagram](./0008-user-experience-flow.png)\n#### Topic creation\n1. A developer creates a Topic CRD in their team namespace, detailing the topic name and configuration (1)\n2. Kafkarator sees the Topic and\n1. Creates the topic in the Aiven cluster\n2. Creates a TopicAccess CRD object, recording the team as owner of this Topic\n#### Getting topic access\n1. A developer creates an AppTopic CRD in their team namespace, connecting an application with topics for either\nproducing or consuming (2)\n2. Kafkarator sees the AppTopic and\n1. If the application does not already have a service user in Aiven, it is created\n2. Downloads credentials for service user\n3. Creates a Kubernetes secret in the team namespace\n4. Adds an access request to the associated TopicAccess\n#### Managing topic access\n1. A topic owner edits the TopicAccess CRD to accept an access request (4)\n2. Kafkarator sees the TopicAccess change and\n1. Updates ACL for all relevant service users, granting or removing access to a Topic (5)\n#### Future work\n(1) It could be useful to create a UI for topic creation, with links to documentation for each option and easy\naccess to ""common"" options, but it does not seem worth the effort at this time.\n(2) As discussed, this could be done indirectly by specifying the topics in the Application CRD and letting Naiserator\ncreate the AppTopic CRD.\n(3) Application developers need to make sure their application requests access to the correct secrets to get the Aiven\ncredentials. This could be handled by Naiserator automatically, by simply mounting the secret in a predefined location\nif it exists.\n(4) Later iterations might want to provide a UI for teams to manage access, instead of editing CRDs directly. This would\nreduce the chance of errors, but requires proper access control in the UI which is already provided by Kubernetes.\n(5) Credentials for topic access is provided as service users per application, but we want access to be granted on a\nteam basis. This allows for limiting access to individual applications should the need arise at a later point.\n\n## Decision\n","- We will keep the CRDs as detailed in [6. Kafkarator API is focused around dedicated CRDs](0006-kafkarator-api-is-focused-around-dedicated-crds.md)\nwith further refinements as described above.\n- Kafkarator will create topic and provide credentials as Secrets based on CRDs\n- We make no integration with Naiserator at this time, but leave an opening for doing so in the future, as detailed\nunder [Future work](#future-work)\n","**Topic CRD and AppTopic CRD as the primary mechanism for user interaction**\n\nThis decision is based on the following factors:\n\n- CRDs are a well-established API in Kubernetes and are familiar to most users.\n- CRDs provide a flexible way to define custom resources, which allows us to model the desired user experience.\n- By using CRDs, we can leverage the existing Kubernetes ecosystem of tools and integrations, such as UIs, editors, and monitoring.\n\nThe suggested flow in the Context section provides a good starting point for designing the user experience for topic creation, access management, and team ownership. We will further refine this flow through user research and feedback.\n\nWe will also explore the use of a UI for topic creation and management in the future. However, for now, we will focus on building a solid foundation with CRDs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe project requires dealing with two relatively separate concerns:\n1. Create topics when needed\n2. Supply credentials for working with topics.\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\nso it makes sense to keep them in the same application.\n\n## Decision\n","We will ignore the SRP in this instance, and keep the two concerns in the same application.\n","**Decision:** Create a single application that contains separate components for each of the two concerns.\n\n**Rationale:**\n\n* **Separation of concerns:** The application will be organized into separate components for each concern, ensuring that the code is maintainable and testable.\n* **Conceptual cohesion:** While the two concerns are implemented separately, they are conceptually related. Keeping them in the same application allows for easy collaboration and knowledge sharing between teams working on each component.\n* **Efficiency:** Having all components within a single application streamlines communication and reduces the overhead of coordination between multiple applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use an Architectural Decision Record (ADR) template to capture and document architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\nWe need to make a decicion, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\nFrom various discussions, we have a few findings that guide our decision:\n- Piggybacking on the existing NAIS CRDs are not encouraged and should be avoided\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\n- We have two separate concerns that needs two separate configurations\n\n## Decision\n",- We will define two new CRD objects (see github issues #3 and #12)\n- App developers will create these in the cluster when deploying their application\n- Kafkarator will watch these two CRDs and take needed actions\n,"As the findings suggest there are two separate concerns with separate configurations, and that the recommended approach is to use a separate CRD.\n\nTherefore **the decision** is:\n\n* Create a new CRD for Kafkarator, with appropriate versions, and with a spec that encompasses both concerns."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGP Data changes on a daily basis.\nNightly ETLs are run to obtain Syndication and POMI data.\nCurrently [profiles-db](https://github.com/nhsuk/profiles-db) requires manually updating from the ETL outputs, and re-deploying to refresh the data.\nNo automated validation of data is performed during the update.\n\n## Decision\n","To avoid a new release of [profiles-db](https://github.com/nhsuk/profiles-db) each time data changes, data will be inserted into a running mongodb instance on a daily schedule.\nThe output from the nightly [gp-data-etl](https://github.com/nhsuk/gp-data-etl) and [pomi-data-etl](https://github.com/nhsuk/pomi-data-etl) will be validated and combined.\nThe contents of the existing database will only be replaced if the new data is of a comparable size.\n","An automated process should be developed to update [profiles-db](https://github.com/nhsuk/profiles-db) data from Nightly ETL outputs, without the need for manual intervention.\n\nTo ensure data quality, automated validation checks should be implemented as part of the process."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe merge of the data sources is a common operation from which the generated asset can be used by several different processes.\nCurrently the generated asset is only available for the immediately following process within this application i.e. updating MongoDB.\nThere is an immediate need to use the same generated asset for updating Elasticsearch.\nThe [mongodb-updater](https://github.com/nhsuk/mongodb-updater) service is able to update a MongoDB database from a JSON file available at a URL.\n\n## Decision\n,"The `gp-data-merged.json` file will be written to the team's preferred cloud hosting platform, enabling the merged data to be used as a\nsource for both the `mongodb-updater` and the forthcoming `elasticsearch-updater`.\n",The data source merge output JSON file will be made available as a URL that can be consumed by the [mongodb-updater](https://github.com/nhsuk/mongodb-updater) service. This would involve implementing a new endpoint (or modifying an existing one) that returns the JSON file. It would also require setting up a process to generate the JSON file and make it available at the URL.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","We will record the architectural decisions made on this project using an Architectural Decision Record (ADR) template. The ADR template will include the following sections:\n\n- Title: A brief title for the decision.\n- Date: The date the decision was made.\n- Authors: The people who made the decision.\n- Status: The current status of the decision (e.g., proposed, accepted, rejected).\n- Context: The context in which the decision was made.\n- Decision: The decision that was made.\n- Rationale: The rationale for the decision.\n- Consequences: The potential consequences of the decision.\n- Alternatives: The other alternatives that were considered."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\n\n## Decision\n","We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\n1. The Java package name will include the version number.\n1. The library name will be part of the group ID.\n1. The group ID will include the version number.\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\nhttp://jakewharton.com/java-interoperability-policy-for-major-version-updates/\n","**Decision:** Upgrade `kotlin-guice` to a new major version when Guice makes a corresponding major version upgrade.\n\n**Reasoning:**\n\n* `kotlin-guice` is heavily tied to Guice, and major version changes in Guice will necessitate corresponding changes in `kotlin-guice`.\n* Upgrading both Guice and `kotlin-guice` simultaneously minimizes the likelihood of dependency conflicts and ensures compatibility between the two libraries.\n* This approach aligns `kotlin-guice`'s versioning with Guice's, simplifying dependency management for consumers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\n\n## Decision\n",All pointer event interactions have a mouse event fallback.\n,Use a cross-browser library like [Hammer.js](http://hammerjs.github.io/) or [Velocity.js](http://velocityjs.org/) as a fallback if PointerEvents isn't available.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want to be able to centralise in a single tool all tasks to be called:\n* During the development cycle.\n* During the build cycle.\n* In the CI/CD pipelines.\n## Decision Drivers\n* Must have a significant user base and community.\n* Must not require significant installation.\n* Must be sufficiently simple to learn and use.\n\n## Decision\n,"* Must have a significant user base and community.\n* Must not require significant installation.\n* Must be sufficiently simple to learn and use.\nChosen option: `GNU Make`, because compared to the other evaluated tools (see\n[Pros and Cons](#pros-and-cons-of-the-options)), it fits the bill where as:\n* `Invoke` is not well maintained, nor well documented, nor a de facto standard, nor has\na sufficient community.\n* `Rake` required to install and learn Ruby.\n* `SCons` is more a build tool and seems difficult to apprehend/get to grips with.\n","**Decision:** Adopt Make as the central task runner.\n\n**Rationale:**\n\nMake is a widely-used and well-supported task runner that meets all of the decision drivers:\n\n* It has a large and active user base and community, ensuring that support and resources are readily available.\n* It is straightforward to install, requiring only a single command to set up.\n* Its syntax is relatively simple, making it easy to learn and use even for developers who are new to task runners.\n* It provides a robust set of features for defining, executing, and automating tasks, meeting the requirements for development, build, and CI/CD pipelines."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on Opinionated Digital Center.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","The architectural decision for Opinionated Digital Center is to record the architectural decisions made on the system. This will help to ensure that the system is built in a consistent and maintainable way. The architectural decisions will be documented in a central location, such as a wiki or a document repository. This will make it easy for stakeholders to understand the rationale behind the decisions that were made."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Technological forces\n* METS Reader & Writer (henceforth [metsrw][0] already exists, is being used in\nArchivematica, and seems like a good tool for abstracting METS file creation.\n* Currently, [Archivematica][1] (AM) and the [Archivematica Storage Service][2]\n(SS) both create or modify METS files (pointer files are METS files too),\nusing mets-rw or various lxml APIs. This inconsistency makes it difficult to\nreliably create predictable METS files.\n* Artefactual has created a [METS validator][3] project, which contains a\nSchematron file that validates AM-generated AIP METS files.\n* metsrw (v. 0.1.1) cannot create pointer files and it does not support METS\nfile validation, either via XMLSchema or Schematron.\n* metsrw (v. 0.2.0) does have support for pointer file creation and for METS\nfile validation generally, both via XMLSchema and Schematron.\n* Changing how we write METS files may inadvertently affect the METS files\nproduced; METS validation and related testing will need to prevent this, or\nformally recognize it if it is intentional.\n* At present in AM and SS, METS/PREMIS data structures are encoded at various\npoints as XML strings, MySQL/SQLite database rows, and Python objects (i.e.,\nmetsrw or lxml instances).\n### Project forces\n* In the context of the project ""AIP Encryption via Mirror Locations"", mirror\nlocations in the Storage Service must replicate stored AIPs and give them\ntheir own pointer files; if replica pointer files are to be generated in the\nSS then it makes sense to generate all pointer files there.\n* An [analysis of pointer file creation in Archivematica][4] has been performed.\n### Political forces\n* Other projects under discussion or on the horizon will involve the need to\nrecognize various ""flavours"" or versions of AM-generated and\nexternally-generated (e.g., DSpace, from-LOCKSS) METS files and process them\naccordingly. One such project relates to re-ingesting old AM-generated AIPs\nwhose METS files may be significantly different from their current\ncounterparts. This implies more sophisticated METS file validation and\nrecognition capabilities than AM currently implements.\n* The Mirror Locations project’s budget does not cover wholesale refactoring of\nhow AM creates METS files, or even significant alterations to metsrw.\n\n## Decision\n","We will use metsrw to create and modify METS\nfiles---including pointer files---in all future code. We will not rewrite\nexisting METS creation code to use metsrw unless a clear opportunity arises to\ndo so, e.g., direct funding for this purpose or a project that requires\nnon-trivial changes to such code.\nIn the AIP Mirror project, metsrw will be used to create METS pointer files,\nboth when AIPs are stored and when they are replicated. This will involve\nremoving the Create Pointer File micro-service in AM and calling a\n`create_pointer_file` method of Package at the end of AIP storage, reingest and\nreplication.\n","**Decision:**\n\nUpgrade mets-rw to version 0.2.0 in both Archivematica and Archivematica Storage Service. This will enable the creation of pointer files and support for METS file validation in both systems, using either XMLSchema or Schematron.\n\n**Additional Considerations:**\n\n* Conduct thorough testing to ensure that changing how METS files are written does not inadvertently affect the METS files produced. METS validation and related testing should be used to prevent this or formally recognize any intentional changes.\n* Explore ways to consolidate METS/PREMIS data structures into a single data structure to improve consistency and maintainability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nArchivematica code style is inconsistent and controversial. Formatting is a\ndistraction from content and a constant dilemma in our code review process.\nThe style guide described by [PEP 8][0] is not specific enough and Python is too\nflexible. Every project ends up with its own style and memorizing the rules is\nhard.\nWith the recent rise of auto formatters such *gofmt* or *Prettier*, we see the\nvalue of ceding control to an opinionated tool.\n\n## Decision\n","Chosen option: ""black"", because:\n* It prioritizes consistency over style.\n* It can check that the reformatted code still produces a valid AST that is\nequivalent to the original.\n* Zero configuration, sane defaults.\n* Adopted by popular projects such [responder][5] or [pytest][6].\n","**Adopt a strict and opinionated code style enforced by an auto formatter.**\n\n### Rationale\n\n* Improves code consistency and readability, reducing distractions and simplifying the code review process.\n* Enforces a specific style, eliminating the need for individual project styling and memorization.\n* Auto formatters like *gofmt* or *Prettier* provide automated formatting, ensuring consistency and reducing the burden on developers.\n\n### Consequences\n\n* Some developers may initially resist the imposition of a strict style.\n* May require modification to existing code to comply with the new style.\n* May require ongoing maintenance efforts to ensure compliance with the code style as the codebase evolves."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nThe Simon Fraser University Archives is a long-time Archivematica user. They\nrecognized that there is often a long-period between receiving digital materials\nfrom donors and an archivist creating a Submission Information Package (SIP) for\nlong-term archival storage. SFU Archives wanted the ability to perform minimal\ndigital preservation tasks, such as those provided by Archivematica's Transfer\nfunctionality, and then return to the creation of SIPs from backlog at an\nundetermined future time, perhaps several years in the future.\nThis meant that transfer backlogs are used for long-term storage of content and\nthat users should expect it to be as durable as Archival Information Package (AIP)\nstorage for maintaining accurate metadata over pipeline upgrades, migrations or\nre-indexes. To enable this, the decision was made to package backlog transfers as\nBagIt packages, supported by a verbose METS file, as is already done for AIPs.\n\n## Decision\n","* Preservation of original content and metadata over long-term gaps in\nprocessing\n* Re-use of existing standards and protocols\nChosen option: Option 2 was chosen because SFU Archives wanted the ability to\ncreate multiple SIPs from multiple transfers. In their use cases, one transfer\ndoes not automatically equal one SIP and one AIP. By sending transfers to\nbacklog they are able to use the Appraisal tab functionality where users can\ncreate SIPs by combining files from different transfers in the Archivematica\nbacklog.\nHowever, this creates a new expectation, namely, that transfer\nbacklogs can be used for long-term storage of content and that users should\nexpect it to be as durable as AIP storage for maintaining accurate metadata\nover pipeline upgrades, migrations or re-indexes. However, prior to release\n1.10, transfers stored using Archivematica's backlog functionality could only be\naccessed by using the same pipeline that the transfer was created in. If the\npipeline was accidently lost or decomissioned, you would be unable to reindex\nand repopulate the backlogged material in a new Archivematica pipeline. The\nreliance on maintaining this backlog information in the pipeline's transactional\ndatabase (MCPServer) and search index (Elasticsearch) is a significant\npreservation risk if the files are kept in backlog for an indefinitely long\nperiod of time, as is the case for SFU Archives and an increasing number of\nArchivematica users.\n![before_archivematica_backlog](/assets/before_archivematica_backlog.png)\nThe decision was made to convert the materials in transfer backlogs to mirror\nArchivematica's AIPs, namely to use the Bagit and METS standards to\npackage and describe the transfer materials. As materials go through transfer\nprocessing, event and object information is written to the transfer METS file\nwherever feasible. Other relevant information is captured in a logs subdirectory\nin an IETF-compliant Bagit package. Functionality was then added in Archivematica\nrelease 1.10 to rebuild backlogs from these self-describing packages in the event\nof a lost or destroyed pipeline.\n![after_archivematica_backlog](/assets/after_archivematica_backlog.png)\n### Positive consequences\nArchivematica users that rely on transfer backlog for long-term storage of\ndigital materials can now be assured that they can rebuild their backlogs from\nthe transfer Bagit packages that Archivematica creates as of release 1.10. This\nsignificantly reduces the risk of loss of materials during the transfer backlog\nphase.\n### Negative consequences\nSome major changes were introduced to the Archivematica codebase to enable it\nto restore from backlog using transfer Bagit packages. Previously, the\ninformation used to accomplish this was maintained primarily in the MCPServer\ndatabase and ElasticSearch index. There is a risk that not all previous\nfunctionality to restore from backlog is fully enabled and that some\nregressions may have been introduced as a result of this architectural change.\n","**Decision:**\n\nPackage transfer backlogs as BagIt packages with verbose METS files to enable long-term storage and maintain accurate metadata over pipeline upgrades, migrations, or re-indexes."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nArtefactual have been working on a client application to provide easier access\nto the functions exposed by the Archivematica Application Programming Interface\n(API). This application is called [AMClient][amclient-1]. The application also\nacts as a Python library which makes it easier to make Archivematica specific\nAPI calls in code. The library was released as a [Python package][amclient-2]\nearly in 2019.\n\n## Decision\n,"* Desire to simplify development processes in Archivematica and the Storage\nService.\n* Desire to make it easier to maintain components of Archivematica downstream\nand push the effects upstream (modularity).\nOption 2. The AMClient library helps us to create consistency across the\nArchivematica code-base. It is already widely used in the default Automation\nTools, and Archivematica Automated Acceptance Tests (AMAUAT).\n### Positive consequences\n* As the AMClient library is adapted as a Python package its use on the command\nline is extended and so it becomes more useful to those embedding it outside\nof Archivematica.\n* As AMClient is extended, tests are added to the package itself, providing a\ngreater level of lower-level testing, thusly the Archivematica API. The focus\nof testing in Archivematica can be on manipulating well understood AMClient\nresponses where this is appropriate for the unit being tested.\n* The number of lines of code required to talk to the AMClient package versus\nthat of creating lower-level API calls via a HTTP requests library should be\nfewer.\n* Missing functionality, e.g. as described by issue [#905][amclient-3] can be\nadded as it is required by Archivematica and the Storage Service. As an\nexternal package, and something used inside Archivematica, the two take on a\nsymbiotic relationship.\n### Negative consequences\n* The release candidate for version 1.0.0 of AMClient does not yet adhere to\nmore traditional Python best practices i.e. it might not be considered to be\nentirely ""Pythonic"" in how it exposes errors to the calling code. This is\ndescribed in issue [#488][amclient-4]. This effect of not being 'Pythonic'\nmay travel upstream to Archivematica and is something for maintainers to be\naware of. Work on an AMClient 2.0.0 should replace these patterns and then\ncascading those changes down to Archivematica should still be easier than\nworking with and replacing the calls that exist in Archivematica using a HTTP\nrequests library.\n",Continue developing the AMClient package as a Python package.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nThere is no open-source software system capable of implementing the functional\nrequirements of the OAIS reference model (ISO 14721:2003). Digital preservation\nspecialists must use multiple tools, which can be difficult to install and use,\nto perform discrete preservation actions. Those tools produce metadata that do\nnot conform to digital preservation and exchange standards and schemas, and do\nnot provide a way to automatically generate standardized, system-independent,\nself-documenting Archival Information Packages (AIPs) that package content and\nPreservation Description Information (PDI) as described by OAIS. Repository\napplications such as Fedora are capable of performing some but not all OAIS\npreservation actions, and tend to be complex to develop and maintain, posing a\nrisk to future retrieval and readability of the AIPs. In fact, any middleware\nrepository or database software that is required to access and read AIPs is\ninherently a risk to their long-term usability.\n\n## Decision\n","Artefactual designed an open-source, web-based archival description and access\nsystem called ICA-AtoM (Access To Memory) that has a broad user base around the\nworld. ICA-AtoM does not provide digital preservation functionality as described\nby OAIS. It would benefit ICA-AtoM users to be able to integrate with a back-end\nsystem designed to preserve digital objects that are linked to access copies in\nICA-AtoM. The system should also be usable on its own or in conjunction with\nother access tools.\nDesign principles:\n1. The application will perform a set of configurable preservation actions on\ningested digital objects, using the file system as the focal point of\npreservation action operations. Making the file system the focal point of\nmicro-services operations is noteworthy as a long-term preservation strategy\nbecause it provides users with the option of direct, unmediated access to\narchival storage. This might be necessary one day because the various layers\nand generations of digital preservation system components are just as\nsusceptible to the risk of technology obsolescence and incompatibility as the\ndigital objects they are attempting to preserve.\n2. The information packages ingested by the application will be moved from one\nmicro-service to the next using the Unix pipeline pattern.\n3. Micro-service functionality will be provided by one or more of the\nopen-source software utilities and applications bundled into the application.\nWhere necessary, these will be supplemented by integration code written as\nPython scripts.\n4. The application will provide a graphical user interface so that the end user\ncan determine and control the status of digital objects moving through the\npipeline.\n5. The application will generate AIPs that are system-independent,\nself-documenting and self-contained. The AIPs will contain PDI that conforms\nto recognized standards and schemas, such as PREMIS, METS and Dublin Core.\n6. The file-based AIP is the canonical source of the preserved digital objects\nand metadata. The preservation application may be supported by databases and\nindexes that are derived and updated from this source, but these are not\nessential to long-term preservation of the AIP.\n7. The application will be designed to integrate with diverse external systems\ncapable of serving as sources of digital objects to be ingested.\n8. The application will be designed to integrate with diverse storage systems\nand protocols for deposit and long-term preservation of AIPs.\n9. The application will be designed to integrate with diverse external systems\ncapable of providing search, browse and display capabilities for\nDissemination Information Packages (DIPs) generated by the application.\n",Develop a modular digital preservation framework tailored to support OAIS-compliant preservation actions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nArchivematica users rely on spreadsheets created in a specific way to perform\ntasks within or after Archivematica. Documentations and examples can be found\n[here](https://www.archivematica.org/en/docs/archivematica-1.9/user-manual/transfer/import-metadata/)\nbut there is no clear validation as can be performed by a machine. As one\nexample, the metadata.csv and rights.csv files are ""special"" and are utilized by\nArchivematica to add metadata or rights metadata into the AIP's METS file.\nAnother example is the Avalon Media System having a specific Manifest.csv file\nthat is used to recreate hierarchical information and additional metadata, which\nis used after a DIP is created from a stored AIP. It would be beneficial if this\nmanifest could be validated prior to going through the preservation process.\nBoth of these examples would benefit from a validation service that a user (or\nautomated system) could access prior to ingest into Archivematica.\n\n## Decision\n","* More closely integrate two open source projects: Archivematica and Avalon\n* [Feature request](https://github.com/archivematica/Issues/issues/563) for\nvalidation of Archivematica-specific CSVs within Archivematica\n* Creation of something for a specific purpose that can later be extended to\nsuit many use cases\nChosen option: ""1. API endpoint for pre-ingest CSV validation"", because it is\nflexible, it lays the groundwork for future work around CSV validation as a step\nto be taken by Archivematica. It doesn't have the complications of the long-term\nmaintenance and testing of a GUI component. It can be more easily automated. The\nsolution allows for custom or institutionally-specific CSV to be used/added.\nTechnically, this would live in the Archivematica codebase and be a new endpoint\nin the [Archivematica API](https://wiki.archivematica.org/Archivematica_API)\nProposed endpoint below:\n```yaml\nURL: /api/transfer/validate_csv\nVerb: POST\nValidates local CSV with validator service Python script\nParameters: CSV\ninput: Path to the CSV\nvalidator: Name of service CSV should be checked against, i.e. ""avalon""\nor ""rights""\nResponse: JSON\nmessage: Approval or non-approval, depending on service output\n```\n### Positive consequences\n* Fulfills requirement of Avalon/Archivematica integration work.\n* Relatively small feature with big potential benefits.\n* Lays foundation for more development in the future.\n### Negative consequences\n* Code must be maintained and tested over time.\n* User would need API expertise to benefit from this feature.\n* Implementation -- timeouts on very big CSVs?\n","Develop a service to validate a single file (metadata.csv, rights.csv, Manifest.csv, etc)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nPython 2 will not be supported by the Python development team past January 1,\n2020\. Operating system vendors will provide bugfix support for some number of\nyears, depending on the OS:\n* [Ubuntu 16.04 through 2021](https://wiki.ubuntu.com/Releases)\n* [Ubuntu 18.04 through 2023](https://wiki.ubuntu.com/Releases)\n* [RHEL/CentOS 7 through 2024](https://wiki.centos.org/About/Product)\nArchivematica is not fully compatible with Python 3 yet. Archivematica and all\nof its required components will eventually need to run in a Python 3\nenvironment, especially as downstream dependencies begin to drop support for\nPython 2 (i.e.\n[Django](https://docs.djangoproject.com/en/2.2/faq/install/#what-python-version-should-i-use-with-django)).\nComponents:\n* [am/compose](https://github.com/artefactual-labs/am)\n* [archivematica-storage-service](https://github.com/artefactual/archivematica-storage-service)\n* [MCPServer](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/MCPServer)\n* [MCPClient](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/MCPClient)\n* [archivematica-common](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/archivematicaCommon)\n* [Dashboard](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/dashboard)\n* [mets-reader-writer](https://github.com/artefactual-labs/mets-reader-writer)\n* [ammcpc](https://github.com/artefactual-labs/ammcpc)\n* [agentarchives](https://github.com/artefactual-labs/agentarchives)\n* [fixity](https://github.com/artefactual/fixity)\n* [automation-tools](https://github.com/artefactual/automation-tools)\n* [archivematica-acceptance-tests](https://github.com/artefactual-labs/archivematica-acceptance-tests)\n* [amclient](https://github.com/artefactual-labs/amclient)\n\n## Decision\n","* Archivematica is built on Python 2\n* Python 2 will no longer be supported after January 1, 2020\n* Archivematica should be refactored to use Python 3 (3.5 at minimum)\nChosen option: 1, because refactoring Archivematica and all required components\nto Python 3 is the only option to ensure that Archivematica continues to be a\nsafe, stable environment.\nPython 3.5 has been set as the minimum version for the following reasons:\n* [Python 3.4](https://www.python.org/dev/peps/pep-0429/) has already been\nretired\n* [Python 3.5](https://www.python.org/dev/peps/pep-0478/) is the oldest version\nthis is still supported\n* Ubuntu 16.04 ships Python 3.5\n* Ubuntu 18.04 ships Python 3.6\n* For CentOS 7, The Software Collections Repository provides packages for\nPython 3.5 and up\nDuring the course of the project, a higher version of Python may be selected.\n[Epic for Python 3 refactoring](https://github.com/archivematica/Issues/issues/805)\n### Positive consequences\n* Archivematica will be upgraded to a maintained version of Python.\n* Archivematica developers will be able to take advantage of new features in\nPython.\n* Python 3 support will be maintained for many years (for example, the latest\nstable release is [3.7, which will be maintained until 2023](https://www.python.org/dev/peps/pep-0537/#release-schedule);\nfurther releases are expected).\n### Negative consequences\n* This is a very large task and components need to be tackled in priority order,\nso it will take a considerable amount of time to get everything refactored.\n* Releasing the Python 3 work once all components are refactored will take the\nplace of a feature release, leaving some development work unreleased for at\nleast 4 extra months (note: all code resides in public branches).\n* We will need to support Archivematica running Python 2.7 for some time because\nusers will need time to upgrade. This results in a dual support burden.\n* Development work currently in progress may need to be refactored to Python 3\nbefore it are releasable, which is unsponsored work.\n","The decision is to migrate Archivematica and all of its required components to a Python 3 environment as soon as possible.\n\n**Rationale**\n\nPython 2 will not be supported by the Python development team past January 1, 2020. Operating system vendors will provide bugfix support for some number of years, depending on the OS, but this support will eventually end. Archivematica and all of its required components will eventually need to run in a Python 3 environment, especially as downstream dependencies begin to drop support for Python 2.\n\nMigrating to Python 3 will require a significant amount of work, but it is necessary to ensure the long-term viability of Archivematica. The earlier we start the migration, the less disruptive it will be.\n\n**Consequences**\n\nMigrating to Python 3 will require a significant amount of work, but it is necessary to ensure the long-term viability of Archivematica. The earlier we start the migration, the less disruptive it will be.\n\n**Alternatives**\n\nOne alternative to migrating to Python 3 is to continue to use Python 2 and rely on operating system vendors to provide bugfix support for as long as possible. However, this is a risky strategy, as there is no guarantee that operating system vendors will continue to support Python 2 indefinitely. Additionally, continuing to use Python 2 will make it more difficult to keep Archivematica up-to-date with the latest security patches and bug fixes.\n\nAnother alternative is to fork Archivematica and continue to develop it using Python 2. However, this would be a significant undertaking, and it would be difficult to maintain compatibility with the main Archivematica codebase. Additionally, forking Archivematica would reduce the number of developers working on the project, which could slow down development and make it more difficult to fix bugs and implement new features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nThe core of Archivematica is a server application that coordinates work across\nmultiple workers. The collection of related tasks (also known as *workflow\ndata*) is stored in the application database using a relation model.\nIntroducing changes to the workflow data is hard because the model is clumsy and\nconvoluted. Changes to the data are delivered as data and schema migrations\nwhich are difficult to implement and they add risk to the process of upgrading\nthe software.\nAdditionally, we want to add full internationalisation support in order to make\nthe system more attractive to a wider range of users and increase adoption. We\nneed a simpler mechanism to deliver translations and other workflow data changes\nin general.\n\n## Decision\n",* Archivematica 1.9 will implement the proposed solution replacing the existing\ndatabase tables.\n* We do not currently intend to expose the ability to modify the workflow data\nto Archivematica users - but this may change in the future as the solution\nmatures and becomes interoperable.\n* The visualization and manipulation of the workflow data will be facilitated by\na new tool that we're calling amflow.\n,The Archivematica core application will use a configuration file format based on YAML\nto represent workflow data. This format will make it easy to add internationalisation\nsupport.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nQuarantine is an optional workflow step in Archivematica that places digital\nobjects in a separate space for a specified period of time for virus definitions\nto be updated. It adds complexity to the workflow and is arguably unnecessary\nbecause ingested digital objects generally speaking pre-date the available virus\ndefinitions, which are updated daily. This proposal is to remove quarantine\nto simplify Archivematica's workflow and ease maintenance and testing.\n\n## Decision\n","* Desire to simplify Archivematica workflows\n* Need to fix known bugs related to quarantine if it remains supported\n* Little evidence of need for or use of quarantine in the user community\nOption 1, because there is no evidence of use in the community to justify the\ncontinued maintenance of the feature and the positive consequences of the\nremoval of the code outweigh the benefits of the functionality. The virus\nscanning micro-service remains unchanged by this decision.\n### Positive consequences\n* Simplified, easier to maintain workflow\n* Removes two processing configurations that users are required to make but\nwhich are rarely implemented.\n* Resolution of the following issues:\n* DSpace workflow issue: [#426][1]\n* Virus scan not triggered after quarantine: [691][2]\n[1]: https://github.com/archivematica/Issues/issues/426\n[2]: https://github.com/archivematica/Issues/issues/691\n","**Decision: Remove quarantine**\n\n**Rationale:**\n\n* Quarantine adds complexity to the workflow without providing significant benefits.\n* Ingested digital objects generally pre-date available virus definitions.\n* Removing quarantine would simplify Archivematica's workflow and ease maintenance and testing.\n\n**Consequences:**\n\n* **Reduced workflow complexity:** The workflow will be simplified by removing an unnecessary step.\n* **Easier maintenance and testing:** The reduced complexity will make it easier to maintain and test Archivematica.\n* **Potential security risk:** There is a small potential security risk if new viruses are detected after ingestion. However, this risk is mitigated by the fact that ingested digital objects generally pre-date available virus definitions.\n\n**Additional Considerations:**\n\n* **Phased removal:** The quarantine step can be removed in a phased approach to minimize the impact on existing workflows.\n* **Monitoring for new viruses:** Archivematica can monitor for new viruses and take appropriate action if necessary."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some ""events bus"" to do that.\n\n## Decision\n",Solution number 1 - In Memory Events Bus</br>\nAt that moment we don't see more advanced integration scenarios in our system than simple publish/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\n,Use Apache Kafka as the event bus for inter-modular communication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs it is monolith, only one language (or platform) must be selected for implementation.\n\n## Decision\n","I decided to use:\n- .NET Core platform - it is new generation multi-platform, fully supported by Microsoft and open-source community, optimized and designed to replace old .NET Framework\n- C# language - most popuplar language in .NET ecosystem, I have 12 years commercial experience\n- F# will not be used, I don't have commercial experience with it\n",Select a language (or platform) that meets the following criteria:\n- It is supported by a majority of the team.\n- It has a large and active community.\n- It is well-documented and easy to learn.\n- It is appropriate for the size and complexity of the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEach module should be autonomous. However, communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\n\n## Decision\n","Solution number 2 - Event-driven (asynchronous)</br>\nWe want to achieve the maximum level of autonomy and loose coupling between modules. Moreover, we don't want dependencies between modules. We allow direct calls in the future, but this should be an exception, not a rule.\n","**Decision:** Utilize a Message Bus architecture for communication and integration between modules.\n\n**Rationale:**\n\n* **Decoupling and Autonomy:** The message bus architecture effectively decouples modules, allowing them to communicate and integrate without direct dependencies. This preserves their autonomy and facilitates changes without impacting the entire system.\n* **Loose Coupling:** Modules can subscribe to and publish messages on the message bus without requiring specific knowledge about other modules. This flexibility enhances the system's resilience and maintainability.\n* **Asynchronous Communication:** The message bus enables asynchronous communication, allowing modules to process messages at their own pace. This eliminates bottlenecks and ensures that messages are delivered reliably even in the presence of failures or delays.\n* **Message Standardization:** The message bus provides a common interface for message exchange, ensuring compatibility and consistency between modules. This simplifies integration and reduces the likelihood of communication errors.\n* **Event-Driven Architecture:** The message bus facilitates event-driven architecture, where modules react to events published by other modules. This promotes a reactive and adaptive system that can respond to changes in real time."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur application should handle 2 types of requests - reading and writing. </br>\nFor now, it looks like:</br>\n- for reading, we need data model in relational form to return data in tabular/flattened way (tables, lists, dictionaries).\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\n\n## Decision\n","We applied the CQRS architectural style/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\n","We will use two different data models for reading and writing:\n\n- For reading, we will use a relational data model. This will allow us to return data in a tabular/flattened way, which is what we need for our reading requests.\n- For writing, we will use a graph of objects. This will allow us to perform more sophisticated work like validations, business rules checks, and calculations, which is what we need for our writing requests."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAn advanced example of Modular Monolith architecture and tactical DDD implementation in .NET is missing on the internet.\n\n## Decision\n,I decided to create nontrivial application using Modular Monolith architecture and Domain-Driven Design tactical patterns.\n,"**Decision: Create a case study showcasing Modular Monolith architecture and tactical Domain-Driven Design (DDD) implementation in .NET.**\n\n**Details:**\n\n* Develop a real-world application that demonstrates the principles of Modular Monolith architecture and tactical DDD.\n* Utilize a modern .NET stack, such as .NET Core or .NET 6, for implementation.\n* Structure the application into independent modules with clearly defined boundaries and responsibilities.\n* Employ DDD techniques, such as domain models, bounded contexts, and repositories, to encapsulate business logic and maintain data integrity.\n* Provide comprehensive documentation, code samples, and architecture diagrams to illustrate the design and implementation.\n* Publish the case study on a publicly accessible platform, such as GitHub, to serve as a resource for the community."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe theory of the CQRS and the CQS principle says that we should not return any information as the result of Command processing. The result should be always ""void"". However, sometimes we need to return some data immediately as part of the same request.\n\n## Decision\n","We decided to allow in some cases return results after command processing. Especially, when we create something and we need to return the ID of created object or don't know if request is Command or Query (like Authentication).\n","In cases where it is necessary to return data immediately as part of the command processing, the following options should be considered:\n\n1. **Use a separate query for the data.** This is the preferred approach, as it keeps the command and query models separate.\n2. **Use a ""Command Query Separation"" (CQS) design pattern.** This pattern allows commands to return data, but it does so in a way that keeps the command and query models separate.\n3. **Use a ""Query on Commit"" (QoC) design pattern.** This pattern allows commands to return data after the command has been committed to the database.\n\nThe best option for a particular application will depend on the specific requirements of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe MyMeetings domain contains 4 main subdomains: Meetings (core domain), Administration (supporting subdomain), Payments (supporting subdomain) and User Access (generic domain).\nWe use Modular Monolith architecture so we need to implement one application which solves all requirements from all domains listed above.\nWe need to modularize our system.\n2. Create 4 modules based on Bounded Contexts which in this scenario maps 1:1 to domains. This solution is more difficult at the beginning. We need to set modules boundaries, communication strategy between modules and have more advanced infrastructure code. It is a more complex solution. On the other hand, it supports autonomy, maintainability, readability. We can develop our Domain Models in all of the Bounded Contexts independently.\n\n## Decision\n","Solution 2.\nWe created 4 modules: Meetings, Administration, Payments, User Access. The key factor here is module autonomy and maintainability. We want to develop each module independently. This is more cleaner solution. It involves more work at the beginning but we want to invest.\n","Implement the 4 modules based on the Bounded Contexts to support autonomy, maintainability, readability, and independent development of the Domain Models in each Bounded Context."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to expose the API of our application to the outside world. For now, we expect one client of our application - FrontEnd SPA application.\n\n## Decision\n",Solution 1.\nCreating separate API projects for each module will add complexity and little value. Grouping endpoints for a particular business module in a special directory is enough. Another layer on top of the module is unnecessary.\n,"We will create a REST API using Django Rest Framework. We chose Django Rest Framework because it is a popular and well-maintained framework that provides a robust set of features for building REST APIs. It is also well-integrated with Django, which makes it a good choice for our application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur API layer should communicate with business modules to fulfill client requests. To support the maximum level of autonomy, each module should expose a minimal set of operations (the module API/contract/interface).\n\n## Decision\n","Each module will provide implementation for one interface with 3 methods:</br>\n```csharp\nTask<TResult> ExecuteCommandAsync<TResult>(ICommand<TResult> command);\nTask ExecuteCommandAsync(ICommand command);\nTask<TResult> ExecuteQueryAsync<TResult>(IQuery<TResult> query);\n```\nThis interface will act as a façade (Façade pattern) between API and module. Only Commands, Queries and returned objects (which are part of this interface) should be visible to the API. Everything else should be hidden behind the façade (module encapsulation).\n","**Decision:** The communication between the API layer and business modules will be based on **fine-grained interfaces**.\n\n**Rationale:**\n\n* **Modularity and Autonomy:** Fine-grained interfaces allow each module to expose only the operations it needs, promoting modularity and autonomy.\n* **Flexibility:** Fine-grained interfaces provide greater flexibility in evolving the system over time, as modules can be easily added, removed, or modified without affecting the API layer or other modules.\n* **Code Reusability:** Fine-grained interfaces encourage code reuse, as operations can be shared among multiple modules.\n* **Maintainability:** By isolating operations into individual interfaces, it becomes easier to maintain and understand the codebase."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn some cases it is not possible to enforce the application architecture, design or established conventions using compiler (compile-time). For this reason, code implementations can diverge from the original design and architecture. We want to minimize this behavior, not only by code review.\n\n## Decision\n",We decided to implement Unit Tests for our architecture. </br>\nWe will implement tests for each module separately and one tests library for general architecture. We will use _NetArchTest_ library which was created exactly for this purpose.\n,Enforce coding conventions by means of a static code analyzer.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAggregates should check business invariants. When the invariant is broken, we should stop processing and return an error immediately to the client.\n\n## Decision\n","Solution number 1 - Use exceptions. </br>\nPerformance cost of throwing an exception is irrelevant, we don't want too many if/else statements in entities, more familiar with exceptions approach.\n","**Decision:** Invariants are checked within the aggregates. When an invariant is broken, an exception is thrown."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to create Domain Models for all of the modules. Each Domain Model should represent a solution that solves a particular set of Domain problems (implements business logic).\n\n## Decision\n,"Solution number 3 - Rich Domain Model</br>\n1 - no, because the procedural style of coding will not be enough. We want to focus on behavior, not on the data.</br>\n2 - no, keeping business logic in the database is not a good idea in that case, object-oriented programming is better than T-SQL to model our domain and we don't have performance architectural drivers to resign from OOD.</br>\nWe expect complex business logic with different rules, calculations and processing so we want to get as much as possible from Object-Oriented Design principles like abstraction, encapsulation, polymorphism. We want to mutate the state of our objects only through methods (abstraction) to encapsulate all logic and hide implementation details from the client (the Application Service Layer and Unit Tests).</br>\n",We should **create a Domain Model for each module**. Each Domain Model will be responsible for representing a solution that solves a particular set of Domain problems (implements business logic). This will allow us to easily identify and reuse Domain logic across different modules. It will also help us to keep our codebase organized and maintainable.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe applied the CQRS style (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), now we need to decide how to handle reading (querying) requests.\n\n## Decision\n","We will use 2 layered architecture to handle queries: API layer and Application Service layer. As we applied the CQRS and created a separated read model, querying should be straightforward so 2 layers are enough. The API layer is responsible for Query creation based on HTTP request and the module Application layer is responsible for query handling.\n","We have decided to use a read-only database replica for handling read (querying) requests. The replica will be updated asynchronously from the primary database using a log-based replication mechanism. This approach will provide us with the following benefits:\n\n* **High availability:** The read-only replica will be available even if the primary database is unavailable, ensuring that our application can continue to serve read requests.\n* **Scalability:** The read-only replica can be scaled independently of the primary database, allowing us to handle increased read traffic without impacting the performance of write operations.\n* **Low latency:** The read-only replica will be located close to the application servers, reducing the latency of read operations.\n\nWe will use a database replication tool, such as [Bucardo](https://wiki.postgresql.org/wiki/Bucardo) or [Slony](https://www.slony.info/), to implement the log-based replication between the primary database and the read-only replica."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)) for each module. We need to define or use some construction elements / building blocks to implement our architecture and business logic.\n\n## Decision\n,"We decided to use **Domain-Driven Design** tactical patterns. They focus on the Domain Model implementation. Especially we will use the following building blocks:\n- Command - public method on Aggregate (behavior)\n- Domain Event - the immutable class which represents important fact occurred on a special point of time (behavior)\n- Entity - class with identity (identity cannot change) with mutable attributes which represents concept from domain\n- Value Object - immutable class without an identity which represents concept from domain\n- Aggregate - cluster of domain objects (Entities, Value Objects) with one class entry point (Entity as Aggregate Root) which defines the boundary of transaction/consistency and protects business rules and invariants\n- Repository - collection-like abstraction to persist and load particular Aggregate\n- Domain Service - stateless service to execute some business logic which does not belong to any of Entity/Value Object\n",Use an Event Sourcing architecture style as the foundation for building our rich domain models.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\n\n## Decision\n",Solution number 2 - IoC Container per module</br>\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\n,Use Dependency Injection (DI) framework to resolve object dependencies.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe applied the CQRS style (see [ADR #7](0007-use-cqrs-architectural-style.md)), now we need to decide how to handle writing operations (Commands).\n\n## Decision\n","We will use **Clean Architecture** to handle commands with 4 layers: **API layer**, **Application Service layer**, **Infrastructure layer** and **Domain layer**. </br>\nWe need to add Domain layer because domain logic will be complex and we want to isolate this logic from other stuff like infrastructure or API. Isolation of domain logic supports testing, maintainability and readability.\n","**Decision:** We will use a Command Bus pattern with in-memory message routing.\n\n**Rationale:**\n\n* **Centralized Command Handling:** The Command Bus provides a central point for handling all commands, ensuring consistency and control over command execution.\n* **Decoupled Execution:** The Command Bus decouples command execution from the domain model, allowing for easier testing and maintenance.\n* **Asynchronous Execution:** The in-memory message routing enables asynchronous command execution, improving performance and scalability.\n* **Extensibility:** The Command Bus can be easily extended to support additional command handling mechanisms, such as RPC or HTTP endpoints.\n\n**Consequences:**\n\n* **Infrastructure Overhead:** Adding a Command Bus introduces additional infrastructure, which can increase complexity and maintenance overhead.\n* **Performance Considerations:** The in-memory message routing may not be suitable for highly concurrent or latency-sensitive systems.\n* **Error Handling:** The Command Bus must handle errors and exceptions gracefully to ensure data integrity and system reliability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, we have a main class called [KonduitServingMain](https://github.com/KonduitAI/konduit-serving/blob/45af79d15abe4912ccd81e78c9d215306472036e/konduit-serving-core/src/main/java/ai/konduit/serving/configprovider/KonduitServingMain.java) that is the entrypoint for a konduit-serving application to run. The main command line arguments are defined inside [KonduitServingNodeConfigurer](https://github.com/KonduitAI/konduit-serving/blob/e791741b80721980f8b66a35ed42f20b30612d5c/konduit-serving-core/src/main/java/ai/konduit/serving/configprovider/KonduitServingNodeConfigurer.java) class. We also have an additional [Python CLI](https://github.com/KonduitAI/konduit-serving/blob/7965965b58217f2b4d983fd41aaea013264491ee/python/cli.py) that can be just implemented in Java. Vert.x Launcher supports the ability to start, stop and list running verticles out of the box.\n\n## Decision\n","- Extend `KonduitServingNodeConfigurer` from [Vert.x Launcher](https://vertx.io/docs/vertx-core/java/#_the_vert_x_launcher) class.\n- Write all the initialization/tear-down logic inside the lifecycle methods of Vert.x Launcher which is present inside both of the above classes (KonduitServingMain and KonduitServingNodeConfigurer).\n- Extend the CLI and write separate classes for every command we want to add to the CLI (see the documentation [here](https://vertx.io/docs/vertx-core/java/#_extending_the_vert_x_launcher)).\n- Depending on the final decided API, register or unregister commands in Vert.x Launcher.\n### Example CLI:\nThe following CLI will be the start for how this will be look:\n#### Starting Verticles\nEvery verticle will be named some sensible value instead of the full classpath so that the verticles can be started with a simple name. For example:\nkonduit serve inference-server --config '{}' # With a JSON string\nkonduit serve inference-server --config config.json # With a JSON file\nkonduit serve inference-server --config config.json --name=mnist-prediction-server # With a server name. If no names are given then a random name\n# is generated.\nThe CLI can also have options for setting up deployment options like:\nkonduit serve inference-server --config config.json --instances 3 # For running 3 instances of a Verticle. Usually it runs on a single port with\n# round robin fashion requests transfer for load balancing\n#### Stopping Verticles\nkonduit stop inference --name=mnist-prediction-server # stops the server by the name of ""mnist-prediction-server""\n#### Listing Verticles\nkonduit list # Lists all of the running verticle services with their names, host, port, configuration\n#### Inspecting Verticles\nPossible details will include:\n- configuration\n- host and port\n- current resource usage\n```bash\nkonduit inspect # Give details of all the running verticles\nkonduit inspect --name=mnist-prediction-server # Details of a specific verticle\n```\n#### Running Predictions\n#### Predict with JSON (application/json)\nkonduit predict --name=mnist-prediction-server --input '{""key"":""value""}' # With JSON string\nkonduit predict --name=mnist-prediction-server --input input.json # With JSON file\n#### Predict with Files (multipart/form-data)\nkonduit predict --name=mnist-prediction-server --input file.npy # Numpy\nkonduit predict --name=mnist-prediction-server --input file.zip # DL4J\nkonduit predict --name=mnist-prediction-server --input image.jspg # Image\nkonduit predict --name=mnist-prediction-server --input file1.zip,file2.zip # DL4J with multiple inputs\n",Implement a `VertxLauncher` to replace the main function.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKonduit Serving is a complex modular tool intended to be deployed in a number of different configurations, in multiple packaging formats.\nFor any given model/pipeline, the deployment/packaging scenarios can vary widely. For example, a user might want to deploy a TensorFlow model via Konduit Serving in one of these configurations (and many more):\n* Docker image packaging using TensorFlow + CUDA 10.1 on an Linux ARM64 system, with serving via HTTP/REST\n* A self-contained .exe (with embedded JVM) using SameDiff TensorFlow import to run the model on CPU, on a Windows x86 + AVX2 system with Intel MKL + MKLDNN (OneDNN) included, with serving being performed via gRPC\nCurrently, packaging for Konduit Serving is done via Maven profiles and Maven modules.\nA user selects the combination of dopendencies and functionality they need by enabling a number of profiles and system properties.\nFor example, building a Windows CPU uber JAR looks something like this:\n```\nmvn clean package -DskipTests -Puberjar -Pcpu -Ppython -Pnative -Ppmml -Ptensorflow -Dchip=cpu -Djavacpp.platform=windows-x86_64\n```\nThe other packaging options are executed by adding different profiles.\nThis approach has got us quite far in terms of packaging (enabling flexible packaging options including uber-JARs, Docker, WARs, DEB/RPMs, tar files and .exe files), we are running up against the limits of this approach.\nSpecifically, this approach has the following problems:\n* The combination of options available to users is only going to continue to grow (too many profiles and combinations for devs/users to know about and understand)\n* Some combinations are difficult or impossible using just profiles and properties (for example, building a binary for both Windows and Linux, but not Mac or PPC etc)\n* It is easy to leave performance on the table - i.e., using ND4J/SameDiff/TensorFlow etc binaries built without AVX support\n* Many incompatibilities will only become apparent at runtime (example: build for a CUDA 10.x version only to find that TensorFlow only releases with CUDA 10.y and hence we have a runtime problem)\n* Now (with the Data/API rewrite) configuration and execution is separate; the one configuration can be run many different ways. For example, a TensorFlow model could be run with TensorFlow, SameDiff, TVM, or (possibly automated) conversion ONNX, etc. This will be challenging to support via a ""profiles and properties"" build approach.\n* Usability issues: For example, users need to know a lot about the different profiles, configuration, etc to get an optimal (or even functional) deployment - or even know what is possible.\n- An example of this: the user might build an uber-JAR without the PMML profile being enabled, only to discover their JAR can't run their pipeline (that has a PMML model)\n* Packaging of custom code, dependencies and other assets (inc. model, vocabulary files etc) is difficult or impossible at present\n* Building Konduit Serving artifacts (Uber-JARs, docker images, etc) requires cloning the main source code repo (though we hide this and do it automatically in the case of Python CLI-based source builds)\n\n## Decision\n","Also Gradle may be beneficial if/when we deploy to Android (though there are many other issues for Android deployments to consider beyond just Maven/Gradle).\nNote however that the pom.xml/build.gradle won't be generated then reused for long-lived projects or anything, instead being generated just before the build from our config.\nSo usability, user experience and IDE support shouldn't matter for Maven vs. Gradle.\nOne benefit is that Alex (who will be either building or overseeing this) and the KS team generally has more Maven experience.\nAlso: it's not necessarily an either-or decision: we could have both Maven and Gradle build tool implementations, or switch at a later date (without users really being aware of the switch).\nGradle has a pom.xml file generator also which might provide another option here.\n> Can we make this build tool ""live inside"" the parent build tool?\nThe motivation for this is to enable users to access the Konduit Serving build functionality without any external installation.\nThough `pip install konduit-serving` as a way to get the build tool will probably serve the average Python developer/data scientist OK, it won't be ideal for JVM-based developers. For them, perhaps a Maven plugin or Gradle task would provide good usability:\n```\nmvn ai.konduit.serving:for-pipeline foo-bar.yaml\ngradle doWhateverINeed\n```\nThe ability to do do Konduit Serving builds with only a JVM and Maven or Gradle installed, nothing else.\nAgain, it's not either or - we can potentially do both.\nAlternative one-liners for getting set up with the Konduit Serving build tools include:\n* `sudo apt-get install konduit-serving`  (for Linux users)\n* A `konduit-buildw` script (i.e., wrapper script like Maven and Gradle wrappers, `mvnw` and `gradlew`)\n> Who is actually going to build Konduit Serving from Source?\nFew people. Using this build tool would not require or even encourage building from source. Given it's just generating a pom.xml/build.gradle it will work with release versions too without a local copy of the source.\n> Who is the target audience for the build tool?\nMost users usually won't interact with this build tool directly, instead only touching (or being aware) of the UI/CLI layer on top of it.\nThe target of the whole package of functionality (CLI+UI+build tool etc) will be pretty much any KS user who needs more than standard ""off-the-shelf"" model serving. i.e., anyone who needs custom code/dependencies, or needs to customize the build for specific hardware or to use a specific execution framework, etc.\nDown the line I see us expanding this to allow other packaging for other deployment targets (like helping people deploy on android) too.\n> Do we expect every single user of Konduit Serving to be building a special build for their deployment?\nNo, but that's kind of tangential to the proposal. There's nothing stopping us from having off-the-shelf builds/artifacts for common scenarios (linux/win/mac TF CPU, for example) with or without this tool, either distributed directly (via docker hub, or whatever) or simply as a ""simple""/default build option.\nOne day we'll provide OSGi-based deployments too, which will allow for automatic/runtime download and installation of modules and dependencies also.\n","**Use a declarative dependency/packaging specification DSL:**\n\n* **Declarative and User-friendly:** Allow users to specify their packaging requirements in a clear and concise manner, using a dedicated DSL.\n* **Flexible and Extensible:** Support a wide range of packaging configurations and options, including dependencies, platforms, and execution environments.\n* **Performance Optimization:** Enable automated performance optimization by selecting the appropriate binaries or dependencies based on system capabilities.\n* **Runtime Compatibility:** Enforce runtime compatibility checks to avoid deployment failures due to version conflicts or missing dependencies.\n* **Configuration and Execution Separation:** Support the separation of configuration and execution, allowing the same configuration to be executed in multiple ways.\n* **Improved Usability:** Provide a user-friendly interface for specifying packaging requirements, eliminating the need for manual profile and property selection.\n* **Custom Asset Packaging:** Enable the packaging of custom code, dependencies, and other assets as part of the deployment artifact.\n* **Repository Cloning Optimization:** Eliminate the need for cloning the main source code repo during artifact building, simplifying the process."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the CLI allows users to launch a pipeline based on a configuration file that is hosted locally, such as:\n```text\nkonduit serve -c /some/path/to/config.json\n```\nHowever, in addition to serving local files, we want to support serving of pipelines that are hosted remotely:\n```text\nkonduit serve -c https://www.somewebsite.com/pipelines/config.json\nkonduit serve -c s3://s3-eu-west-1.amazonaws.com/myaccount/models/myconfig.yaml\nkonduit serve -c wasb://mycontainer@myaccount.blob.core.windows.net/directory/myconfig.json\n```\nIn addition to launching a server from a configuration hosted at a remote location, we also want the ability to load models\nthat are hosted at some non-local URI.\nThat is: when loading any model type, the location of the model is specified in terms of a URI in the configuration\n(such as the `modelUri` field in DL4JModelPipelineStep and TensorFlowPipelineStep) which needs to be downloaded (and perhaps cached).\nConsider the following simple use case:\n```text\n$ pip install konduit-serving\n$ konduit serve -c https://serving.konduit.ai/pipelines/images/classification/mobilenetv2.json\n```\nAssume the model specified in mobilenetv2.json is hosted remotely also at say `https://serving.konduit.ai/models/mobilenetv2.pb`.\nThis use case (remote configuration and remote model) should ""just work"".\nOverall, we will aim to support the following URI types for both configurations and model locations:\n* HTTP/HTTPS, HTTPS + basic authentication, and _maybe_ other options like HTTPS + OAuth 2.0?\n* FTP, FTP + basic authentication\n* HDFS (Hadoop filesystem, hdfs://)\n* Amazon S3 (s3://)\n* Azure (Storage account / blob storage - wasb://, wasbs://)\n* Google Cloud Storage (gs://)\nFor all of these, we simply need the ability to download a single file given the URI. At this point, uploading, directory\nlisting, etc is not required.\nThe supported types of URIs should be extensible also - i.e., users should be able to add other types if needed.\n\n## Decision\n","We utilize standard Java APIs for this: URI, URL, URLConnection and InputStream:\n```java\nURI u = URI.create(""hdfs://somepath:12345/pipelines/config.json"");\nURL url = u.toURL();\nURLConnection connection = url.openConnection();\ntry(InputStream is = connection.getInputStream()){\n//Read data\n}\n```\nThere are three issues we need to resolve in this approach.\n1. URI --> URL - i.e., we need protocol handlers for each protocol type\nA reminder: URIs describe a resource; URLs additionally provide a mechanism to access the resource.\n2. Ensuring the protocol handlers are on the classpath when required\nHaving every protocol handler always on the classpath is a simple solution/option here, though doing so will pollute\nthe classpath with a bunch of additional dependencies most users won't need, and risks introducing  dependency divergence\n(version) issues.\n3. Authentication - how do we deal with authentication (which may be optionally present or absent in many URI types)\nNote also we may need multiple sets of different credentials for launching the one pipeline.\nFor example, we may require one set of credentials for `https://somewebsite.com/myconfig.json` and an entirely different\nset of credentials for `https://othersite.com/mymodel.pb`.\n### URI --> URL + Protocol Handlers\nFor the ""standard"" protocol types such as HTTPS and FTP, these will be built into `konduit-serving-pipeline`.\nThe other protocols (requiring 3rd party dependencies - `hdfs://`, `s3://` etc) will be placed into separate modules with\nnames `konduit-serving-protocol/konduit-serving-<protocol_name>` i.e., all protocols are in their own module under the\n`konduit-serving-protocol` parent module, with name (for example, for Azure `wasb://` and `wasbs://`) `konduit-serving-azure`.\nIn each of these modules (and also in konduit-serving-pipeline) we will provide the following:\n* `SomeURLStreamHandler extends java.net.URLStreamHandler`\n* `SomeURLConnection extends java.net.URLConnection`\nSome of the underlying dependencies (S3 or Azure APIs for example) may provide these already.\nFinally, we need to actually register these 2 new classes so they are used when we attempt to do the URI --> URL conversion.\nApparently there are 2 ways to do this: https://accu.org/index.php/journals/1434\n* Register a factory\n* Register via system property\nThe system property approach is somewhat ugly, but might be the easiest solution. If possible, we will set it at runtime\nusing System.setProperty before anything that would attempt to read a URL would be used.\nFor the content of that system property (i.e., the fully qualified class names), we can likely simply collect all available\nhandlers using a mechanism similar to what konduit-serving-meta already uses for collecting all available pipeline step types.\nThe alternative is to use URL.setURLStreamHandlerFactory but this has the unfortunate design of being callable only once\nfor the entire JVM: https://docs.oracle.com/javase/7/docs/api/java/net/URL.html#setURLStreamHandlerFactory(java.net.URLStreamHandlerFactory)\ni.e., the second call to URL.setURLStreamHandlerFactory will throw an exception.\nThis is also an option but risks being incompatible with any 3rd party dependencies that also call this method.\nSee also: https://www.oreilly.com/library/view/learning-java/1565927184/apas02.html\n### Authentication and Credentials\nUnfortunately, credentials are likely going to be protocol-specific, with different ways of setting the credentials\nfor different protocols. For example, Azure may require environment variables so set for example an access key, whereas\nother protocols may require system properties or some other mechanism.\nWe will need to look into the available authentication methods for each of the protocols we want to support.\nFor now, where system properties or environment variables can be used, we'll provide a mechanism to do so when launching\na server (or calling the CLI).\n### Protocols for CLI\nUnless the dependencies for the other protocol types (s3, azure, hdfs, etc) are especially large (file size) or are problematic\n(dependency problems) we will simply include all of the protocol types by default in the CLI.\nIf there are problems with this ""always available in CLI"" design, we will switch to an alternative approach:\n* Only the standard protocols (http, ftp etc) are available by default\n* If another protocol is required (s3, wasbs, hdfs etc) we download the appropriate module dependencies using the build tool\n* Each module has an entry point used for downloading files, which we run to download the file\nThat is: we launch a new, temporary process from the CLI that simply calls the module's main method in order to download\nthe configuration file and then exit\nNote however that we'll probably need this ""alternative approach"" in the future anyway to enable us to support custom protocols\nvia the CLI.\n### Protocols for Runtime (Model Servers)\nThere are two use cases here, as discussed in the ADR 0006-CI_Launching_vs_Build.md.\nFirst is the ""immediate CLI launch case"", and second is the ""deployment artifact"" launch case.\nFor both cases, we will start by always including all protocol modules as per the CLI.\nAnd again, if this proves problematic, we will consider an alternative.\nAlternative approach, to be implemented if necessary: We will use the same approach that we use already for determining\nmodules and dependencies for servers via the build tool.\nThat is: We will look at the provided configuration (JSON/YAML) and find any relevant URIs. From that, we will determine\nthe modules needed. For example, if the configuration contains `""modelUri"" : ""s3://some/path/to/model.pb` then we know\nwe need to include the `konduit-serving-s3` module when launching the server.\n### Extensibility - Custom Protocol Types\nThe likely approach for custom protocols is to use the (soon to be added) `additionalDependencies` / `additionalJars` options\nfor the CLI and build tool. By setting one of these, a user can make a protocol handler for their custom type available\nboth for the CLI and at runtime.\n### File Caching\nAn additional consideration here is file caching.\nSuppose I'm launching a server from a remote URI. I shut down and then immediately restart the server.\nUpon that restart: should I download the model again or not?\nFor large models, this could be a significant problem - we don't want to redundantly download a model every time we launch\na server.\nTo account for this, we will cache downloaded models in say `~/.konduit_cache`.\nThen, when launching servers, we will:\n1. Check this cache. If no file exists, download as normal\n2. If a file does exist, we will open a URLConnection and check that both of the values returned by `URLConnection.getLastModified()`\nand `URLConnection.getContentLength()`. If either differs, we will delete the old file and download from scratch.\nNote that if opening the URL fails after say 3 retries (hence we can't check if the cached file is the latest): by default\nwe will provide a warning and use the cached version anyway. This has the downside of potentially using an old model,\nfor a server, but reduces the probability of intermittent network problems causing a server launch failure even when\nthe model we've got cached is totally fine.\nHowever, this ""use anyway"" default behaviour should be switchable to ""fail if we can't validate"" via some system property\nor environment variable - perhaps `konduit.serving.cache.validation.warnonly` set to true by default, false if ""exception\non validation failure"" is preferred instead.\nTo implement this, when downloading a model into the cache, we need to record (in a separate file) the content length and\nlast modified values. We should also record the most recent time a cache entry was accessed, which will allow us to delete\nold entries (for example, those that haven't been used for say 60 days or more by default). We will thus implement a method\nto check and clear the cache once per JVM launch of a server, after the cache has been used/accessed.\nTo implement this caching for downloading models, we will add a `URIResolver` class to `konduit-serving-pipeline` with\n`public static File getFile(URI uri)` and `getFile(String uri)` methods that handles the downloading, caching, etc.\nThis URIResolver class should then be used within each of the pipeline step runners to launch models.\nNote that when no scheme is provided for the `getFile(String)` method, URIResolver should assume it's a local file (and\nhence add the `file://` scheme to the provided path). Users shouldn't get obscure URI syntax exceptions when trying to\nlaunch from a local file if they forget to add `file://` to the start of every single path.\nFinally, as a workaround for potentially stale cache entries, we will implement a system property `konduit.serving.cache.clear`\nthat, when set to true, will clear the cache before launching. This should be rarely needed, but will be available if the\nlastModified and contentLength checks fail to identify an updated remote file for some reason.\n","**HTTP / HTTPS and S3 should be supported for remote loading of both configurations and models.**\n\nHTTP / HTTPS and S3 are the most commonly used protocols for remote file access. They are well-supported by existing libraries and tools, and they provide a wide range of features and options.\n\n**Support for other protocols (such as FTP and HDFS) can be added in the future, if there is demand.**\n\nFTP and HDFS are less commonly used than HTTP / HTTPS and S3, and they are not as well-supported by existing libraries and tools. However, if there is sufficient demand, support for these protocols can be added in the future.\n\n**The supported types of URIs should be extensible, so that users can add other types if needed.**\n\nExtensibility is important because it allows users to add support for new types of URIs as needed. This is especially important for new and emerging protocols.\n\n**The following steps should be taken to implement this decision:**\n\n1. Add support for HTTP / HTTPS and S3 to the Konduit Serving CLI.\n2. Add support for HTTP / HTTPS and S3 to the Konduit Serving Java API.\n3. Add support for HTTP / HTTPS and S3 to the Konduit Serving Python API.\n4. Add support for HTTP / HTTPS and S3 to the Konduit Serving C++ API.\n5. Document the new features in the Konduit Serving documentation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the past, Konduit Serving pipelines have been essentially a 'stack' of pipeline steps - each connected to only to the next: `A -> B -> C` and so on.\nHowever, some use cases require a more complex structure, allowing parallel and/or conditional execution of pipeline steps.\nExamples of use cases:\n* ""Select one of N models"" as part of a pipeline\n- A/B split testing (i.e., test different models for different inputs/users)\n- One model per X, selected dynamically (per region, language, time of day, etc)\n- ""Select N of M models"" might occasionally be useful in some cases (like sensor fusion: image + text + sound in, but these are all optional/unreliable inputs)\n* Parallel branches\n- Ensembles of ML models\n- Parallelization: Execute slow steps in parallel to reduce overall pipeline execution time (database access, network communication, etc)\n* Fallback models (i.e., ""RNN has 100ms to provide a response otherwise we return 'X'""; or ""if model fails, return Y"")\nThis ADR proposed the GraphPipeline, which will enable these use cases and more.\nThe existing SequencePipeline functionality (i.e., ""stack of steps"" approach) would not be changed by this proposal.\nFor GraphPipeline, there are a two considerations here:\n* Functionality to provide\n* API for providing that functionality\n\n## Decision\n","### Functionality\nGraphPipeline will, like SequencePipeline, have a single Data input and a single Data output. This allows for a number of things including:\n* Embedding a GraphPipeline within another Pipeline (SequencePipeline or GraphPipeline)\n* Same API and serving methods and serving code for SequencePipeline and GraphPipeline\n- i.e., users don't\nInternally, GraphPipeline can have any amount of branching, parallelism, etc.\nThis single input, single output restriction should not be cause any usability problems due to the fact that Data instances can contain any number of values (i.e., any number of (key,value) pairs). Hence anything we can do with a multi-Data input design can be achieved by combining and splitting Data instances.\nIt is proposed that we provide support for directed acyclic graphs only - no loops are allowed within graph pipelines.\nWe provide 5 types of ""graph steps""\n* Standard (1-to-1): single input, single output - a normal PipelineStep in a graph\n* Switch operation (1 to 1-of-N): 1 Data instance in, which is routed to one of N outputs, based on some criteria (value in Data instance, or otherwise)\n- Example use case: A/B testing (switch op selects which model step to use)\n- Merge (N-to-1): simply copy the content of all input Data instances into one output Data instance\n- Any (N-to-1): simply forwards on the first (possibly only) available Data instance\n- Typically used in conjunction with a switch step, where only one of N branches is executed\n- Combine function (N-to-1): An arbitrary N-to-1 function, with or without all the inputs being available first. In addition to allowing custom Java/Python UDFs, we will provide a small number of built-in functions for:\n- Ensemble: allows weighted averaging, etc\n- an integer aggregation selection (`inIdx = argMax(in[i][""score""])`)\n- a timeout condition (""return X if we get the value within N ms, otherwise return Y"")\nInternally (but not for JSON) Merge and Any will probably be implemented as special cases of CombineFn - so we have really have just 3 types from an implementation perspective (1->1, 1->(1-of-N), N->1).\nExamples use cases:\n- Any: conditional execution with 2 branches: `in -> Switch(a,b), a->X, b->Y, Merge[Any](X,Y) -> output`. We either execute the left branch (`in->a->X->Merge(ANY)->out`) or the right branch (`in->b->Y->Merge(ANY)->out`)\n- CombineFn: Select and return the predictions of the model with the highest probability\nWe could also introduce a ""split"" operation, that does 1-to-N by splitting up a single Data instance; in practice we can do this simply by a number of simple ""subset"" pipeline steps in parallel. For example, `in -> SubsetPipelineStep -> A`, `in -> SubsetPipelineStep -> B`, where SubsetPipelineStep simply copies a subset of the input Data (key,value) pairs to the output Data instance.\nNote that routing an input to ""N of M"" outputs is not easily supported in this proposal (where N changes on each inference step). If needed, it can be added later, or it can be approximated by a series of switch, no-op pipeline steps (returns empty Data), and merge operations.\n### API (Java)\nThe goal of the API is to make it as easy as passible to create graph pipelines, that do exactly (and unambiguously) what users expect.\nAt least two options exist:\n* Functional-style API\n* Builder style API (like DL4J ComputationGraphConfiguration)\nIt is propesd to use a semi-functional API, as follows:\n```java\nGraphBuilder b = new GraphBuilder();\nGraphStep input = b.input();\n//Standard PipelineStep:\nGraphStep myStep = input.then(""myStep"", new SomePipelineStep());    //Always require a name\n//Merge:\nGraphStep merged = myStep.mergeWith(""myMerged"", input);             //Name is optional\n//Any:\nGraphStep any = b.any(step1, step2);                                //Name is optional\n//Combine\nCombineFn fn = ...\nGraphStep combined = b.combine(fn, step1, step2);                   //Name is optional\n//Switch: note exact API here is TBD, but it's essentially a Function<List<Data>,Integer> with a numOutputs() method\nSwitchFn sf = ...\nGraphStep[] switched = b.switch(fn, myStep)\n//Construct the final GraphPipeline\nPipeline p = b.build(combined);                                     //Build method takes the final output step\n```\nAssuming we go with the functional-style design, there's not too many design decisions here, mainly related to naming:\n* The method name for adding a step - ""then"", ""call"", ""followedBy"", ""inputTo"", and probably a lot more are possible\n* The method name for merging: ""merge"", ""mergeWith"", etc\n* The method name for any: ""any"", ""merge"", ""first"", etc\n* The method name for combine: ""combine"", ""combineFn"", ""combineFunction"", ""aggregate"", etc\nThere's also the concern that ""merge"" and ""combine"" are too close in name/meaning, to confuse people. (Suggestions here are welcome)\n### API (Python)\nIn Python, it will be almost idestical, other than being a true functional interface for pipeline steps\n```python\nb = GraphBuilder()\ninput = b.input()\n# Stardard PipelineStep:\nmyStep = input(""myStep"", SomePipelineStep())\n# Merge:\nmerged = myStep.mergeWith(""myMerged"", input)\n# Any:\nany = b.any(step1, step2)\n# CombineFn\nfn = ...\ncombined = b.combine(fn, step1, step2)\n# Switch\nsf = ...\nswitched = b.switch(sf, myStep)\n# Construct final GraphPipeline:\np = b(combined)                     #OR: b.build(combined)?\n```\n### JSON\nWe should consider JSON part of the public API also - we want people to be able to write graph steps using JSON/YAML by hand.\nFor SequencePipeline, defining steps is simple: Users just provide an array/list of steps, like so:\n```json\n{\n""steps"" : [\n{\n""@type"" : ""<step type>"",\n""config1"" : ""value1"",\n""config2"" : ""value2""\n},\n{\n""@type"" : ""<step type>"",\n""config1"" : ""value1"",\n""config2"" : ""value2""\n},\n]\n}\n```\nFor Graph pipelines, we have to encode extra information:\n* Graph structure - i.e., names and inputs\n* Graph components other than just PipelineSteps\nProposal: We stay as close to the SequnencePipeline representation as possible, changing only the following:\n* ""steps"": Becomes an object (map) not a list. Object keys are step names.\n* We add an ""@input"" (alias: ""@inputs"") field within each pipeline step\n* For pipeline steps: can take a single value, or a size 1 list\n* For Merge/Any/CombineFn - take a list/array\nBy calling the field `@input` / `@inputs` we avoid to avoid ambiguity / clashing names when it comes to JSON serialization time for arbitrary configuration classes. That is, if we called it `input` and the user had a field called `input` in their configuration class, we have a problem.\nExample JSON for graph pipelines, with 1 pipeline step (connected to pipeline input), and one merge step (connected to input and the step ""myStep"")\n```json\n{\n""steps"" : {\n""myStep"" : {\n""@input"" : ""input"",\n""@type"" : ""<step type>"",\n""config1"" : ""value1"",\n""config2"" : ""value2""\n},\n""myMerged"" : {\n""@input"" : [""input"", ""myStep""],\n""@type"" : ""MERGE"",\n}\n}\n}\n```\n",**Decision:** Implement a Graph-based Pipeline API and functionality\n\n**Rationale:**\n\n* The existing SequencePipeline approach is inflexible and insufficient for the complex use cases described in the Context.\n* A GraphPipeline API will provide the necessary flexibility to create pipelines with parallel and conditional execution of steps.\n* Separating the API for GraphPipeline from the existing SequencePipeline API allows for the possibility of different implementations in the future.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are multiple ways a user might want to use Konduit Serving to serve a model.\nFor the sake of this ADR, I'll refer to two use cases as:\n1. Immediate deployment case: ""Deploy a server for this pipeline right now on this machine via the CLI""\n2. Deployment artifact case: ""Create an artifact (JAR, docker image, etc) for deployment somewhere else""\nThe ADR 0005-Packaging_System_Design.md dealt with the ""create deployment artifact"" (2) case above.\nHowever, the build system in its current form it does well not serve the needs of the immediate deployment use case\nparticularly well, mainly due to its uber-jar design. This causes a few usability problems:\n1. Deploying a pipeline with different modules (dl4j vs. samediff vs. TF, or CPU vs. GPU) requires building an uber-jar\n2. Uber-jar builds can take a long time (30-120+ seconds, plus download time)\n3. Either we always rebuild (slow launches) or we have an uber-JAR cache (potentially lots of unnecessary disk space used)\nThe old API CLI dependency/launching approach for this use case also had/has the following problems:\n1. Requiring the user to build a JAR ahead of time and manually include the modules they need\n2. Not being able to launch (for example) both CPU and GPU servers simultaneously without rebuilding the whole Konduit\nServing uber-jar in between launching the different servers\nFor the CLI-based ""deploy right now"" use case, an alternative is proposed.\n\n## Decision\n","For the ""immediate deployment via CLI"" scenario, we will not create an uber-jar; instead, we will use the Konduit Serving\nbuild tool to work out the dependencies we need, download them, and return a list of all dependencies (i.e., a list of JAR file\npaths) that the Konduit Serving server will be launched with.\nThis is similar to how IDEs like IntelliJ work. Consider for example the command IntelliJ uses when launching unit tests etc: (some parts omitted)\n```\n""C:\Program Files\AdoptOpenJDK\jdk-8.0.242.08-hotspot\bin\java.exe"" -ea -Dfile.encoding=UTF-8 -classpath ""C:\Program Files\JetBrains\IntelliJ IDEA Community Edition 2019.3.3\lib\idea_rt.jar;...;C:\DL4J\git\konduit-serving\konduit-serving-models\konduit-serving-deeplearning4j\target\test-classes;...;C:\Users\Alex\.m2\repository\org\nd4j\nd4j-api\1.0.0-SNAPSHOT\nd4j-api-1.0.0-SNAPSHOT.jar;C:\Users\Alex\.m2\repository\com\jakewharton\byteunits\byteunits\0.9.1\byteunits-0.9.1.jar;C:\Users\Alex\.m2\repository\com\google\flatbuffers\flatbuffers-java\1.10.0\flatbuffers-java-1.10.0.jar;C:\Users\Alex\.m2\repository\org\nd4j\protobuf\1.0.0-SNAPSHOT\protobuf-1.0.0-SNAPSHOT.jar;C:\Users\Alex\.m2\repository\commons-net\commons-net\3.1\commons-net-3.1.jar... "" com.intellij.rt.junit.JUnitStarter -ideVersion5 -junit4 ai.konduit.serving.deeplearning4j.TestDL4JStep\n```\nNote the `-classpath ""<list of JAR paths>""` component here.\nIn practice, we won't pass a list of JAR file paths directly due to constraints on the maximum command line length on Windows.\nInstead, we will use a small JAR containing a manifest file, which will list the absolute path of all dependencies:\nhttps://www.baeldung.com/java-jar-manifest\nThis single manifest JAR is then passed via the `-classpath <path>` arg during launch.\nNote that this exact Manifest JAR approach is also used by IntelliJ as an option for command line shortening to work around\nthe maximum command line length problem.\nThe key aspects of this design:\n**1 - Static CLI JAR**\nThe CLI JAR is a static JAR without any modules/dependencies that are needed to run pipeline steps; it never gets modified,\nrebuilt, etc no matter what type of pipeline is being launched.\n**2 - Konduit Serving Build Tool - Downloads and Resolves Dependencies**\nWhen a user launches a server based on some Konduit Serving pipeline configuration, the following occurs:\n1. The CLI calls the build tool\n2. The build tool resolves all dependencies that need to be included to run that pipeline\n3. All direct and transitive dependencies are downloaded as normal via Gradle and stored in their usual location\n4. The build tool creates the required manifest JAR\n**3 - We introduce a concept of ""device profiles"" in the CLI**\nIn practice, this will allow users to switch between different targets when launching (CPU vs. CUDA, and x86 instead of x86_AVX2 if needed\nfor some reason). Specifically:\n1. On the first run of the Konduit Serving CLI, we automatically create a set of appropriate device profiles based on\nhardware and software available on this system. We will also set the default device profile to the CUDA profile if present.\nIn practice, this will usually be just a CPU profile (highest level supported - avx2, avx512, etc) and a CUDA profile\nif a CUDA device is present on the system. For the CUDA profile, we'll also detect if the CUDA is installed (if not,\nwe'll use the JavaCPP presets CUDA redist binaries to provide it at runtime, avoiding the need for a manual install)\n2. When running, we'll use the default profile unless the user passes a `-p <profile_name>` during launch. That is:\n`konduit serve -c config.json -p CPU`\nIn practice most users won't need to worry about device profiles, unless they need:\n(a) to run on CPU only for a GPU-enabled device, or\n(b) (very rarely, if ever) need to downgrade the target (for example, x86 instead of x86_avx2 on an avx2 compatible system)\nas a workaround to some issue with an avx2 or higher binary.\n### Example Workflow: Launch Locally\nSuppose a user wants to deploy a server for inference, on system without a Konduit Serving installation.\nHere's what that could look like:\n```text\n$ pip install konduit-serving           #Or any other easy installation method - apt, yum, etc etc\n$ konduit serve -c config.json\nKonduit Serving\n--- Konduit Serving First Run Initialization ---\nDetecting hardware... done\nCPU:                 ARM64 (aarch64) - 4 core\nCUDA GPU:            <device name>, 4GB\nCUDA installation:   Found CUDA 10.2 (/usr/local/cuda)\nCreating device profiles...\nProfile 1: ""CUDA_10.2"" - ARM64, CUDA 10.2 installed\nProfile 2: ""CPU""       - ARM64, CPU execution\nCreating device profiles complete\nSetting default profile: ""CUDA_10.2""\nUse <some_command> to set default profile or pass ""-p <profile>"" when launching to override\n--- First run initialization complete ---\nLaunching server using default device profile ""CUDA_10.2""\nAcquiring dependencies... done\n<usual Konduit Serving launch info>\n```\nNote that users need only 2 lines here to go from a brand new system (no KS install) to hosting a model server using the\noptimal hardware/configuration for that device (i.e., CUDA, or highest supported AVX level for x86 systems, etc).\nFurthermore there is no slow ""build uberjar"" step that delays the launching of the server by 30-120 seconds, on top of\ndependency downloading.\n### Launching for the ""Deployment Artifact"" case\nThis ""manifest JAR"" approach can likely be used in other situations:\n* Docker: Could use either uber-JAR or switching to an assembly-JAR style (i.e., embed the original/unmodified dependency\nJARs instead of an uberjar)\n* RPM and DEB: As per docker\n* Stand-alone .exe: Continue to use uber-JAR approach\nIf we decide an assembly-JAR style approach is useful for these deployment artifacts, we can implement that at a later date.\nAlso, in principle we can add extra dependencies on top of an uberjar... it may not be an especially elegant design, but\ncombining uber-jars with 'extra' classpath dependencies may be possible if we ever really need it. However that won't\nbe something we support for now\n### Detecting Hardware and Creating Profiles\nDetecting the CPU details should be straightforward, at least on x86-based systems using a library such as OSHI:\nhttps://github.com/oshi/oshi\nHow well OSHI supports ARM-based platforms is something we need to explore, though Raspberry Pi (armhf) support does\nseem to be available: https://github.com/oshi/oshi/issues/864\nFalling back on a system utility (`cat /proc/cpuinfo` or similar) is also a possibility here.\nDetecting the presence or absence of a compatible CUDA GPU may be harder. When CUDA is installed and available on the\npath, this becomes easier (CUDA install -> assume CUDA device present, or parse output of `nvidia-smi`).\nFor the ""no CUDA installed but CUDA GPU available"" case, OSHI may show it up, or there may be a command line based approach\nto find it (like cpuinfo).\nIn principle this is a solveable problem but additional work is required to find a robust solution for detecting hardware\n(including CUDA GPUs, and maybe other GPUs in the future) that will work across all devices and operating systems we expect\nto deploy on in practice.\n","The decision is to use a dynamic module loading approach, where the user only includes the dependencies their specific model serving use case requires. This approach will address the following issues with the uber-jar design:\n\n- Deploying a pipeline with different modules (dl4j vs. samediff vs. TF, or CPU vs. GPU) requires building an uber-jar\n- Uber-jar builds can take a long time (30-120+ seconds, plus download time)\n- Either we always rebuild (slow launches) or we have an uber-JAR cache (potentially lots of unnecessary disk space used)"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nConsidering only inferences, we have two endpoints that are responsible to take inputs for prediction.\n- application/json\n- /:predictionType/:inputDataFormat # For JSON inputs\n- multipart/form-data\n- /:predictionType/:inputDataFormat # For multipart inputs (FILES)\nGiven those two endpoints we have done a ton of work on creating a python client that (as of right now) needs more proper documentation, examples and maintenance planning for making the APIs more adaptable.\nSince, we're planning to have APIs in multiple languages (python, java, C# and others), it might get difficult to maintain and document them separately in the future.\n\n## Decision\n","### Using swagger annotations to generate and document client APIs.\n[Swagger annotations](https://github.com/swagger-api/swagger-core/wiki/Swagger-2.X---Annotations) are a quick and easy way to generate openapi specifications from the source code. Annotations apply to classes, methods and arguments. Using this way, it's easier to get rid of client APIs maintenance (generation, documentation and packaging) in different languages.\n#### How it works?\nBy refactoring konduit-serving source code into classes that contain APIs for different types of verticles. For example:\n```java\n@Path(""/"")\n@Produces(MediaType.APPLICATION_JSON)\npublic class InferenceApi {\n@GET\n@Path(""/config"")\npublic InferenceConfiguration getConfig() {\nreturn new InferenceConfiguration();\n}\n@POST\n@Path(""/{predictionType}/{inputDataFormat}"")\n@Consumes(MediaType.MULTIPART_FORM_DATA)\n@Operation(summary = ""Get inference result with multipart data."",\ntags = {""inference""},\ndescription = ""You can send multipart data for inference where the input names will be the names of the inputs of a transformation process. "" +\n""or a model input and the corresponding files containing the data for each input."",\nresponses = {\n@ApiResponse(description = ""Batch output data"",\nresponseCode = ""200"",\ncontent = @Content(schema = @Schema(oneOf = {\nClassifierOutput.class,\nRegressionOutput.class,\nDetectedObjectsBatch.class,\nManyDetectedObjects.class\n}))\n),\n}\n)\npublic BatchOutput predict(@PathParam(""predictionType"") Output.PredictionType predictionType,\n@PathParam(""inputDataFormat"") Input.DataFormat inputDataFormat,\n@Parameter(description = ""An array of files to upload."") File[] multipartInput) {\nreturn new ClassifierOutput();\n}\n}\n```\nThis will require similar refactoring for the other verticles and their respective routers. Currently, the following classes will have to be refactored based on the above details:\n- PipelineRouteDefiner\n- MemMapRouteDefiner\n- ConverterInferenceVerticle\n- ClusteredVerticle\n#### How it would look at the end?\nHaving an API that looks like the class above will generate an API specification that will look like:\n```yaml\nopenapi: 3.0.1\ninfo:\ntitle: Konduit Serving REST API\ndescription: RESTful API for various operations inside konduit-serving\ncontact:\nname: Konduit AI\nurl: https://konduit.ai/contact\nemail: hello@konduit.ai\nlicense:\nname: Apache 2.0\nurl: https://github.com/KonduitAI/konduit-serving/blob/master/LICENSE\nversion: 0.1.0-SNAPSHOT\nexternalDocs:\ndescription: Online documentation\nurl: https://serving.oss.konduit.ai\ntags:\n- name: inference\ndescription: Tag for grouping inference server operations\n- name: convert\ndescription: Tag for grouping converter operations\n- name: memmap\ndescription: Tag for grouping memory mapping operations\npaths:\n/config:\nget:\noperationId: getConfig\nresponses:\ndefault:\ndescription: default response\ncontent:\napplication/json:\nschema:\n$ref: '#/components/schemas/InferenceConfiguration'\n/{predictionType}/{inputDataFormat}:\npost:\ntags:\n- inference\nsummary: Get inference result with multipart data.\ndescription: You can send multipart data for inference where the input names\nwill be the names of the inputs of a transformation process. or a model input\nand the corresponding files containing the data for each input.\noperationId: predict\nparameters:\n- name: predictionType\nin: path\nrequired: true\nschema:\ntype: string\nenum:\n- CLASSIFICATION\n- YOLO\n- SSD\n- RCNN\n- RAW\n- REGRESSION\n- name: inputDataFormat\nin: path\nrequired: true\nschema:\ntype: string\nenum:\n- NUMPY\n- JSON\n- ND4J\n- IMAGE\n- ARROW\nrequestBody:\ndescription: An array of files to upload.\ncontent:\nmultipart/form-data:\nschema:\ntype: array\nitems:\ntype: string\nformat: binary\nresponses:\n""200"":\ndescription: Batch output data\ncontent:\napplication/json:\nschema:\noneOf:\n- $ref: '#/components/schemas/ClassifierOutput'\n- $ref: '#/components/schemas/RegressionOutput'\n- $ref: '#/components/schemas/DetectedObjectsBatch'\n- $ref: '#/components/schemas/ManyDetectedObjects'\ncomponents:\nschemas:\nInferenceConfiguration:\ntype: object\nproperties:\nsteps:\ntype: array\nitems:\n$ref: '#/components/schemas/PipelineStep'\nservingConfig:\n$ref: '#/components/schemas/ServingConfig'\nmemMapConfig:\n$ref: '#/components/schemas/MemMapConfig'\nMemMapConfig:\ntype: object\nproperties:\narrayPath:\ntype: string\nunkVectorPath:\ntype: string\ninitialMemmapSize:\ntype: integer\nformat: int64\nworkSpaceName:\ntype: string\nPipelineStep:\ntype: object\nproperties:\ninput:\n$ref: '#/components/schemas/PipelineStep'\noutput:\n$ref: '#/components/schemas/PipelineStep'\noutputColumnNames:\ntype: object\nadditionalProperties:\ntype: array\nitems:\ntype: string\ninputColumnNames:\ntype: object\nadditionalProperties:\ntype: array\nitems:\ntype: string\ninputSchemas:\ntype: object\nadditionalProperties:\ntype: array\nitems:\ntype: string\nenum:\n- String\n- Integer\n- Long\n- Double\n- Float\n- Categorical\n- Time\n- Bytes\n- Boolean\n- NDArray\n- Image\noutputSchemas:\ntype: object\nadditionalProperties:\ntype: array\nitems:\ntype: string\nenum:\n- String\n- Integer\n- Long\n- Double\n- Float\n- Categorical\n- Time\n- Bytes\n- Boolean\n- NDArray\n- Image\noutputNames:\ntype: array\nitems:\ntype: string\ninputNames:\ntype: array\nitems:\ntype: string\nServingConfig:\ntype: object\nproperties:\nhttpPort:\ntype: integer\nformat: int32\nlistenHost:\ntype: string\noutputDataFormat:\ntype: string\nenum:\n- NUMPY\n- JSON\n- ND4J\n- ARROW\nuploadsDirectory:\ntype: string\nlogTimings:\ntype: boolean\nincludeMetrics:\ntype: boolean\nmetricTypes:\ntype: array\nitems:\ntype: string\nenum:\n- CLASS_LOADER\n- JVM_MEMORY\n- JVM_GC\n- PROCESSOR\n- JVM_THREAD\n- LOGGING_METRICS\n- NATIVE\n- GPU\nClassifierOutput:\ntype: object\nproperties:\ndecisions:\ntype: array\nitems:\ntype: integer\nformat: int32\nprobabilities:\ntype: array\nitems:\ntype: array\nitems:\ntype: number\nformat: double\nlabels:\ntype: array\nitems:\ntype: string\nbatchId:\ntype: string\nRegressionOutput:\ntype: object\nproperties:\nvalues:\ntype: array\nitems:\ntype: array\nitems:\ntype: number\nformat: double\nbatchId:\ntype: string\nDetectedObjectsBatch:\ntype: object\nproperties:\ncenterX:\ntype: number\nformat: float\ncenterY:\ntype: number\nformat: float\nwidth:\ntype: number\nformat: float\nheight:\ntype: number\nformat: float\npredictedClassNumbers:\ntype: array\nitems:\ntype: integer\nformat: int32\npredictedClasses:\ntype: array\nitems:\ntype: string\nconfidences:\ntype: array\nitems:\ntype: number\nformat: float\nbatchId:\ntype: string\nManyDetectedObjects:\ntype: object\nproperties:\ndetectedObjectsBatches:\ntype: array\nitems:\n$ref: '#/components/schemas/DetectedObjectsBatch'\nbatchId:\ntype: string\nwriteOnly: true\nBatchOutput:\ntype: object\nproperties:\nbatchId:\ntype: string\nwriteOnly: true\n```\nAnd from this yaml the clients will be generated using [openapi-generator](https://github.com/OpenAPITools/openapi-generator). For example:\n```bash\njava -jar openapi-generator-cli.jar generate -i openapi.yaml -g python -o python_api_client\n```\nThe above command will generate the python clients for us and the related docs for using the API in python.\nThis will be a similar process for other languages as well.\n",Implement a REST API that uses a unified endpoint for both JSON and multipart/form-data inputs. This will simplify the API design and make it easier to maintain and document across multiple languages.\n\n**Endpoint:**\n\n```\n/:predictionType/:inputDataFormat\n```\n\n**Request body:**\n\n* **For JSON inputs:** Content-Type: `application/json`\n* **For multipart inputs:** Content-Type: `multipart/form-data`\n\n**Response body:**\n\n* Content-Type: `application/json`\n\n**Benefits:**\n\n* Simplified API design\n* Easier to maintain and document\n* Consistent behavior across multiple languages\n* Reduced development time\n\n**Considerations:**\n\n* Additional documentation may be required to explain the unified endpoint and input formats.\n* Error handling should be consistent for both input formats.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, we have the ""/predict"" endpoint for performing inference - this takes a Konduit Serving `Data` instance in,\nreturns a `Data` instance out, in either JSON or binary (protobuf) format. We also have the ability ot create Grafana dashboards\nbased on our metrics functionality/endpoints.\nGrafana dashboards are totally fine for some use cases (simple passive visualization). However, in we might want:\n(a) more flexibility and control than Grafana is able to provide for displaying predictions/metrics (Grafana doesn't really\nsupport images or video, other than maybe via custom plugins),\n(b) The ability to provide interactive dashboards, that can also take input, not just show outputs\n(c) the ability to provide input/output in some ""application specific"" format (for example, allow users to post a raw PDF\nto a custom endpoint, instead of having to do preprocessing on the client side post create image/text via the Konduit\nServing Data).\nAdditionally, for developing real apps we might need to (for example) provide a configuration endpoint, or a debugging\nendpoint, etc that hosts an actual HTML page the user can view and interact with.\n\n## Decision\n","This proposal suggests the addition of custom endpoint functionality for Konduit Serving.\nThe idea: to add a custom endpoint (get/post etc, with or without inteference), users need to:\n1. Provide a class that implements an interface - i.e., `MyCustomEndpoint implements HttpEndpoint`\n2. Provide the fully qualified class name in their pipeline configuration - i.e., `""customEndpoint"" : ""com.company.MyCustomEndpoint`\n3. When launching the server, they provide a JAR with their custom code (using the ""additionalDependencies"" and ""additionalClasspath""\nmechanism already proposed for the CLI / build tool) - i.e., the JAR containing the custom endpoint code is provided\nvia a JAR location, or via GAV coordinates\nThe `HttpEndpoint` interface is defined with just one method:\n`endpoints(Pipeline, PipelineExecutor) : List<Endpoint>`\nThis gives the endpoint access to the pipeline/executor for use in the endpoints\nThe `Endpoint` interface then has the following methods:\n`type() : io.vertx.core.http.HttpMethod` - i.e., GET, POST, etc\n`path() : String` - i.e., ""/myEndpoint"" - may optionally include path parameters etc\n`consumes() : List<String>` - MIME types for the input\n`produces() : List<String>` - MIME types of the response\n`handler() : Handler<RoutingContext>` - i.e., the actual endpoint-handling code.\nNote that the endpoints don't necessarily have to call the underlying pipeline / pipeline executor, though some will.\nFor example, two use cases supported by this design include:\n(a) Static HTML page serving - that may have the option to call another endpoint (example: a form that allows uploading\nof an image to another endpoint)\n(b) POST endpoints for proving for example a PDF, that does conversion, calls the executor, and returns a `Data` instance\nor some other type\n",**Decision:** Implement a simple web application framework based on Flask and Flask-RESTful for building bespoke dashboards that more fully suit our needs and the needs of our users.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn some machine learning deployments, we need to perform inference periodically, regardless of whether anyone is actively\nquerying the pipeline from a REST/GRPC etc endpoint.\nExamples 1: When processing video, we want to perform inference on each frame, or N times per second, etc regardless of\nwhether someone calls a REST endpoint or not.\nExample 2: When processing video, we want the number of pipeline executions to be independent of the number of clients\nquerying the REST endpoint (i.e., we should run at 20 FPS and return the latest output/prediction available, regardless of\nwhether there is 1 or 100 clients querying per second).\nExample 3: anomaly detection use case without any sort of REST/GRPC/MQTT etc server. Imagine we want to perform\ninference every second, and (conditionally, if a fault is detected) execute a ""HTTP post"" pipeline step to report the fault.\nExample 4: ""I want to perform inference at most 5 times per second. If no requests occur, we shouldn't perform any\ninference. If more than 5 requests per second occur, we want to return the last cached result instead""\nExample 5: ""periodic push based"" pipeline execution (microservices style). Suppose we want to perform inference on a camera\nfeed at 5 FPS and for every frame post the prediction (for example, predicted class or detected objects) to a HTTP endpoint,\na Kafka queue, an MQTT endoint, or simply writing to a file (where these are implemented as pipeline steps).\nThese use cases aren't yet supported in Konduit Serving.\n\n## Decision\n","We introduce an additional Pipeline type, `AsyncPipeline`. It implements the `Pipeline` interface (same as `SequencePipeline`\nand `GraphPipeline`) so from an API and execution point of view it is the same as the other Pipeline types.\nThis AsyncPipeline is a Decorator/Wrapper pattern - i.e.,\n```java\nPipeline p = SequencePipeline.builder()... .build();\nPipeline asyncPipeline = new AsyncPipeline(p, Trigger);\n```\nThe AsyncPipeline does two things:\n* Performs execution of the underlying Pipeline based on some sort of trigger\n* (Usually) stores the last output of the underlying pipeline, and returns it when query() is called\nConsider an AsyncPipeline set to perform inference of the underlying pipeline once per second. If we query the AsyncPipeline\n100 times per second (for example, by 100 difference users all querying the same REST endpoint), we get the same result\nreturned 100 times, not 100 independent (potentially redundant) execution of the underlying pipeline.\nThe ""trigger"" for the is configurable. It allows different ways of performing inference on the underlying model.\nThe `Trigger` interface would have the following API:\n```text\nquery(Data) : Data                       - Called when the AsyncPipelineExecutor.exec(...) is called. Returns either a\ncached Data instance, or optionally blocks for perform a new inference.\nsetCallback(Function<Data,Data>) : void  - The function provided here is used by the Trigger to perform execution of the\nunderlying Pipeline whenever the Trigger wants to (with the provided Data),\nirrespective of whether there is a query() call or not\n```\nThe idea is the Trigger would call the function whenever it wants inference to be performed, whether or not the external\nPipeline/PipelineExecutor has been called or not (i.e., irrespective of whether query(Data) is called or not).\nNote that the Trigger instances should be thread safe.\nBuilt-in implementations would initially include:\n* `SimpleLoopTrigger`: performs inference in a loop as fast as possible, unless an optional configuration `frequencyMs`\noption is set (in which case, it calls the underlying pipeline every `frequencyMs` milliseconds).\n* `TimeLoopTrigger`: calls every N `TimeUnit`s, with some offset. For example, ""Every hour, at the start of the hour"", or\n""3 hours past the start of the day, every day"", etc\n* `CachingTrigger`: performs inference ""at most every N milliseconds"". For example, if we say ""at most once per 1000ms"",\nand we get a query(Data) call at T=0, we block and call the `Function<Data,Data>`. For all subsequent queries up to T=1000ms,\nwe return the cached value from the T=0 call. The next call immediately after T=1000ms results in another blocking call\nand an update of the cached value (until T=2000ms, and so on).\n### JSON Format\nThe JSON format for SequencePipeline and GraphPipeline is something that users are supposed to be able to understand, edit\nand potentially even write from scratch if they so desire. It is also (with only a few exceptions) programming language\nindependent.\nThe SequencePipeline vs. GraphPipeline is differentiated by the form of the ""steps"" field: if it's a list, it's a SequencePipeline;\nif it's an object/map, it's a GraphPipeline.\nWe can do a simple extension to this idea: Again noting that the AsyncPipeline has a decorator pattern, we either have a\nAsyncPipeline(SequencePipeline) or a AsyncPipeline(GraphPipeline).\nIn either case, we can simply add a new field  - `@AsyncTrigger`, and otherwise leave the existing JSON format unchanged.\ni.e., the presence of this field means that we have a AsyncPipeline decorator.\n",Implement a recurrent workflow that runs without the need of a trigger.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are multiple ways one can represent a point with the existing data types. However, all of those ways can be classified as a workaround. A bounding box with zero width and height, a list of doubles, or an NDArray all don't communicate the intent of their contents well.\nBecause communicating intent is important when building maintainable systems, a Point data type was suggested.\n\n## Decision\n","We create a Point datatype that is explicitly meant to be used for point based data. The Point datatype can represent points with any number of dimensions. It is **not** limited to just 2 or 3 dimensional points.\nIn order to make conversion between bounding boxes and points easier, points like bounding boxes will also provide optional ""label"" and ""probability"" fields.\nTo access a specific dimension of a point, a direct `.get(dimension)` method is provided. Because we anticipate that 2 and 3 dimensional points are going to be used very often, the typical x, y, z based notation will also be allowed trough the use of `.x()`, `.y()` and `.z()` methods which internally call `.get(0)`, `.get(1)` or `.get(2)` respectively.\nThe value of a point will *usually* fall between 0 and 1, i.e. will be a relative measure. This is especially useful when used with other data types like image. However, it is not required to be within this range meaning absolute values are also allowed - interpreting the meaning of a point value is up to the user.\nThe point data type is going to be implemented with an interface / implementation split, however only a single implementation that can take n-dimensional points will be used. If this should ever become a problem, we should be able to provide specialized implementations without breaking existing code.\n","Create a new data type called ""Point"" that encapsulates the concept of a point in a way that is clear and concise, making it easier to understand and maintain code that works with points."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nShould `Extended` support geometry shaders?\n## Decision Drivers <!-- optional -->\n* Metal does not support geometry shaders.\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\n\n## Decision\n","* Metal does not support geometry shaders.\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\nNone taken yet.\n",Geometry shaders are not supported.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome AIS data sources include NMEA sentences that are not correctly formed. We sometimes see truncated messages, for example. This causes problems when a corrupt message was a fragment. At some point we need to let go of the fragment because its fellow fragments are never going to arrive.\n\n## Decision\n","We default to abandoning fragmented messages if 8 other messages have arrived since the first fragment. This is configurable, but we chose 8 because there's a limit of 9 fragmented messages in progress at any one time if you use the AIVDM-level group identifiers. It is configurable because systems that provide tag block group identifiers often use larger group IDs (e.g., 4 digits are common), making it possible for more to be in flight at once. But in practice, most message fragments are adjacent, so a large window is usaully unnecessary.\n",Create a policy that will purge AIS message fragments after a configurable period of time.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNMEA messages containing AIS data are sometimes malformed. The messages are often transferred from transponder devices to computer equipment over a serial port, which have limited error detection and correction capabilities, making it possible for messages to lose characters or contain garbage characters.\n\n## Decision\n","We deal with malformed NMEA messages at two levels.\nThe `NmeaLineParser` detects failures to conform to the basic structure required of an NMEA message. It reports these errors by throwing an `ArgumentException` with a text message describing the way in which the message structure does not match the specification. For example, if the normal `!` start character is missing, the error message is `Invalid data. Expected '!' at sentence start`. In cases where the message is well formed but with features that Ais.Net cannot currently parse, we throw a `NotSupportedException` exception.\nCode that processes large numbers of messages will typically not construct the `NmeaLineParser` itself, and instead relies on the high-throughput methods offered by `NmeaStreamParser`. Code working this way passes in an implementation of either `INmeaLineStreamProcessor` or `INmeaAisMessageStreamProcessor` depending on whether it wants to process each line in the file directly, or it wants `NmeaStreamParser` to handle the task of reassembling AIS payloads that have been fragmented across multiple NMEA message lines. Both of these interfaces require an `OnError` method that looks like this:\n```csharp\nvoid OnError(in ReadOnlySpan<byte> line, Exception error, int lineNumber);\n```\nWhen the `NmeaLineParser` constructed by the `NmeaStreamParser` reports an error in the manner described above, the exception it throws will be passed to this `OnError` method, along with a span providing access to the problematic line's full content, and the line number within the stream at which the error was detected.\nThere are also errors that can occur with fragmented messages. While each individual fragment might be well-formed, they might be wrong in combination. For example, it is not permitted for a fragmented message to have padding at the end of the first fragment. In these cases, which will be detected when working at the message level, we report errors through the same mechanism.\n",Implement a protocol interpreter that will perform basic error checking and handle malformed messages gracefully.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\nsuch as Looked After Children and Foster Families. These will be sourced from Local\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\nas well as central Departmental data collections such as SSDA903.\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\npathways through the Care System. Each individuals pathway can be regarded as a\nseries of events and touchpoints that the individual has had with various other actors\nin the system, such as IFAs and Foster Families.\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\n\n## Decision\n","* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\nto LAs and IFAs, serving an [Event Architecture] available internally.\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\n* Publish example client code on the Internet to ease LA and IFA adoption.\n* Events will become the durable and persistent data which can be used to hydrate\nthe Department's services, such as Diary Log and Matchmaker Assistant,\n* [Event Sourcing] should be trialled as a solution.\n",Create a single event stream in the Data Lake where all of the LA and IFA CMS Event Streams are brought together.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\n\n## Decision\n","The placement alpha prototypes and development work will only process fake data.\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the ""Authority to Operate"" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\n",Decision: Use a test data generator to create synthetic data for the Childrens Social Care Placement Service.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nArchitectural Decisions are not made in isolation and need to be discussed and agreed amongst the Service Team.\n\n## Decision\n,"Utilise [GitHub Workflow](https://guides.github.com/introduction/flow/) to manage the acceptance, or not, of ADRs by the Service Team. The Pull Request review process, prior to merging into `main`, provides suitable mechanisms for tracking comments, amendments, approvals and so on.\n","The provided context does not contain a specific decision to be made, therefore, I cannot extract the required data from the provided context."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe CSC Placement service is taking an [Evolutionary Architecture] approach.\n\n## Decision\n,"Since we are in a highly uncertain phase of the Service, and we want to optimise around developer productivity and feature prototyping, the adoption of a [Monolith First] architecture is considered best.\nAlthough Martin Fowler uses this as a precursor for a [Microservice Architecture], it is not definitely regarded as a transition to Microservices here, but rather a reasonable first step before deciding future architectural quanta.\n",**Decision:**\n\nUse [Event-Driven Architecture (EDA)] as the architectural style for the CSC Placement service to support its evolutionary development.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPostcode Validation/Verification, Address & Geocoding searches is a requirement in\nthe Diary Logging Service and more particularly in the Placement Shortlisting Service.\nCurrently there is no adopted DfE Architectural pattern for postcode searches, address\nsearches & Geocoding searches.\nThere is a Common Components Initiate (CCI) currently in development by DfE where several\nSaaS offerings are being considered.  Postcode, address & geocoding is part of this initiative,\nbut this will not be approved and an adopted practise for some time. For further information see\n[here](https://dfe-digital.github.io/architecture/common-components/#postcode-lookup).\nCCI will also include Geospatial Mapping and the following products and service will\nbe available across the department:\n* [Azure Maps]\n* [Google Maps]\n* [Bing Maps]\nEach product provides a slightly different set of features and capabilities that\nfit different use cases.\n\n## Decision\n","This Alpha is working to build out a simple API postcode service, in the first utilising\n[Postcodes.io].\nAddress validation and geocoding is not required during the alpha phase and\nthis decision will be replaced by a substantive revised decision, including address lookup\nand geocoding when the detail of the data model is clear, and the project moves to the Beta phase.\nA summary of the availability statistics for the services are available on this link:\n* [Ideal Postcodes Status Page]\n","**Decision:**\n\nAdopt Azure Maps for postcode validation, address, and geocoding searches in the Diary Logging Service and Placement Shortlisting Service.\n\n**Rationale:**\n\n* Azure Maps is a Microsoft-owned service that is part of the CCI, ensuring compatibility with future DfE initiatives.\n* It provides a comprehensive range of features, including postcode validation, address search, and geocoding, which meet the requirements of the Diary Logging Service and Placement Shortlisting Service.\n* It is a reliable and scalable service that can handle the high volume of searches expected in the applications.\n* The cost of using Azure Maps is within the budget constraints of the project.\n\n**Consequences:**\n\n* The use of Azure Maps will ensure that the Diary Logging Service and Placement Shortlisting Service are compliant with DfE architectural patterns when they are adopted.\n* The applications will benefit from the robust and reliable features provided by Azure Maps.\n* The project will incur the cost of using Azure Maps, which is within the budget constraints."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis decision is being made during Alpha. It also comes at a time that the Digital organisation within DfE are building developer capability and want the members of the developer community to readily switch between services. Hence a default preferred language and framework is desired, Digital are coalescing around Ruby and the associated Rails framework.\nRails is a development tool which gives web developers a framework, providing structure for all the code they write. The Rails framework helps developers to build websites and applications, because it abstracts and simplifies common repetitive tasks.\nRails is written in Ruby, the programming language which is also used alongside Rails. Ruby is to Rails as PHP is to Symfony and Zend, or as Python is to Django. The appeal of Ruby to developers lies in the elegance and terseness of the language.\n\n## Decision\n","The decision is to align with the central Digital preference for Ruby and Rails.\nKey reasons for choosing Ruby and Rails are:-\n* Alignment with a large portion of the Government Digital Service sites and services, including cross service development skills within DfE. [Here](https://dfe-digital.github.io/technical-guidance/guides/default-technology-stack/#the-ruby-stack) is the DfE Technical Guidance around this.\n* Optimisation around developer productivity and lower costs.\n* It was created specifically for building web applications.\n* Numerous code libraries (Gems) which provide free, open-source code for developers to fulfil specific needs.\n* A very strong community with a great sense of collaboration and support.\n* It is robust and high-quality. All Ruby developers are encouraged to follow a set of community-supported coding conventions and standards, which in turn helps produce better code, and thereby high quality digital products and services.\n* It’s conventions make it much easier for developers to move between Rails services, which will tend to use the same structure and coding practices.\n* A strong focus on testing, with many good testing frameworks.\n* It emphasises RESTful application design. REST (REpresentational State Transfer) is a style of software architecture based around a client-server relationship. It encourages a logical structure which can be exposed as an API (Application Programing Interface) which can be used to expose platform functionality (as desired for this service).\n* Is an intuitive and easy-to-read language. It uses fewer characters than other languages allowing for intuitive, natural language, coding rather than having to use lots of special characters. The flip side is that reading code is made simpler, allowing for quick on-boarding to a service and peer reviewing for example.\n* Enables rapid development. Due to many of the above points, such as readily available code libraries specifically designed for websites and apps, an active community which promotes high-quality development, and code that is simple to read and write then using Rails can achieve rapid development with high-quality results.\nReasons why you might not choose Ruby and Rails are:-\n* Not all hosting can support Rails - but since we are choosing where to host our service we can ensure this isn’t an issue.\n* Java and PHP are more widely used and there are more developers in these languages. However there is still a strong market in Ruby developers and the very extensive open-source libraries available mean a lot can be achieved by a few developers.\n* Performance and Scalability. Rails is not as fast as Java or C, but it is fast enough for this service and will horizontally scale out when needed. There are plenty of high-profile organisations relying on Rails, examples include AirBnB, Yellow Pages, Groupon, Channel 5, GitHub, Shopify and Gov.uk.\n",**Use Ruby and Rails as the default preferred language and framework for DfE Digital services.**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\n\n## Decision\n,"The decision is to use the [Gov.UK PaaS].\n**What does Gov.UK PaaS offer?**\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\n* Fast application delivery, with short cycles\n* Flexible development tools\n* Simplified DevOps\n* Seamless scalability\n* Integrated data services\n* A secure, trusted platform\n* 24x7 support\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\n**Why not DfE Azure?**\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\n**Is it secure?**\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\nSome useful URLs for security in general on Gov.UK PaaS are [here](https://www.cloud.service.gov.uk/security/) and the [security principles](https://www.cloud.service.gov.uk/cloud-security-principles/).\nAnd for the DfE there is a specific [paper](https://docs.google.com/document/d/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ/edit#heading=h.du8tl2830srf) from MBS.\n","**Decision:** Use a microservices architecture with a focus on developer productivity.\n\n**Rationale:**\n\nIn the Alpha phase of the SDLC, the primary goal is to explore solutions and optimize for developer productivity. A microservices architecture allows developers to work independently on smaller, more manageable components, which can reduce development time and increase flexibility. By focusing on developer productivity, the service can quickly iterate and adapt to the evolving requirements of the domain problem."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo put down an architecture for the system as a whole.\n\n## Decision\n,"The system consists of the following components:\n* Global components:\n* Alexa Smart Home Skill\n* AWS Lambda\n* AWS API Gateway websocket endpoint\n* Per user-components:\n* Google Account containing an automatically set-up Google Spreadsheet\n* Universal Remote Controller (URC) device\n### Alexa Smart Home Skill\n* Takes care of activation and hands off control to AWS Lambda\n* Links to user's Google Account\n### AWS Lambda\n* Receives and keeps websocket connection from each URC and sends commands to the URCs\n* Reads configuration data from user's Google Spreadsheet\nTODO: this needs to be worked out further, following\n### Google Account with Spreadsheet\nThe Spreadsheet contains all necessary configuration data to control the user's specific environment:\n* URCID\n* IR/RF transmission sequences grouped by appliance\n* mappings of commands to transmission sequences, e.g. ""Turn On TV"" --> specific IR transmission sequence to turn on TV set\n### Universal Remote Controller\n* ESP8266 system with IR and RF transmitter\n* has a baked-in unique URCID (from DS2401)\n* upon startup establishes websocket connection to AWS API Gateway (passing its URCID), and keeping that connection permanently alive\n* receives and executes commands that AWS Lambda sends via AWS API Gateway\n",We will follow a microservices architecture.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",Establish an Architectural Decision Record (ADR) process to document and track architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build/maintain our own images.\n\n## Decision\n","We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\n","Use Marketplace images for common images like CentOS, Ubuntu, and Windows Server for multiple accounts.\nUse AWS images for less popular images to save on cost. \nDeploy images that will be used in multiple accounts to a shared account."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAny operating systems that we use are likely to be patched for bug-fixes or security reasons at least once per month. Ideally we would pick up the latest stable release all the time, however this implies a great deal of churn. Such change would also come in spikes of activity e.g. around Windows patch tuesdays.\n\n## Decision\n",We will not patch our operating systems in AWS. Instead we will pin every instance to a specific version of an operating system and move the pin (via a PR) only when necessary.\n,"Create a plan for system patching that balances risk, downtime, and effort. Patch images should be tested in staging environment before deployment to production."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are moving from platforms such as VMWare, where instances are kept small and are encouraged to scale out. If we were to scale up instead, then we would require either\n1. A workload that is able to make efficient use of multi-threading.\n1. Workloads that are able to co-exist peacefully.\nIt cannot be assumed that the workloads we are moving into AWS will fit neatly into either category.\n\n## Decision\n",Our preference for enterprise applications is to consolidate onto a smaller number of instances.\n,"We will use containers to package our workloads, allowing us to isolate them from each other and control their resource usage."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n- Windows Defender is active by default in the Windows Server 2019 AMIs shipped by AWS. Although there is a good deal of overlap between the functionality of AWS Security Groups and Windows Defender, AWS has decided to keep Defender active for one specific reason, namely; ""the Windows firewall gives you audit details about packet drops, which may be important to meet your security policy or compliance requirements"". There is currently no such requirement in RI Tech.\n- Windows Defender is difficult, but not impossible, to configure in code.\n- AWS Security Groups are relatively easy to configure in code.\n- When Defender is in use, the best way to work out what it is doing, is to log into the instance as Administrator and use the Defender GUI, but logging in as Administrator is strongly discouraged.\n\n## Decision\n",Windows Defender is to be de-activated on all Windows Servers in AWS.\n,"**Decision:** Disable Windows Defender on Windows Server 2019 AMIs shipped by AWS.\n\n**Rationale:**\n\n* AWS Security Groups provide similar functionality to Windows Defender, and are easier to configure in code.\n* There is currently no requirement for audit details about packet drops in RI Tech.\n* Logging into the instance as Administrator is strongly discouraged, which makes it difficult to configure Windows Defender in code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently use RDP for Windows Server and SSH for Linux to connect to the servers to be migrated, when we have server administration tasks to perform. However, if EC2 instances are stateless, then we would not need to log into these servers, if they are sick, then then can be terminated.\n\n## Decision\n","RDP and SSH are allowed for all non-production EC2 istances, but are left blocked for production EC2 instances.\n","**Decision:** Refrain from using RDP and SSH for EC2 instances.\n\n**Rationale:** Due to the stateless nature of EC2 instances, logging into them for administrative tasks is unnecessary. If an instance becomes sick, it can be terminated and recreated, eliminating the need for manual intervention. This approach aligns with the best practices for managing EC2 instances and enhances operational efficiency."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are migrating from several different versions of Windows and Linux in DXC to AWS. The easiest migration path would be to move applications and databases to exactly the same platform in AWS. The lowest TCO for the target solution would be provided by migrating onto a small number of consolidated Linux platforms (assuming we can't go serverless for everything). These two options are mutually exclusive, so we need a decision on the way forward.\n\n## Decision\n","Our strong preference is to go serverless. Where we can't go serverless we prefer to use the latest AWS Linux, but accept that the choice of OS is often dictated by the application and/or database layers.\n",We will migrate to a small number of consolidated Linux platforms.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\n\n## Decision\n","We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\n",Automate all processes related to maintaining services undergoing migration or recently migrated to reduce project costs.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur postcode checker uses a database to store valid service areas and allowed post codes.\nCurrently our requirements involve looking up a postcode using the [postcodes.io](https://postcodes.io).\nIn future we may want to build features that involve more sophisticated geolocation capabilities. Most databases do not support geolocation natively.\nPostgres is the most geolocation capable SQL database. In future we can enable the PostGIS extension.\n\n## Decision\n,We will use Postgres for our database.\n,Migrate the postcode checker to Postgres to enable future geolocation capabilities.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThroughout the process of development and release, the deployed components, codebase and data may vary in stability and quality (despite all used effort by the teams 😛). Also, since several teams with different use cases work on the same project, potentially at the same time, there might be collision(s) in development and integration.\n\n## Decision\n","In order to ensure that only stable and quality code is shipped to the public viewer (app visitor), we will use several environments. This will allow siloting the code updates in a controlled manner.\nWe will use the following environment:\n- **Production**: Visitor facing environment. **It must be stable**, i.e. code and data shipped to production must be of the best quality and validated by product team.\n- **Staging**: Internal purposed environment. _Components, code and data therein might be unstable_ and thus could potentially not reflect what would be deployed to production. This is the environment used by the product team to validate new improvements, fix and so... Since there is ""only"" one environment of that kind, several updates might concurrently live there.\n- **Local**: Development purposed environment. Each developer (independently of its team) might use such an environment while developing. Since our stack involves components on GCP and Algolia, the following has been decided: GCP CloudFunctions can be ran locally (refer to `README.md` for usage); Algolia indices can only exist in Algolia (i.e. SAAS), thus one must create its own (i.e. developer bound) index (refer to `functions/README.md` for usage) and refer to it locally by updating `algolia.index` in the `functions/.runtimeconfig.json` file.\n","Implement a software release pipeline that can handle the **maximum parallelism** in development and integration. Optimize the pipeline for **fast and frequent releases**, while keeping the **quality at a high level**."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\neLife has numerous projects written completely and partly with the [Python programming language](https://www.python.org/).\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\nWe previously have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\nPython official images make it easy to support a new Python version without custom PPAs.\n\n## Decision\n,We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\nWe will use Python 3.6 as our default supported version for any containerized project that solely uses or supports Python 3.\nWe will use Python 3.5 as our default supported version for any project that is not containerized at the moment.\n,We should support Python 3.6 and 3.7.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAWS RDS are managed database servers.\neLife uses RDS to alleviate the task of database management tasks like\ndistribution, fault tolerance, monitoring.\neLife uses PostgreSQL RDS instances for several projects.\neLife provisions these RDS instances, their databases and database users using\nCloudformation.\nBecause RDS is a *managed* database server, there are constraints in it's usage.\nCommonly encountered constraints are:\n* no ssh access to the machine(s) hosting the database server\n* no 'root' user access within the database\n* [a similar-to-but-not-really 'root' user called 'rds_superuser'](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.MasterAccounts.html)\nDuring the life of an application, the database provisioned by Cloudformation\nmay be deleted and re-created, losing the permissions the original database was\ncreated and successfully provisioned with.\nIf the new owner of the database is not the 'root' user or a member of the 'rds_superuser' role, some non-application activities such as backups and testing may fail with permission errors.\nRe-instating permissions in a PostgreSQL database is an involved process.\n\n## Decision\n","The RDS *root* user provisioned shall always be the `owner` of the application database.\nThe RDS *application* user will have enough permissions to read and write to the application database.\nThe RDS *application* user will not have permission to drop/re-create the application database *in continuumtest, end2end and prod environments*.\n","Provision RDS database instances, their databases and database users with a consistent set of permissions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\neLife has a CiviCRM monolithic system that covers the contacts with key groups such as authors, editors, and newsletter subscribers.\nCiviCRM is difficult to develop on, test and debug, and no in-house expertise is available.\nCiviCRM is usually in the hands of (several) external consultants, which lack contact with the rest of the architecture.\n\n## Decision\n","We will push responsibilities away from CiviCRM, favoring [Separate Ways](http://culttt.com/2014/11/26/strategies-integrating-bounded-contexts/) or API-based integration of an external microservice over developing inside CiviCRM.\n","**Decision:** Migrate CiviCRM to a more modern, modular architecture.\n\n**Rationale:**\n\n* CiviCRM's monolithic architecture makes it difficult to maintain and evolve.\n* The lack of in-house expertise and reliance on external consultants creates knowledge and maintenance gaps.\n* A modular architecture will allow for more agile development, testing, and debugging.\n* It will provide greater flexibility and scalability to meet changing business needs.\n* By involving the internal team in the migration process, the organization can build in-house knowledge and reduce reliance on external contractors."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\neLife has numerous projects written completely and partly with the [Python programming language](https://www.python.org/).\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\nWe have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\n\n## Decision\n,We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\nWe will use Python 3.5 as our default supported version for any project that solely uses or supports Python 3.\n,"The following Python versions will be supported by eLife:\n\n* Python 3.5\n* Python 3.6\n* Python 3.7\n\nThese versions are all LTS (Long Term Support) versions, which means they will receive security and bug fixes for many years to come. They are also all commonly available on the major Linux distributions.\n\nWe will continue to monitor the Python ecosystem and may add additional versions to this list in the future. However, we will not support any Python versions that are not LTS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\neLife has several microservices (and larger projects) written in PHP.\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\nThere is an exception, `crm` not being upgraded on PHP 7 (using 5.6 instead) but supporting it.\nStaying up to date with the major and minor versions of PHP is important:\n- to be able to use new features and libraries (nice to have)\n- to keep working on a version that receives bug fixes and security updates (must have)\nAll infrastructure is currently based on:\n- Ubuntu 14.04 (doesn't have PHP 7.x by default)\n- Ubuntu 16.04 (does have PHP 7.0 by default)\n- a popular [PPA](https://launchpad.net/~ondrej/+archive/ubuntu/php) filling in the blanks, supporting 7.0, 7.1, and 7.2.\n- official [PHP Docker images](https://hub.docker.com/_/php/) supporting 5.6, 7.0, 7.1, and 7.2.\nPHP 7.0 has ceased active support, but has [security support](http://php.net/supported-versions.php) until 2018-12-03.\n\n## Decision\n","We will use PHP 7.0 on all existing and new PHP projects bar exceptions that do not support it (`crm` if needed).\nWe will upgrade to PHP 7.1, PHP 7.2 and similar minor versions as a concerted effort on all libraries and projects, before the end of the security support.\n","**Decision**: We will support PHP versions 7.1 and 7.2 for all new projects and all upgrades to existing projects, with the exception of `crm`, which will remain on PHP 5.6 until further notice.\n\n**Rationale**: Our infrastructure supports PHP versions 7.0, 7.1, and 7.2. PHP 7.0 has ceased active support, so we will not support it for new projects or upgrades to existing projects. PHP 7.1 and 7.2 are both actively supported, and they offer new features and security updates that are important for our applications.\n\n**Consequences**: \n- We will need to upgrade our infrastructure to support PHP 7.1 and 7.2. \n- We will need to upgrade all of our existing PHP applications to PHP 7.1 or 7.2. \n- We will need to ensure that all of our new PHP applications are developed using PHP 7.1 or 7.2."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\neLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.\nThere are tools that are peculiar to our infrastructure, such as [goaws](https://github.com/p4tin/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).\nThere are also tools that were originally written in another language but are being adopted by us, like [INK](https://gitlab.coko.foundation/INK/ink-api) for document conversion, written in Ruby.\nThese tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.\nSome tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.\n\n## Decision\n","We will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.\n",All **foreign** languages will be integrated into our infrastructure via a containerization strategy.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSoftware is built incrementally as an accumulation of changes.\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\nShort-lived pull requests allow:\n- visibility of who is changing what.\n- discussion and review from other people in the team.\n- automated testing and other kind of checks to run, offloading work from humans to machines.\n\n## Decision\n",We will open short-lived pull requests as the primary means for deploying a change.\n,Use Pull Requests to manage code changes and protect the mainline.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\neLife has projects built in Node.\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\nStaying up to date with the major and minor versions of Node is important:\n- to be able to use new features (nice to have)\n- to keep working on a version that receives bug fixes and security updates (must have)\n\n## Decision\n","To use 8.x, the current Long Term Support (LTS) line.\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https://github.com/nodejs/Release#release-schedule)).\nTo use whichever npm version is bundled with the Node version we use.\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\n","eLife will support the Node.js versions that are in Long Term Support (LTS). Currently, this includes Node.js version 10.x and 12.x.\n\neLife will support the latest npm version that is compatible with the supported Node.js versions. Currently, this is npm version 6.x."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use Java 8 in all projects so far. However the most current version of Java as of now is version 13.\n\n## Decision\n,Use Java 13 to start the Java version of these services.\n,"**Migrate to Java 11**\n\n**Rationale:**\n\n* Java 11 is the recommended long-term support (LTS) release and provides significant performance and security enhancements over Java 8.\n* Java 13 introduces minor improvements that are not critical for our current needs.\n* Upgrading to Java 11 will allow us to take advantage of its performance benefits and security features, while staying aligned with our current Java 8 development practices."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want a generic runtime that let's us deploy the services a uniform way in various environments like AWS, Azure, Kubernetes or locally.\nDocker is the de-facto standard today and is the system used by us for all projects.\n\n## Decision\n",Use docker to deploy all services.\n,Use Docker as the underlying runtime for deploying services.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nInitially the directory structure was to have the language as a top folder and have services below. However this makes it harder to use different language versions of the services together.\n\n## Decision\n,Keep a monorepo for easier handling the project (it will never be grow too large and will not be maintained by different teams).\nMove the services to top level. And have language versions below each service.\n,"**Decision:**\n\nGroup services by domain and layer, with language-specific implementations as subfolders within each service folder.\n\n**Rationale:**\n\n* This structure allows for easier use of different language versions of services together, as the services are grouped by functionality rather than language.\n* It also makes it easier to locate and maintain code for specific services, as all code for a service is contained in one folder."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",Create an Architectural Decision Record (ADR) template to document architectural decisions. The template should include the following information:\n\n* Decision title\n* Decision date\n* Contributors\n* Decision description\n* Rationale\n* Consequences\n* Trade-offs\n* References
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n- [Context](#context)\n- [Proposed Design](#proposed-design)\n- [Decision](#decision)\n- [Consequences](#consequences)\n- [References](#references)\n<!--te-->\n\n## Decision\n,"- [Consequences](#consequences)\n- [References](#references)\n<!--te-->\nIt was decided to move forward with the above design\nAfter initial ADR was approved, it was decided to retain the `-r/--registry` command-line flag and not add the `Enabled` field in the Registry provider configuration.\n","This context does not contain a Decision, so I cannot extract the requested data from the provided context."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR seeks to define the EdgeX direction on using encryption\nto secure ""in-cluster"" EdgeX communications, that is,\ninternal microservice-to-microservice communication.\nThis ADR will seek to clarify the EdgeX direction\nin several aspects with regard to:\n- EdgeX services communicating within a single host\n- EdgeX services communicating across multiple hosts\n- Using encryption for confidentiality or integrity in communication\n- Using encryption for authentication between microservices\nThis ADR will be used to triage EdgeX feature requests in this space.\n\n## Decision\n","At this time, EdgeX is primarily a single-node IoT application framework.\nShould this position change, this ADR should be revisited.\nBased on the single-node assumption:\n- TLS will not be used for confidentiality and integrity of internal on-host microservice communication.\n- TLS will be avoided as an authentication mechanism of peer microservices.\n- Integrity and confidentiality of microservice communcations crossing host boundaries is required to secure EdgeX, but are an EdgeX customer responsibility.\n- EdgeX customers are welcome to add extra security to their own EdgeX deployments.\n","EdgeX will not encrypt ""in-cluster"" communications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n- [Context](#context)\n- [Existing Behavior](#existing-behavior)\n* [Device Services](#device-services)\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\n* [Core and Support Services](#core-and-support-services)\n* [Security Proxy Setup](#security-proxy-setup)\n- [History](#history)\n- [Problem Statement](#problem-statement)\n- [Decision](#decision)\n- [References](#references)\n<!--te-->\n\n## Decision\n,"- [References](#references)\n<!--te-->\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\nthe address of the other service.\n",This ADR does not include a decision.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n- [Context](#context)\n* [Existing Implementations](#existing-implementations)\n+ [What is a Secret?](#what-is-a-secret)\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\n+ [Known and Unknown Services](#known-and-unknown-services)\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\n* [Interfaces](#interfaces)\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\n- [App SDK's current implementation](#app-sdks-current-implementation)\n* [Interface](#interface)\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\n- [Decision](#decision)\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\n* [Abstraction Interface](#abstraction-interface)\n* [Implementation](#implementation)\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\n+ [Caching of Secrets](#caching-of-secrets)\n+ [Insecure Secrets](#insecure-secrets)\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\n+ [Mocks](#mocks)\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\n- [Go Services](#go-services)\n- [C Device Service](#c-device-service)\n* [Consequences](#consequences)\n\n## Decision\n,"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\n* [Abstraction Interface](#abstraction-interface)\n* [Implementation](#implementation)\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\n+ [Caching of Secrets](#caching-of-secrets)\n+ [Insecure Secrets](#insecure-secrets)\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\n+ [Mocks](#mocks)\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\n- [Go Services](#go-services)\n- [C Device Service](#c-device-service)\n* [Consequences](#consequences)\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\n### Only Exclusive Secret Stores\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\n```yaml\nEDGEX_ADD_SECRETSTORE_TOKENS: ""<service-name1>,<service-name2>""\n```\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\n```yaml\nADD_SECRETSTORE: ""app-service-xyz[appservice/redisdb]""\n```\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `/secret/edgex/appservice/redisdb`*\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `/secret/edgex/app-service-xyz/redis`.\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common/messagebus` as the secret identifier.\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\n```\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\n```\nExample with one service specifying IDs for static secrets and one without static secrets\n```yaml\nADD_SECRETSTORE: ""appservice-xyz[appservice/redisdb; common/messagebus], appservice-http-export""\n```\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\n### Abstraction Interface\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\n```go\ntype SecretProvider interface {\n// Stores new secrets into the service's exclusive SecretStore at the specified path.\nStoreSecrets(path string, secrets map[string]string) error\n// Retrieves secrets from the service's exclusive SecretStore at the specified path.\nGetSecrets(path string, _ ...string) (map[string]string, error)\n// Sets the secrets lastupdated time to current time.\nSecretsUpdated()\n// Returns the secrets last updated time\nSecretsLastUpdated() time.Time\n}\n```\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\n### Implementation\n#### Factory Method and Bootstrap Handler\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\n#### Caching of Secrets\nSecrets will be cached as they are currently in the Application Service implementation\n#### Insecure Secrets\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\n```toml\n[Writable.InsecureSecrets]\n[Writable.InsecureSecrets.DB]\npath = ""redisdb""\n[Writable.InsecureSecrets.DB.Secrets]\nusername = """"\npassword = """"\n```\n##### Handling on-the-fly changes to `InsecureSecrets`\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\n#### Mocks\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\n#### Where will `SecretProvider` reside?\n##### Go Services\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\n##### C Device Service\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\n### Consequences\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\n- Go Device SDK will add the SecretProvider to it's bootstrapping\n- C Device SDK implementation could be big lift?\n- ` SecretStore`configuration section will be added to all Device Services\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\n","**Only Exclusive Secret Stores**\n\nBootstrap should only work with exclusive secret stores. All App SDK services will support secret stores. Services operating in non-secure mode are not required to initialize the `exclusive` secret store. However, doing so will not harm service operations.\n\n**Abstraction Interface**\n\nWe will define an abstraction interface for the secret manager. This interface can define a set of methods to populate secrets, read secrets, and secrets cache priority.\n\nThe abstraction interface will make it easier to test secret manager implementations and to change the default secret manager implementation in the future.\n\n**Implementation**\n\nThe App SDK will provide a default `SecretManager` implementation that uses the `InsecureSecrets` singleton for services running in non-secure mode, and the `exclusive` secret store for all other services.\n\n**Factory Method and Bootstrap Handler**\n\nBootstrap will define a factory method to instantiate the `SecretManager` interface. Bootstrap will also define a bootstrap handler to register the default `SecretManager` implementation with the factory.\n\n**Caching of Secrets**\n\nThe `SecretManager` abstraction interface will define a client-side cache for storing secrets. This cache will be populated when a secret is first used. Subsequent requests for the same secret will be served from the cache. The secret cache may be cleared by changing the secret value or by calling a flush method on the `SecretManager` interface.\n\n**Insecure Secrets**\n\nThe `SecretManager` abstraction interface will define a method to read secrets from `InsecureSecrets`. This method will only be available to services running in non-secure mode. Services operating in non-secure mode are not required to initialize the `exclusive` secret store.\n\n**Handling on-the-fly changes to `InsecureSecrets`**\n\nThe `SecretManager` implementation in the App SDK will watch for changes to `InsecureSecrets`and update the client-side cache accordingly. This will allow services to respond to changes to `InsecureSecrets` without restarting.\n\n**Mocks**\n\nMocks of the `SecretManager` abstraction interface will be made available for testing. This will make it easier to test services that use secrets.\n\n**Where will `SecretProvider` reside?**\n\nThe `SecretProvider` interface will reside in the `cloud.google.com/go/secretmanager` package. The `SecretManager` implementation will reside in the `cloud.google.com/go/appengine` package.\n\n**Go Services**\n\nFor Go services, all secret operations will be unchanged. Services will call `secrets.Get()` or `secrets.GetAll()` and the `SecretProvider` interface in `cloud.google.com/go/secretmanager` will handle the rest.\n\n**C Device Service**\n\nFor the C Device Service, the `cds::Secrets` and `cds::InsecureSecrets` objects will be replaced with a `cds::SecretsProvider` object that implements the `cds::Secrets` interface. The `cds::SecretsProvider` object will use the `cloud.google.com/go/appengine/secrets` package to manage secrets."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince its debut, EdgeX has had a configuration seed service (config-seed) that, on start of EdgeX, deposits configuration for all the services into Consul (our configuration/registry service).  For development purposes, or on  resource constrained platforms, EdgeX can be run without Consul with services simply reading configuration from the filesystem.\nWhile this process has nominally worked for several releases of EdgeX, there has always been some issues with this extra initialization process (config-seed), not least of which are:\n- race conditions on the part of the services, as they bootstrap, coming up before the config-seed completes its deposit of configuration into Consul\n- how to deal with ""overrides"" such as environmental variable provided configuration overrides. As the override is often specific to a service but has to be in place for config-seed in order to take effect.\n- need for an additional service that is only there for init and then dies (confusing to users)\nNOTE - for historical purposes, it should be noted that config-seed only writes configuration into the configuration/registry service (Consul) once on the first start of EdgeX.  On subsequent starts of EdgeX, config-seed checks to see if it has already populated the configuration/registry service and will not rewrite configuration again (unless the --overwrite flag is used).\nThe design/architectural proposal, therefore, is:\n- removal of the config-seed service (removing cmd/config-seed from the edgex-go repository)\n- have each EdgeX micro service ""self seed"" - that is seed Consul with their own required configuration on bootstrap of the service.  Details of that bootstrapping process are below.\n### Command Line Options\nAll EdgeX services support a common set of command-line options, some combination of which are required on startup for a service to interact with the rest of EdgeX. Command line options are not set by any configuration.  Command line options include:\n- --configProvider or -cp (the configuration provider location URL - prefixed with `consul.` - for example:  `-cp=consul.http://localhost:8500`)\n- --overwrite or -o (overwrite the configuration in the configuration provider)\n- --file or -f (the configuration filename - configuration.toml is used by default if the configuration filename is not provided)\n- --profile or -p (the name of a sub directory in the configuration directory in which a profile-specific configuration file is found. This has no default. If not specified, the configuration file is read from the configuration directory)\n- --confdir or -c (the directory where the configuration file is found - ./res is used by default if the confdir is not specified, where ""."" is the convention on Linux/Unix/MacOS which means current directory)\n- --registry or -r (string indicating use of the registry)\nThe distinction of command line options versus configuration will be important later in this ADR.\nTwo command line options (-o for overwrite and -r for registry) are not overridable by environmental variables.\nNOTES: Use of the --overwrite command line option should be used sparingly and with expert knowledge of EdgeX; in particular knowledge of how it operates and where/how it gets its configuration on restarts, etc.  Ordinarily, --overwrite is provided as a means to support development needs.  Use of --overwrite permanently in production enviroments is highly discouraged.\n### Configuration Initialization\nEach service has (or shall have if not providing it already) a local configuration file.  The service may use the local configuration file on initialization of the service (aka bootstrap of the service) depending on command line options and environmental variables (see below) provided at startup.\n**Using a configuration provider**\nWhen the configuration provider _is_ specified, the service will call on the configuration provider (Consul) and check if the top-level (root) namespace for the service exists.  If configuratation at the top-level (root) namespace exists, it indicates that the service has already populated its configuration into the configuration provider in a prior startup.\nIf the service finds the top-level (root) namespace is already populated with configuration information it will then read that configuration information from the configuration provider under namespace for that service (and ignore what is in the local configuration file).\nIf the service finds the top-level (root) namespace is not populated with configuration information, it will read its local configuration file and populate the configuration provider (under the namespace for the service) with configuration read from the local configuration file.\nA configuration provider can be specified with a command line argument (the -cp / --configProvider) or environment variable (the EDGEX_CONFIGURATION_PROVIDER environmental variable which overrides the command line argument).\n> NOTE:  the environmental variables are typically uppercase but there have been inconsistencies in environmental variable casing (example:  edgex_registry).  This should be considered and made consistent in a future major release.\n**Using the local configuration file**\nWhen a configuration provider _isn't_ specified, the service just uses the configuration in its local configuration file.  That is the service uses the configuration in the file associated with the profile, config filename and config file directory command line options or environmental variables.  In this case, the service does not contact the configuration service (Consul) for any configuration information.\nNOTE:  As the services now self seed and deployment specific changes can be made via environment overrides, it will no longer be necessary to have a Docker profile configuration file in each of the service directories (example:  https://github.com/edgexfoundry/edgex-go/blob/master/cmd/core-data/res/docker/configuration.toml).  See Consequences below.  It will still be possible for users to use the profile mechanism to specify a Docker configuration, but it will no longer be required and not the recommended approach to providing Docker container specific configuration.\n### Overrides\nEnvironment variables used to override configuration always take precedence whether configuration is being sourced locally or read from the config provider/Consul.\nNote - this means that a configuration value that is being overridden by an environment variable will always be the source of truth, even if the same configuration is changed directly in Consul.\nThe name of the environmental variable must match the path names in Consul.\nNOTES:\n- Environmental variables overrides remove the need to change the ""docker"" profile in the res/docker/configuration.toml files - Allowing removal of 50% of the existing configuration.toml files.\n- The override rules in EdgeX between environmental variables and command line options may be counter intuitive compared to other systems.  There appears to be no standard practice.  Indeed, web searching ""Reddit & Starting Fights Env Variables vs Command Line Args"" will layout the prevailing differences.\n- Environment variables used for configuration overrides are named by prepending the the configuration element with the configuration section inclusive of sub-path, where sub-path's "".""s are replaced with underscores. These configuration environment variable overrides must be specified using camel case.  Here are two examples:\n~~~~~\nRegistry_Host  for\n[Registry]\nHost = 'localhost'\nClients_CoreData_Host for\n[Clients]\n[Clients.CoreData]\nHost = 'localhost'\n~~~~~\n- Going forward, environmental variables that override command line options should be all uppercase.\nAll values overriden get logged (indicating which configuration value or op param and the new value).\n\n## Decision\n",These features have been implemented (with some minor changes to be done) for consideration here:  https://github.com/edgexfoundry/go-mod-bootstrap/compare/master...lenny-intel:SelfSeed2.  This code branch will be removed once this ADR is approved and implemented on master.\nThe implementation for self-seeding services and environmental overrides is already implemented (for Fuji) per this document in the application services and device services (and instituted in the SDKs of each).\n,"The decision is to remove the config-seed service and have each EdgeX micro service ""self seed"" - that is seed Consul with their own required configuration on bootstrap of the service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR defines the motiviation and approach used to secure access\nto the Consul component in the EdgeX architecture\nfor *security-enabled configurations only*.\nNon-secure configuations continue to use Consul in\nanonymous read-write mode.\nAs this Consul security feature requires Vault to function,\nif `EDGEX_SECURITY_SECRET_STORE=false` and Vault is not present,\nthe legacy behavior (unauthenticated Consul access) will be preserved.\nConsul provides several services for the EdgeX architecture:\n- Service registry (see ADR in references below)\n- Service health monitoring\n- Mutable configuration data\nUse of the services provided by Consul is optional on a service-by-service basis.\nUse of the registry is controlled by the `-r` or `--registry` flag provided to an EdgeX service.\nUse of mutable configuration data is controlled by the `-cp` or `--configProvider` flag provided to an EdgeX service.\nWhen Consul is enabled as a configuration provider,\nthe `configuration.toml` is parsed into individual settings\nand seeded into the Consul key-value store on the first start of a service.\nConfiguration reads and writes are then done to Consul if it is specified as the configuration provider,\notherwise the static `configuration.toml` is used.\nWrites to the `[Writable]` section in Consul trigger per-service callbacks\nnotifying the application of the changed data.\nUpdates to non-`[Writable]` sections are parsed only once at startup\nand require a service restart to take effect.\nSince configuration data can affect the runtime behavior of services,\ncompensating controls must be introduced in order to mitigate the risks introduced\nby moving configuration from a static file into to an HTTP-accessible service with mutable state.\nThe current practice is that Consul is exposed via unencrypted HTTP in anonymous read/write mode\nto all processes and EdgeX services running on the host machine.\n\n## Decision\n","Consul will be configured with access control list (ACL) functionality enabled,\nand each EdgeX service will utilize a Consul access token to authenticate to Consul.\nConsul access tokens will be requested from the Vault Consul secrets engine\n(to avoid introducing additional bootstrapping secrets).\nDNS will be disabled via configuration as it is not used in EdgeX.\n**Consul Access Via API Gateway**\nIn security enabled EdgeX, the API gateway will be configured to\nproxy the Consul service over the `/consul` path,\nusing the `request-transformer` plugin\nto add the global management token to incoming requests\nvia the `X-Consul-Token` HTTP header.\nThus, ability to access remote APIs also grants the ability\nto modify Consul's key-value store.\nAt this time, service access via API gateway is all-or-nothing,\nbut this does not preclude future fine-grained authorization\nat the API gateway layer to specific microservices, including Consul.\nProxying of the Consul UI is problematic and there is no current solution,\nwhich would involve proper balacing of the externally-visible URL,\nthe path-stripping effect (or not) of the proxy,\nConsul's `ui_content_path`,\nand UI authentication\n(the `request-transfomer` does not work on the UI).\n",EdgeX will leverage Consul's support for Vault authentication for the purposes\nof securing access to configuration data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR details the REST API to be provided by Device Service implementations in EdgeX version 2.x. As such, it supercedes the equivalent sections of the earlier ""Device Service Functional Requirements"" document. These requirements should be implemented as far as possible within the Device Service SDKs, but they also apply to any Device Service implementation.\n\n## Decision\n","### Common endpoints\nThe DS should provide the REST endpoints that are expected of all EdgeX microservices, specifically:\n* *config*\n* *metrics*\n* *ping*\n* *version*\n### Callback\n| Endpoint | Methods\n| --- | ---\n| *callback/device* | `PUT` and `POST`\n| *callback/device/name/{name}* | `DELETE`\n| *callback/profile* | `PUT`\n| *callback/watcher* | `PUT` and `POST`\n| *callback/watcher/name/{name}* | `DELETE`\n| parameter | meaning\n| --- | ---\n| *{name}* | the name of the device or watcher\nThese endpoints are used by the Core Metadata service to inform the device service of metadata updates. Endpoints are defined for each of the objects of interest to a device service, ie Devices, Device Profiles and Provision Watchers. On receipt of calls to these endpoints the device service should update its internal state accordingly. Note that the device service does not need to be informed of the creation or deletion of device profiles, as these operations may only occur where no devices are associated with the profile. To avoid stale profile entries the device service should delete a profile from its cache when the last device using it is deleted.\n#### Object deletion\nWhen an object is deleted, the Metadata service makes a `DELETE` request to the relevant *callback/{type}/name/{name}* endpoint.\n#### Object creation and updates\nWhen an object is created or updated, the Metadata service makes a `POST` or `PUT` request respectively to the relevant *callback/{type}* endpoint. The payload of the request is the new or updated object, ie one of the Device, DeviceProfile or ProvisionWatcher DTOs.\n### Device\n| Endpoint | Methods\n| --- | ---\n| *device/name/{name}/{command}* | `GET` and `PUT`\n| parameter | meaning\n| --- | ---\n| *{name}* | the name of the device\n| *{command}* | the command name\nThe command specified must match a deviceCommand or deviceResource name in the device's profile\n**body** (for `PUT`): An `application/json` SettingRequest, which is a set of key/value pairs where the keys are valid deviceResource names, and the values provide the command argument for that resource. Example: `{""AHU-TargetTemperature"": ""28.5"", ""AHU-TargetBand"": ""4.0""}`\n| Return code | Meaning\n| --- | ---\n| **200** | the command was successful\n| **404** | the specified device does not exist, or the command/resource is unknown\n| **405** | attempted write to a read-only resource\n| **423** | the specified device is locked (admin state) or disabled (operating state)\n| **500** | the device driver is unable to process the request\n**response body**: A successful `GET` operation will return a JSON-encoded EventResponse object, which contains one or more Readings. Example: `{""apiVersion"":""v2"",""deviceName"":""Gyro"",""origin"":1592405201763915855,""readings"":[{""deviceName"":""Gyro"",""name"":""Xrotation"",""value"":""124"",""origin"":1592405201763915855,""valueType"":""int32""},{""deviceName"":""Gyro"",""name"":""Yrotation"",""value"":""-54"",""origin"":1592405201763915855,""valueType"":""int32""},{""deviceName"":""Gyro"",""name"":""Zrotation"",""value"":""122"",""origin"":1592405201763915855,""valueType"":""int32""}]}`\nThis endpoint is used for obtaining readings from a device, and for writing settings to a device.\n#### Data formats\nThe values obtained when readings are taken, or used to make settings, are expressed as strings.\n| Type | EdgeX types | Representation\n| --- | --- | ---\n| Boolean | `Bool` | ""true"" or ""false""\n| Integer | `Uint8-Uint64`, `Int8-Int64` | Numeric string, eg ""-132""\n| Float | `Float32`, `Float64` | Decimal with exponent, eg ""1.234e-5""\n| String | `String` | string\n| Binary | `Bytes` | octet array\n| Array | `BoolArray`, `Uint8Array-Uint64Array`, `Int8Array-Int64Array`, `Float32Array`, `Float64Array` | JSON Array, eg ""[""1"", ""34"", ""-5""]""\nNotes:\n- The presence of a Binary reading will cause the entire Event to be encoded using CBOR rather than JSON\n- Arrays of String and Binary data are not supported\n#### Readings and Events\nA Reading represents a value obtained from a deviceResource. It contains the following fields\n| Field name | Description\n| --- | ---\n| *deviceName* | The name of the device\n| *profileName* | The name of the Profile describing the Device\n| *resourceName* | The name of the deviceResource\n| *origin* | A timestamp indicating when the reading was taken\n| *value* | The reading value\n| *valueType* | The type of the data\nOr for binary Readings, the following fields\n| Field name | Description\n| --- | ---\n| *deviceName* | The name of the device\n| *profileName* | The name of the Profile describing the Device\n| *resourceName* | The name of the deviceResource\n| *origin* | A timestamp indicating when the reading was taken\n| *binaryValue* | The reading value\n| *mediaType* | The MIME type of the data\nAn Event represents the result of a `GET` command. If the command names a deviceResource, the Event will contain a single Reading. If the command names a deviceCommand, the Event will contain as many Readings as there are deviceResources listed in the deviceCommand.\nThe fields of an Event are as follows:\n| Field name | Description\n| --- | ---\n| *deviceName* | The name of the Device from which the Readings are taken\n| *profileName* | The name of the Profile describing the Device\n| *origin* | The time at which the Event was created\n| *readings* | An array of Readings\n#### Query Parameters\nCalls to the device endpoints may include a Query String in the URL. This may be used to pass parameters relating to the request to the device service. Individual device services may define their own parameters to control specific behaviors. Parameters beginning with the prefix `ds-` are reserved to the Device SDKs and the following parameters are defined for GET requests:\n| Parameter | Valid Values      | Default | Meaning\n| --- |-------------------|---------| ---\n| *ds-pushevent* | ""true"" or ""false"" | ""false"" | If set to true, a successful `GET` will result in an event being pushed to the EdgeX system\n| *ds-returnevent* | ""true"" or ""false"" | ""true""  | If set to false, there will be no Event returned in the http response\n!!! edgey ""EdgeX 3.0""\nThe valid values of **ds-pushevent** and **ds-returnevent** is changed to `true/false` instead of `yes/no` in EdgeX 3.0.\n#### Device States\nA Device in EdgeX has two states associated with it: the Administrative state and the Operational state. The Administrative state may be set to `LOCKED` (normally `UNLOCKED`) to block access to the device for administrative reasons. The Operational state may be set to `DOWN` (normally `UP`) to indicate that the device is not currently working. In either case access to the device via this endpoint will be denied and HTTP 423 (""Locked"") will be returned.\n#### Data Transformations\nA number of simple data transformations may be defined in the deviceResource. The table below shows these transformations in the order in which they are applied to outgoing data, ie Readings. The transformations are inverted and applied in reverse order for incoming data.\n| Transform | Applicable reading types | Effect\n| --- | --- | ---\n**mask** | Integers | The reading is masked (bitwise-and operation) with the specified value.\n**shift** | Integers | The reading is bit-shifted by the specified value. Positive values indicate right-shift, negative for left.\n**base** | Integers and Floats | The reading is replaced by the specified value raised to the power of the reading.\n**scale** | Integers and Floats | The reading is multiplied by the specified value.\n**offset** | Integers and Floats | The reading is increased by the specified value.\nThe operation of the **mask** transform on incoming data (a setting) is that the value to be set on the resource is the existing value bitwise-anded with the complement of the mask, bitwise-ored with the value specified in the request.\nie, `new-value = (current-value & !mask) | request-value`\nThe combination of mask and shift can therefore be used to access data contained in a subdivision of an octet.\nIt is possible that following the application of the specified transformations, a value may exceed the range that may be represented by its type. Should this occur on a set operation, a suitable error should be logged and returned, along with the `Bad Request` http code 400. If it occurs as part of a get operation, the Reading's value should be set to the String `""overflow""` and its valueType to `String`.\n#### Assertions and Mappings\nAssertions are another attribute in a device resource's PropertyValue, which specify a string which the reading value is compared against. If the comparison fails, then the http request returns a string of the form *""Assertion failed for device resource: \<name>, with value: \<value>""*, this also has a side-effect of setting the device operatingstate to `DISABLED`. A 500 status code is also returned. Note that the error response and status code should be returned regardless of the `ds-returnevent` setting.\nAssertions are also checked where an event is being generated due to an AutoEvent, or asynchronous readings are pushed. In these cases if the assertion is triggered, an error should be logged and the operating state should be set as above.\nAssertions are not checked for settings, only for readings.\nMappings may be defined in a deviceCommand. These allow Readings of string type to be remapped. Mappings are applied after assertions are checked, and are the final transformation before Readings are created. Mappings are also applied, but in reverse, to settings (`PUT` request data).\n#### lastConnected timestamp\nEach Device has as part of its metadata a timestamp named `lastConnected`, this\nindicates the most recent occasion when the device was successfully interacted\nwith. The device service should update this timestamp every time a GET or PUT\noperation succeeds, unless it has been configured not to do so (eg for\nperformance reasons).\n### Discovery\n| Endpoint | Methods\n| --- | ---\n| *discovery* | `POST`\nA call to this endpoint triggers the device discovery process, if enabled. See\n[Discovery Design](../../legacy-design/device-service/discovery.md) for details.\n",The decision of the Architectural Decision Record is not provided in the given text.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR seeks to clarify and prioritize the secret handling approach taken by EdgeX.\nEdgeX microservices need a number of secrets to be created and distributed\nin order to create a functional, secure system.\nAmong these secrets are:\n- Privileged administrator passwords (such as a database superuser password)\n- Service account passwords (e.g. non-privileged database accounts)\n- PKI private keys\nThere is a lack of consistency on how secrets are created and distributed to EdgeX microservices,\nand when developers need to add new components to the system,\nit is unclear on what the preferred approach should be.\nThis document assumes a threat model wherein the EdgeX services are sandboxed\n(such as in a snap or a container) and the host system is trusted,\nand all services running in a single snap share a trust boundary.\n### Terms\nThe following terms will be helpful for understading the subsequent discussion:\n- _SECRETSLOC_ is a protected file system path where bootstrapping secrets are stored.\nWhile EdgeX implements a sophisticated secret handling mechanism,\nthat mechanism itself requires secrets.\nFor example, every microservice that talks to Vault\nmust have its own unique secret to authenticate:\nVault itself cannot be used to distribute these secrets.\n_SECRETSLOC_ fulfills the role that the non-routable\ninstance data IP address, 169.254.169.254,\nfulfills in the public cloud:\ndelivery of bootstrapping secrets.\nAs EdgeX does not have a hypervisor nor virtual machines for this purpose,\na protected file system path is used instead.\n_SECRETSLOC_ is implementation-dependent.\nA desirable feature of _SECRETSLOC_ would be that data written here\nis kept in RAM and is not persisted to storage media.\nThis property is not achieveable in all circumstances.\nFor Docker, a list of suggested paths--in preference order--is:\n* `/run/edgex/secrets` (a `tmpfs` volume on a Linux host)\n* `/tmp/edgex/secrets` (a temporary file area on Linux and MacOS hosts)\n* A persistent docker volume (use when host bind mounts are not available)\nFor snaps, a list of suggested paths-in preference order--is:\n* `/run/snap.`_$SNAP_NAME_`/` (a `tmpfs` volume on a Linux host)\n* _$SNAP_DATA_`/secrets` (a snap-specific persistent data area)\n* _TBD_ (a content interface that allows for sharing of secrets from the core snap)\n### Current practices survey\nA survey on the existing EdgeX secrets reveals the following appoaches.\nA designation of ""compliant"" means that the current implementation\nis aligned with the recommended practices documented in the next section.\nA designation of ""non-compliant"" means that the current implementation\nuses an implemention mechanism outside of the recommended practices documented in the next section.\nA ""non-compliant"" implementation is a candidate for refactoring\nto bring the implementation into conformance with the recommended practices.\n#### System-managed secrets\n- PKI private keys\n* Docker: PKI generated by standalone utility every cold start of the framework. Distribution via _SECRETSLOC_. (Compliant.)\n* Snaps: PKI generated by standalone utility every cold start of the framework. Deployed to _SECRETSLOC_. (Compliant.)\n- Secret store master password\n* Docker: Distribution via persistent docker volume. (Non-compliant.)\n* Snaps: Stored in `$SNAP_DATA/config/security-secrets-setup/res`. (Non-compliant.)\n- Secret store per-service authentication tokens\n* Docker: Distribution via _SECRETSLOC_ generated every cold start of the framework. (Compliant.)\n* Snaps: Distribution via _SECRETSLOC_, generated every cold start of the framework. (Compliant.)\n- Postgres superuser password\n* Docker: Hard-coded into docker-compose file, checked in to source control. (Non-compliant.)\n* Snaps: Generated at snap install time via ""apg"" (""automatic password generator"") tool, installed into Postgres, cached to `$SNAP_DATA/config/postgres/kongpw` (non-compliant), and passed to Kong via `$KONG_PG_PASSWORD`.\n- MongoDB service account passwords\n* Docker: Direct consumption from secret store. (Compliant.)\n* Snaps: Direct consumption from secret store. (Compliant.)\n- Redis authentication password\n* Docker: Server--staged to secrets volume and injected via command line. (Non-compliant.). Clients--direct consumption from secret store. (Compliant.)\n* Snaps: Server--staged to `$SNAP_DATA/secrets/edgex-redis/redis5-password` and injected via command line. (Non-compliant.). Clients--direct consumption from secret store. (Compliant.)\n- Kong client authentication tokens\n* Docker: System of reference is unencrypted Postgres database. (Non-compliant.)\n* Snaps: System of reference is unencrypted Postgres database. (Non-compliant.)\nNote: in the current implementation,\nConsul is being operated as a public service.\nConsul will be a subject of a future ""bootstrapping ADR""\ndue to its role in serivce location.\n#### User-managed secrets\nUser-managed secrets functionality is provided by `app-functions-sdk-go`.\nIf security is enabled, secrets are retrieved from Vault.\nIf security is disabled, secrets are retreived from the configuration provider.\nIf the configuration provider is not available, secrets are read from the underlying `.toml`.\nIt is taken as granted in this ADR that secrets originating in the configuration provider\nor from `.toml` configuration files are not secret.\nThe fallback mechanism is provided as a convienience to the developer,\nwho would otherwise have to litter their code with ""if (isSecurityEnabled())"" logic leading to implementation inconsistencies.\nThe central database credential is supplied by `GetDatabaseCredentials()`\nand returns the database credential assigned to `app-service-configurable`.\nIf security is enabled, database credentials are retreived using the standard flow.\nIf security is disabled, secrets are retreived from the configuration provider\nfrom a special section called `[Writable.InsecureSecrets]`.\nIf not found there, the configuration provider is searched\nfor credentials stored in the legacy `[Databases.Primary]` section\nusing the `Username` and `Password` keys.\nEach user application has its own exclusive-use area of the secret store\nthat is accessed via `GetSecrets()`.\nIf security is enabled, secret requests are passed along to `go-mod-secrets`\nusing an application-specific access token.\nIf security is disabled, secret requets are made to the configuration provider\nfrom the `[Writable.InsecureSecrets]` section.\nThere is no fallback configuration location.\nAs user-managed secrets have no framework support for initialization,\na special `StoreSecrets()` method is made available\nto the application for the application to initialize its own secrets.\nThis method is only available in security-enabled mode.\nNo changes to user-managed secrets are being proposed in this ADR.\n\n## Decision\n","### Creation of secrets\nManagement of hardware-bound secrets is platform-specific\nand out-of-scope for the EdgeX framework.\nEdgeX open source will contain only the necessary hooks to integrate\nplatform-specific functionality.\nFor software-managed secrets, the\n[_system of reference_](http://www.grcdi.nl/dqglossary/record%20of%20reference.html)\nof secrets in EdgeX is the EdgeX secret store.\nThe EdgeX secret store provides for encryption of secrets at rest.\nThis term means that if a secret is replicated,\nthe EdgeX secret store is the authoritative source of truth of the secret.\nWhenever possible, the EdgeX secret store should also be the\n[_record of origin_](http://www.grcdi.nl/dqglossary/record%20of%20origin.html)\nof a secret as well.\nThis means creating secrets inside of the EdgeX secret store\nis preferable to importing an externally-created secret into the secret store.\nThis can often be done for framework-managed secrets,\nbut not possible for user-managed secrets.\n### Choosing between alternative forms of secrets\nWhen given a choice between plain-text secrets\nand cryptographic keys,\ncryptographic keys should be preferred.\nAn example situation would be the introduction of an MQTT message broker.\nA broker may support both TLS client authentication as well as username/password authentication.\nIn such a situation, TLS client authentication would be preferred:\n- The cryptographic key is typically longer in bits than a plain-text secret.\n- A plain-text secret will require transport encryption in order to protect confidentiality of the secret, such as server-side TLS.\n- Use of TLS client authentication typically eliminates the need for additional assets on the server side (such as a password database) to authenticate the client, by relying on digital signature instead.\nTLS client authentication **should not be used**\nunless there is a capability to revoke a compromised certificate,\nsuch as by replacing the certificate authority,\nor providing a certificate revokation list to the server.\nIf certificate revokation is not supported,\nplain-text secrets (such as username/password) should be used instead,\nas they are typically easier to revoke.\n### Distribution and consumption of secrets\n#### Prohibited practices\nUse of hard-coded secrets is an instance of\n[CWE-798: Use of hard-coded credentials](https://cwe.mitre.org/data/definitions/798.html)\nand is not allowed.\nA hard-coded secret is a secret that is the same across multiple EdgeX instances.\nHard-coded secrets make devices susceptible to BORE (break-once-run-everywhere) attacks,\nwhere collections of machines can compromised by a single replicated secret.\nSpecific cases where this is likely to come up are:\n- Secrets embedded in source control\nEdgeX is an open-source project.\nAny secret that is present in an EdgeX repository is public to the world,\nand therefore not a secret, by definition.\nConfiguration files, such as .toml files, .json files, .yaml files\n(including `docker-compose.yml`) are specific instances of this practice.\n- Secrets embedded in binaries\nBinaries are usually not protected against confidentiality threats,\nand binaries can be easily reverse-engineered to find any secrets therein.\nBinaries included compile executables as well as Docker images.\n#### Recommended practices\n1. Direct consumption from process-to-process interaction with secret store\nThis approach is only possible for components that have native support for\n[Hashicorp Vault](https://www.vaultproject.io/).\nThis includes any EdgeX service that links to go-mod-secrets.\nFor example, if secretClient is an instance of the go-mod-secrets\nsecret store client:\n```go\nsecrets, err := secretClient.GetSecrets(""myservice"", ""username"", ""password"")\n```\nThe above code will retrieve the `username` and `password` properties\nof the `myservice` secret.\n2. Dynamic injection of secret into process environment space\nEnvironment variables are part of a process' environment block\nand are mapped into a process' memory.\nIn this scenario,\nan intermediary makes a connection to the secret store to fetch a secret,\nstore it into an environment variable,\nand then launches a target executable,\nthereby passing the secret _in-memory_ to the target process.\nExisting examples of this functionality include\n[vaultenv](https://github.com/channable/vaultenv),\n[envconsul](https://github.com/hashicorp/envconsul),\nor [env-aws-params](https://github.com/gmr/env-aws-params).\nThese tools authenticate to a remote network service,\ninject secrets into the process environment,\nand then exec's a replacment process\nthat inherits the secret-enriched enviornment block.\nThere are a few potential risks with this approach:\n* Environment blocks are passed to child processes by default.\n* Environment-variable-sniffing malware (introduced by compromised 3rd party libaries) is a proven attack method.\n3. Dynamic injection of secret into container-scoped `tmpfs` volume\nAn example of this approach is [consul-template](https://github.com/hashicorp/consul-template).\nThis approach is useful when a secret is required to be in a configuration file\nand cannot be passed via an environment variable\nor directly consumed from a secret store.\n4. Distribution via _SECRETSLOC_\nThis option is the most widely supported secret distribution mechanism by container orchestrators.\nEdgeX supports runtime environments such as standard Docker and snaps\nthat have no built-in secret management features.\n* Generic Docker does not have a built-in secrets mechanism.\nManual configuration of a _SECRETSLOC_ should utilize either\na host file file system path or\na Docker volume.\n* Snaps also do not have a built-in secrets mechanism.\nThe options for _SECRETSLOC_ are limited\nto designated snap-writable directories.\nFor comparison:\n* Docker Swarm:\nSwarm swarm mode is not officially supported by the EdgeX project.\nDocker Swarm secrets are shared via the `/run/secrets` volume,\nwhich is a Linux `tmpfs` volume created on the host and shared with the container.\nFor an example of Docker Swarm secrets, see the\n[docker-compose secrets stanza](https://docs.docker.com/compose/compose-file/#secrets).\nSecrets distributed in this manner become part of the RaftDB,\nand thus it becomes necessary to enable swarm autolock mode,\nwhich prevents the Raft database encryption key\nfrom being stored plaintext on disk.\nSwarm secrets have an additional limitation in that they are not\nmutable at runtime.\n* Kubernetes:\nKubernetes is not officially supported by the EdgeX project.\nKubernetes also supports the secrets volume approach,\nthough the secrets volume can be mounted anywhere in the container namespace.\nFor an example of Kubernetes secrets volumes, see the\n[Kubernetes secrets documentation](https://kubernetes.io/docs/concepts/configuration/secret/).\nSecrets distributed in this manner become part of the `etcd` database,\nand thus it becomes necessary to specify a\n[KMS provider for data encryption](https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/)\nto prevent `etcd` from storing plaintext versions of secrets.\n","The decision is to standardize on the following secret handling practices:\n\n**System-managed secrets**\n\n* **PKI private keys**: Generated by a trusted utility and distributed via _SECRETSLOC_.\n* **Secret store master password**: Stored in Vault.\n* **Secret store per-service authentication tokens**: Distributed via _SECRETSLOC_.\n* **Database superuser password**: Generated by a trusted utility and stored in Vault.\n* **Mongo service account passwords**: Generated by a trusted utility and stored in Vault.\n* **Redis authentication password**: Generated by a trusted utility and stored in Vault.\n* **Kong client authentication tokens**: Generated by a trusted utility and stored in Vault.\n\n**User-managed secrets**\n\n* **User-managed secret parameters**: To be managed by the application. The framework can provide a secret storage mechanism for user-managed secrets but not the secrets themselves.\n\n**Notes**\n\n* The location and implementation of _SECRETSLOC_ should be consistent with the principles discussed in this ADR.\n* The ""trusted utility"" used to generate secrets should be implemented in a secure manner and resist brute-force attacks.\n* The decision described above pertains to bootstrapping secrets. Secrets management over the lifetime of the application is outside the scope of this ADR."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen deploying the EdgeX Docker containers some security measures are recommended to ensure the integrity of the software stack.\n\n## Decision\n,"When deploying Docker images, the following flags should be set for heightened security.\n- To avoid escalation of privileges each docker container should use the `no-new-privileges` option in their Docker compose file (example below). More details about this flag can be found [here](https://docs.docker.com/engine/reference/run/#security-configuration). This follows Rule #4 for Docker security found [here](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-4-add-no-new-privileges-flag).\n```docker\nsecurity_opt:\n- ""no-new-privileges:true""\n```\n> NOTE: Alternatively an AppArmor security profile can be used to isolate the docker container. More details about apparmor profiles can be found [here](https://docs.docker.com/engine/security/apparmor/)\n```docker\nsecurity_opt:  [ ""apparmor:unconfined"" ]\n```\n- To further prevent privilege escalation attacks the user should be set for the docker container using the `--user=<userid>` or `-u=<userid>` option in their Docker compose file (example below). More details about this flag can be found [here](https://docs.docker.com/engine/reference/run/#user). This follows Rule #2 for Docker security found [here](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\n```docker\nservices:\ndevice-virtual:\nimage: ${REPOSITORY}/docker-device-virtual-go${ARCH}:${DEVICE_VIRTUAL_VERSION}\nuser: $CONTAINER-PORT:$CONTAINER-PORT # user option using an unprivileged user\nports:\n- ""127.0.0.1:49990:49990""\ncontainer_name: edgex-device-virtual\nhostname: edgex-device-virtual\nnetworks:\n- edgex-network\nenv_file:\n- common.env\nenvironment:\nSERVICE_HOST: edgex-device-virtual\ndepends_on:\n- consul\n- data\n- metadata\n```\n> NOTE: exception\nSometimes containers will require root access to perform their fuctions. For example the System Management Agent requires root access to control other Docker containers. In this case you would allow it run as default root user.\n- To avoid a faulty or compromised containers from consuming excess amounts of the host of its resources `resource limits` should be set for each container. More details about `resource limits` can be found [here](https://docs.docker.com/config/containers/resource_constraints/). This follows Rule #7 for Docker security found [here](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-7-limit-resources-memory-cpu-file-descriptors-processes-restarts).\n```docker\nservices:\ndevice-virtual:\nimage: ${REPOSITORY}/docker-device-virtual-go${ARCH}:${DEVICE_VIRTUAL_VERSION}\nuser: 4000:4000 # user option using an unprivileged user\nports:\n- ""127.0.0.1:49990:49990""\ncontainer_name: edgex-device-virtual\nhostname: edgex-device-virtual\nnetworks:\n- edgex-network\nenv_file:\n- common.env\nenvironment:\nSERVICE_HOST: edgex-device-virtual\ndepends_on:\n- consul\n- data\n- metadata\ndeploy:  # Deployment resource limits\nresources:\nlimits:\ncpus: '0.001'\nmemory: 50M\nreservations:\ncpus: '0.0001'\nmemory: 20M\n```\n- To avoid attackers from writing data to the containers and modifying their files the `--read_only` flag should be set. More details about this flag can be found [here](https://docs.docker.com/compose/compose-file/#domainname-hostname-ipc-mac_address-privileged-read_only-shm_size-stdin_open-tty-user-working_dir). This follows Rule #8 for Docker security found [here](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-8-set-filesystem-and-volumes-to-read-only).\n```docker\ndevice-rest:\nimage: ${REPOSITORY}/docker-device-rest-go${ARCH}:${DEVICE_REST_VERSION}\nports:\n- ""127.0.0.1:49986:49986""\ncontainer_name: edgex-device-rest\nhostname: edgex-device-rest\nread_only: true # read_only option\nnetworks:\n- edgex-network\nenv_file:\n- common.env\nenvironment:\nSERVICE_HOST: edgex-device-rest\ndepends_on:\n- data\n- command\n```\n> NOTE: exception\nIf a container is required to have write permission to function, then this flag will not work. For example, the vault needs to run setcap in order to lock pages in memory. In this case the `--read_only` flag will not be used.\nNOTE: Volumes\nIf writing persistent data is required then a volume can be used. A volume can be attached to the container in the following way\n```docker\ndevice-rest:\nimage: ${REPOSITORY}/docker-device-rest-go${ARCH}:${DEVICE_REST_VERSION}\nports:\n- ""127.0.0.1:49986:49986""\ncontainer_name: edgex-device-rest\nhostname: edgex-device-rest\nread_only: true # read_only option\nnetworks:\n- edgex-network\nenv_file:\n- common.env\nenvironment:\nSERVICE_HOST: edgex-device-rest\ndepends_on:\n- data\n- command\nvolumes:\n- consul-config:/consul/config:z\n```\n> NOTE: alternatives\nIf writing non-persistent data is required (ex. a config file) then a temporary filesystem mount can be used to accomplish this goal while still enforcing `--read_only`. Mounting a `tmpfs` in Docker gives the container a temporary location in the host systems memory to modify files. This location will be removed once the container is stopped. More details about `tmpfs` can be found [here](https://docs.docker.com/storage/tmpfs/)\nfor additional docker security rules and guidelines please check the Docker security [cheatsheet](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html)\n",Implement an AppArmor container profile to restrict the file system access and capabilities of the EdgeX containers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn EdgeX today, sensor/device data collected can be ""filtered"" by [application services](../../../microservices/application/ApplicationServices.md) before being exported or sent to some [north side](../../../general/Definitions.md#south-and-north-side) application or system. Built-in application service functions (available through the app services SDK) allow EdgeX event/reading objects to be filtered by device name or by device ResourceName.  That is, event/readings can be filtered by:\n- which device sent the event/reading (as determined by the Event device property).\n- the classification or origin (such as temperature or humidity) of data produced by the device as determined by the Reading's name property (which used to be the value descriptor and now refers to the device ResourceName).\n### Two Levels of Device Service Filtering\nThere are potentially two places where ""filtering"" in a device service could be useful.\n- One (Sensor Data Filter) - after the device service has communicated with the sensor or device to get sensor values (but before the service creates `Event/Reading` objects and pushes those to core data).  A sensor data filter would allow the device service to essentially ignore some of the raw sensed data.  This would allow for some device service optimization in that the device service would not have perform type transformations and creation of event/reading objects if the data can be eliminated at this early stage.  This first level filtering would, **if put in place**, likely occur in code associated with the read command gets done by the `ProtocolDriver`.\n- Two (Reading Filter) - after the sensor data has been collected and read and put into `Event/Reading` objects, there is a desire to filter some of the `Readings` based on the `Reading` values or `Reading` name (which is the device ResourceName) or some combination of value and name.\nAt this time, **this design only addresses the need for the second filter (Reading Filter)**.  At the time of this writing, no applicable use case has yet to be defined to warrant the Sensor Data Filter.\n### Reading Filters\nReading filters will allow, not unlike application service filter functions today, to have `Readings` in an `Event` to be removed if:\n- the value was outside or inside some range, or the value was greater than, less than or equal to some value\n- based on the `Reading` value (numeric) of a `Reading` outside a specified range (min/max) described in the service configuration.  Thus avoiding sending in outlier or jittery data `Readings` that could negatively effect analytics.\n- Future scope:  based on the `Reading` value (numeric) equal to or near (with in some specified range) the last reading.  This allows a device service to reduce sending in `Event/Readings` that do not represent any significant change.  This differs from the already implemented onChangeOnly in that it is filtering `Readings` within a specified degree of change.  **Note:** this feature would require caching of readings which has not fully been implemented in the SDK.  The existing mechanism for `autoevents` provides a partial cache.  Added for future reference, but this feature would not be accomplished in the initial implementation; requiring extra design work on caching to be implemented.\n- the value was the same as some or not the same as some specified value or values (for strings, boolean and other non-numeric values)\n- the value matches a pattern (glob and/or regex) when the value is a string.\n- the name (the device ResourceName) matched a particular value; in other words match `temperature` or `humidity` as example device resources.\nUnlike application services, there is not a need to filter on a device name (or identifier).  Simply disable the device in the device service if all `Event/Readings` are to be stopped for the device.\nIn the case that all `Readings` of an `Event` are filtered, it is assumed the entire `Event` is deemed to be worthless and not sent to core data by the device service.  If only some `Readings` from and `Event` are filtered, the `Event` minus the filtered `Readings` would be sent to core data.\nThe filter behaves the same whether the collection of `Readings` and `Events` is triggered by a scheduled collection of data from the underlying sensor/device or triggered by a command request (as from the command service).  Therefore, the call for a command request still results in a successful status code and a return of no results (or partial results) if the filter causes all or some of the readings to be removed.\n### Design / Architecture\nA new function interface shall be defined that, when implemented, performs a Reading Filter operation.  A ReadingFilter function would take a parameter (an `Event` containing readings), check whether the `Readings` of the `Event` match on the filtering configuration (see below) and if they do then remove them from the `Event`.  The ReadingFilter function would return the `Event` object (minus filtered `Readings`) or `nil` if the `Event` held no more `Readings`.  Pseudo code for the generic function is provided below.  The results returned will include a boolean to indicate whether any `Reading` objects were removed from the `Event` (allowing the receiver to know if some were filtered from the original list).\n``` go\nfunc (f Filter) ReadingFilter(lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {\n// depending on impl; filtering for values in/out of a range, >, <, =, same, not same, from a particular name (device resource), etc.\n// The boolean will indicate whether any Readings were filtered from the Event.\nif (len(event.Reading )) > 0)\nif (len filteredReadings > 0)\nreturn event, true\nelse\nreturn event, false\nelse\nreturn nil, true\n}\n```\nBased on current needs/use cases, implementations of the function interface could include the following filter functions:\n``` go\nfunc (f Filter) FilterByValue (lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {}\nfunc (f Filter) FilterByResourceNamesMatch (lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {}\n```\n!!! Note\nThe app functions SDK comes with `FilterByDeviceName` and `FilterByResourceName` functions today. The FilterByResourceName would behave similarly to FilterByResourceNameMatch.\nThe Filter structure houses the configuration parameters for which the filter functions work and filter on.\n!!! Note\nThe app functions SDK uses a fairly simple Filter structure.\n``` go\ntype Filter struct {\nFilterValues []string\nFilterOut    bool\n}\n```\nGiven the collection of filter operations (in range, out of range, equal or not equal), the following structure is proposed:\n``` go\ntype Filter struct {\nFilterValues []string\nTargetResourceName string\nFilterOp string  // enum of in (in range inclusive), out (outside a range exclusive), eq (equal) or ne (not equal)\n}\n```\nExamples use of the Filter structure to specify filtering:\n``` go\nFilter {FilterValues: {10, 20}, ""Int64"", FilterOp: ""in""} // filter for those Int64 readings with values between 10-20 inclusive\nFilter {FilterValues: {10, 20}, ""Int64"", FilterOp: ""out""} // filter for those Int64 readings with values outside of 10-20.\nFilter {FilterValues: {8, 10, 12}, ""Int64"", FilterOp: ""eq""} //filter for those Int64 readings with values of 8, 10, or 12.\nFilter {FilterValues: {8, 10}, ""Int64"", FilterOp: ""ne""}  //filter for those Int64 readings with values not equal to 8 or 10\nFilter {FilterValues: {""Int32"", ""Int64""}, nil, FilterOp: ""eq""} //filter to be used with FilterByResourceNameMatch.  Filter for resource names of Int32 or Int64.\nFilter {FilterValues: {""Int32""}, nil, FilterOp: ""ne""} //filter to be used with FilterByResourceNameMatch.  Filter for resource names not equal to (excluding) Int32.\n```\nA NewFilter function creates, initializes and returns a new instance of the filter based on the configuration provided.\n``` go\nfunc NewReadingNameFilter(filterValues []string, filterOp string) Filter {\nreturn Filter{FilterValues: filterValues, TargetResourceName string, FilterOp: filterOp}\n}\n```\n### Sharing filter functions\nIf one were to explore the filtering functions in the app functions SDK [filter.go](https://github.com/edgexfoundry/app-functions-sdk-go/blob/master/pkg/transforms/filter.go) (both `FilterByDeviceName` and `FilterByValueDescriptor`), the filters operate on the `Event` model object and return the same objects (`Event` or nil).  Ideally, since both app services and device services generally share the same interface model (from `go-mod-core-contracts`), it would be the desire to share the same filter functions functions between SDKs and associated services.\nDecisions on how to do this in Go - whether by shared module for example - is left as a future release design and implementation task - and as the need for common filter functions across device services and application services are identified in use cases.  C needs are likely to be handled in the SDK directly.\n#### Additional Design Considerations\nAs Device Services do not have the concept of a functions pipeline like application services do, consideration must be given as to how and where to:\n- provide configuration to specify which filter functions to invoke\n- create the filter\n- invoke the filtering functions\nAt this time, custom filters will not be supported as the custom filters would not be known by the SDK and therefore could not be specified in configuration.  This is consistent with the app functions SDK and filtering.\n#### Function Inflection Point\nIt is precisely after the convert to `Event/Reading` objects (after the async readings are assembled into events) and before returning that result in `common.SendEvent` (in utils.go) function that the device service should invoke the required filter functions.  In the existing V1 implementation of the device-sdk-go, commands, async readings, and auto-events all call the function `common.SendEvent()`.  *Note: V2 implementation will require some re-evaluation of this inflection point.*  Where possible, the implementation should locate a single point of inflection if possible.  In the C SDK, it is likely that the filters will be called before conversion to Event/Reading objects - they will operate on commandresult objects (equivalent to CommandValues).\nThe order in which functions are called is important when more than one filter is provided.  The order that functions are called should be reflected in the order listed in the configuration of the filters.\nEvents containing binary values (event.HasBinaryValue), will not be filtered.  Future releases may include binary value filters.\n#### Setting Filter Function and Configuration\nWhen filter functions are shared (or appear to be doing the same type of work) between SDKs, the configuration of the similar filter functions should also look similar.  The app functions SDK configuration model for filters should therefore be followed.\nWhile device services do not have pipelines, the inclusion and configuration of filters for device services should have a similar look (to provide symmetry with app services). The configuration has to provide the functions required and parameters to make the functions work - even though the association to a pipeline is not required.  Below is the common app service configuration as it relates to filters:\n``` toml\n[Writable.Pipeline]\nExecutionOrder = ""FilterByDeviceName, TransformToXML, SetOutputData""\n[Writable.Pipeline.Functions.FilterByDeviceName]\n[Writable.Pipeline.Functions.FilterByDeviceName.Parameters]\nDeviceNames = ""Random-Float-Device,Random-Integer-Device""\nFilterOut = ""false""\n```\nSuggested and hypothetical configuration for the device service reading filters should look something like that below.\n``` toml\n[Writable.Filters]\n# filter readings where resource name equals Int32\nExecutionOrder = ""FilterByResourceNamesMatch, FilterByValue""\n[Writable.Filter.Functions.FilterByResourceNamesMatch]\n[Writable.Filter.Functions.FilterByResourceNamesMatch.Parameters]\nFilterValues = ""Int32""\nFilterOps =""eq""\n# filter readings where the Int64 readings (resource name) is Int64 and the values are between 10 and 20\n[Writable.Filter.Functions.FilterByValue]\n[Writable.Filter.Functions.FilterByValue.Parameters]\nTargetResourceName = ""Int64""\nFilterValues = {10,20}\nFilterOp = ""in""\n```\n\n## Decision\n","#### Additional Design Considerations\nAs Device Services do not have the concept of a functions pipeline like application services do, consideration must be given as to how and where to:\n- provide configuration to specify which filter functions to invoke\n- create the filter\n- invoke the filtering functions\nAt this time, custom filters will not be supported as the custom filters would not be known by the SDK and therefore could not be specified in configuration.  This is consistent with the app functions SDK and filtering.\n#### Function Inflection Point\nIt is precisely after the convert to `Event/Reading` objects (after the async readings are assembled into events) and before returning that result in `common.SendEvent` (in utils.go) function that the device service should invoke the required filter functions.  In the existing V1 implementation of the device-sdk-go, commands, async readings, and auto-events all call the function `common.SendEvent()`.  *Note: V2 implementation will require some re-evaluation of this inflection point.*  Where possible, the implementation should locate a single point of inflection if possible.  In the C SDK, it is likely that the filters will be called before conversion to Event/Reading objects - they will operate on commandresult objects (equivalent to CommandValues).\nThe order in which functions are called is important when more than one filter is provided.  The order that functions are called should be reflected in the order listed in the configuration of the filters.\nEvents containing binary values (event.HasBinaryValue), will not be filtered.  Future releases may include binary value filters.\n#### Setting Filter Function and Configuration\nWhen filter functions are shared (or appear to be doing the same type of work) between SDKs, the configuration of the similar filter functions should also look similar.  The app functions SDK configuration model for filters should therefore be followed.\nWhile device services do not have pipelines, the inclusion and configuration of filters for device services should have a similar look (to provide symmetry with app services). The configuration has to provide the functions required and parameters to make the functions work - even though the association to a pipeline is not required.  Below is the common app service configuration as it relates to filters:\n``` toml\n[Writable.Pipeline]\nExecutionOrder = ""FilterByDeviceName, TransformToXML, SetOutputData""\n[Writable.Pipeline.Functions.FilterByDeviceName]\n[Writable.Pipeline.Functions.FilterByDeviceName.Parameters]\nDeviceNames = ""Random-Float-Device,Random-Integer-Device""\nFilterOut = ""false""\n```\nSuggested and hypothetical configuration for the device service reading filters should look something like that below.\n``` toml\n[Writable.Filters]\n# filter readings where resource name equals Int32\nExecutionOrder = ""FilterByResourceNamesMatch, FilterByValue""\n[Writable.Filter.Functions.FilterByResourceNamesMatch]\n[Writable.Filter.Functions.FilterByResourceNamesMatch.Parameters]\nFilterValues = ""Int32""\nFilterOps =""eq""\n# filter readings where the Int64 readings (resource name) is Int64 and the values are between 10 and 20\n[Writable.Filter.Functions.FilterByValue]\n[Writable.Filter.Functions.FilterByValue.Parameters]\nTargetResourceName = ""Int64""\nFilterValues = {10,20}\nFilterOp = ""in""\n```\n*To be determined*\n","**Implement a new filter function interface to filter Readings based on values or names.**\n\nThe filter functions will be defined as follows:\n\n``` go\nfunc (f Filter) ReadingFilter(lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {}\nfunc (f Filter) FilterByValue (lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {}\nfunc (f Filter) FilterByResourceNamesMatch (lc logger.LoggingClient, event *models.Event) (*models.Event, error, boolean) {}\n```\n\nThe Filter structure will house the configuration parameters for which the filter functions work and filter on:\n\n``` go\ntype Filter struct {\n    FilterValues []string\n    TargetResourceName string\n    FilterOp string  // enum of in (in range inclusive), out (outside a range exclusive), eq (equal) or ne (not equal)\n}\n```\n\nThe filter functions will be invoked after the async readings are assembled into events and before returning that result in `common.SendEvent()`. The order in which functions are called is important when more than one filter is provided. The order that functions are called should be reflected in the order listed in the configuration of the filters.\n\nThe configuration of the filter functions will be similar to the app functions SDK configuration model for filters:\n\n``` toml\n[Writable.Filters]\n# filter readings where resource name equals Int32\nExecutionOrder = ""FilterByResourceNamesMatch, FilterByValue""\n[Writable.Filter.Functions.FilterByResourceNamesMatch]\n[Writable.Filter.Functions.FilterByResourceNamesMatch.Parameters]\nFilterValues = ""Int32""\nFilterOps =""eq""\n# filter readings where the Int64 readings (resource name) is Int64 and the values are between 10 and 20\n[Writable.Filter.Functions.FilterByValue]\n[Writable.Filter.Functions.FilterByValue.Parameters]\nTargetResourceName = ""Int64""\nFilterValues = {10,20}\nFilterOp = ""in""\n```"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n- [Context](#context)\n- [Decision](#decision)\n* [Which Message Bus implementations?](#which-message-bus-implementations)\n* [Go Device SDK](#go-device-sdk)\n* [C Device SDK](#c-device-sdk)\n* [Core Data and Persistence](#core-data-and-persistence)\n* [V2 Event DTO](#v2-event-dto)\n+ [Validation](#validation)\n* [Message Envelope](#message-envelope)\n* [Application Services](#application-services)\n* [MessageBus Topics](#messagebus-topics)\n* [Configuration](#configuration)\n+ [Device Services](#device-services)\n- [[MessageQueue]](#messagequeue)\n+ [Core Data](#core-data)\n- [[MessageQueue]](#messagequeue)\n+ [Application Services](#application-services)\n- [[MessageBus]](#messagebus)\n- [[Binding]](#binding)\n* [Secure Connections](#secure-connections)\n- [Consequences](#consequences)\n\n## Decision\n,"* [Which Message Bus implementations?](#which-message-bus-implementations)\n* [Go Device SDK](#go-device-sdk)\n* [C Device SDK](#c-device-sdk)\n* [Core Data and Persistence](#core-data-and-persistence)\n* [V2 Event DTO](#v2-event-dto)\n+ [Validation](#validation)\n* [Message Envelope](#message-envelope)\n* [Application Services](#application-services)\n* [MessageBus Topics](#messagebus-topics)\n* [Configuration](#configuration)\n+ [Device Services](#device-services)\n- [[MessageQueue]](#messagequeue)\n+ [Core Data](#core-data)\n- [[MessageQueue]](#messagequeue)\n+ [Application Services](#application-services)\n- [[MessageBus]](#messagebus)\n- [[Binding]](#binding)\n* [Secure Connections](#secure-connections)\n- [Consequences](#consequences)\n### Which Message Bus implementations?\nMultiple Device Services may need to be publishing Events to the MessageBus concurrently.  `ZMQ` will not be a valid option if multiple Device Services are configured to publish. This is because `ZMQ` only allows for a single publisher. `ZMQ` will still be valid if only one Device Service is publishing Events. The `MQTT` and `Redis Streams` are valid options to use when multiple Device Services are required, as they both support multiple publishers. These are the only other implementations currently available for Go services. The C base device services do not yet have a MessageBus implementation.  See the [C Device SDK](#c-device-sdk) below for details.\n> *Note: Documentation will need to be clear when `ZMQ` can be used and when it can not be used.*\n### Go Device SDK\nThe Go Device SDK will take advantage of the existing `go-mod-messaging` module to enable use of the EdgeX MessageBus. A new bootstrap handler will be created which initializes the MessageBus client based on configuration. See [Configuration](#configuration) section below for details.  The Go Device SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\n### C Device SDK\nThe C Device SDK will implement its own MessageBus abstraction similar to the one in `go-mod-messaging`.  The first implementation type (MQTT or Redis Streams) is TBD. Using this abstraction allows for future implementations to be added when use cases warrant the additional implementations.  As with the Go SDK, the C SDK will be enhanced to optionally publish Events to the MessageBus anywhere it currently POSTs Events to Core Data. This publish vs POST option will be controlled by configuration with publish as the default.  See [Configuration](#configuration) section below for details.\n### Core Data and Persistence\nWith this design, Events will be sent directly to Application Services w/o going through Core Data and thus will not be persisted unless changes are made to Core Data. To allow Events to optionally continue to be persisted, Core Data will become an additional or secondary (and optional) subscriber for the Events from the MessageBus. The Events will be persisted when they are received. Core Data will also retain the ability to receive Events via HTTP, persist them and publish them to the MessageBus as is done today. This allows for the flexibility to have some device services to be configured to POST Events and some to be configured to publish Events while we transition the Device Services to all have the capability to publishing Events. In the future, once this new `Publish` approach has been proven, we may decide to remove POSTing Events to Core Data from the Device SDKs.\nThe existing `PersistData` setting will be ignored by the code path subscribing to Events since the only reason to do this is to persist the Events.\nThere is a race condition for `Marked As Pushed` when Core Data is persisting Events received from the MessageBus. Core Data may not have finished persisting an Event before the Application Service has processed the Event and requested the Event be `Marked As Pushed`. It was decided to remove `Mark as Pushed` capability and just rely on time based scrubbing of old Events.\n### V2 Event DTO\nAs this development will be part of the Ireland release all Events published to the MessageBus will use the V2 Event DTO. This is already implemented in Core Data for the V2 AddEvent API.\n#### Validation\nServices receiving the Event DTO from the MessageBus will log validation errors and stop processing the Event.\n### Message Envelope\nEdgeX Go Services currently uses a custom Message Envelope for all data that is published to the MessageBus. This envelope wraps the data with metadata, which is `ContentType` (JSON or CBOR), `Correlation-Id` and the obsolete `Checksum`. The `Checksum` is used when the data is CBOR encoded to identify the Event in V1 API to be mark it as pushed. This checksum is no longer needed as the V2 Event DTO requires the ID be set by the Device Services which will always be used in the V2 API to mark the Events as pushed. The Message Envelope will be updated to remove this property.\nThe C SDK will recreate this Message Envelope.\n### Application Services\nAs part of the V2 API consumption work in Ireland the App Services SDK will be changed to expect to receive V2 Event DTOs rather than the V1 Event model. It will also be updated to no longer expect or use the `Checksum` currently on the  Message Envelope. Note these changes must occur for the V2 consumption and are not directly tied to this effort.\nThe App Service SDK will be enhanced for the secure MessageBus connection described below. See **[Secure Connections](#secure-connections)** for details\n### MessageBus Topics\n> *Note: The change recommended here is not required for this design, but it provides a good opportunity to adopt it.*\nCurrently Core Data publishes Events to the simple `events` topic. All Application Services running receive every Event published, whether they want them or not. The Events can be filtered out using the `FilterByDeviceName` or `FilterByResourceName` pipeline functions, but the Application Services still receives every Event and process all the Events to some extent. This could cause load issues in a deployment with many devices and large volume of Events from various devices or a very verbose device that the Application Services is not interested in.\n> *Note: The current `FilterByDeviceName` is only good if the device name is known statically and the only instance of the device defined by the `DeviceProfileName`. What we really need is `FilterByDeviceProfileName` which allows multiple instances of a device to be filtered for, rather than a single instance as it it now. The V2 API will be adding `DeviceProfileName` to the Events, so in Ireland this  filter will be possible.*\nPub/Sub systems have advanced topic schema, which we can take advantage of from Application Services to filter for just the Events the Application Service actual wants. Publishers of Events must add the `DeviceProfileName`, `DeviceName` and `SourceName` to the topic in the form `edgex/events/<device-profile-name>/<device-name>/<source-name>`. The `SourceName` is the `Resource` or `Command` name used to create the Event. This allows Application Services to filter for just the Events from the device(s) it wants by only subscribing to those `DeviceProfileNames` or the specific `DeviceNames` or just the specific `SourceNames`  Example subscribe topics if above schema is used:\n- **edgex/events/#**\n- All Events\n- Core Data will subscribe using this topic schema\n- **edgex/events/Random-Integer-Device/#**\n- Any Events from devices created from the **Random-Integer-Device** device profile\n- **edgex/events/Random-Integer-Device/Random-Integer-Device1**\n- Only Events from the **Random-Integer-Device1** Device\n- **edgex/events/Random-Integer-Device/#/Int16**\n- Any Events with Readings from`Int16` device resource from devices created from the **Random-Integer-Device** device profile.\n- **edgex/events/Modbus-Device/#/HVACValues\n- Any Events with Readings from `HVACValues` device command from devices created from the **Modbus-Device** device profile.\nThe MessageBus abstraction allows for multiple subscriptions, so an Application Service could specify to receive data from multiple specific device profiles or devices by creating multiple subscriptions. i.e.  `edgex/Events/Random-Integer-Device/#` and  `edgex/Events/Random-Boolean-Device/#`. Currently the App SDK only allows for a single subscription topic to be configured, but that could easily be expanded to handle a list of subscriptions. See [Configuration](#configuration) section below for details.\nCore Data's existing publishing of Events would also need to be changed to use this new topic schema. One challenge with this is Core Data doesn't currently know the `DeviceProfileName` or `DeviceName` when it receives a CBOR encoded event. This is because it doesn't decode the Event until after it has published it to the MessageBus. Also, Core Data doesn't know of `SourceName` at all. The V2 API will be enhanced to change the AddEvent endpoint from `/event` to `/event/{profile}/{device}/{source}` so that `DeviceProfileName`, `DeviceName`, and `SourceName` are always know no matter how the request is encoded.\nThis new topic approach will be enabled via each publisher's `PublishTopic` having the `DeviceProfileName`, `DeviceName`and `SourceName`  added to the configured `PublishTopicPrefix`\n```toml\nPublishTopicPrefix = ""edgex/events"" # /<device-profile-name>/<device-name>/<source-name> will be added to this Publish Topic prefix\n```\nSee [Configuration](#configuration) section below for details.\n### Configuration\n#### Device Services\nAll Device services will have the following additional configuration to allow connecting and publishing to the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopic` will include the `DeviceProfileName` and `DeviceName`.\n##### [MessageQueue]\nA  MessageQueue section will be added, which is similar to that used in Core Data today, but with `PublishTopicPrefix` instead of `Topic`.To enable secure connections, the `Username` & `Password` have been replaced with ClientAuth & `SecretPath`, See **[Secure Connections](#secure-connections)** section below for details. The added `Enabled` property controls whether the Device Service publishes to the MessageBus or POSTs to Core Data.\n```toml\n[MessageQueue]\nEnabled = true\nProtocol = ""tcp""\nHost = ""localhost""\nPort = 1883\nType = ""mqtt""\nPublishTopicPrefix = ""edgex/events"" # /<device-profile-name>/<device-name>/<source-name> will be added to this Publish Topic prefix\n[MessageQueue.Optional]\n# Default MQTT Specific options that need to be here to enable environment variable overrides of them\n# Client Identifiers\nClientId =""<device service key>""\n# Connection information\nQos          =  ""0"" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\nKeepAlive    =  ""10"" # Seconds (must be 2 or greater)\nRetained     = ""false""\nAutoReconnect  = ""true""\nConnectTimeout = ""5"" # Seconds\nSkipCertVerify = ""false"" # Only used if Cert/Key file or Cert/Key PEMblock are specified\nClientAuth = ""none"" # Valid values are: `none`, `usernamepassword` or `clientcert`\nSecretpath = ""messagebus""  # Path in secret store used if ClientAuth not `none`\n```\n#### Core Data\nCore data will also require additional configuration to be able to subscribe to receive Events from the MessageBus. As describe above in the  [MessageBus Topics](#messagebus-topics) section, the `PublishTopicPrefix` will have `DeviceProfileName` and `DeviceName` added to create the actual Public Topic.\n##### [MessageQueue]\nThe `MessageQueue` section will be  changed so that the `Topic` property changes to `PublishTopicPrefix` and `SubscribeEnabled` and `SubscribeTopic` will be added. As with device services configuration, the `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details. In addition, the Boolean `SubscribeEnabled` property will be used to control if the service subscribes to Events from the MessageBus or not.\n```toml\n[MessageQueue]\nProtocol = ""tcp""\nHost = ""localhost""\nPort = 1883\nType = ""mqtt""\nPublishTopicPrefix = ""edgex/events"" # /<device-profile-name>/<device-name>/<source-name> will be added to this Publish Topic prefix\nSubscribeEnabled = true\nSubscribeTopic = ""edgex/events/#""\n[MessageQueue.Optional]\n# Default MQTT Specific options that need to be here to enable evnironment variable overrides of them\n# Client Identifiers\nClientId =""edgex-core-data""\n# Connection information\nQos          =  ""0"" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\nKeepAlive    =  ""10"" # Seconds (must be 2 or greater)\nRetained     = ""false""\nAutoReconnect  = ""true""\nConnectTimeout = ""5"" # Seconds\nSkipCertVerify = ""false"" # Only used if Cert/Key file or Cert/Key PEMblock are specified\nClientAuth = ""none"" # Valid values are: `none`, `usernamepassword` or `clientcert`\nSecretpath = ""messagebus""  # Path in secret store used if ClientAuth not `none`\n```\n#### Application Services\n##### [MessageBus]\nSimilar to above, the Application Services `MessageBus` configuration will change to allow for secure connection to the MessageBus. The `Username` & `Password` have been replaced with `ClientAuth` & `SecretPath` for secure connections. See **[Secure Connections](#secure-connections)** section below for details.\n```toml\n[MessageBus.Optional]\n# MQTT Specific options\n# Client Identifiers\nClientId =""<app sevice key>""\n# Connection information\nQos          =  ""0"" # Quality of Sevice values are 0 (At most once), 1 (At least once) or 2 (Exactly once)\nKeepAlive    =  ""10"" # Seconds (must be 2 or greater)\nRetained     = ""false""\nAutoReconnect  = ""true""\nConnectTimeout = ""5"" # Seconds\nSkipCertVerify = ""false"" # Only used if Cert/Key file or Cert/Key PEMblock are specified\nClientAuth = ""none"" # Valid values are: `none`, `usernamepassword` or `clientcert`\nSecretpath = ""messagebus""  # Path in secret store used if ClientAuth not `none`\n```\n##### [Binding]\nThe `Binding` configuration section will require changes for the subscribe topics scheme described in the [MessageBus Topics](#messagebus-topics) section above to filter for Events from specific device profiles or devices. `SubscribeTopic` will change from a string property containing a single topic to the `SubscribeTopics` string property containing a comma separated list of topics. This allows for the flexibility for the property to be a single topic with the `#` wild card so the Application Service receives all Events as it does today.\nReceive only Events from the `Random-Integer-Device` and `Random-Boolean-Device` profiles\n```toml\n[Binding]\nType=""messagebus""\nSubscribeTopics=""edgex/events/Random-Integer-Device, edgex/events/Random-Boolean-Device""\n```\nReceive only Events from the  `Random-Integer-Device1` from the `Random-Integer-Device` profile\n```toml\n[Binding]\nType=""messagebus""\nSubscribeTopics=""edgex/events/Random-Integer-Device/Random-Integer-Device1""\n```\nor receives all Events:\n```toml\n[Binding]\nType=""messagebus""\nSubscribeTopics=""edgex/events/#""\n```\n### Secure Connections\nAs stated earlier,  this ADR is dependent on the  **Secret Provider for All**(Link TBD) ADR to provide a common Secret Provider for all Edgex Services to access their secrets. Once this is available, the MessageBus connection can be secured via the following configurable client authentications modes which follows similar implementation for secure MQTT Export and secure MQTT Trigger used in Application Services.\n- **none** - No authentication\n- **usernamepassword** - Username & password authentication.\n- **clientcert** - Client certificate and key for authentication.\n- The secrets specified for the above options are pulled from the `Secret Provider` using the configured `SecretPath`.\nHow the secrets are injected into the `Secret Provider` is out of scope for this ADR and covered in the **Secret Provider for All**( Link TBD) ADR.\n",### Which Message Bus implementations?\n\nWe will use Google Cloud Pub/Sub as the Message Bus implementation for the Go Device SDK.\nWe will use Google Cloud IoT Core for the C Device SDK.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n* [Context](#context)\n+ [History](#history)\n* [Decision](#decision)\n+ [Stage-gate mechanism](#stage-gate-mechanism)\n+ [Docker-specific service changes](#docker-specific-service-changes)\n- [""As-is"" startup flow](#as-is-startup-flow)\n- [""To-be"" startup flow](#to-be-startup-flow)\n- [New Bootstrap/RTR container](#new-bootstraprtr-container)\n* [Consequences](#consequences)\n+ [Benefits](#benefits)\n+ [Drawbacks](#drawbacks)\n* [Alternatives](#alternatives)\n+ [Event-driven vs commanded staging](#event-driven-vs-commanded-staging)\n+ [System management agent (SMA) as the coordinator](#system-management-agent-sma-as-the-coordinator)\n+ [Create a mega-install container](#create-a-mega-install-container)\n+ [Manual secret provisioning](#manual-secret-provisioning)\n* [References](#references)\n\n## Decision\n","+ [Stage-gate mechanism](#stage-gate-mechanism)\n+ [Docker-specific service changes](#docker-specific-service-changes)\n- [""As-is"" startup flow](#as-is-startup-flow)\n- [""To-be"" startup flow](#to-be-startup-flow)\n- [New Bootstrap/RTR container](#new-bootstraprtr-container)\n* [Consequences](#consequences)\n+ [Benefits](#benefits)\n+ [Drawbacks](#drawbacks)\n* [Alternatives](#alternatives)\n+ [Event-driven vs commanded staging](#event-driven-vs-commanded-staging)\n+ [System management agent (SMA) as the coordinator](#system-management-agent-sma-as-the-coordinator)\n+ [Create a mega-install container](#create-a-mega-install-container)\n+ [Manual secret provisioning](#manual-secret-provisioning)\n* [References](#references)\n### Stage-gate mechanism\nThe stage-gate mechanism must work in the following environments:\n* docker-compose in Linux on a single node/system\n* docker-compose in Microsoft* Windows* on a single node/system\n* docker-compose in Apple* MacOS* on a single node/system\nStartup sequencing will be driven by two primary mechanisms:\n1. Use of entrypoint scripts to:\n- Block on stage-gate and service dependencies\n- Perform first-boot initialization phase activities as noted in [Context](#context)\nThe bootstrap container will inject entrypoint scripts into\nthe other containers in the case where EdgeX is directly consuming\nan upstream container.  Docker will automatically retry\nrestarting containers if its entrypoint script is missing.\n2. Use of open TCP sockets as semaphores to gate startup sequencing\nUse of TCP sockets for startup sequencing is commonly used in Docker environments.\nDue to its popularlity, there are several existing tools for this, including\n[wait-for-it](https://github.com/vishnubob/wait-for-it),\n[dockerize](https://github.com/jwilder/dockerize), and\n[wait-for](https://github.com/Eficode/wait-for).\nThe TCP mechanism is portable across platforms\nand will work in distributed multi-node scenarios.\nAt least three new ports will be added to EdgeX for sequencing purposes:\n* `bootstrap` port. This port will be opened once first-time initialization has been completed.\n* `tokens_ready` port. This port signals that secret-store tokens have been provisioned and are valid.\n* `ready_to_run` port. This port will be opened once stateful services have completed initialization\nand it is safe for the majority of EdgeX core services to start.\nThe stateless EdgeX services should block on `ready_to_run` port.\n### Docker-specific service changes\n#### ""As-is"" startup flow\nThe following diagram shows the ""as-is"" startup flow.\n![AS-IS](./bootstrap-as-is.jpg)\nThere are several components being removed via activity unrelated with this ADR.\nThese proposed edits are shown to reduce clutter in the TO-BE diagram.\n* secrets-setup is being eliminated through a separate ADR to eliminate TLS for single-node usage.\n* kong-migrations is being combined with the kong service via an entrypoint script.\n* bootstrap-redis will be incorporated into the Redis entrypoint script to set the Redis password before Redis starts to fix the time delay before a Redis password is set.\n#### ""To-be"" startup flow\nThe following diagram shows the ""to-be"" startup flow.\nNote that the bootstrap flows are always processed,\nbut can be short-circuited.\n![TO-BE](./bootstrap-to-be.jpg)\nAnother difference to note in the ""to-be"" diagram is that\nthe Vault depdendency on Consul is reversed\n[in order to provide better security](https://github.com/edgexfoundry/edgex-docs/issues/296).\n#### New Bootstrap/RTR container\nThe purpose of this new container is to:\n* Inject entrypoint scripts into third-party containers\n(such as Vault, Redis, Consul, PostgreSQL, Kong)\nin order to perform first-time initialization and\nwait on service dependencies\n* Raise the `bootstrap` semaphore\n* Wait on dependent semaphores required to raise the `ready_to_run` semaphore\n(these are the stateful components such as databases,\nand blocking waiting for sercret store tokens to be provisioned)\n* Raise the `ready_to_run` semaphore\n* Wait forever (in order to leave TCP sockets open)\n","**Decision:**\n\n**Adopt a ""To-be"" startup flow that incorporates a new Bootstrap/RTR container.**\n\n## Stage-gate mechanism\n\nThe ""As-is"" startup flow will be replaced with a ""To-be"" startup flow that includes the following stage-gate mechanism:\n\n1. **Bootstrap**: The Bootstrap container is responsible for initializing the system and provisioning secrets.\n2. **RTR**: The RTR container is responsible for starting the main application and managing its lifecycle.\n3. **Stage-gate**: The stage-gate mechanism ensures that each container is ready before starting the next one.\n\nDocker-specific service changes are required to implement the stage-gate mechanism.\n\n## Docker-specific service changes\n\nThe following Docker-specific service changes are required:\n\n- **Create a new Bootstrap container**: The Bootstrap container will be created with an image that includes the necessary tools and scripts to initialize the system and provision secrets.\n- **Create a new RTR container**: The RTR container will be created with an image that includes the main application and the necessary tools and scripts to manage its lifecycle.\n- **Modify the Docker service definition**: The Docker service definition will be modified to include the Bootstrap and RTR containers and the stage-gate mechanism.\n\n## New Bootstrap/RTR container\n\nThe new Bootstrap/RTR container will be responsible for the following tasks:\n\n- **Bootstrap**:\n    - Initialize the system\n    - Provision secrets\n- **RTR**:\n    - Start the main application\n    - Manage the lifecycle of the main application\n\nThe Bootstrap/RTR container will be created using an image that includes the necessary tools and scripts to perform these tasks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn security-enabled EdgeX,\nthere is a component called `security-secretstore-setup`\nthat seeds authentication tokens for Hashicorp Vault--EdgeX's secret store--into\ndirectories reserved for each EdgeX microservice.\nThe implementation is provided by a sub-component, `security-file-token-provider`,\nthat works off of a static configuration file (`token-config.json`)\nthat configures known EdgeX services,\nand an environment variable that lists additional services that require tokens.\nThe token provider creates a unique token for each service\nand attaches a custom policy to each token that limits token access\nin a manner that paritions the secret store's namespace.\nThe current solution has some problematic aspects:\n* These tokens have an initial TTL of one hour (1h)\nand become invalid if not used and renewed within that time period.\nIt is not possible to delay the start of EdgeX services until a later time\n(that is, greater than the default token TTL),\nas they will not be able to connect to the EdgeX secret store\nto obtain required secrets.\n* Transmission of the authentication token requires one or more shared file systems\nbetween the service and `security-secretstore-setup`.\nIn the Docker implementation,\nthis shared file system is constructed by bind-mounting a host-based directory\nto multiple containers.\nThe snap implementation is similar, utilizing a content-interface between snaps.\nIn a Kubernetes implementation limited to a single worker node,\na CSI storage driver that provided RWO volumes would suffice.\n* The current approach cannot support distributed services\nwithout an underlying distributed file system to distribute tokens,\nsuch as GlusterFS, running across the participating nodes.\nFor Kubernetes, the requirement would be a remote shared file system\npersistent volume (RWX volume).\n\n## Decision\n","EdgeX will create a new service, `security-spiffe-token-provider`.\nThis service will be a mutual-auth TLS service that exchanges\na [SPIFFE](https://spiffe.io/) X.509 SVID for a secret store token.\nAn SPIFFE identifier is a URI of the format `spiffe://trust domain/workload identifier`.\nFor example: `spiffe://edgexfoundry.org/service/core-data`.\nA SPIFFE Verifiable Identity Document (SVID) is a cryptographically-signed\nversion of a SPIFFE ID, typically a X.509 certificate with\nthe SPIFFE ID encoded into the `subjectAltName` certificate extension,\nor a JSON web token (encoded into the `sub` claim).\nThe EdgeX implementation will use a naming convention on\nthe path component, such as the above, in order to be\nable to extract the requesting service from the SPIFFE ID.\nThe SPIFFE token provider will take three parameters:\n1. An X.509 SVID used in mutual-auth TLS for the token provider\nand the service to cross-authenticate.\n2. The reqested service key.  If blank, the service key will\ndefault to the service name encoded in the SVID.\nIf the service name follows the pattern `device-(name)`,\nthen the service key must follow the format\n`device-(name)` or `device-name-*`.\nIf the service name is `app-service-configurable`,\nthen the service key must follow the format `app-*`.\n(This is an accomodation for the Unix workload attester\nnot being able to distingish workloads that are launched\nusing the same executable binary.\nCustom app services that support multiple instances\nwon't be supported unless they name the executable\nthe same as the standard app service binary or\nmodify this logic.)\n3. A list of ""known secret"" identifiers that will allow\nnew services to request database passwords or\nother ""known secrets"" to be seeded into their service's\npartition in the secret store.\nThe `go-mod-secrets` module will be modified to enable a new mode\nwhereby a secret store token is obtained by:\n1. Obtaining an X.509 SVID by contacting a local SPIFFE agent's\nworkload API on a local Unix domain socket.\n2. Connecting to the `security-spiffe-token-provider` service\nusing the X.509 SVID to request a secret store token.\nThe SPIFFE authentication mode will be an opt-in feature.\nThe SPIFFE implementation will be user-replaceable;\nspecifically, the workload API socket will be configurable,\nas well as the parsing of the SPIFFE ID.\nReasons for doing so might include: changing the name of\nthe trust domain in the SPIFFE ID, or moving the SPIFFE\nserver out of the edge.\nThis feature is estimated to be a ""large"" or ""extra large""\neffort that could be implemented in a single release cycle.\n### Technical Architecture\n![SPIFFE Architecture and Workflow](0020-spiffe-architecture.jpg)\nThe work flow is as follows:\n1. Create a root CA for the SPIFFE user to use for creation of sub-CA's.\n1. The SPIFFE server is started.\n1. The server creates a sub-CA for issuing new identities.\n1. The trust bundle (certificate authority) data is exported from the SPIFFE server\nand stored on a shared volume readable by other EdgeX microservices\n(i.e. the existing secrets volume used for sharing secret store tokens).\n1. A join token for the SPIFFE agent is created using `token generate`\nand shared to the EdgeX secrets volume.\n1. Workload entries are loaded into the SPIFFE server database,\nusing the join-identity of the agent created in the previous step\nas the parent ID of the workload.\n1. The SPIFFE agent is started with the join token\ncreated in a previous step to add it to the cluster.\n1. Vault is started and `security-secret-store-setup`\ninitializes it and creates an admin token for `security-spiffe-token-provider` to use.\n1. The `security-spiffe-token-provider` service is started.\nIt obtains an SVID from the SIFFE agent and uses it as a TLS server certificate.\n1. An EdgeX microservice starts and obtains another SVID from the SPIFFE agent\nand uses it as a TLS client certificate to contact the\n`security-spiffe-token-provider` service.\nThe EdgeX microservice uses the trust bundle as a server CA\nto verify the TLS certificate of the remote service.\n1. `security-spiffe-token-provider` verifies the SVID using the trust bundle as client CA\nto verify the client,\nextracts the service key,\nand issues an appropriate Vault service token.\n1. The EdgeX microservice accesses Vault as usual.\n#### Workload Registration and Agent Sockets\nThe server uses a workload registration Unix domain socket that allows\nauthorization entries to be added to the authorization database.\nThis socket is protected by Unix file system permissions to control\nwho is allowed to add entries to the database.\nIn this proposal, a subcommand will be added to the EdgeX `secrets-config`\nutility to simplify the process of registering new services\nthat uses the registration socket above.\nThe agent uses a workload attesation Unix domain socket that\nis open to the world.  This socket is shared via a snap content-interface\nof via a shared host bind mount for Docker.  There is one agent per node.\n#### Trust Bundle\nSVID's must be traceable back to a known issuing authority (certificate authority)\nto determine their validity.\nIn the proposed implementation, we will generate a CA on first boot and store it persistently.\nThis root CA will be distributed as the trust bundle.\nThe SPIFFE server will then generate a rotating sub-CA for issuing SVIDs,\nand the issued SVID will include both the leaf certificate and the intermediate certificate.\nThis implementation differs from the default implementation,\nwhich uses a transient CA that is rotated periodically\nand that keeps a log of past CA's.\nThe default implementation is not suitable because only the Kubernetes\nreference implementation of the SPIRE server has a notification hook\nthat is invoked when the CA is rotated.\nCA rotation would just result in issuing of SVIDs that are not\ntrusted by microservices that received only the initial CA.\nThe SPIFFE implementation is replaceable.\nThe user is free to replace this default implementation with\npotentally a cloud-based SPIFFE server and a cloud-based CA.\n#### Workload Authorization\nWorkloads are authenticated by connecting to the `spiffe-agent`\nvia a Unix domain socket, which is capable of identifying\nthe process ID of the remote client.\nThe process ID is fed into one of following workload attesters,\nwhich gather additional metadata about the caller:\n* The Unix workload attester gathers UID, GID, path, and SHA-256 hash of the executable.\nThe Unix workload attester would be used native services and snaps.\n* The Docker workload attester gathers container labels\nthat are added by docker-compose when the container is launched.\nThe Docker workload attester would be used for Docker-based EdgeX deployments.\nAn example label is `docker:label:com.docker.compose.service:edgex-core-data`\nwhere the service label is the key value in the `services` section of the `docker-compose.yml`.\nIt is also possible to refer to labels built-in to the container image.\n* The Kubernetes workload attester gathers a wealth of pod and container metadata.\nOnce authenticated, the metadata is sent to the SPIFFE server\nto authorize the workload.\nWorkloads are authorized via an authorization database\nconnected to the SPIFFE server.\nSupported databases are SQLite (default), PostgreSQL, and MySQL.\nDue to startup ordering issues, SQLite will be used.\n(Disclaimer: SQlite, according for the\n[Turtle book](https://thebottomturtle.io/Solving-the-bottom-turtle-SPIFFE-SPIRE-Book.pdf)\nis intended for development and test only.\nWe will use SQlite anyway because because Redis is not supported.)\nThe only service that needs to be seeded to the database as this time\nis `security-spiffe-token-provier`.  For example:\n```sh\nspire-server entry create -parentID ""${local_agent_svid}"" -dns edgex-spiffe-token-provider -spiffeID ""${svid_service_base}/edgex-spiffe-token-provider"" -selector ""docker:label:com.docker.compose.service:edgex-spiffe-token-provider""\n```\nThe above command associates a SPIFFE ID with a _selector_,\nin this case, a container label, and configures a DNS\nsubjectAltName in the X.509 certificate for server-side TLS.\nA snap-based installation of EdgeX would use a `unix:path` or `unix:sha256` selector instead.\nThere are two extension mechanims for authorization additional workloads:\n1. Inject a config file or environment variable to authorize additional workloads.\nThe container will parse and issue `spire-server entry create` commands\nfor each additional service.\n2. Run the `edgex-secrets-config` utility\n(that will wrap the `spire-server entry create` command)\nfor ad-hoc authorization of new services.\nThe authorization database is persistent across reboots.\n","Implement a token renewal service to address the issues with the static token file approach:\n\n* **Replace the static token file with a token renewal service.** This service will generate new tokens for services on demand, and will renew tokens before they expire. This will eliminate the need for a static token file and the associated problems with it.\n* **Use a distributed key-value store to store tokens.** This will allow tokens to be accessed from any node in the cluster, eliminating the need for a shared file system.\n* **Implement a token renewal service that is highly available.** This will ensure that tokens are always available, even if one or more nodes in the cluster fails."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR presents a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.\n\n## Decision\n,"1. Use standardized command-line args/flags\n| Argument/Flag      | Description |\n| ----------- | ----------- |\n| `-d`, `--debug`      | show additional output for debugging purposes (e.g. REST URL, request JSON, …). This command-line arg will replace -v, --verbose and will no longer trigger output of the response JSON (see -j, --json).       |\n| `-j`, `--json`   | output the raw JSON response returned by the EdgeX REST API and *nothing* else. This output mode is used for script-based usage of the client.    |\n| `--version`   | output the version of the client and if available, the version of EdgeX installed on the system (using the version of the metadata data service)   |\n2. Restructure the Go code hierarchy to follow the [most recent recommended guidelines](https://github.com/golang-standards/project-layout). For instance /cmd should just contain the main application for the project, not an implementation for each command - that should be in /internal/cmd\n3. Take full advantage of the features of the underlying command-line library, [Cobra](https://github.com/spf13/cobra), such as tab-completion of commands.\n4. Allow overlap of command names across services by supporting an argument to specify the service to use: `-m/--metadata`, `-c/--command`, `-n/--notification`, `-s/--scheduler` or `--data` (which is the default). Examples:\n- `edgex-cli ping --data`\n- `edgex-cli ping -m`\n- `edgex-cli version -c`\n5. Implement all required V2 endpoints for core services\n**Core Command**\n- **`edgex-cli command`** `read | write | list`\n**Core Data**\n- **`edgex-cli event`** `add | count | list | rm | scrub**`\n- **`edgex-cli reading`** `count | list`\n**Metadata**\n- **`edgex-cli device`**  `add | adminstate | list | operstate | rm | update`\n- **`edgex-cli deviceprofile`**  `add | list | rm | update`\n- **`edgex-cli deviceservice`** ` add | list | rm | update`\n- **`edgex-cli provisionwatcher`**  `add | list | rm | update`\n**Support Notifications**\n- **`edgex-cli notification`** `add | list | rm`\n- **`edgex-cli subscription`** `add | list | rm`\n**Support Scheduler**\n- **`edgex-cli interval`** `add | list | rm | update`\n**Common endpoints in all services**\n- **`edgex-cli version`**\n- **`edgex-cli ping`**\n- **`edgex-cli metrics`**\n- **`edgex-cli status`**\nThe commands will support arguments as appropriate. For instance:\n- `event list` using `/event/all` to return all events\n- `event list --device {name}` using `/event/device/name/{name}` to return the events sourced from the specified device.\n6.  Currently, some commands default to always displaying GUIDs in objects when they're not really needed. Change this so that by default GUIDs aren't displayed, but add a flag which causes them to be displayed.\n7. **scrub** may not work with Redis being secured by default. That might also apply to the top-level `db` command (used to wipe the entire db). If so, then the commands will be disabled in secure mode, but permitted in non-secure mode.\n8. Have built-in defaults with port numbers for all core services and allow overrides, avoiding the need for static configuration file or configuration provider.\n9. *(Stretch)* implement a `-o`/`--output` argument which could be used to customize the pretty-printed objects (i.e. non-JSON).\n10. *(Stretch)* Implement support for use of the client via the API Gateway, including being able to connect to a remote EdgeX instance. This might require updates in go-mod-core-contracts.\n",Create a new version of edgex-cli that supports the new V2 REST APIs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA redesign of the EdgeX Foundry API is proposed for the Geneva release. This is understood by the community to warrant a 2.0 release that will not be backward compatible. The goal is to rework the API using solid principles that will allow for extension over the course of several release cycles, avoiding the necessity of yet another major release version in a short period of time.\nBriefly, this effort grew from the acknowledgement that the current models used to facilitate requests and responses via the EdgeX Foundry API were legacy definitions that were once used as internal representations of state within the EdgeX services themselves. Thus if you want to add or update a device, you populate a full device model rather than a specific Add/UpdateDeviceRequest. Currently, your request model has the same definition, and thus validation constraints, as the response model because they are one and the same! It is desirable to separate and be specific about what is required for a given request, as well as its state validity, and the bare minimum that must be returned within a response.\nFollowing from that central need, other considerations have been used when designing this proposed API. These will be enumerated and briefly explained below.\n**1.) Transport-agnostic**\nDefine the request/response data transfer objects (DTO) in a manner whereby they can be used independent of transport. For example, although an OpenAPI doc is implicitly coupled to HTTP/REST, define the DTOs in such a way that they could also be used if the platform were to evolve to a pub/sub architecture.\n**2.) Support partial updates via PATCH**\nGiven a request to, for example, update a device the user should be able to update only some properties of the device. Previously this would require an endpoint for each individual property to be updated since the ""update device"" endpoint, facilitated by a PUT, would perform a complete replacement of the device's data. If you only wanted to update the LastConnected timestamp, then a separate endpoint for that property was required. We will leverage PATCH in order to update an entity and only those properties populated on the request will be considered. Properties that are missing or left blank will not be touched.\n**3.) Support multiple requests at once**\nEndpoints for the addition or updating of data (POST/PATCH) should accept multiple requests at once. If it were desirable to add or update multiple devices with one request, for example, the API should facilitate this.\n**4.) Support multiple correlated responses at once**\nFollowing from #3 above, each request sent to the endpoint must result in a corresponding response. In the case of HTTP/REST, this means if four requests are sent to a POST operation, the return payload will have four responses. Each response must expose a ""code"" property containing a numeric result for what occurred. These could be equivalent to HTTP status codes, for example. So while the overall call might succeed, one or more of the child requests may not have. It is up to the caller to examine each response and handle accordingly.\nIn order to correlate each response to its original request, each request must be assigned its own ID (in GUID format). The caller can then tie a response to an individual request and handle the result accordingly, or otherwise track that a response to a given request was not received.\n**5.) Use of 207 HTTP Status (Multi-Result)**\nIn the case where an endpoint can support multiple responses, the returned HTTP code from a REST API will be 207 (Multi-status)\n**6.) Each service should provide a ""batch"" request endpoint**\nIn addition to use-case specific endpoints that you'd find in any REST API, each service should provide a ""batch"" endpoint that can take any kind of request. This is a generic endpoint that allows you to group requests of different types within a single call. For example, instead of having to call two endpoints to get two jobs done, you can call a single endpoint passing the specific requests and have them routed appropriately within the service. Also, when considering agnostic transport, the batch endpoint would allow for the definition and handling of ""GET"" equivalent DTOs which are now implicit in the format of a URL.\n**7.) GET endpoints returning a list of items must support pagination**\nURL parameters must be supported for every GET endpoint to support pagination. These parameters should indicate the current page of results and the number of results on a page.\n\n## Decision\n","Commnunity has accepted the reasoning for the new API and the design principles outlined above. The approach will be to gradually implement the V2 API side-by-side with the current V1 APIs. We believe it will take more than a single release cycle to implement the new specification. Releases of that occur prior to the V2 API implementation completion will continue to be major versioned as 1.x. Subsequent to completion, releases will be major versioned as 2.x.\n","**Adopt the proposed API redesign for the Geneva release.**\n\n**Reasons:**\n\n* **Improved Extensibility:** The new API uses solid principles that will allow for extension over multiple release cycles, preventing the need for frequent major release updates.\n* **Transport Agnostic:** The data transfer objects (DTOs) are defined independently of transport, allowing for flexibility in evolving from an HTTP/REST architecture to a pub/sub architecture.\n* **Partial Updates with PATCH:** The API supports partial updates via PATCH, enabling users to update only specific properties of an entity.\n* **Multi-Request Handling:** Endpoints support handling multiple requests at once, improving efficiency.\n* **Multi-Response Tracking:** Each request is assigned a unique ID, allowing callers to correlate responses with their respective requests.\n* **Batch Request Support:** Each service provides a ""batch"" endpoint that can handle any type of request, simplifying integration.\n* **Pagination:** GET endpoints returning item lists support pagination, allowing for easier handling of large result sets."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n- [Context](#context)\n- [Decision](#decision)\n- [Consequences](#consequences)\n<!--te-->\n\n## Decision\n,"- [Consequences](#consequences)\n<!--te-->\n### DeviceProfile extension\nThe permitted values of the `Type` field in `PropertyValue` are extended to include:\n""BoolArray"", ""Uint8Array"", ""Uint16Array"", ""Uint32Array"", ""Uint64Array"", ""Int8Array"", Int16Array"", ""Int32Array"", ""Int64Array"", ""Float32Array"", ""Float64Array""\n### Readings\nIn the API (v1 and v2), `Reading.Value` is a string representation of the data. If this is maintained, the representation for Array types will follow the JSON array syntax, ie `[""value1"", ""value2"", ...]`\n","Sorry, I can't provide a Decision for the Context given below, as there is no Decision provided in the text provided."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOut of the proposal for releasing on time, the community suggested that we take a closer look at feature-flags.\nFeature-flags are typically intended for users of an application to turn on or off new or unused features. This gives user more control to adopt a feature-set at their own pace – i.e disabling store and forward in App Functions SDK without breaking backward compatibility.\nIt can also be used to indicate to developers the features that are more often used than others and can provided valuable feedback to enhance and continue a given feature. To gain that insight of the use of any given feature, we would require not only instrumentation of the code but a central location in the cloud (i.e a TIG stack) for the telemetry to be ingested and in turn reported in order to provide the feedback to the developers. This becomes infeasible primarily because the cloud infrastructure costs, privacy concerns, and other unforeseen legal reasons for sending “Usage Metrics” of an EdgeX installation back to a central entity such as the Linux Foundation, among many others. Without the valuable feedback loop, feature-flags don’t provide much value on their own and they certainly don’t assist in increasing velocity to help us deliver on time.\nPutting aside one of the major value propositions listed above, feasibility of a feature flag “module” was still evaluated. The simplest approach would be to leverage configuration following a certain format such as FF_[NewFeatureName]=true/false. This is similar to what is done today. Turning on/off security is an example, turning on/off the registry is another. Expanding this further with a module could offer standardization of controlling a given feature such as `featurepkg.Register(“MyNewFeature”)` or `featurepkg.IsOn(“MyNewFeature”)`. However, this really is just adding complexity on top of the underlying configuration that is already implemented. If we were to consider doing something like this, it lends it self to a central management of features within the EdgeX framework—either its own service or possibly added as part of the SMA. This could help address concerns around feature dependencies and compatibility. Feature A on Service X requires Feature B and Feature C on Service Y. Continuing down this path starts to beget a fairly large impact to EdgeX for value that cannot be fully realized.\n\n## Decision\n","The community should NOT pursue a full-fledged feature flag implementation either homegrown or off-the-shelf.\nHowever, it should be encouraged to develop features with a wholistic perspective and consider leveraging configuration options to turn them on/off. In other words, once a feature compiles, can work under common scenarios, but perhaps isn’t fully tested with edge cases, but doesn’t impact any other functionality, should be encouraged.\n","Do not implement feature-flags at this time. Instrumentation and feedback loop is not feasible which is a core value proposition of feature-flags. Without this, the additional complexity of a feature-flag module is not warranted."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome customers who operate in regulated industries or offer SaaS products to their end customers, require Zero Trust environments; meaning that 3rd parties (including Microsoft Partners) should not have access to any environment which contains PII, financial or medical data. This requirement is often backed by legal contracts with the customer's end-customer, who are often also regulated. Although this offers benefits to the 3rd parties around mitigating risk and liability, and limits GDPR responsibilities, it causes problems with access rights based attribution of Azure Consumed Revenue (ACR), required for partner competency consumption targets.\nThe process for creating a Zero Trust Partner Admin Link is as follows:\n1) The Partner shares their MPN ID with the Customer.\n2) The Customer exports their list of subscriptions as a CSV, which then can then edited to remove any non-relevant subscriptions.\n3) The Customer (with the relevant permissions) creates an AAD Application that is linked to The Partner's MPN ID and retains complete control over the associated credentials that need not be shared with The Partner.\n\n## Decision\n","We can automate this process by encapsulating it in two PowerShell Cmdlets packaged as a module available from the [PowerShell Gallery](https://www.powershellgallery.com/).\n`Export-CustomerSubscriptionsAsCsvForPartnerAdminLink` - run by The Customer to generate a CSV file listing their Azure Subscriptions. This can be edited to remove non-applicable subscriptions.\n`Set-ZeroTrustPartnerAdminLink` - run by The Customer to create a dedicated AAD Application. Then assigns this Application the ""Contributor Role"" to each of the subscriptions listed in the CSV file, and then uses the `AzManagementPartner` module to link the Application to The Partner's MPN Id to complete the Partner Admin Link.\n","**Decision:** Implement a self-service portal for customers to create Zero Trust Partner Admin Links.\n\n**Rationale:**\n\nThe self-service portal will streamline the process for creating Zero Trust Partner Admin Links, making it easier for customers and partners to comply with regulatory requirements while meeting their business needs. The portal will allow customers to:\n\n* Share their MPN ID with the partner\n* Export their list of subscriptions as a CSV\n* Create an AAD Application that is linked to the partner's MPN ID and retain complete control over the associated credentials\n\nThis solution will:\n\n* **Reduce the time and effort required** to create Zero Trust Partner Admin Links.\n* **Eliminate the need for manual intervention** by Microsoft or the partner.\n* **Provide customers with greater control** over the process and the associated credentials.\n* **Help ensure compliance** with regulatory requirements.\n\n**Consequences:**\n\n* The self-service portal will require development and maintenance resources.\n* Customers will need to be trained on how to use the portal.\n* The portal will need to be secure and auditable to meet regulatory requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA list of IDs retrieved from Azure Storage is used to seed the ETL scrape.\nThe `modifiedsince` end point in syndication is used to detect when records have changed, or new records\nhave been added, enabling an incremental scrape of data.\nThe `modifiedsince` endpoint does not currently include records that have been removed, hidden, or unhidden.\nRecords may also be added much later than the modification date, i.e. a record modified in January may not\nbe published to Syndication until March. In these cases the record changes will not be reflected in the ETL.\n\n## Decision\n","The `modifiedsince` end point will only be used to detect newly added pharmacies, which will be added to the\nseed ID list.\nAll entries in the seed ID list will be refreshed overnight. If an ID has been deleted, the details will be recorded\nin the summary file as reporting a 404 error, and the record will not be present in the output JSON.\n",Implement a polling mechanism that checks for recent modification of records outside of the current modifiedSince timestamp to ensure records are not missed.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe output from the ETL is only available in the container and needs to be exposed to consuming applications.\n\n## Decision\n,"When the ETL has completed the output will be stored in an Azure blob, Azure being the current preferred cloud platform.\nThe output file will be exposed on a publicly available URL.\n",**Decision:** Expose the ETL output as a REST API endpoint.\n\n**Rationale:**\n\n* This approach provides a consistent and standardized way for consuming applications to access the ETL output.\n* It allows for easy integration with a wide range of applications and tools.\n* It provides a clear and well-defined interface for data access.\n* It enables the ETL output to be easily consumed by both internal and external applications.\n\n**Alternatives Considered:**\n\n* **Expose the ETL output via a message queue:** This approach would require consuming applications to subscribe to the message queue and implement logic to process the messages. It would also introduce additional complexity in terms of message handling and error handling.\n* **Create a separate data storage layer:** This approach would require creating a separate data store to store the ETL output. It would add additional overhead in terms of data management and maintenance.\n* **Provide direct access to the container filesystem:** This approach would not provide a well-defined interface for data access and would be difficult to manage and secure.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe [NHS Organisation API](http://api.nhs.uk/organisations) holds information about pharmacies.\nThe [Connecting to Services](https://github.com/nhsuk/connecting-to-services) applications needs to make use of Pharmacy data.\n\n## Decision\n,The NHS Organisation API will be scraped nightly to generate a file containing pharmacy data.\n,**Decision:** To use the NHS Organisation API to retrieve pharmacy data for the Connecting to Services applications.\n\n**Rationale:**\n\n* The NHS Organisation API is the authoritative source of information about pharmacies in the NHS.\n* The API provides a consistent and reliable way to access pharmacy data.\n* Using the API will reduce the risk of data duplication and inconsistency.\n* The API is well-documented and easy to use.\n\n**Alternatives Considered:**\n\n* **Scraping the NHS website:** This would be a manual and error-prone process.\n* **Using a third-party data provider:** This would be more expensive than using the NHS Organisation API.\n* **Developing our own API:** This would be a time-consuming and expensive process.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\n\n## Decision\n","The ETL will be re-entrant, storing state on a regular basis.\nRestarting the ETL will pick up from the last point.\n",**Decision:** Implement a checkpointing mechanism within the ETL process to allow for resuming execution in the event of an interruption.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nData structure may change between releases of the ETL.\nThe ETL uses a scrape of previous data to reduce unnecessary work.\n\n## Decision\n,The major and minor version will be included in the seed data file to identify a change of data structure.\n,Create an immutable store of the data structure.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","Use an Architectural Decision Record (ADR) template to document architectural decisions. The ADR should include the following sections:\n- **Title:** A short, descriptive title for the decision.\n- **Date:** The date the decision was made.\n- **Status:** The current status of the decision (e.g., proposed, accepted, rejected).\n- **Context:** A brief description of the context in which the decision was made.\n- **Decision:** A clear statement of the decision that was made.\n- **Rationale:** The rationale for the decision, including the key factors that were considered.\n- **Consequences:** A list of the potential consequences of the decision.\n- **References:** any relevant documentation or links to the decision."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose a language to in which to code this project.\n\n## Decision\n,Use Golang.\n,**Decision:**\n\nUse Python as the primary programming language for this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe point of the tool is to enforce good commit hygiene, which is easy to compromise while trying to just get good code out. We should pick a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility.\n\n## Decision\n","Use [Conventional Commits v1.0.0-beta.4]. It's similar enough to [Semantic Commits] and its inspiration [Angular Commits], but loose enough and working on being an independent standard.\n",Adopt the [Conventional Commits](https://www.conventionalcommits.org/) specification for standardizing commit messages.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome records are consistently followed and should be centralized for consistency.\n\n## Decision\n,Use jncmaguire/adr.\n,**Decision:** Centralize management of consistent records.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe planned to use so called [render prop](https://reactjs.org/docs/render-props.html) pattern to be able to swap out content of cells (or headers, or axes) in `PivotGrid`.\nThen we discovered that we need code like this:\n```tsx\nconst axes = {\nrow: ({ row, width, height }): { row: number; width: number; height: number } => {\nconst heightWithoutPadding = height - 2 * padding;\nconst yScale = useScaleBand({\nframe: pivotedFrame.row(row) as IteratableFrame<string>,\ncolumn: cityCursor,\nrange: [0, height],\n});\nreturn (\n<svg\nwidth={width}\nheight={heightWithoutPadding}\nviewBox={`0 0 ${width} ${heightWithoutPadding}`}\nstyle={{ margin: `${padding} 0` }}\n>\n<Axis scale={yScale} transform={`translate(${width}, -${padding})`} position=""left"" />\n</svg>\n);\n},\n};\n```\nbut we can't use hooks inside the render prop, so we [decided to switch from hooks to components](https://github.com/contiamo/operational-visualizations/pull/87).\nThen we discovered that we need code like this:\n```tsx\nconst axes = (\ndata: DataFrame<string>,\npivotedFrame: PivotFrame<string>,\ncategorical: string,\nmeasuresInRow: boolean,\n) => {\nconst Row = ({ row, measure, width, height }: { row: number; measure?: string; width: number; height: number }) => {\nconst scale = useScaleBandOrLinear(\nmeasuresInRow && !!measure\n? {\nframe: data,\ncolumn: data.getCursor(measure),\n```\nThis is because we need access to ""root"" `data` to get cursor (see previous discussion about cursors) and other params.\nI wonder if we overcomplicated our own life witout any benefit 🤔?\nMaybe it was premature optimisation to use hooks for scales, maybe it is very cheap to calculate it again and again (on React re-render)? Maybe we need to remove hooks and instead worry about caching of components?\nLimitation of not having `getCursor` on all substructures forces to pass root data everywhere. Either it shows that we are doing something wrong or this limitation was a bad idea.\nThis code was premature optimisation - I thought it would be more performant than do pattern matching on some tag field (like `.type` in Redux actions):\n```ts\ncase ""Empty"":\nif (prop.measure && prop.rowIndex === undefined) {\nreturn {\nmeasure: true,\n};\n} else if (prop.axis && prop.rowIndex === undefined) {\nreturn {\naxis: true,\n};\n} else {\nreturn {\nrowIndex: prop.rowIndex!,\n};\n}\n```\nBut it caused so much confusion in the code.\n\n## Decision\n","- [We added getCursor everywhere](https://github.com/contiamo/operational-visualizations/pull/93)\n- [We added data to every sub-component](https://github.com/contiamo/operational-visualizations/pull/96)\n- We didn't touch scale hooks for now, let's see how it goes\n- We undid some of premature optimisation and simplified code\n","**Decision:** Convert scales to class components, which will get data as props."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome decisions were taken about how data is organised and how we do caching, but they weren't explicitly documented. Let's fix that.\nWe have 3 methods of caching:\n- flru in PivotFrame\n- WeakMaps in stats utils\n- hooks for scales\n### Referential transparency\nReact relies a lot on **referential transparency** when it decides if it needs to rerender or not e.g. it compares objects as reference (so called shallow comparison), it doesn't compare structure. It is a very cheap operation, because underneath it compares pointers.\nFor example, if you compare primitive values:\n```ts\n1 === 1; //true\n```\nbut when you compare objects\n```ts\n{} === {}; // false\nconst a = {};\na === a; // true\n```\nEvery `{}` creates new instance of the object, and a new reference. Referential transparency is closely related to the ""purity"" of function.\nThis is a pure function:\n```ts\nconst add = (x: number, y: number) => x + y;\n```\nbut is the next one a pure function as well?\n```ts\nconst getPoint = (x: number) => new Point(x);\ngetPoint(0) === getPoint(0); // false\n```\nIt doesn't seem to have any side effects, but it produces a new instance every time. One of the solutions to this problem is to use memoisation so it would always return the same result\n```ts\nconst memoise = f => {\nconst cache = new Map();\nreturn x => {\nif (!cache.has(x)) cache.set(x, f(x));\nreturn cache.get(x);\n};\n};\nconst getPoint = memoise((x: number) => new Point(x));\ngetPoint(0) === getPoint(0); // true\n```\nThe problem here is that memory is not infinite, and we need to clean up it at some point. How are we going to do it? There are many approaches to this problem. We chose to store some number (1024 in our case for PivotFrame) of recently created references. It is ok, because we need to show only part of the PivotFrame (PivotGrid) on the screen (we use ""windowing"" technique). In practice it means the following\n```ts\nconst first = frame.column(0);\nframe.column(1);\n// frame.column(2) ... frame.column(500)\nfirst === frame.column(0); // true, still in cache\n// frame.column(500) ... frame.column(1024)\nfirst === frame.column(0); // false, evicted from cache\n```\nNot all operations are referentially transperent, for example DataFrames `pivot` is not.\n### Performance optimisation\nAnother point of caching is that you can trade computational cycles for memory e.g. once you have done a computation you can cache it and next time when you need it would be faster, but you need to store results in memory. For example, when we need to calculate maximum value of or total value of the column - we need to go through the whole data set so it's O(N) operations, which is not much from one side, but on the other side if you do this for every cell in PivotGrid (to calculate axes and on each React rerender) the whole cost can turn into O(N^2).\nCaching is easy to do with something like `memoise`, but the question is when and how to clean up cache. Imagine we load one data set, do a calculation, load another data set, do more calculations - we will run out of cache at some point.\nWe can employ a nice trick to solve this problem (especially if we deal with a referentially transparent system). We can store cache until the initial object for which we did the calculation is in memory. As soon as it goes away (swiped by Garbage Collector), we can drop the cache as well. To do this in JS we can use `WeakMap`.\n```ts\nconst weakMemoise = f => {\nconst cache = new WeakMap();\nreturn x => {\nif (!cache.has(x)) cache.set(x, f(x));\nreturn cache.get(x);\n};\n};\n```\nThe limitation of `WeakMap` is that keys can't be primitive values, but this is ok because we can use `FragmentFrame` and `DataFrame` as cache keys.\nInside each cache ""slot"" we can store an empty object, inside which we can store cache for each column, for each type of operation.\nSo cache of `max` of `firstColumn` of `frame`, would be stored as:\n```ts\ncache.get(frame)[firstColumn][""max""];\n```\nAs soon as `frame` is removed by GC, the whole slot (`cache.get(frame)`) will be removed as well.\nThis system works only with referentially transparent functions, so for example it will not work with FragmentFrames `groupBy`. (Maybe we need to change `groupBy` and return some kind of `GroupFrame` 🤔).\n### Hooks\nSome values, like scales, are hard to cache with the `WeakMap` style cache, because you have a lot of components so you will need really deep nesting in ""slot"" object (we already use 2 levels :/), which is hard to work with.\nTo solve such cases we decided to use hooks, which simplify caching (because in React they can attach cache to React tree node). The downside of this, is that we can't use render prop, we were forced to switch to components. See 0002-about-render-props for details.\n\n## Decision\n","So we have 3 approaches for caching, but we have room for improvement. For example, we can't have (or can we?) big number of arguments in stats functions.\nNot all of our methods are referentially transparent. We may want to address this later.\n","We will use the following caching methods:\n\n- For referentially transparent functions, we will use a Least Recently Used (LRU) cache with a capacity of 1024.\n- For non-referentially transparent functions, we will use a WeakMap cache.\n- For values that are difficult to cache with the WeakMap style cache, we will use hooks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we draw charts we iterate over the frame, as the result there are some issues with continuous charts (Line, Area):\n- it will draw lines in case of missing data (to prevent this we added special code)\n- there is a chance it will draw a ""twist"" (or tangle, or knot), because **there is no guarantee that points in Scale and in Frame are in the same order**\nThis is the same issue as reported in [#94](https://github.com/contiamo/operational-visualizations/issues/94).\n### uniqueValues issue\nBandScale is built from unqiue values of the frame (`uniqueValues(frame, column)`). Let's assume we have following frame\n| age  | city    |\n| ---- | ------- |\n| < 50 | Dresden |\n| < 50 | London  |\n| > 50 | Berlin  |\n| > 50 | Dresden |\nUnqiue values of the frame: `Dresden, London, Berlin`. But if we would group frame by `age` and take each fragment separately\n- `< 50`: `Dresden, London`\n- `> 50`: `Berlin, Dresden` (vs scale's `Dresden, London, Berlin`)\nIf we would iterate over the Frame and oreder of points in Scale and Frame is different (as example above), we would get ""twist"":\n```\n^              3        4\n|              *--------*--\n|               \\n|                \\n|      1          \\n|  ----*-----------*2\n|\n+---------------------------->\nA      B     C    D\n```\nThis is wrong, instead it should look like:\n```\n^             2         4\n|            /*\       *--\n|           /   \     /\n|          /     \   /\n|      1  /       \ /\n|  ----*-/         *3\n|\n+---------------------------->\nA      B     C    D\n```\n### uniqueValues solution\nThe only solution I see is to sort Frame by the same values we use for grouping and for graphing (e.g. in this case we would sort by age and city). This way we can sort unique values as well:\n```js\ncacheItem.unique = [...unique].sort();\n```\n_Note_: The default sort order is built upon converting the elements into strings, then comparing their sequences of UTF-16 code units values (for numbers it produce unexpected results).\nOn the other hand if we would want to sort the data on the server, then we can't use `[...unique].sort();`, because we wouldn't know which kind of sorting criteria server is using.\n### Missing data handling\nIf we would have consistent order of items in Frame and Scale we could iterate over Scale instead of Frame and this way we can easily track if there are missing data points or not.\n```ts\nconst xTicks = xScale.domain();\nlet offset = 0;\nxTicks.map((z, i) => {\nconst row = data.row(i + offset) || [];\nif (z !== x(row)) {\noffset -= 1;\nreturn; // undefined will be skipped by `.defined()` d3 function\n}\nreturn row;\n});\n```\n\n## Decision\n",Decision here...\n,"**Decision:**\n\nTo ensure consistent order of points between the scale and the frame, the frame should be sorted by the same values used for grouping and graphing, and the unique values should be sorted accordingly. Additionally, the iteration should occur over the scale instead of the frame to handle missing data points effectively."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe visualization library contains a `Labels` component, which can be used in the same way as all other axial chart renderers:\n```jsx\n<Chart>\n<Bars ... />\n<Labels ... />\n</Chart>\n```\n\n## Decision\n","However, the user can also display labels by passing a `showLabels` flag. This solves multiple problems:\n- In the case of stacked bars or area charts, using the above API does not take stacking into account, so the labels will be in the wrong places.\n- The `Labels` component only works for axial charts, not for pie charts, so `PieChart` would have needed a flag anyway - this ensures consistency across all renderers.\n- For scatter plots, the labels are now automatically offset by the radius of the dots. Any other renderer-specific styling that may be required is now also easier.\n",**Decision:** Upgrade the `Labels` to use the same API as all other axial chart renderers.\n\n**Rationale:**\n\n* Consistency: Using the same API for all axial chart renderers will make it easier for developers to use and understand the library.\n* Extensibility: Making the `Labels` component use the same API will make it easier to add new features and functionality in the future.\n* Performance: Upgrading the `Labels` component to use the same API as all other axial chart renderers will likely improve performance.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use `RawRow` in `frame` library to expose rows from data stored in the `frame`. At the moment `RawRow` implemented as `export type RawRow = any[];`. This approach exposes implementation details - that `frame` is row oriented storage. If we would want to swap implementation from row oriented to column oriented storage, we may need to change implementation details and it will break code for people who use our library. We need to hide implemntation details.\n### Row-oriented storage vs Column-oriented storage\nThe main difference between two storages is the cost of accessing rows and columns:\n|            | row-oriented | column-oriented |\n| ---------- | ------------ | --------------- |\n| get row    | O(1)         | O(N)            |\n| get column | O(N)         | O(1)            |\n(the simplified version, really depends on the implementation).\nAs well you can have some hybrid solution, like return cursors instead of raw data, to get data lazily.\nAt the moment we have all data loaded in the memory, so this distinction is not so critical, but as soon as we would want to switch implementation to some proxy which will read data from the remote server, this can be important.\n### Rename\nThe minimal improvement we can make is to rename `RawRow` to `RowCursor`, to shift away from mentality of row-oriented storage. To get the value from the table you would need to use two cursors, like x and y coordinates (or latitude and longitude):\n```ts\ncolumnCursor(rowCursor); // this is what we have right now\n// or\ngetValue(rowCursor, columnCursor);\n// or\nrowCursor(columnCursor);\n```\n### Hide implementation\nThe next step would be to hide implementation. We can:\n- wrap rows into objects, to hide the shape of the data\n- in Flow you can hide implementation with [opaque type](https://flow.org/en/docs/types/opaque-types/), maybe something similar is possible with interfaces in TS\n### Premature optimisation\nAt the moment it is possible to write this:\n```ts\nrowCursor[columnCursor.index];\n// which is the same as\ncolumnCursor(rowCursor);\n```\nAssumption was that array access is faster than function call (which inside still will do array access). It may happen that it was premature optimisation and it exposes a lot of implementation details.\nPossible middle-ground here would be [macros](https://github.com/Microsoft/TypeScript/issues/4892): developer would write `columnCursor(rowCursor);` but generated JS contains `rowCursor[columnCursor.index];`, so in case we would change implmentation we can as well change implementation of macro. And user of our library doesn't need to worry about implemntation details.\n\n## Decision\n","For now we only took first step (rename, in this PR). We will think about other steps in the future.\n","**Decision:** Hide the implementation of `RawRow` using opaque types in Flow or interfaces in TypeScript.\n\n**Rationale:**\n\nHiding the implementation of `RawRow` prevents exposure of internal data structures and allows for future changes in implementation without breaking existing code.\n\n**Action Items:**\n\n- Rename `RawRow` to `RowCursor` to convey its cursor-like functionality.\n- Implement opaque types or interfaces to wrap rows and hide the underlying data structure.\n- Explore the use of macros to optimize array access if desired, while maintaining implementation flexibility."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe tried to employ type system to prevent wrong combination of options for charts. For examples, bar chart expects one band and one continious scale:\n```ts\nexport type BaseAxialChartProps<Name extends string> =\n| {\nxScale: ScaleBand<string>;\nyScale: ScaleLinear<number, number>;\n}\n| {\nxScale: ScaleLinear<number, number>;\nyScale: ScaleBand<string>;\n};\n```\nBut this won't work, because TS can't work with disjoint union without actual discriminant, so we need to add tag (`metricDirection`):\n```ts\nexport type BaseAxialChartProps<Name extends string> =\n| {\nmetricDirection: ""vertical"";\nxScale: ScaleBand<string>;\nyScale: ScaleLinear<number, number>;\n}\n| {\nmetricDirection: ""horizontal"";\nxScale: ScaleLinear<number, number>;\nyScale: ScaleBand<string>;\n};\n```\nBut this resulted in clumsy code, which we refactored down to:\n```ts\nexport type BaseAxialChartProps<Name extends string> = {\nmetricDirection: ""vertical"" | ""horizontal"";\ncategorical: ScaleBand<string>;\nmetric: ScaleLinear<number, number>;\n};\n```\nWhich was ok-ish, except it confuses people who are not familiar with terminology (categorical, metric values). But then we got to scatter plot, which can take any combination of scales:\n```tsx\nexport interface DotsProps<Name extends string> {\nxScale: ScaleLinear<number, number> | ScaleBand<string>;\nyScale: ScaleLinear<number, number> | ScaleBand<string>;\n}\n```\nAnd now we want to refactor all chart to have consistent interface, so it would be easy to interchange them.\nPretty similar thing happens in `PivotGrid`:\n```tsx\n/**\n* We support text only pivot grid out of the box,\n* for this case you don't need to provide cell render prop, but you need to provide measures\n*/\ntype TextOnlyPivotGridProps<Name extends string> =\n| {\ntype?: ""text"";\nrowMeasures: Name[];\ncolumnMeasures?: undefined;\n}\n| {\ntype?: ""text"";\nrowMeasures?: undefined;\ncolumnMeasures: Name[];\n};\n/**\n* This is props for general PivotGrid, you need to provide cell render prop.\n* It can return any React component which will be rendered in cells\n*/\ntype GeneralPivotGridProps<Name extends string> =\n| {\ntype: ""general"";\ncell: (prop: CellPropsWithoutMeasure<Name>) => React.ReactElement | null;\n}\n| {\ntype: ""generalWithMeasures"";\nrowMeasures?: Name[];\ncolumnMeasures?: Name[];\ncell: (prop: CellPropsWithMeasure<Name>) => React.ReactElement | null;\n};\n```\n`type` is a tag for the disjoint union, which defines what combinations of parameters is supported. The problem is that it generates very confusing TypeScript errors, because it has huge type signature:\n```tsx\ntype Props<Name extends string = string> = (TextOnlyPivotGridProps<Name> | GeneralPivotGridProps<Name>) & {\n...\n}\n```\n\n## Decision\n","We want to avoid a static check and instead rely on dynamic type checking for charts:\n```tsx\nexport const Bars: DiscreteAxialChart<string> = ({ xScale, yScale }) => {\nif (isScaleBand(xScale) && isScaleContinuous(yScale)) {\nreturn <g>{bars}</g>;\n} else if (isScaleBand(yScale) && isScaleContinuous(xScale)) {\nreturn <g>{bars}</g>;\n} else {\nthrow new Error(""Unsupported case of scales"");\n}\n};\n```\nThere is a chance that the visualisation can crash at runtime in the user's face. We can be more graceful about this and replace `throw` with `return ""Error""` in production mode.\nMaybe later we will do the same change for `PivotGrid`.\n","We'll use a normal union for `BaseAxialChartProps` and introduce a new generic type `XY`:\n\n```typescript\nexport type XY = | ""x"" | ""y"";\n\nexport type BaseAxialChartProps<Name extends string> = {\n  metricDirection: ""vertical"" | ""horizontal"";\n  [x in XY]?: ScaleLinear<number, number> | ScaleBand<string>; // only one of those scales can be specified\n};\n```\n\nThis way, we avoid the need for a union of unions (which is confusing for TypeScript), and we also get type safety for the scale types.\n\nFor `PivotGrid`:\n\n```typescript\nexport type TextOnlyPivotGridProps<Name extends string> = {\n  type: ""text"";\n  rowMeasures?: Name[];\n  columnMeasures?: undefined;\n};\n\nexport type GeneralPivotGridProps<Name extends string> = {\n  type: ""general"";\n  cell: (prop: CellPropsWithoutMeasure<Name>) => React.ReactElement | null;\n};\n\nexport type PivotGridProps<Name extends string = string> = (\n  | TextOnlyPivotGridProps<Name>\n  | GeneralPivotGridProps<Name>\n) & {\n  // common props for both types\n};\n```\n\nThis way, we avoid the need for a disjoint union, and we also get type safety for the props."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\nCurrent implementation of **cursor** looks like this\n```tsx\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\n(row: RowCursor): ValueInRawRow;\ncolumn: Name;\nindex: number;\n}\n```\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\nCursor at the moment can be recieved from ""root"" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\n**Question** raised in [one of PR](https://github.com/contiamo/operational-visualizations/pull/70/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to ""root"" `DataFrame` along all derivative structures and ""proxy"" `getCursor` method call to it.\n\n## Decision\n","At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\n","**Decision**: Provide `getCursor` functionality in `FragmentFrame`.\n\n**Rationale**:\n\n* Simplifies code by providing a consistent way to access values in raw rows for all data structures.\n* Maintain referential transparency by caching cursors in the ""root"" `DataFrame`.\n* Avoids potential performance overhead associated with passing references to the ""root"" `DataFrame` along all derivative structures."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to provide a nicely ergonomic API to programs using Converge,\nand we wanted to adhere closely to the Clojure Way (TM).  We decided\nto use the Atom API of swap!/reset!/deref.\nWhen altering the value of the ConvergentRef via swap!/reset!, we need\nto create a covering set of Ops to describe the change being made.  We\nconsidered the following alternatives:\n1. Using `clojure.data/diff` on old-value and new-value, with metadata\nfor Id accounting\n2. Using [`differ`](https://github.com/Skinney/differ) on old-value\nand new-value, with metadata for Id accounting\n3. Using [`editscript`](https://github.com/juji-io/editscript) on\nold-value and new-value, with metadata for Id accounting\n4. Implementing the Map/Vector interfaces a la\n[Schism](https://github.com/aredington/schism/blob/master/src/schism/impl/types/nested_map.cljc)\nand do our tracking and accounting within those implementation\n\n## Decision\n","On initial analysis, it appears that #4 above is flawed, as\nimplementing our own tracking/accounting nested map/list types\nwouldn't maintain the necessary context to translate to an opset.\nSo that left us with a strategy based on diffing/patching. Based on\nanalysis in [this blog\npost](https://juji.io/blog/comparing-clojure-diff-libraries/), as well\nas our own usage testing with `clojure.data/diff`, `differ`, and\n`editscript`, we have decided to use\n[`editscript`](https://github.com/juji-io/editscript) for creating\nopset patches.\n","We decided to use [`editscript`](https://github.com/juji-io/editscript) on old-value and new-value, with metadata for Id accounting because it is the most efficient and flexible solution."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want a message broker so the devices and other applications can communicate with the backend.\nWhat broker and protocol should we use?\n## Decision Drivers <!-- optional -->\n- Applicability regarding IoT projects : low-resources clients, etc\n- Possibility to use it to stream frames/images coming from cars cameras\n- Ease of deployment on Kubernetes\n- Existing knowledge of the team\n\n## Decision\n","- Applicability regarding IoT projects : low-resources clients, etc\n- Possibility to use it to stream frames/images coming from cars cameras\n- Ease of deployment on Kubernetes\n- Existing knowledge of the team\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https://www.rabbitmq.com/mqtt.html)**, because:\n- It is already well-known among the team\n- It has some [existing ""official"" Helm chart](https://github.com/helm/charts/tree/master/stable/rabbitmq)\n- It seems like a good fit to iterate fast\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\n","Use Apache Kafka with AMQP as the protocol.\n\n## Rationale\n- Kafka is a popular message broker that is well-suited for IoT projects. It can handle large volumes of data from low-resources clients, and it can be used to stream frames/images coming from cars cameras.\n- Kafka is also easy to deploy on Kubernetes, and it has a large community of users and contributors.\n- The team has existing knowledge of Kafka, which will make it easier to implement and maintain."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want to code a backend that will handle inbound informations through the\nmessage broker, and expose them to the dashboard front-end.\nWhat programming language should we use?\n## Decision Drivers\n- Should be interesting (fun, new, ...) for people\n- Either already adopted by Xebians or not too hard to adopt based on existing\nknowledge\n- Library support for our usecases\n\n## Decision\n","- Should be interesting (fun, new, ...) for people\n- Either already adopted by Xebians or not too hard to adopt based on existing\nknowledge\n- Library support for our usecases\nChosen option: **""Go""**, because:\n- It has good library support for things such as **HTTP/2** and **Protobuf**\n(separately, we're not talking about gRPC  here). This will help establish\nstrong bidirectionnal and persistant streaming with the dashboard (HTTP/2).\nIt will also open the way to Protobuf serialized messages over the message\nbus that will maybe comme in addition to RabbitMQ.\n- People on the project want to play more with it!\n- It seems appropriate for small (micro)services like the one we're going to\nbuild\nHowever, we are **not excluding other languages** depending on usecases and\nwishes of people working on it. We want people to contribute so we won't force\nanyone to use a given language. **People are free to use the language they\nwant**, but we'll try to have **Go as a default main language as much as\npossible**.\n",Use Python
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe are deploying a RabbitMQ cluster on GKE and people will need to connect to\nit. We want to provide some kind of authentication so it's not totally\nopen-bar.\nWhat authentication mecanism should we use?\n\n## Decision\n,"Chosen option: **""User/Pawword""**, because:\n- We want to make the RabbitMQ cluster quickly available to teams\n- It will be easy to change to client certificate auth later on\n- It is better than not providing auth at all\nWe **will replace it by client certificates authentication later**.\n","**Decision:** Use client certificates for authentication.\n\n**Rationale:**\n\n* Client certificates provide a strong level of authentication by verifying the identity of the client based on their certificate.\n* They are easy to manage and can be revoked if necessary.\n* They do not require sharing secrets, which makes them more secure than shared secrets or passwords.\n* They are supported by RabbitMQ and GKE.\n\n**Consequences:**\n\n* Clients will need to have a valid client certificate to connect to the cluster.\n* The certificates will need to be managed and distributed to authorized clients.\n* The RabbitMQ cluster will need to be configured to use client certificates for authentication."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want to **deploy** pods, services, and other resources on **Kubernetes**,\nwhich is a GKE cluster for now.\nWhat tool should we use to do this?\n## Decision Drivers\n- Ease of use\n- How ""far"" is the tool from the Kubernetes lifecycle? - _""Is it trailing\nafter upstream and if so, how far away from it""_\n- Ability to express deployments in a descriptive ""as-code"" way\n- Ease of use in a CI/CD pipeline - _Does it require a local state? Weird\nrequirements? Auth to other systems?_\n- Reliability - _""Will the tool usage often lead to messed up\nservices/cluster?""_\n\n## Decision\n","- Ease of use\n- How ""far"" is the tool from the Kubernetes lifecycle? - _""Is it trailing\nafter upstream and if so, how far away from it""_\n- Ability to express deployments in a descriptive ""as-code"" way\n- Ease of use in a CI/CD pipeline - _Does it require a local state? Weird\nrequirements? Auth to other systems?_\n- Reliability - _""Will the tool usage often lead to messed up\nservices/cluster?""_\nChosen option: ""A mix of **kubectl** and **Helm**"", because:\n- `kubectl` is the native way of deploying to Kubernetes and it thus the most\ndocumented, widely-used, appropriate and up-to-date.\n- Helm fits best for complex stacks deployment and we're eager to try things\nsuch as Helm charts repositories!\nHelm will be used as an alternative to `kubectl` in a second part, when we'll\nramp up things and when we will need to deploy more complex stacks. The\nbeginning will be `kubectl` for our own services, and Helm for\ncomponents/middleware/databases with existing charts such as RabbitMQ!\nThis imply that the provided Kubernetes cluster(s) for this project will need\nto be **provisionned with Helm requirements such as Tiller**.\n",Helm
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWorking with microservices and a issues / Pull Requests based workflow, labels\nfor these issues and PRs are a must have and should be explicit about what the\nissue/PR is about.\nHowever, there are numerous labels that represent different kind of\ninformation (scope, lifecycle step, concerned component, etc). Colors help to\nidentify these different kind of labels. In the end, managing all of this\nmanually is painful, and implies duplicate work across multiple services\nrepositories for common labels (closing reasons, PR lifecycle, etc.)\nWe want to find a way to handle this cleanly.\n## Decision Drivers <!-- optional -->\n- How easy it will be to integrate new repositories\n- How much work we have to do to for minor changes\n\n## Decision\n","- How easy it will be to integrate new repositories\n- How much work we have to do to for minor changes\nChosen option: **""Automate GitHub repositories management with\ninfrastructure-as-code, namely Terraform""**, because:\n- We don't want to do things manually\n- Having everything as-code is cool\n- It will be easy to apply to many repositories\n- We already use Terraform for a bunch of stuff so it's known by people\nHowever, it will not be run automatically by our CI/CD pipelines at the\nbeginning, since Terraform needs an API access to GitHub and this would imply\ncreating a ""machine user"" GitHub account, which will be more thing to manage\nthan we want for now.\n","Establish a set of repositories that contain the labels to use across the different services, each of them associated with one kind of label (scope, lifecycle, component, etc). Then, in each service define only the labels that are really needed for the service, and that are not defined in the central repositories. When creating a label for a given kind, use the central repositories as a template. Do this with a separate pull request for each missing label.\n\nMost of the labels can be shared across all services. Labels that are specific to a group of services or even one service should only be defined in the repositories of those services.\n\nFor the purpose of this decision, define central repository as a repository that contains labels that are used across multiple services. All services should define only the labels that are really needed for the service, and that are not defined in the central repositories."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe are deploying **Kubernetes Services** of type `LoadBalancer` in order to\nexpose services such as RabbitMQ. This creates a Google Cloud Network Load\nBalancer on GCP, with the associated public IP address.\nIn order to **connect to services using a name** and not a raw IP address, we\nneed to be able to **create DNS records** automatically and in a clean way,\npointing on these GCLB.\n## Decision Drivers <!-- optional -->\n- Ease of use for developers writing their Kubernetes manifests to expose\nServices\n- Ability to plug on Google CloudDNS and potentially other providers\n- Must be dynamic\n\n## Decision\n","- Ease of use for developers writing their Kubernetes manifests to expose\nServices\n- Ability to plug on Google CloudDNS and potentially other providers\n- Must be dynamic\nChosen option: **""ExternalDNS""**, because:\n- Is directly uses **annotations** on Kubernetes Services and Ingress to know\nwhat records to add and to which IP they should point. Therefore, it enable\npeople to define their own records in their K8s manifests **without having to\ndefine anything in another repo**.\n- It is maintained, wealthy and keeps growing\n- It is clean and answer our need perfectly\nHowever, we still didn't test it for Ingresses.\n","**Decision:** We will use [ExternalDNS](https://github.com/kubernetes-sigs/external-dns) to manage DNS records for Kubernetes Services.\n\n**Rationale:**\n\n* ExternalDNS meets all of our decision drivers.\n* It is easy to use for developers, as it can be configured to automatically create and update DNS records for Services of type `LoadBalancer`.\n* It can be plugged into Google CloudDNS and other DNS providers.\n* It is dynamic, meaning that it will automatically update DNS records when Services are created, updated, or deleted.\n\n**Consequences:**\n\n* Using ExternalDNS will require us to install and configure the ExternalDNS controller in our Kubernetes cluster.\n* ExternalDNS will create and manage DNS records for all Services of type `LoadBalancer` in our cluster.\n* We will need to grant ExternalDNS the necessary permissions to create and update DNS records in our DNS provider."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want to describe our infrastructure in a textual format and use infrastructure-as-code tools to create it.\nWhat Infrastructure-as-Code tool should we use?\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* … <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* … <!-- numbers of drivers can vary -->\nChosen option: ""[option 1]"", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | … | comes out best (see below)].\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* …\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* …\n",Use Terraform as the Infrastructure-as-Code tool.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\n`BernoulliDistribution`.\nTwo possible solutions where suggested:\n1. Add an additional property that provides us with the correct class name\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\n\n## Decision\n,For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\nname.\n,2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome Ops (esp. convolution) have many parameters. Many of them can have reasonable defaults, but even then creating\nsignatures for evey reasonable configuration may be impossible, as those signatures would require different naming in\norder to be actually distinguishable from each other.\nIn other cases, an op may have a lot of same typed parameters that are required (e.g. GRU, LSTM, SRU) but it is very\neasy to mix them up.\nFor both of those cases (many optional parameters, easily mixed up required parameters) it is reasonable to use a\nconfig holder with builder pattern in languages that do not support named or default parameters.\nIn our current codebase those configurations are often used across several related ops.\n\n## Decision\n","We add a `Config(""name""){ ... }` section to the namespace context. It supports `Input` and `Arg` definitions in the same\nway that `Op` does.\nOps that want to use that config can use `useConfig(conf)`. As configs are often reused across related objects, this\nwill have the effect of a mixin: All inputs and args defined in that config will also be automatically defined on that\nOp. If there is a naming conflict, an exception will be thrown at construction time.\nFor default signatures, configs will be passed at the end, in the order that they were added to the Op.\nIf other signatures are desired, configs, like regular inputs and args, can be passed to `Signature`.\nIn languages that do not support default or named parameters, a config holder will be created, that will take the\nparameters of the config using a builder pattern. For languages with default and named parameters, no additional config\nholder will be created, and the parameters of the config will be treated as if they were directly configured on the Op.\n### Example\nThis example shows a very simple case in order to highlight how this feature would be used.\n```kotlin\nfun RNN() = Namespace(""RNN""){\nval sruWeights = Config(""SRUWeights""){\nInput(FLOATING_POINT, ""weights""){ description = ""Weights, with shape [inSize, 3*inSize]"" }\nInput(FLOATING_POINT, ""bias""){ description = ""Biases, with shape [2*inSize]"" }\n}\nOp(""SRU""){\nInput(FLOATING_POINT, ""x""){ description = ""..."" }\nInput(FLOATING_POINT, ""initialC""){ description = ""..."" }\nInput(FLOATING_POINT, ""mask""){ description = ""..."" }\nuseConfig(sruWeights)\nOutput(FLOATING_POINT, ""out""){ description = ""..."" }\n}\nOp(""SRUCell""){\nval x = Input(FLOATING_POINT, ""x""){ description = ""..."" }\nval cLast = Input(FLOATING_POINT, ""cLast""){ description = ""..."" }\nval conf = useConfig(sruWeights)\nOutput(FLOATING_POINT, ""out""){ description = ""..."" }\n// Just for demonstration purposes\nSignature(x, cLast, conf)\n}\n}\n```\n","Use a config holder with builder pattern for ops with many parameters, either optional or required, to avoid having to create multiple signatures for every reasonable configuration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn many cases ops have a similar interface. For example all transform ops take a single input, but some of them take\nadditional arguments; all pairwise ops take two inputs, and so on. The documentation of those ops is often the result\nof copy & paste with just a few little modifications, and changing anything later on suddenly becomes a huge undertaking\nbecause what should effectively be a change in a single place, has to be changed in many places.\nAnother issue that copy & paste based definitions bring to the table is that this practice effectively makes any\nrelationship between those ops implicit.\nWhen defining ops with the DSL we prefer to make things as explicit as possible, while also reducing repetition and\nboilerplate.\nThe existing inheritance mechanism, added without a formal proposal at the beginning of the project, allows the\ndefinition of an abstract base op. The new op based on it, can copy any of the given parts before it gets to define its\nown properties. This approach has the problem that we can't inherit from multiple base ops, and that we do not get any\ndirect access to its fields for ease of use.\n# Decision\nWe introduce an explicit mixin mechanism `Mixin(""name"") {...}` which can define any parts of any op, but isn't an Op\ndefinition on its own. Mixins can be defined at top-level, thereby being usable across namespaces.\nA mixin is mixed into an op with `useMixin(mixinReference, ...options...)` within an op context. It will add all (if\nnot otherwise configured) definitions of the mixin to the current op as if they were copied into its place.\nIf `useMixin(ref)` is used as the first thing within an op definition, then it will behave exactly like the old\ninheritance mechanism.\n`useMixin(ref)` returns a holder object, that can be used to reference its parameters.\nThe available options on `useMixin` are `keepInputs`, `keepArgs`, `keepOutputs`, `keepSignatures`, `keepDoc`,\n`keepConstraints`. They default to `true`.\nIf there is a naming conflict between mixins or between mixin and op definition, the last definition wins.\n### Example\n```kotlin\nval indexAccum = Mixin(""indexAccum""){\nlegacy = true\njavaPackage = ""org.nd4j.linalg.api.ops.impl.indexaccum""\nval input = Input(NUMERIC, ""in"") { description = ""Input variable"" }\nval keepDims = Arg(BOOL, ""keepDims"") { description = ""If true: keep the dimensions that are reduced on (as length 1). False: remove the reduction dimensions""; defaultValue = false }\nval dims = Arg(INT, ""dimensions""){ count = AtLeast(1); description = ""Dimensions to reduce over. If dimensions are not specified, full array reduction is performed"" }\nOutput(NUMERIC, ""output""){ description = ""Reduced array of rank (input rank - num dimensions)"" }\nSignature(input, dims)\nAllParamSignature(withOutput = false)\n}\nNamespace(""math""){\nOp(""firstIndex"") {\nval idxAccum = useMixin(indexAccum, keepSignatures=false)\nvar c = Arg(CONDITION, ""condition"") { description = ""Condition to check on input variable"" }\nSignature(idxAccum.input(""in""), c, idxAccum.arg(""dimensions""))\nSignature(idxAccum.input(""in""), c, idxAccum.arg(""keepDims""), idxAccum.arg(""dimensions""))\nDoc(Language.ANY, DocScope.ALL){\n""""""\nFirst index reduction operation.\nReturns a variable that contains the index of the first element that matches the specified condition (for each\nslice along the specified dimensions)\nNote that if keepDims = true, the output variable has the same rank as the input variable,\nwith the reduced dimensions having size 1. This can be useful for later broadcast operations (such as subtracting\nthe mean along a dimension).\nExample: if input has shape [a,b,c] and dimensions=[1] then output has shape:\nkeepDims = true: [a,1,c]\nkeepDims = false: [a,c]\n"""""".trimIndent()\n}\n}\n}\n```\n\n## Decision\n","We introduce an explicit mixin mechanism `Mixin(""name"") {...}` which can define any parts of any op, but isn't an Op\ndefinition on its own. Mixins can be defined at top-level, thereby being usable across namespaces.\nA mixin is mixed into an op with `useMixin(mixinReference, ...options...)` within an op context. It will add all (if\nnot otherwise configured) definitions of the mixin to the current op as if they were copied into its place.\nIf `useMixin(ref)` is used as the first thing within an op definition, then it will behave exactly like the old\ninheritance mechanism.\n`useMixin(ref)` returns a holder object, that can be used to reference its parameters.\nThe available options on `useMixin` are `keepInputs`, `keepArgs`, `keepOutputs`, `keepSignatures`, `keepDoc`,\n`keepConstraints`. They default to `true`.\nIf there is a naming conflict between mixins or between mixin and op definition, the last definition wins.\n### Example\n```kotlin\nval indexAccum = Mixin(""indexAccum""){\nlegacy = true\njavaPackage = ""org.nd4j.linalg.api.ops.impl.indexaccum""\nval input = Input(NUMERIC, ""in"") { description = ""Input variable"" }\nval keepDims = Arg(BOOL, ""keepDims"") { description = ""If true: keep the dimensions that are reduced on (as length 1). False: remove the reduction dimensions""; defaultValue = false }\nval dims = Arg(INT, ""dimensions""){ count = AtLeast(1); description = ""Dimensions to reduce over. If dimensions are not specified, full array reduction is performed"" }\nOutput(NUMERIC, ""output""){ description = ""Reduced array of rank (input rank - num dimensions)"" }\nSignature(input, dims)\nAllParamSignature(withOutput = false)\n}\nNamespace(""math""){\nOp(""firstIndex"") {\nval idxAccum = useMixin(indexAccum, keepSignatures=false)\nvar c = Arg(CONDITION, ""condition"") { description = ""Condition to check on input variable"" }\nSignature(idxAccum.input(""in""), c, idxAccum.arg(""dimensions""))\nSignature(idxAccum.input(""in""), c, idxAccum.arg(""keepDims""), idxAccum.arg(""dimensions""))\nDoc(Language.ANY, DocScope.ALL){\n""""""\nFirst index reduction operation.\nReturns a variable that contains the index of the first element that matches the specified condition (for each\nslice along the specified dimensions)\nNote that if keepDims = true, the output variable has the same rank as the input variable,\nwith the reduced dimensions having size 1. This can be useful for later broadcast operations (such as subtracting\nthe mean along a dimension).\nExample: if input has shape [a,b,c] and dimensions=[1] then output has shape:\nkeepDims = true: [a,1,c]\nkeepDims = false: [a,c]\n"""""".trimIndent()\n}\n}\n}\n```\n","Introduce an explicit mixin mechanism `Mixin(""name"") {...}` which can define any parts of any op, but isn't an Op definition on its own. Mixins can be defined at top-level, thereby being usable across namespaces."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe API surface area is very large over all those namespaces. For consistency with our previous manually created API we want to be able to alias ops into different namespaces. Those aliases are meant as a convenience for users, so they can find ops more easily.\n# Decision\nWe introduce an aliasing mechanism `Alias(NamespaceName().op(""opName""))` at the Namespace level, which can reference an op directly.\nWhen an op is aliased like that, all of that ops signatures will be available in the referencing namespace. However, they will get an additional note in their documentation saying that it is an alias to the original op. In addition, the implementation of an alias signature, is a direct call of the same signature in the original namespace.\nIt is not allowed to alias an op from a namespace that only has it because it has aliased it itself.\nWhen the requested op is not part of the given namespace, trying to alias it will throw an OpNotFoundException.\n### Example\n#### Definitions\n```kotlin\nfun BaseOps() = Namespace(""BaseOps""){\nOp(""mmul"") {\njavaPackage = ""org.nd4j.linalg.api.ops.impl.reduce""\nInput(NUMERIC, ""x"") { description = ""First input variable"" }\nInput(NUMERIC, ""y"") { description = ""Second input variable"" }\nArg(BOOL, ""transposeX"") { description = ""Transpose x (first argument)""; defaultValue=false }\nArg(BOOL, ""transposeY"") { description = ""Transpose y (second argument)""; defaultValue=false }\nArg(BOOL, ""transposeZ"") { description = ""Transpose result array""; defaultValue=false }\nOutput(NUMERIC, ""output""){ description = """" }\nDoc(Language.ANY, DocScope.ALL){\n""""""\nMatrix multiplication: out = mmul(x,y)\nSupports specifying transpose argument to perform operation such as mmul(a^T, b), etc.\n"""""".trimIndent()\n}\n}\n}\nfun Linalg() = Namespace(""Linalg""){\nAlias(BaseOps(), ""mmul"")\n}\n```\n#### Output\nThis output is meant as a possible example. It is not what it will look like exactly once this feature is implemented.\n```java\npublic class Linalg{\n/**\n* Matrix multiplication: out = mmul(x,y)<br>\n* Supports specifying transpose argument to perform operation such as mmul(a^T, b), etc. <br>\n*\n* Alias of basepackage.baseOps.mmul(x, y)<br>\n*\n* @param x First input variable\n* @param y Second input variable\n*/\npublic static INDArray mmul(INDArray x, INDArray y){\nreturn basepakage.baseOps.mmul(x,y);\n}\n}\n```\n\n## Decision\n","We introduce an aliasing mechanism `Alias(NamespaceName().op(""opName""))` at the Namespace level, which can reference an op directly.\nWhen an op is aliased like that, all of that ops signatures will be available in the referencing namespace. However, they will get an additional note in their documentation saying that it is an alias to the original op. In addition, the implementation of an alias signature, is a direct call of the same signature in the original namespace.\nIt is not allowed to alias an op from a namespace that only has it because it has aliased it itself.\nWhen the requested op is not part of the given namespace, trying to alias it will throw an OpNotFoundException.\n### Example\n#### Definitions\n```kotlin\nfun BaseOps() = Namespace(""BaseOps""){\nOp(""mmul"") {\njavaPackage = ""org.nd4j.linalg.api.ops.impl.reduce""\nInput(NUMERIC, ""x"") { description = ""First input variable"" }\nInput(NUMERIC, ""y"") { description = ""Second input variable"" }\nArg(BOOL, ""transposeX"") { description = ""Transpose x (first argument)""; defaultValue=false }\nArg(BOOL, ""transposeY"") { description = ""Transpose y (second argument)""; defaultValue=false }\nArg(BOOL, ""transposeZ"") { description = ""Transpose result array""; defaultValue=false }\nOutput(NUMERIC, ""output""){ description = """" }\nDoc(Language.ANY, DocScope.ALL){\n""""""\nMatrix multiplication: out = mmul(x,y)\nSupports specifying transpose argument to perform operation such as mmul(a^T, b), etc.\n"""""".trimIndent()\n}\n}\n}\nfun Linalg() = Namespace(""Linalg""){\nAlias(BaseOps(), ""mmul"")\n}\n```\n#### Output\nThis output is meant as a possible example. It is not what it will look like exactly once this feature is implemented.\n```java\npublic class Linalg{\n/**\n* Matrix multiplication: out = mmul(x,y)<br>\n* Supports specifying transpose argument to perform operation such as mmul(a^T, b), etc. <br>\n*\n* Alias of basepackage.baseOps.mmul(x, y)<br>\n*\n* @param x First input variable\n* @param y Second input variable\n*/\npublic static INDArray mmul(INDArray x, INDArray y){\nreturn basepakage.baseOps.mmul(x,y);\n}\n}\n```\n","We introduce an aliasing mechanism `Alias(NamespaceName().op(""opName""))` at the Namespace level, which can reference an op directly.\n\nWhen an op is aliased like that, all of that ops signatures will be available in the referencing namespace. However, they will get an additional note in their documentation saying that it is an alias to the original op. In addition, the implementation of an alias signature, is a direct call of the same signature in the original namespace.\n\nIt is not allowed to alias an op from a namespace that only has it because it has aliased it itself.\n\nWhen the requested op is not part of the given namespace, trying to alias it will throw an OpNotFoundException."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\nencountered while trying to define a language that would work for a few simple examples.\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\nread and write those files from any programming language. However, the drawback with this approach is that writing json\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\ncontributors would have to use.\nUsing a Java builder pattern based approach is very verbose.\n\n## Decision\n",We use a Kotlin-based DSL to define Ops.\n,Use a Kotlin DSL to define Ops.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNot all inputs or args (= parameters) are always required.\nOften there are sensible defaults available. We want to be able to make those defaults explicit where possible.\nEven though some parameters may be optional, they might become required in the presence of other optional parameters.\nWe need a way to explicitly define what combinations are possible.\n\n## Decision\n","We drop the `optional` property on parameters. Instead, parameters get an additional property `defaultValue`. It can be\nset to either a fixed literal value (e.g. `7`, `""something""`, `null`), an Arg, or it may reference the specific methods\n`shape()` and `dataType()` on inputs and outputs. Parameters with `defaultValue` specified are treated as optional.\nTo be able to deal with languages that do not support default values for arguments, Signatures will be specified.\nSignatures are specified using a `Signature(a,b,c){ ""signature specific documentation"" }` section for each signature.\nWith the signature specific documentation being optional.\nSignatures making use of outputs will only be generated for NDArray programming mode, not in SameDiff mode. This also\nmeans that parameters with a `defaultValue` based on an output will be treated as required in SameDiff mode.\nIf signatures are specified, only the specified signatures will be generated.\nIf no signatures are explicitly specified, only the ""all-arg"" and ""no-optional-arg"" signatures will be generated. In\nNDArray programming mode, the default signatures also include a variant that includes the output.\n",Allow arguments to be marked as optional or required with default values. Create a specification around conditional dependencies between optional arguments where combinations are only allowed if all required arguments are present.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSerialization and Code Generation have different needs when it comes to how the object graph should be laid out. When\ngenerating code, it is a lot easier to be able to directly access referenced objects and traverse through the graph.\nHowever, if the same object graph is used for serialization, the same object will appear in multiple places.\nThis becomes very apparent when defining constraints. A single constraint might be referring to an input multiple times,\nand then there can also be multiple constraints that refer to multiple inputs.\nThe main reason why we want to keep serialization in mind, is that we want to keep code generators in other languages as\na viable option. Just serializing the graph that is meant to be used during runtime in code generation however, would\ncan easily become a problem when object identity is required for equality comparisons.\nAn implementer in a different language would therefore need work through that graph and find identical objects AND would\nhave to know where that identity is a coincidence and where it is meant to be that way. By creating an object graph that\nmakes this explicit, we make that work easier.\n\n## Decision\n","We use two distinct object graphs. One is used for code generation, and the other is used for serialization.\n","Since serialization and code generation have different requirements for how the object graph should be laid out, we will use a hybrid approach that splits the graph into two parts:\n\n1. A core object graph that is used for code generation. This graph will contain only the objects that are necessary for generating code.\n2. A serialization object graph that is used for serialization. This graph will contain all of the objects in the core object graph, plus any additional objects that are needed for serialization.\n\nThe core object graph will be laid out in a way that makes it easy to traverse and access referenced objects. The serialization object graph will be laid out in a way that makes it easy to serialize and deserialize the graph.\n\nThis approach will allow us to meet the requirements of both serialization and code generation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome operations work in-place on the inputs that they are given in ND4J, but in SameDiff the same operations will\ngenerate an array from a given shape. Examples for this include `BernoulliDistribution`, and other random ops, that\neffectively initialize the array that they are given.\nFrom a consistency point of view, it would be nice if both API's would support both ways of using those ops.\n\n## Decision\n","We introduce an option to mark inputs as `inPlace = true` to make it clear that this input is going to be changed\nin-place. In addition we introduce an option `supportsInPlaceInit = true` to mark an input as initialize-able. If the\n`supportsInPlaceInit` option is enabled, two signatures for the Op will be created, one that takes an input, and one\nthat takes the appropriate shape and data type information in its stead.\n",SameDiff will support both in-place and non-inplace modes of operation for ops.\nThis will be implemented by introducing a flag to specify the mode of operation for ops.\nThe default mode for SameDiff ops will be non-inplace.\nThe in-place mode can be enabled by setting `inplace=True` in the op constructor.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\nmakes usage and documentation easier.\n\n## Decision\n,"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\n`0`.\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\nvalues match one of the possible values (if applicable).\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\nthe generated enum will be derived from the name of the arg.\n### Example\n```kotlin\nArg(ENUM, ""padMode""){\npossibleValues = listOf(""CONSTANT"", ""REFLECT"", ""SYMMETRIC"")\ndescription = ""padding mode""\n}\n```\n",Define an enum type for each parameter with the possible values.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe services need to look and feel the same. They need to comply with [adr 0002 - Use a consistent user experience](0002-use-a-consistent-user-experience.md).\nNQC’s SRS and DPS services are already built. They comply with GOV.UK styles to a certain extent. They should be partially exempted from this decision for the moment\nGDS have released their new [Design System](https://design-system.service.gov.uk). It does not yet support non-GOV.UK designs but that is on the [roadmap](https://design-system.service.gov.uk/roadmap/).\n[CCS Website](https://www.crowncommercial.gov.uk/s/) has complied with, and contributed to, GDS patterns, but has a divergent colour palette.\n\n## Decision\n","- All new services should share a common front end toolkit\n- There may be different modules per language, sharing the same HTML, CSS, JavaScript assets\n- SRS stylesheets should comply with this kit as closely as practicable. We should revisit this decision as new phases of work are considered for transparency and supplier registration including DPS\n- Comply with GDS Design System except where CCS styling requires differences; document the differences\n- Use a checkout of the Frontend toolkit as the base for front end resources\n- Add a new repository for style and other resource overrides to meet CCS needs,\n- Periodically pull new versions of Frontend toolkit and test\n- Work with GDS to help them support non-GOV.UK styles\n",Services must use the GDS Design System in order to comply with [ADR 0002](0002-use-a-consistent-user-experience.md).\n\nServices that require an exemption should apply to the Technology Leadership Board (TLB) for approval.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\nThe notifications can, for example, be:\n* Telling a supplier they have a task to complete,\n* Telling a supplier that a deadline is approaching,\n* Telling a supplier that a buyer has been shortlisted for a procurement,\n* Providing a receipt,\n* Providing status updates,\n* Etc...\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\n\n## Decision\n","We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal/no training.\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\n",Use GOV.UK Notify for sending all supplier notifications.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSalesforce is currently live within CCS and is used for a number of processes, including managing commercial agreements.  It has not so far been used as a pure CRM tool.\nIt has been agreed by Exec to implement a CRM system.  As Salesforce was originally designed as a CRM tool, it is proposed to utilise Sf in this context.  The existing instance, however, has been significantly customised and contains a large quantity of poor-quality data.\n\n## Decision\n","The current Sf instance be maintained, but that a new Sf instance specifically for use as a CRM, without significant customisation.\n",Utilize Salesforce as the CRM system.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we replace services and content their location on the internet may change.\nFor example:\n* When MISO is replaced by Report MI, https://miso.ccs.cabinetoffice.gov.uk will be replaced with https://reportmi.crowncommercial.gov.uk\n* When we migrate content from the CCS Agreements website to the new CCS website, content will move from https://ccs-agreements.cabinetoffice.gov.uk/abc to https://www.crowncommercial.gov.uk/xyz.\nMany of these domain names and URLs have existed for a long period of time, and can be found all over the internet.\nLots of users will have bookmarked pages, URLs will be published in documents, on company intranets, internal guidance, in the National Archives, buried in inboxes, written on posters and marketing material etc.\nIf we switch off those URLs, we will break user journeys and make things more difficult for users of our services.\nThis will also affect SEO rankings in search engines.\nWe should stop this from happening wherever we can by redirecting users to the replacement content and services.\nWhen GOV.UK replaced Directgov and Business Link, the Government Digital Service adopted a policy of “[no link left behind](https://gds.blog.gov.uk/2012/10/11/no-link-left-behind/)”. They redirected hundreds of thousands of pages of content from these two sites to their equivalents on GOV.UK (or the National Archives for pages that weren’t migrated).\nThis process of redirection continued for all the sites migrated to GOV.UK and is part of the standard for operating government websites. Prior to GOV.UK, providing redirects was also part of the COI-TG109 minimum technical standards for operating government websites.\n\n## Decision\n","When we replace a service or move content from one service to another, we should redirect users to either the replacement location or the appropriate cache on the National Archives.\nWe will issue HTTP 301 (permanent) redirects.\nThese redirects will be maintained for a minimum of 3 years, after which, they will continue if data shows users continue to access the old URLs.\nIt will be the responsibility of each project to ensure that this is done - and should form part of the service and operational acceptance criteria.\n","We will redirect users from old URLs to new URLs wherever possible, to maintain user journeys and SEO rankings. We will use 301 redirects to ensure that search engines and other referrers are aware of the new location of the content."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a number of frameworks being delivered in a series of separate but closely related projects:\nInitial marketing activities will be strongly focused on particular buyer segments - for instance with the education sector.\nWe will use a number of partners to help build these frameworks, but CCS will be the final owners and operators. The frameworks will be built at pace, but will need to share many elements of data, user experience, and logic in order to meet CCS strategy and be deliverable in time at an acceptable level of quality.\n\n## Decision\n","As defined below, in line with technology strategy, common CMP components will be built and/or used:\n- store Agreement data including service offering and supplier interests in a common API\n- use common repository for agreement definitions as part of the build process\n- use CCS ID for buyer registration and authentication\n- use SRS for supplier registration and authentication\n- use common API for Party (Buyer and Supplier) details\n- synchronise data to and from SalesForce as needed\n- all services will be built and integrated together\n- use sector specific (initially education) services for needs that are common across agreements\n- share common services wherever practicable\n- use a common store for questions and answer tasks for framework buyer needs\n- use the same languages and frameworks wherever practicable\n![Overall architecture](../images/CMP%20applications%20high%20level%20architecture.jpg)\n### Commercial agreement specific services\n#### Apprenticeships Buyer Service\nCode and services to support Apprenticeship Commercial Agreement. This is a Dynamic Purchasing system agreement, so that suppliers can qualify in and out of the agreement over time. The structure of the service includes items relating to different apprentice training needs.\n#### Consulting Buyer Service\nServices for the new framework for buying consulting services, with a further competition workflow. The further competition design will be built based on the DOS framework. Award and call-off creation will be created as soon as possible. We don’t yet know whether common services can be built to facilitate the competition phase, but code will almost certainly be shared with Legal Services, which is very similar.\n#### Legal Services Buyer Service\nServices for the new framework for buying legal services, very like Consulting services.\n#### Supply Teachers Buyer Service\nServices for the new framework for finding supply teachers, with location specific and skills based features.\n#### Facilities Management Buyer Service\nServices for the new framework for buying facilities management, with two lots: one for large procurements, and one for smaller procurements. Features include locality, a large number of service items, and agreement specific rules for when to require further competition. Initially we expect the competition rules to be coded in this service.\n### Common components\n#### CCS ID\nA service for authenticating buyers.\n#### SRS - Supplier Registration Service\nA service for registering suppliers, which will need to be extended to support some of these frameworks, if only to support SAML2 authentication to the services for suppliers.\n#### DPS - Dynamic Purchasing Services\nA service supporting qualification of suppliers to a number of commercial agreements, which will need to be supplemented for, at least, Apprenticeships.\n#### Sector (e.g Education) specific Registries & Services\nCode and services that support sector specific buyers. For example, several services may wish to consult Dfe data sources to understand various things about schools.\n#### common services\nCode and services that are shared between teams and commercial agreement services\n#### User Needs Q&A Tasks\nA store for task data in commercial agreement journeys that may be shared. This allows us to define journeys where buyers don’t have to repeat themselves.\n#### Search Index\nA logical search capability to locate commercial agreement elements, to be used in CMp phases when buyer directed matching starts to become important.\n#### Agreements Records\nData store and API for commercial agreements, including Frameworks, DPS, call-offs\n#### Party Records\nData store and API for parties (buyer and supplier organisations)\n#### Documents (Attachments)\nData store for documents linked in agreements and elsewhere\n#### Agreements Definitions\nA repository defining agreement types, used at build time by commercial agreement related services.\n#### SalesForce\nCCS CRM and general workflow environment, and public website.\n","**Decision:** Adopt a shared architecture and development approach across the frameworks.\n\n**Rationale:**\n\n* **Data sharing:** A shared architecture will ensure seamless data sharing between the frameworks, supporting CCS's strategy and the need to meet specific buyer segment requirements.\n* **UX consistency:** By using a common development approach, the user experience for all frameworks will be consistent, enhancing usability and stakeholder satisfaction.\n* **Code reusability:** Sharing logic and code components across the frameworks will reduce development effort, optimize resource allocation, and ensure a consistent quality level.\n* **Timeliness:** A shared architecture and approach will streamline development processes, enabling the frameworks to be delivered more efficiently and meet deadlines.\n* **Ownership and control:** CCS will maintain ownership and operation of all frameworks, ensuring alignment with overall strategy and governance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen developing a new system, there are many possible language choices available.\n-CCS uses a mixture of different languages for existing systems including:\n- PHP for the Supplier Registration Service and Contracts Finder\n- C# for MISO\n- Java for the CCS ID and DigiTS services\n- Python for the Digital Marketplace\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\nAny decision we make about languages and frameworks should consider:\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\n- Cost - what is the cost of developing in this way? Are there licence costs?\n- Hosting - would picking a language restrict hosting options?\nWe should also consider the Service Manual guide on [choosing technology](https://www.gov.uk/service-manual/technology/choosing-technology-an-introduction).\n\n## Decision\n","New services developed by CCS should use the following languages:\n- Java\n- Python\n- Ruby\n- Javascript\nDigital services should also use HTML5, CSS3 and JSON as required.\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\nNew services which do not use these languages will require an Architectural Decision to explain why.\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\n","**1. Establish a Core Set of Approved Languages**\n\n* **Java:** Proven stability, scalability, and enterprise-grade support.\n* **Python:** Versatility, strong data science capabilities, and vast ecosystem.\n* **C#:** Suitability for high-performance applications, close integration with Microsoft technologies.\n\n**2. Allow Flexibility for Specialized Use Cases**\n\n* For specific requirements, consider using additional languages such as PHP, Ruby, or Go if they offer a distinct advantage (e.g., legacy code integration, specialized frameworks).\n\n**3. Encourage Modern Frameworks and Libraries**\n\n* Utilize frameworks like Spring (Java), Django (Python), or ASP.NET Core (C#) to enhance productivity and maintain code quality.\n\n**Reasoning**\n\nThis decision ensures flexibility in supplier choice while balancing support complexity and cost. The core set of approved languages provides a foundation for general development, while allowing for exceptions in specialized cases. The emphasis on modern frameworks and libraries promotes efficient and maintainable code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\nIn the future we will want to more automatically match buyers’ articulations of need so as to work out which products CCS can offer to meet their need.\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\n\n## Decision\n",Use a shared component to store user tasks in a question and answer format during buying journeys\nThe Q&A definitions should be defined from a common repository defining the agreements\nAgreed outcomes must be stored in the agreement records\n,Create a common core of buyer need questions and their associated buyer responses that can be assembled into sections relevant to each agreement type and stored in the buyer journey.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Current landscape\nThere are many existing services and applications in CCS, each of which has made their own decision about how to host their systems.\nFor example, we have recently developed a private beta service for Travel bookings (DigiTS) which uses Amazon Web Service (AWS). We have also procured a database and reporting platform (Powerbuilder and SQL Server) which is hosted on Microsoft Azure.\nThis is a non-exhaustive list of hosting platforms in use across CCS:\n|System           | Hosting arrangement |\n|-------          |---------------------|\n|DigiTS (Travel)  |   AWS managed by Keytree|\n|Data Warehouse   | Azure (Powerbuilder and SQL Warehouse)|\n|CaSIE            | Azure  |\n|Contracts Finder, TRUK | Azure managed by NQC |\n|Digital Marketplace | GDS Government-as-a-Platform managed service on top of AWS |\n|Emptoris, Bravo, SalesForce, MISO, Fleet Portal, Optima | Supplier specific |\n### Imperative for decision\nCCS will be producing several services over the next few months which will need to make decisions relating to appropriate cloud hosting.\nCrown Marketplace (CMp) already operate the DigiTS service on AWS. A decision on the future hosting and support for this service will be required shortly.\nCMp also need to decide on a hosting platform for the next 4 frameworks, starting with Facilities Management.\nThe Facilities Management service will be built using a Digital Marketplace approach and will potentially reuse code and assets from there. A Digital Outcomes and Specialists framework tender has already been issued for Facilities Management. All CMp frameworks are likely to operate on the same software, so a single provider would be advantageous\nThe first phase of core CMp deliverables may also require cloud hosting, as will the new Data Submission Service.\nThere is likely to be an overhead for having a different approach for each project, and we arguably don't have sufficient capabilities and controls for any of the existing solutions.\n\n## Decision\n","We use Amazon Web Services (AWS) as the default cloud provider for newly built services which require Infrastructure as a Service (IaaS) or Platform as a Service (PaaS) hosting.\nThis will provide clarity for the immediate in-flight projects (eg Facilities Management, CMp first phase, Data Submission Service, DigiTS etc) and allow us to consider consolidating support and operations of these services.\nNew services which are not to be hosted on AWS will require an Architectural Decision to explain why.\nWe will re-evaluate this decision in 6 months, taking account of current market trends, industry moves, the upcoming pipeline of work and the success of the current in flight projects.\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist.\n","The decision is **to standardise on AWS for all CCS cloud hosting**.\n\n**Rationale:**\n\n* **Consolidation:** Standardising on AWS will reduce the overhead and complexity of managing multiple cloud platforms.\n* **Improved capabilities and controls:** AWS provides a comprehensive suite of capabilities and controls that can be leveraged across all CCS cloud services.\n* **Consistency:** Using the same platform for all services will ensure consistency in terms of security, scalability, and performance.\n* **Leveraging existing expertise:** CMp already has expertise in managing AWS for DigiTS, which can be leveraged for other services.\n* **Potential cost savings:** Standardising on AWS may lead to cost savings as CMp can negotiate better pricing with a single provider.\n* **Alignment with Digital Marketplace:** The Facilities Management service will be built using a Digital Marketplace approach, which is already hosted on AWS. Standardising on AWS will ensure compatibility and reduce integration challenges.\n* **Support for core CMp deliverables:** The first phase of core CMp deliverables and the Data Submission Service will also require cloud hosting. Standardising on AWS will provide a consistent platform for these services."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Many services\nWe have many services at the moment each providing a different part of the CCS user journey.\nWe don't expect there to be a single service that will do all of what CCS needs. In fact even within one part of the user journey there are likely to be several services.\nWe need an architecture that delivers a consistent and joined up journey for users, and which stores data in a consistent and shareable way, but which allows more than one service.\nWe envisage the solution to this to be like a 'hamburger' with the top being the consistent web experience, the bottom being the consistent data, and the filling being the services. Our approach to providing a consistent web experience is outlined in ADR-002.\n### Distributed data\nData for services is entered many times in different places. For example:\nAn agreement (eg framework) is first conceived of in a SalesForce pipeline activity\nIt then gets detailed in word documents\nIt is entered into competition tools - either Bravo or Emptoris\nIt is then manually retyped into SalesForce (with automated assistance in Bravo)\nIt is re-entered into MISO\nThere is a similar convoluted journey for customer data. For instance customer contact data is entered into SalesForce directly, and into various systems across CCS.\n### One source of the truth\nSome data will be common across services and software components. We call these common records. These common records will be used by many services.\nIn addition, all services should make their data available via API to encourage new patterns of use, even if we don’t know they will be common at first. However, these APIs are less urgent than APIs for common records.\n\n## Decision\n","### Directive\nWhen designing services you should ensure that:\nAll common records are accessed through a common API\nService specific data are made available via an API so it can be used by others\nAll data types used in APIs are described and published\nCommon data types are used wherever possible\nGovernment open standards are used where appropriate\nData quality is improved as a result of the service development\n### Recommendation\nWe should start with an area of data required by the next service we develop, such as user details. We can define APIs so services can access user details without having to reenter or replicate them (which helps with data protection). The API may either store canonical data in SalesForce or in a separate database.\nWe will commit to improving data quality when we do this.\n","We will have a distributed service architecture, but with an agreement as to how services should interact and how data is stored.\nThe data architecture will be based on the concept of a single source of truth.\nAll services will have access to the single source of truth via APIs.\nServices will be able to store data in the single source of truth, but they must do so in a consistent way.\nThe single source of truth will be implemented using a distributed database.\nThe distributed database will be managed by a team of database administrators.\nThe database administrators will be responsible for ensuring that the database is reliable, scalable, and secure."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCCS is developing many new services over the coming months, each of which will be producing new application source code.\nThese services will be developed by several independent teams including people from different supplier organisations.\nThere is a risk that each workstream operates in a different way using different standards which could lead to confusion and complexity in the future.\n#### Version control\nIt is standard industry practice that all source code should be stored somewhere that allows it to be properly versioned and controlled.\nThere are several options for software version control, including for example SVN, Git, Mercurial and Team Foundation Server (TFS) etc.\nCCS already uses [Git](https://en.wikipedia.org/wiki/Git#Adoption) for the Data Submission Service and Salesforce. Git is also widely used across government, and the industry as a whole (for example Microsoft is migrating Windows development to use Git).\nGit is a distributed version control system, which means that a full copy of the repository is stored locally on the developer’s device. It is standard practice for a copy of a Git repository to be stored in a central place so that code can be shared between developers.\nCCS uses GitHub as this central place for Salesforce and Data Submission Service, while DigiTS uses BitBucket. GitHub is widely used across government including for [all GDS services](https://github.com/alphagov). Other services also exist including BitBucket, GitLab, AWS CodeCommit and Google CloudSource.\n### Trunk-based development\n[Trunk-based development](https://trunkbaseddevelopment.com/) is a source code management methodology which is aimed at making it easier for multiple people to make changes to code at the same time without breaking each other’s work.\nWith many teams working on common interfaces, there is a risk that development activities will work to different versions, which can cause integration issues later on. Trunk based development reduces the risk of this occurring.\n### Continuous integration (CI)\n[Continuous integration](https://en.wikipedia.org/wiki/Continuous_integration) (CI) is a software development practice that involves regularly and automatically running unit and integration tests on code as it is being developed. The aim of this practice is to reduce potential integration issues later in development.\nThe adoption of CI can also reduce the time-to-deployment by introducing automated testing and standardised builds. This is important part of being able to [deploy software regularly](https://www.gov.uk/service-manual/technology/deploying-software-regularly) as recommended in the Service Manual.\n\n## Decision\n","All new services will use Git for version control and GitHub as a central code repository. We will use public repositories by default, unless there is a convincing reason not to (which must approved by TDDA and then be documented in the README of the private repository).\nAll new services will adopt Trunk-based development and Continuous Integration practices. Any branches should be short-lived feature branches which last for no more than a few days before being merged into the trunk. Feature switches should be deployed where necessary.\nServices should use a common continuous integration tool and build monitor, with a common set of build phases. A decision on this common tool and common build phases isn’t made in this ADR, but will be subject to a future ADR.\nThe decision to use GitHub as the central code repository should be reviewed in 6 months to determine if a better alternative is available.\nCCS-owned source code should be migrated to use Git and GitHub.\nWe should develop an approach to evaluate the impacts of migrating existing services to use Trunk-based development and Continuous Integration practices.\n",**Decision:** CCS will use Git and GitHub as the standard source code control for all new applications. Trunk-based development will be used as the standard source code management methodology. Continuous integration will be used as the standard software development practice for all new applications.\n\n**Rationale:**\n1. Git is a distributed version control system that is widely used across government and the industry as a whole.\n2. GitHub is a central place to store code that is widely used across government.\n3. Trunk-based development is a source code management methodology that is aimed at making it easier for multiple people to make changes to code at the same time without breaking each other’s work.\n4. Continuous integration is a software development practice that involves regularly and automatically running unit and integration tests on code as it is being developed.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCCS provides many online services, each available through different web domains and providing a different look and feel.\nThis provides an inconsistent user experience to people who use CCS services, which makes their use more difficult as users have to re-learn how things work for each new service they access.\nFor example:\n|Service|Domain|Look & Feel|\n|-------|------|-----------|\n|MISO|miso.ccs.cabinetoffice.gov.uk|Custom\nFleet Portal|fleetportal.crowncommercial.gov.uk|Custom\nEnergy Portal|ccsutilities.energycloud.com|Custom\nBravo|crowncommercialservice.bravosolution.co.uk|Custom\nEmptoris|gpsesourcing.cabinetoffice.gov.uk|Custom\nDigital Marketplace|digitalmarketplace.service.gov.uk|GOV.UK Standard\nDigital Travel Service (DigiTS)|travel.crowncommecial.gov.uk|GOV.UK Standard\nPurchasing Platform|purchasingplatform.crowncommercial.gov.uk|Custom with GOV.UK elements\nSupplier Registration Service (SRS / SID4GOV)|supplierregistration.crowncommercial.gov.uk|Similar to GOV.UK\nCCS Here to Help|ccsheretohelp.uk|Custom\nCCS Website|crowncommercial.gov.uk|Custom based on GOV.UK with new CCS branding\nCCS Salesforce (SSO edition)|buying.crowncommercial.gov.uk|Custom\nWe don't expect there to be a single service that will do all of what CCS needs - in fact even within one part of the user journey there are likely to be several services.\nWe need an architecture that delivers a consistent and joined up journey for users.\nGOV.UK Design Patterns\nGOV.UK provides a series of consistent design patterns which have been developed to make government services look and work the same.\nThese patterns have been through extensive user research and are used on many high-value and high-traffic government transactions.\nThese patterns are accompanied by a selection of toolkits and frameworks to allow government services to adopt them easily and quickly. These frameworks also provide useful underlying accessibility features for users.\nRecreating these patterns, toolkits, frameworks will take an significant effort.\n### Government Digital Service Standard\nThe [Digital Service Standard](https://www.gov.uk/service-manual/service-standard) applies to all newly built government services.\nStandard #13 requires that all services have a user experience consistent with GOV.UK using the GOV.UK Design Patterns.\n### CCS Branding\nCCS is developing its own brand to support its marketing activities.\nThis new brand includes an exemption from the GOV.UK domain for the marketing website.\n### Consistency\nJumping between standard GOV.UK design and the custom new CCS brand may cause surprise or confusion for users, and so we should minimise the potential impact from this.\nAdditionally, users shouldn't be particularly aware that they are moving between subdomains and different services.\n### Content Management\nContent is a fundamental part of providing a consistent user experience. CCS stores content in various systems, including Wordpress (CCS Here to Help / Energy Portal), Salesforce, Drupal and other locations.\nWe should review our approach to ensure our content is stored in a consistent manner that meets our needs.\n\n## Decision\n","For all new services we should use the standard GOV.UK Design Patterns and Frontend Toolkit.\nWe should apply a minimal CCS-brand to this toolkit, replacing the ‘GOV.UK’ top bar with the CCS colour and brand, and replacing the footer where appropriate. All other design patterns should remain consistent with GOV.UK where possible.\nAll new services should use the `crowncommercial.gov.uk` domain name. Where new services do not use this domain, there should be an Architectural Decision to explain why.\nAs we develop new services, we should contribute our research and work back to the GOV.UK design patterns and toolkits to support other government services.\nWe will review our content management needs before beginning the third phase of the CCS website development.\nBefore starting the third phase of the website project, we should evaluate our needs around content design and produce an appropriate Architectural Decision.\n",Adopt GOV.UK Design Patterns for the consistent user experience across different CCS services.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nArchitectural Decision 003 Use a common records layer decides to use a common records layer. Common Sourcing design defines how to interact various components in a coordinated sourcing strategy.\nAll Agreement services need party and agreement records to coordinate supplier registration and DPS qualification with the buyer journey.\n\n## Decision\n,"All Agreement services need party and agreement records to coordinate supplier registration and DPS qualification with the buyer journey.\n- Store all new suppliers and updates using Party API\n- Store all new buyers and updates using Party API\n- Retrieve details of services and supplier offers from Agreement API\n- Retrieve attachments from Document store (S3, with metadata in agreements)\n- Store purchase orders in Agreements API where appropriate\n- Synchronise with SalesForce as needed\nAs Suppliers are added to, removed from or change their offer on agreements, the service will call the common record API to update all required consumers.\nStore relevant supporting documents and attachments directly in AWS S3, with a supporting document API to synchronise with SalesForce content linked to document URI. Salesforce API links may be fragile and have access limits. S3 documents have an easier API for volume based consumption. The S3 store can be used to insulate the service for SalesForce web releases.\nThe agreement and Party APIs have not been defined yet but will use json over HTTPS with RESTful access patterns, with OAuth identity tokens. This is in line with GDS guidelines. OAuth is used instead of simple shared secrets because the number of API clients is likely to be large and fluid. Data definitions will be controlled by CCS but will map to OCDS standards wherever practical.\nAgreement records will contain:\n- Agreements, lot structure, items (services) and related content, and call-offs\n- Interests linking suppliers (by ID) to Agreements and their offers\n- Offers from Suppliers linked to the relevant part of agreements services\n- Purchase orders where appropriate\nParty records will contain details about buyer and supplier organizations (not individuals):\n- Supplier identifiers and details\n- supplier qualifications questions and answers\n- other details not specific to agreements\n- Buyer identifiers and details.\n- articulation of various user needs not specific to agreements, such as details of physical estate, people, plans, budgets.\nIndividual data, where needed, will be stored as user ID references to CCS ID only.\n","**Decision:** Implement a Common Records Layer.\n\n**Rationale:**\n\nThe common sourcing design requires coordination between various components in a unified sourcing strategy. All Agreement services heavily depend on party and agreement records to facilitate supplier registration and DPS qualification during the buyer journey. Utilizing a shared records layer ensures consistent data management, eliminates data duplication, and simplifies data exchange among these services."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe same agreement definitions for CMP services will be needed for suppliers to define their offers, and buyer journeys to select offers from. Also there will be many common elements shared between agreements, such as the tasks for buyers (and suppliers) to complete in order to complete a buying journey.\nFor each version of a CMp agreement, both buyer and supplier services, will have to release their code in sync to make sure that supplier responses to their service offer questions line up with buyer selection activities.\nFor example, a framework will have a number of lots, each with a number of services. The services may have various parameters - like cost models, date ranges, locations. Suppliers to that framework will need to answer questions that map to these services. These things should be defined in a consistent way so build pipelines can be built consistently.\nBuild pipelines may find it more convenient to pull Agreement definition files from a backing repository rather than an API.\n\n## Decision\n","The structure of all agreements will be defined in a common CCS GitHub repository. For example, the lot and item structure of frameworks should be defined in the repository so that web forms can be built in a repeatable way without having to code the agreement.\nThe structure of the data is yet to be defined but will map to OCDS standards wherever practical. It will include _elements like_:\n- Agreement schema\n- Lot structure\n- Service items\n- descriptions for buyers\n- value parameters (price, description, location, timelines etc)\n- question and answer formats for suppliers\n- Qualification criteria\n- links to supporting Documents (in S3)\n- supporting content\n- relevant tasks and Q&A text\n- Interests  (links supplier offer to agreement)\n-Supplier Offers\n-(service, offer details)\nThe ‘agreement schema’ should be built in a technology agnostic way, using a data schema. Example domain models for different languages can be generated from the schema.\nWe will probably use a data schema like format, such as yaml.\n![flow of defeinitions changes](../images/cmp-shared-agreement-definition.jpg)\n1. edit a new version of Agreement -  e.g. new services\n2. add / link new related attachments\n3. update Agreement API\n4. update SalesForce content / pages\n5. new content and questions/answer forms for suppliers\n6. update purchase services\n7. define text and structure for user tasks with relation to agreement\n","We will create a git repository to hold the agreement data\n\nThe repository will contain the schema used to validate the data and the data itself, represented as JSON or YAML.\nThe repository will be versioned.\nThe source of truth will always be the repository."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need users to be able to browse and search the datasets within the platform so that they can find what they need, and we can break down data siloes. There are various different ways to implement this.\n## Decision Drivers\n- Ease of set up and maintenance\n- Features and user experience\n- Ability to ingest metadata from the data lake\n- Cost\n\n## Decision\n","- Ease of set up and maintenance\n- Features and user experience\n- Ability to ingest metadata from the data lake\n- Cost\nWe have decided to use an DataHub, an open source tool, as our data catalogue tool because:\n- We have been able to successfully set it up in our AWS environment without much difficulty [please add here]\n- It has the basic features we require to catalogue datasets (e.g. a range of metadata available which users can add to, search and browse functionality) as well as additional functionality that may enable us to catalogue data pipelines, dashboards, models etc in future.\n- Users responded positively to it in user research.\n- It is capable of ingesting metadata from Hive/AWS Glue and thus our data lake.\n- It is open-source and therefore cost is limited.\n- It has an active community where we have an opportunity to influence its future development.\n","**Decision:** Implement a data catalog powered by Apache Atlas.\n\n**Rationale:**\n\n* **Ease of set up and maintenance:** Atlas is open source and requires minimal configuration. It can be integrated with the data lake via connectors.\n* **Features and user experience:** Atlas offers a comprehensive set of features for browsing, searching, and managing data assets, including lineage tracking and governance capabilities. Its user interface is intuitive and provides a good user experience.\n* **Ability to ingest metadata from the data lake:** Atlas supports ingestion of metadata from various data sources, including Hadoop Hive, HDFS, and relational databases. This allows it to centralize metadata from the data lake and provide a single point of access for users.\n* **Cost:** Atlas is open source, which eliminates licensing costs. Operational costs will depend on the scale of the deployment and the underlying infrastructure. However, Atlas is known for its efficiency and low resource consumption."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHackney has at least one data source which is provided to it in `mysqldump`\nformat. This flat file is generated on a daily basis, and we've identified\na need for this data to also be imported into the platform on a daily basis.\nWe hope this the design proposed will be extendable for further SQL flat files\nthat the platform may need to support.\n\n## Decision\n","1. Leverage the existing platforms ability to ingesting RDS snapshots\n2. Use ECS as the execution environment.\n3. Have a reusable RDS instance running which will be the target for importing\nand exporting.\n### Process\nA CloudWatch event will be configured to run an ECS Task when a flat file is\nuploaded to an S3 Bucket, this ECS task will responsible for downloading the\nSQL file from S3, creating a temporary database on the reusable RDS instance and\nrunning an import to that the temp database based on the SQL file name.\nOnce the import has completed the ECS task will trigger an RDS snapshot. Once\nthe snapshot creation has completed the ECS task will drop the temp database\nthat is created, while the snapshot ingestion process starts processing the\nnew snapshot.\n### Discounted options during a spike\nDuring testing we found that the file provided as a test took longer than\nthe 15 minute AWS Lambda execution limit, so decided we needed to use ECS\ninstead.\nWe explored having a MySQL instance inside of a container in ECS. This would\nhave given us more flexibility to a wider range of databases, but we found\ndifficulty in exporting the data out of the MySQL into parquet file format.\n","**Decision:** Use a nightly batch job to import the `mysqldump` file into the platform.\n\n**Rationale:**\n\n* This approach is simple and straightforward to implement.\n* It is a batch process, so it will not impact the performance of the platform during the day.\n* It is extendable to other SQL flat files that the platform may need to support in the future.\n\n**Consequences:**\n\n* The import job will need to be scheduled to run nightly.\n* The import job will need to be monitored to ensure that it is running successfully.\n* The import job may need to be scaled up if the size of the `mysqldump` file grows."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAn SQL dump of the liberator database is put in a SFTP server as a zip file by a supplier. This data is important to the parking team so we need to ingest it into the platform. In order to do this we first need to move the data from the SFTP server into the landing zone S3 bucket.\nThere was also the option to pay for the supplier to directly drop the file into S3.\n\n## Decision\n,"We will use a lambda function to achieve this. The function will run at 06:00 UTC, triggered by a cloudwatch event, finding the file date stamped for that day then streaming it into the given S3 bucket.\nIt finds the file by searching for a filename pattern, if there is more than one file that matches then it will get all of them.\nThe connection details for the SFTP server are stored in ssm parameter store.\n","**Decision:** Move the data from the SFTP server to the landing zone S3 bucket using an automated process.\n\n**Rationale:**\n\n* This option is more cost-effective than paying for the supplier to directly drop the file into S3.\n* It allows for greater control over the data ingestion process.\n* It can be automated, which will reduce the operational overhead associated with data ingestion."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe will be storing sensitive council data within S3 and therefore need to restrict access to this data based on the\ndepartment the user belongs to.\n\n## Decision\n,"In order to limit access, we propose to store all S3 buckets in a single AWS account. Users accessing this account\ndirectly will have little or no access to the owned S3 buckets, instead through the infrastructure deployment process\n(terraform) we will share a partition of the S3 buckets to relevant department accounts.\nE.g. s3://s3-bucket/social-care/\* -> Social Care Account\n",To implement role based access control (RBAC) using AWS Identity and Access Management (IAM) and bucket policies to control access to S3 objects based on the user's department.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\nExamples include:\n- Address matching\n- Address cleaning\n- Repairs sheets data cleaning\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\nBy introducing testing practices, frameworks and tools we hope to:\n- Improve the speed at which PySpark scripts can be developed\n- Provide documentation for each script with example data they expect, and what results they output\n- Increase the proportion of defects found before they reach staging environment\n\n## Decision\n","We will:\n- Use a Python testing framework, [pytest][pytest]\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\n- Integrate that framework into Apache Spark, and provide example test code\n- Create documentation and guidance around how to productively test PySpark scripts\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\n[pytest]: https://docs.pytest.org\n[awsglue-dockerhub]: https://hub.docker.com/r/amazon/aws-glue-libs\n","Introduce a dedicated test environment for PySpark scripts. This environment should be provisioned with a sandboxed copy of the production data, allowing developers to run tests without affecting the live system. The test environment should also be equipped with appropriate testing frameworks and tools to facilitate the development of automated tests. It is recommended to use Pytest as it is a popular and well-established testing framework for Python. Additionally, consider using a mocking library such as Mock to simulate external dependencies and isolate the test from external factors. Regularly scheduled automated tests should be implemented to ensure the continued stability of the PySpark scripts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHackney have recovered a number of datasets from prior to the Cybersecurity incident, these datasets will need to be\nmerged with datasets created after the incident.\n\n## Decision\n","Recovered datasets that are in a consumable format (json, csv, .sql) will be deposited into the landing zone directly,\nusing the same partition storage strategy as other data sets [see ADR 0004](partition-strategy)\n","**Decision:** Merge the recovered datasets with the datasets created after the incident.\n\n**Justification:**\n\n* The recovered datasets contain valuable data that should be integrated into the current data ecosystem.\n* Merging the datasets will provide a more comprehensive and accurate data foundation for Hackney.\n* The integration process will require careful planning and execution to ensure data integrity and consistency.\n\n**Alternatives Considered:**\n\n* **Do not merge the datasets:** This would result in a fragmented data ecosystem and limit the value of the recovered datasets.\n* **Merge the datasets manually:** This would be a time-consuming and error-prone process, making it impractical for large datasets.\n* **Use data integration tools:** This would automate the merging process and reduce the risk of errors, but would require additional investment and technical expertise.\n\n**Decision Outcome:**\n\nThe decision was made to merge the recovered datasets with the datasets created after the incident using data integration tools. This approach will provide the best balance of efficiency, accuracy, and cost-effectiveness."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Data Platform is currently broken up into three separate accounts, Development, Staging, and Production.\nProduction has been designed to run reporting, repeating ETL process and provide a stable environment for staff to run Qlik\nor other BI tools against.\nDevelopment has been designed for engineers to independently test out new features, functionality or\ninfrastructure relating to the platform.\nStaging, a term normally used to refer to a mirror, clone or close approximation of the Production environment which is\nnormally used to test out changes intended on being placed in Production.\nAnalysts/Engineers require a place to build, modify and test new or existing ETL process in an environment that will not\nhave an impact on the Production environment.\nThe data that the Analysts/Engineers will need to use in order to develop these process will need to have the same\nschema structure and underlying data structure as the production data. Generating such data as ""dummy data"" would be\ncomplex and time-consuming based on the quantity and quality of the data being consumed by Hackney.\n\n## Decision\n","Analysts/Engineers will use the Staging environment to build, modify and test new or existing ETL process.\nThe Staging environment will contain copies of Production data. Documentation on how the data is copied to staging can be found here: [Production to pre-production sync](../../docs/production-to-pre-production-sync)\n",- Create a separate account within the Data Platform for use as an Analyst/Engineering sandbox (accessible only to Analyst/Engineer staff)\n- Copy the relevant production data tables into this environment to ensure the sandboxes data structure and quality is a true representation of the live data environment.\n- Setup a CI/CD pipeline which will regularly update the mock data to ensure its up to date with the production data (possibly weekly or monthly depending on the relevant data sets frequency of change).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Data Platform currently has no capacity to ingest data from external APIs\n\n## Decision\n,"![API ingestion](../images/api-ingestion.png)\nDeploy a Lambda that will call the the required API and save the returned data\nobject into S3, once all the records have been downloaded the lambda will\ntrigger an AWS glue workflow that will convert JSON objects into parquet and\ncrawl the data enabling users to access that form a data catalogue in AWS Athena.\n","**Decision:** Build an API Gateway service to ingest data from external APIs.\n\n**Rationale:**\n\n* The API Gateway service will provide a centralized and secure point of entry for external APIs to interact with the Data Platform.\n* The service will be responsible for authenticating and authorizing API requests, transforming and validating data, and routing it to the appropriate destination within the Data Platform.\n* Building an API Gateway service will ensure that data is ingested securely, reliably, and efficiently from external APIs.\n* The service will also provide visibility and control over the data ingestion process, making it easier to monitor and troubleshoot issues.\n\n**Consequences:**\n\n* **Benefits:**\n    * Improved security and reliability of data ingestion from external APIs.\n    * Increased visibility and control over the data ingestion process.\n    * Reduced development effort and complexity for integrating with external APIs.\n* **Costs:**\n    * Additional infrastructure and operational costs associated with running the API Gateway service.\n    * Potential performance overhead introduced by the API Gateway service.\n    * Additional development and maintenance effort to build and manage the API Gateway service."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\nefforts. We need to get this information pulled into the data platform for processing.\n\n## Decision\n,We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\n,**Decision:** Utilize Google Sheets API to programmatically extract data from the distributed Google Sheets documents and import it into the data platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen storing data into S3, access performance can be increased by partitioning datasets down into folder tree with the\nleaf folders containing subsets of the overall dataset. When products like AWS Athena access the overall dataset, it can\nlimit it's read processes to a selection of these leaf directories based on the data that needs to be queried.\n\n## Decision\n","Datasets on the platform will be stored in the following folder structure:\n`{department/service}/{dataset-name}/import_year={import-year}/import_month={import-month}/import_day={import-day}/`\n- department/service - The top level folder should always be the server/department that owns the data.\n- dateset-name - The name of the dataset, this is chosen by the user importing the data but should descriptive.\n- import-year/import-month/import-day - The year/month/day the dataset was written to the data platform, or in cases of ingesting\nthird-party data, the year/month/day the data was originally generated.\nDataset that require additional partitioning for optimisation should be done so under this structure. E.g. A dataset that\nstores PCNs may choose to break the data down further by issue date:\n`{department/service}/{dataset-name}/import_year={import-year}/import_month={import-month}/import_day={import-day}/issue_date={pcn-issue-date}`\n",Partition data into folder trees on S3 to improve access performance for analytical tools like AWS Athena.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use the Architectural Decision Record (ADR) template for documenting and tracking architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEdward will be provided as a command-line tool, ideally across multiple operating systems. It will need a simple means of installation and updating.\n\n## Decision\n",Edward shall be implemented using Go.\n,Create an executable installer script that will download and install the latest version of Edward on the user's machine. The script will be made available for download on the Edward website and will be compatible with multiple operating systems.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEdward provides a rich command-line interface intended to use a command structure familiar to most developers.\n\n## Decision\n,[Cobra](https://github.com/spf13/cobra) will be used as a CLI framework.\n,We decide to build a terminal-based API that reads and executes code from the command line.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Use an Architectural Decision Record (ADR) template to document and track architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the Proof of Skill assignment several auxillary technology have been used on top of the required terraform. As terraform\nas a infrastructure as source is a given in the assignment, others will be explained here.\n\n## Decision\n","### Repeatable project creation\nThe use of ansible as a project templater. Ansible is selected to have a reentry method to create projects from a template. In this case this is more used for documentation than template but it is used\nto illustrate its use. The author is aware that ask nicely is using Puppet, but the author is more familliar with ansible and as this is an anxillary\ntool this one is selected over puppet.\n",Use the following auxiliary technologies:\n\n* **GitHub Actions:** for continuous integration and deployment\n* **Docker:** for packaging and deploying the Terraform code\n* **Jenkins:** for pipeline management\n* **Artifactory:** for binary storage\n* **SonarQube:** for code quality analysis
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen creating a piece of infrastructure it is implied that this code would be able to be run in several environments. To illustrate this, there are three environments created, dev, uat, and prod. There will be one and only one\nterraform module and each environment will feed it with different parameters to create distinct instances.\n\n## Decision\n","The terraform code is made multi environment and multi account through there are better tools to do that with, like terragrunt, which is outside the scope of the PoS.\nThe credentials are assumed to be in the credentials file. the author is aware that the are more secure ways of doing this , but as the PoS needs to be easily reproducible the author has chosen to make it easy to use by dropping in the credential files in the home directory.\nThe credentials are that of an administrator, there has been no attempt made to create assumed roles as would be done in production systems, as the goal in this case is speed.\n","**Decision:** Utilize Terraform modules to manage infrastructure configuration for different environments.\n\n**Benefits:**\n\n* **Centralized configuration:** Maintain a single Terraform module that defines the infrastructure configuration.\n* **Environment-specific parameters:** Allow different environments (dev, uat, prod) to provide specific parameters to the module, creating distinct infrastructure instances.\n* **Consistency and standardization:** Ensure consistent and standardized infrastructure configurations across environments.\n* **Flexibility and adaptability:** Easily modify and deploy infrastructure changes by updating the Terraform module and providing environment-specific parameters.\n* **Reduced code duplication:** Avoid writing multiple Terraform scripts for each environment, reducing code redundancy and maintenance effort."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn AWS the use of AIM images is used to create and customize base line and derived application images from. These can be created with a tool like packer that will take a base image, configure it, and then has a ready to use AIM image ready.\n\n## Decision\n",Packer is a multi cloud vendor tool specialized in provision images. It has been decided by the author to use this as a quick prototype example of how to use this in a simple CI/CD example.\n,Use packer to create AIM images that are stored in S3.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need machine-readable logging so that we can do better analytics and our logging is consistent with the Python services, which use Structlog.\nWe are aiming to meet the ONS's logging standard, which is documented here: https://github.com/ONSdigital/software-engineer-community/blob/master/standards/logging.md\n\n## Decision\n","We investigated the available open source projects which would give us an enhanced API to make it easier to log key-value pairs in the desired JSON format. Projects with few contributors and without recent releases were not considered to be viable candidates. It seems that Godaddy have a nice enhancement to the vanilla SLF4J API, which is quite convenient for the developer to use.\nThe proposal to use the Godaddy logger was put on Slack in the developers channel and a vote was held. 6 developers voted ""YES"" and there weren't any ""NO"" votes or votes for other solutions.\nGodaddy logging API allows you to do logging like this: `log.with(""key"", ""value"").error(""Here be error messagez"", exception);`\nMore on the Godaddy logger: https://github.com/godaddy/godaddy-logger/\n","We will use the `structlog` Python package for logging.\n\n`structlog` is a popular Python package for structured logging. It allows us to log messages in a consistent format, which makes it easier to parse and analyze the logs. `structlog` also supports logging to a variety of destinations, including files, databases, and logging services.\n\nThe ONS's logging standard requires that logs be machine-readable and consistent with the Python services, which use `structlog`. By using `structlog`, we can meet both of these requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA Java checked exception called `CTPException` has become ubiquitous throughout our entire SDC codebase, occurring in **1,017** places, many of which are the mandatory declaration of `throws CTPException` as a consequence of using a _checked_ exception, when in most cases, a _runtime_ exception was what we wanted, because the error was unexpected and unrecoverable.\nJava usefully provides IllegalArgumentException and IllegalStateException - for example - which cater for almost almost all of our needs to raise an exception, when an unexpected and unrecoverable error in our system occurs.\nFor the REST APIs, an HTTP status code should be specifically chosen in the logic which is closest to the definition of the API endpoint itself, to tell the API client whether it made a bad request, resource not found, or indeed an internal server error occurred (in the event that our system has an unrecoverable error, due to missing/corrupt data, mangled state and a whole host of other problems which are _internal_ and could only be investigated and fixed by ops).\n`CTPException` was introduced as a convenience class to return different HTTP status codes in the REST APIs, but it has been completely mis-used and spread around the codebase.\n\n## Decision\n","We will remove the CTPException class wherever it exists and replace it with a runtime exception, in most cases, with the exception of these specific REST API errors:\n- REST API is sent garbage: bad request (400)\n- REST API can't find requested resource: resource not found (404)\n- REST API can't perform 'action' because of business rules (e.g. invalid state transition): forbidden (403)\nWe will use **runtime** exceptions in the event of the following REST API errors:\n- REST API didn't find data it expected (and needed) to find, nulls, missing configuration and all the very many other kinds of system state corruption which are possible: internal server error (500)\n- REST API can't connect to database, Rabbit, Redis, or to another API it needs: service unavailable (503)\nA very great deal of the code deals with Rabbit queue messages and cron/scheduled/poller jobs, which of course cannot report a HTTP status code, because there is no interactive 'user' - it's asynchronous. The use of checked exceptions in this code is inappropriate, because the errors are fatal and unrecoverable: we rely totally on our logging and alerting to instigate an investigation and fix the issue. These exceptions are _runtime_ exceptions. The vast majority of exceptions raised in the SDC system should be runtime exceptions.\n","**Migrate** all instances of `CTPException` to `IllegalArgumentException` or `IllegalStateException` where appropriate, and to `RuntimeException` where not."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn computing, an **idempotent operation** is one that has no additional effect if it is called more than once with the same input parameters.\nA vast majority of critical flows of a web services we're running is required to provide this guarantee for their clients.\nThus, it makes sense to extract this functionality into a library with support for common use-cases.\nThere a two common approaches in the industry when it comes to implementing support for idempotency:\n1) distributed **in-memory** data stores (i.e. Redis)\n2) **database** - master node to avoid any issues caused by possible replication lag\n### Client\nTypically, the only additional requirement for the clients to enable idempotency is to add a unique identifier for the request\nthat is only expected to have a side effect once. Sometimes APIs make this a mandatory property of the request.\nFrom a security considerations and to lower a chance of collisions it's recommended to use UUIDv4 as a unique request identifier.\n### Service\nCritical requirements for the processing service:\n1) **persisted result** for a successful and failed with non-retriable error request\n2) **non-blocking** - any concurrent request with a same identifier should be rejected with a transient error\nResult persistence aims to avoid any heavy computations on a re-try or duplicate requests if original request completed with a **success** or **non-retriable** error.\nClients should have a freedom of choice on what exactly is being persisted and which format to use.\nQuick failure in case of the aggressive retry policy on the client side ensures that no server resource is blocked and wasted for such concurrent requests with a same identifier.\n3) request processing procedure and persistence of its result has to be transactional\nWe require request execution logic and idempotence status update to happen in a same transaction.\nThat requirement comes at a cost of the client flexibility. Allowing clients to control transactional context\nintroduces edge-cases such as a failure to persist idempotency status whereas action has been completed and side effect taken place.\n\n## Decision\n","Provide code idempotence service based on a database approach, specific database integration is pluggable.\nBoth **in-memory** and **database** approaches have their own pros and cons, we're not going to go in depth comparision here\nmostly due to the lack of adoption of Redis in our current infrastructure. That said we should definitely consider providing such support in the future.\nWe require request execution logic and idempotence status update to happen in a **same transaction**.\nThat requirement comes at a cost of the client flexibility, but allowing clients to have full control over transaction context\nintroduces edge-cases such as a **failure** to update imdepotency action status after **side effect** already took place.\nThat would require clients to verify if side effect has taken place on each execution - which we consider suboptimal for most common cases.\nIf that approach proves to be limiting for certain flows we can extend core service interface to allow clients to choose different execution strategies and have better control over transactional context.\n","**Implement idempotency support backed by a database master node.**\n\n**Reasoning:**\n\n* **Reliability:** A database master node provides strong consistency and durability, ensuring that the result of an idempotent operation is persisted and accessible even in the event of failures.\n* **Performance:** In-memory data stores like Redis can be faster, but they may experience performance issues during high load or outages, which could impact idempotency guarantees.\n* **Simplicity:** Implementing idempotency with a database is simpler than using an in-memory data store, as it does not require additional setup or maintenance.\n\n**Details:**\n\n* The idempotency library will store a unique request identifier and the status of the request (e.g., success, failure) in a database table.\n* When a client sends a request with an idempotency key, the library will check the status in the database.\n* If the request has already been processed with a success or non-retriable error, the library will return the result without executing the request again.\n* If the request is new or has a retriable error, the library will execute the request and update the status in the database transactionally."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIdempotency guarantees for an action are normally provided for a **limited period of time**. That creates an opportunity for a service to periodically do a **cleanup** of stored actions and as a result avoid ever growing storage requirements and performance implications.\nWe’ve considered few approaches to this problem:\n- partition\n- periodical purge\nPerformance wise introducing partitions in idempotent_action table seemed like the best idea. Performance benefits outweighed any complications of facilitating and dropping partitions (i.e. there is no way to automatically create partition in Postgres). The issue comes with an absence of global index across partitions in some databases (i.e. Postgres) that creates a limitation that any idempotent action key has to **uniquely map into a partition**. Since the most natural choice of a partition key is a date, it would mean each `action_id` would need to come along with a date or have a date bit embedded, i.e. **UUIDv2**. So far we’ve tried to avoid imposing any limitations on a key format, hence we put the idea of partitioning on hold.\nPeriodical purge seemed like the only other feasible approach for a database that is lacking time-to-live (TTL) indexes.\nIt comes with its own challenges like multi-instance set up and DB performance.\nClients have to be extremely careful with batch size of purging to avoid any negative effect on database performance.\nTo avoid any need for co-ordination of purge job execution in a multi instance setup or a dependency on Zookeeper/etcd for a leader election we opted for a lightweight database scheduler `db-scheduler` that guarantees single instance execution.\n\n## Decision\n","Allow clients to configure retention policy for idempotent actions.\nClients can specify:\n- retention period (ISO-8601 format)\n- cron schedule for purge job (spring format)\n- batch-size for removal operation\n```yaml\nidempotence4j:\nretention:\nenabled: true\nperiod: P0Y0M20D\npurge:\nschedule: ""*/15 * * * * ?""\nbatchSize: 150\n```\nPurge job will only be executed on a single node in a multi instance set up.\n",Implement idempotency guarantees for actions with limited time-to-live (TTL) using periodic purge. Use a lightweight database scheduler (`db-scheduler`) to ensure single-instance execution of the purge job in a multi-instance setup.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSvelte offers a very interesting `readable` API that could work with this library.\nAn implementation might look similar to this:\n```ts\nexport function readable<T>(initialValue: T, updater?: Updater<T>) {\nconst value = ref(initialValue);\nupdater?.((newValue: T) => {\nvalue.current = newValue;\n});\nreturn {\nget current() {\nreturn value.current;\n},\n};\n}\n```\nThe idea behind it is that it would provide a readonly way to having changing content. Similar to what an Observable would provide.\nOne of the major questions, however, is whether this API would be beneficial or whether we should aim for something else.\n\n## Decision\n","While the `readable` API on its own offers a very nice functionality, it does not add enough to make up for the required learning effort as it does not blend in\nwell enough with the framework.\n","**Decision:**\n\nImplement the `readable` API as described in the Context, with the understanding that it provides a readonly way to access changing content similar to an Observable.\n\n**Advantages:**\n\n* Allows for a reactive approach to consuming data.\n* Provides a straightforward and concise way to manage changing data.\n* Aligns with the existing Svelte ecosystem and its emphasis on reactivity.\n\n**Considerations:**\n\n* Ensure that the implementation is performant and does not introduce any unnecessary overhead.\n* Explore potential use cases and provide examples to demonstrate the benefits of the API.\n* Consider whether additional features or optimizations are necessary based on user feedback and requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a communication mechanism between modules. We can let each module directly consume the API of another module, or\nwe could let each module subscribe to events published by other modules\n\n## Decision\n","Since in [ADR 2](0002-start-with-modular-monolith-backend.md) we decided to be ready for microservices, we will use the\nsecond approach - events as the communication mechanism\n",We decided to use an event-driven architecture. This will allow us to decouple the modules and make them more independent. It will also make it easier to add new modules in the future.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\nfor reads and writes at the same time in regard to performance and maintainability.\n\n## Decision\n",We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\ndomain entities each respectively on the Command (write) and Query (read) sides\n,"**Decision**: Use a polyglot persistence approach.\n\n**Rationale**: A polyglot persistence approach allows you to choose the right database for the right job. For example, you could use a relational database for structured data that needs to be queried frequently, and a NoSQL database for unstructured data that needs to be stored and retrieved quickly. This approach provides the flexibility and scalability you need to meet the demands of your application.\n\n**Consequences**:\n\n* Increased complexity: Managing multiple databases can be more complex than managing a single database.\n* Data consistency: You need to ensure that data is consistent across all of your databases.\n* Performance: The performance of your application will depend on the performance of the individual databases you choose."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR 5](0005-use-cqrs-architectural-style.md) and [ADR 6](0006-use-ddd-tactical-design-patterns.md) we decided to\nadopt CQRS and tactical DDD patterns respectively. In [ADR 7](0007-use-events-as-communication-mechanism-between-modules.md)\nwe decided that we'll use events as a means of communication between modules. Maintenance of practicalities behind these\npatterns can be quite overwhelming, especially that we are not very experienced.\n\n## Decision\n",We will use AxonFramework to aid in\n- Implementing EDA thanks to the provided EventBus support\n- Adopting the DDD building blocks\n- Implementing read model projections\n- Adopting EventSourcing (optional)\n,"In order to ease adoption of the CQRS, tactical DDD and event-driven paradigms, a framework should be applied. This will reduce the complexity of the overall solution and improve developer efficiency."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR 9](0009-develop-admin-tool-as-spa.md) we decided to develop admin tool as an SPA. Unlike admin tool, shopping\nhas different requirements:\n- User experience matters\n- Should be accessible for unauthenticated users\n- User interface should have unique look and feel, more suited for public use\n- Should be SEO friendly\n\n## Decision\n",We will develop shopping as a separate frontend application employing Server Side Rendering technique to improve the\nuser experience without compromising interactivity.\n,Develop shopping as a server side rendered application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhether to start with a Microservices or not. There are 2 schools each advocating for the exact opposites.\n### 1. [Don’t start with a monolith](https://martinfowler.com/articles/dont-start-monolith.html) - Stefan Tilkov\n> Microservices’ main benefit, in my view, is enabling parallel development by establishing a hard-to-cross boundary between different parts of your system. By doing this, you make it hard – or at least harder – to do the wrong thing: Namely, connecting parts that shouldn’t be connected, and coupling those that need to be connected too tightly. In theory, you don’t need microservices for this if you simply have the discipline to follow clear rules and establish clear boundaries within your monolithic application; in practice, I’ve found this to be the case only very rarely\n### 2. [MonolithFirst](https://martinfowler.com/bliki/MonolithFirst.html) - Martin Fowler\n> The first reason for this is classic Yagni. When you begin a new application, how sure are you that it will be useful to your users? It may be hard to scale a poorly designed but successful software system, but that's still a better place to be than its inverse. As we're now recognizing, often the best way to find out if a software idea is useful is to build a simplistic version of it and see how well it works out. During this first phase you need to prioritize speed (and thus cycle time for feedback), so the premium of microservices is a drag you should do without.\n> The second issue with starting with microservices is that they only work well if you come up with good, stable boundaries between the services - which is essentially the task of drawing up the right set of BoundedContexts. Any refactoring of functionality between services is much harder than it is in a monolith. But even experienced architects working in familiar domains have great difficulty getting boundaries right at the beginning. By building a monolith first, you can figure out what the right boundaries are, before a microservices design brushes a layer of treacle over them\n\n## Decision\n","We believe that in the early stages, when domain is still being explored and the project is small, monolith is the\neasiest and safest way to start.\nWe also acknowledge that as the project complexity and demand for shorter time-to-market increases, microservices\narchitecture becomes more and more valuable.\nTherefore, we decide to start with the monolith, with the strict boundaries between the modules, in order to benefit from\nthe ease of initial development and enable ourselves to evolve towards the microservices in the future\n","**Decision:** Start with a monolith and gradually migrate to microservices as needed.\n\n**Rationale:**\n\n* **Yagni principle:** It is not clear upfront if microservices provide significant value and complexity should be avoided until necessary.\n* **Refactoring difficulty:** Microservices introduce boundaries that make it harder to refactor functionality compared to a monolith.\n* **Bounded context uncertainty:** Identifying appropriate service boundaries upfront is difficult and can lead to architectural rigidity.\n* **Parallel development:** While microservices enable parallel development, this benefit can be achieved with proper discipline and boundaries within a monolith.\n\n**Additional Considerations:**\n\n* **Gradual migration:** Start with a monolith and extract specific components to microservices as they grow in complexity and need for isolation.\n* **Boundary identification:** Use monolith development to better understand the natural boundaries between system components.\n* **Validate first:** Build a simplistic monolith to validate the product idea and gather feedback before investing in microservices complexity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\ntechnology brings, in this case Spring Boot/Framework. Especially that it is designed with the abstraction and\nextensibility in mind, so introducing another layer of abstraction is senseless.\n\n## Decision\n",We accept direct coupling with Spring Boot/Framework code without any abstraction layers on top of it.\n,"**Decision:** Use Spring Boot/Framework with caution.\n\n**Rationale:**\n\nWhile Spring Boot/Framework offers benefits such as rapid development and ease of use, it is important to consider the potential impact on software flexibility. By tightly coupling code to Spring, replacing or updating these technologies in the future may become more challenging.\n\n**Additional Considerations:**\n\n* **Assess the long-term technology roadmap:** Determine whether the use of Spring Boot/Framework aligns with the organization's future technology plans.\n* **Explore alternative abstraction layers:** If additional flexibility is required, consider implementing abstraction layers on top of Spring that allow for easier technology replacement.\n* **Monitor technology progress:** Keep track of emerging technologies that may offer better flexibility or performance than Spring.\n* **Use a modular approach:** Structure the codebase in a modular manner to minimize coupling with Spring and facilitate technology replacements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince the backend is monolithic, we need to choose a language or at least a platform\n\n## Decision\n","We will develop on JVM platform, specifically we will use Kotlin for production code and Groovy for tests\n",Use Java with Spring Boot and Spring Cloud.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need some building blocks to model the domain in the software\n\n## Decision\n,"We will adopt tactical design patterns from DDD, that is we'll model the domain in terms of following objects:\n- ValueObject - immutable object without identity\n- Entity - object encapsulating some business entity with identity\n- Aggregate - graph of Entities and ValueObjects enforcing consistency boundaries and business invariants\n- AggregateRoot - entity acting as a root object in the Aggregate\n- Command - immutable object representing the intention to trigger the behavior of the aggregate\n- Event - immutable object representing the important fact that happened in the domain\n- Repository - collection-like storage for aggregates\n- DomainService - encapsulates the behavior that does not belong to any particular aggregate\n- Saga - long-running business transaction\n",Use Domain-Driven Design
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",We will use an Architectural Decision Record (ADR) template to document the architectural decisions made on this project. The ADR template will include the following sections:\n\n* Title: A brief title for the decision.\n* Date: The date the decision was made.\n* Author: The name of the person who made the decision.\n* Context: A brief description of the context in which the decision was made.\n* Decision: A clear and concise statement of the decision that was made.\n* Rationale: The reasons why the decision was made.\n* Consequences: The expected consequences of the decision.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the functional requirements, we need to have a means of managing the content through an intuitive web\ninterface. There are a couple of options to achieve this\n### 1. As a ""traditional"" web application in the MVC architectural style, embedded within the backend (monolith)\n#### Pros\n- Server-side rendering sometimes can provide better user experience since content is immediately visible once\nthe page is loaded\n- Compatibility issues between the frontend and backend can be detected at the earlier stage\n#### Cons\n- Tools and libraries for the backend and frontend are mixed in the same project, making it a bit messy and mentally harder to grasp\n- Strong coupling between web layer and backend discourages us to design general-purpose API for other types of potential consumers\n- Provides limited level of interactivity\n### 2. As a Single Page Application in the MVVM architectural style, packaged and deployed separately from the backend\n#### Pros\n- Frontend is more decoupled from backend technologies since they interact with each other via API\n- Backend API can be potentially used by other consumers too: CLI, Native & Mobile (as long as it is general-purpose)\n- Frontend and backend can be delivered independently (and hence faster) from each other\n- Provides greater level of interactivity\n#### Cons\n- More pipelines need to be maintained in CI\n- Compatibility issues between the frontend and backend may be detected later, during the integration stage\n- Complete client-side rendering may degrade the user experience a bit\n\n## Decision\n",We will go for the SPA approach\n,"**Decision:**\n\nOption 2: As a Single Page Application in the MVVM architectural style, packaged and deployed separately from the backend.\n\n**Reasons:**\n\n* **Decoupling:** The MVVM architectural style provides greater decoupling between the frontend and backend, allowing for greater flexibility and independence in development and deployment.\n* **Reusable API:** The backend API can be potentially used by other consumers, such as CLI, Native, and Mobile applications, enabling cross-platform functionality.\n* **Independent Delivery:** The frontend and backend can be delivered independently, reducing the time and effort required for deployment.\n* **Enhanced User Experience:** Single Page Applications provide a greater level of interactivity, improving the user experience.\n\n**Considerations:**\n\n* The decision to use a Single Page Application requires additional pipelines in CI for maintenance.\n* Compatibility issues between the frontend and backend may be detected later, during the integration stage.\n* Client-side rendering may slightly degrade the user experience.\n\nHowever, these considerations are outweighed by the benefits of decoupling, reusable API, independent delivery, and enhanced user experience provided by the Single Page Application option."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR 5](0005-use-cqrs-architectural-style.md) we decided to adopt CQRS architectural style. Therefore, we need a\nstrategy, or a guideline for handling commands issued to admin sub-system. Couple of things we can take into the\naccount:\n- Admin tool is intended for ""internal use"" only, i.e. it won't be exposed to the wild (customers)\n- There is no strict requirement about predictable and stable latency\n\n## Decision\n",Commands issued to admin will be handled synchronously by default. An asynchronous mode is still allowed though for\nexceptional cases only.\n,"Due to the internal usage of the admin tool and the lack of strict latency requirements, it is acceptable to use direct calls to the database when executing commands issued via the admin tool.\n\nThis approach provides the necessary flexibility and simplifies the implementation compared to using a message broker. However, it is important to ensure that appropriate security measures are in place to protect the database from unauthorized access."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEnsuring a consistent style in our current code and future changes on it. Make more clear and explicit for contributors\nto know what rules to follow.\n\n## Decision\n,"Establish an initial version of a code style guide based on (in priority order):\n1. Custom Rules\n1. [Airbnb Style Guide](https://github.com/airbnb/javascript)\n1. [ESLint `recommended` guide](https://github.com/eslint/eslint/blob/main/conf/eslint-recommended.js)\nThe first step allow us to iterate and modify existing rules, for instance: we do not agree with [Airbnb 7.1 section\nabout functions](https://github.com/airbnb/javascript#functions--declarations), that's why we don't have the\n`func-style` rule.\nTechnically, we have an ESLint configuration. We don't depend on Airbnb Style Guide (because of decision #0003) but we\nmanually selected the rules we'd like to have.\nIt's worth mentioning that this is just a start, it should be possible at any time to iterate and modify ANY rule.\n",**Decision:** Adopt a static code analyzer and autoformatter to enforce coding style consistency.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt's necessary for a testing tool to support asynchronous code, the community is using a lot async/await in their code.\nWe want Testy to be competitive with other testing tools, so we need to provide a solution for asynchronous code.\n\n## Decision\n",Fully implement asynchronous code support ([#106](https://github.com/ngarbezza/testy/issues/106)). This will allow us to\nuse async/await in both tests and hooks (before/after).\n,"**Decision:** Implement a mocking library for the testing tool.\n\n**Rationale:**\n\n* Mocks can be used to simulate asynchronous code behavior, allowing for reliable testing of asynchronous functions.\n* Mocking provides a structured and controlled way to test asynchronous code, isolating it from external dependencies.\n* A mocking library would enhance the testing tool's capabilities and make it more competitive with other offerings.\n\n**Pros:**\n\n* Increased reliability and control in testing asynchronous code.\n* Improved productivity and efficiency for developers.\n* Enhanced competitiveness of the testing tool.\n\n**Cons:**\n\n* Requires additional development effort to implement the mocking library.\n* May introduce complexity into the testing workflow.\n\n**Alternatives:**\n\n* **Use a third-party mocking library:** This option would save development time but may limit integration with the testing tool and require additional configuration.\n* **Create custom mocking solutions:** This approach would provide complete control but require significant development effort and maintenance.\n\n**Recommendation:**\n\nGiven the need for robust asynchronous testing support and the importance of competitiveness, we recommend implementing a mocking library for the testing tool. This solution would provide a reliable and efficient way to test asynchronous code, enhancing the tool's capabilities and aligning it with industry best practices."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKeeping the tool simple, minimal, easy to install and understand.\n\n## Decision\n",Do not depend on any NPM package unless there's a strong reason to do that. Keep the zero dependence counter as much\nas possible.\n,"Choose a simple, well-documented, widely-used tool with a small footprint."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nReducing complexity in the `ConsoleUI` object and make it more reusable and testable.\n\n## Decision\n,"`ConsoleUI` now only knows when to output things, but not the contents of messages, which is now responsibility of a\n`Formatter`. This object can be replaced by other formatters in the future.\n","Refactor the `ConsoleUI` object into smaller, more cohesive classes that are responsible for specific tasks. This will make the `ConsoleUI` object more manageable and easier to test."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving control of all the utilitary functions the tool might need.\n\n## Decision\n,"Use the `Utils` module for any responsibility whose domain does not seem to be part of other objects' essence. Every time you feel you need a ""helper"" function, it is a good candidate for `Utils`. If you need to rely on `typeof`, it is also a good sign for needing `Utils`. We want to maximize reuse of these functions by having in a single, ""static"" place. We understand this might be a temporary solution.\n","**Decision:** Develop an internal Python library for utility functions.\n\n**Rationale:**\n\n* Provides centralized control over all utilitary functions, ensuring consistency and maintainability.\n* Allows for code reuse and reduces duplication of effort.\n* Streamlines the development process by providing a common set of functions to all tool components.\n* Enables easy updates and maintenance of utility functions in a single location.\n* Enhances code readability and understanding by separating utility functions from business logic."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTests against the real system instead of test doubles was proven to be useful detecting bugs and exercising better our\nsystem.\n\n## Decision\n,Avoid introducing test doubles as much as possible. The only allowed exception is to stub/simulate external systems we\ncan't control. Library code should never be stubbed.\n,Use real system testing in addition to test doubles.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is an initiative about questioning the technical vocabulary we use, and avoiding some words that are widely used\nand may offend people. This is a project that adheres to this initiative, therefore...\n\n## Decision\n","Remove all existing technical vocabulary that might be offensive, and prevent those terms to be added in the future.\nFor instance, the use of ""master/slave"" replaced by ""main/replica"" (or similar), or ""whitelist/blacklist"" by\n""safelist/blocklist"" (or similar).\n",The decision is to avoid using offensive language in technical vocabulary.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMaking clear which versions are supported and how this will be updated as time passes.\n\n## Decision\n,"Only support Node versions with active and security support. Do not support newer, unstable versions.\nWe can use [endoflife.date](https://endoflife.date/nodejs) as a reference. Example: at the moment\nof this decision, only Node 12, 14 and 16 should be supported.\n",The supported versions of the software will be listed in the documentation. The documentation will be updated as new versions are released.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","Use a dedicated tool or platform to record and manage architectural decisions, such as ArchiMate, Archimate Model Exchange (AMX), or Decisions as Code. These tools provide a structured way to document decisions, capture rationale, and track their impact on the architecture."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\nThere isn't an obvious fit for this in the official REST API URI specifications.\n\n## Decision\n",The `/vpcs/{vpcId}/route-tables/{routeTableId}` URI will be used.\n* `{vpcId}` is the VPC ID.\n* `{routeTableId}` is the main route table ID\n,Encapsulate the main logic to construct the request in a re-usable module and pass the relevant information to it in the log event.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\nOf the available HTTP methods, there isn't a perfect fit for this use case.\nPUT & PATCH were generally recommended for similar scenarios.\n\n## Decision\n",The HTTP PATCH method will be used.\n,"**Decision:** Use the PUT method with a conditional request.\n\n**Rationale:**\n\n* **PUT:** Allows creating or updating a resource (the route in this case) with a specific identifier (the route name).\n* **Conditional request:** Ensures that the operation is only performed if the specified condition is met (e.g., the route doesn't already exist).\n\n**Implementation:**\n\n1. Construct a PUT request to update the route table with the desired route.\n2. Include an ""If-None-Match"" header with a value of ""*"", which indicates that the request should only be performed if the route doesn't already exist.\n\nThis approach ensures that the route is either added or replaced, depending on whether it exists in the route table."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe requested functionality was an API endpoint that would synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event.\nAll resources should be managed programmatically for an optimal possible user experience.\n\n## Decision\n","The AWS Amplify CLI toolchain will be used for programmatically creating, updating, and destroying project resources.\nThe endpoint will be defined in an AWS API Gateway, and the synchronization functionality will be defined in a Lambda function.\n",**Decision:** Create a Lambda function triggered by Cloud Logging that will:\n* Parse the log event to get the route information.\n* Use the Cloud Router API to create/update the route in the custom route tables.\n* Use the Projects API to assign IAM permissions to the custom route tables for the VPC network.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event.\nAccess to the API endpoint should be restricted since it can modify, custom route tables, but needs to be very simplistic and either accepts or rejects the call on the first request given the nature of the trigger.\nOptimally, the Lambda proxy function should not be called if the user does not present the proper information to minimize costs.\n\n## Decision\n",API Gateway API keys will be used to limit access given the constraints.\n,"Implement API Gateway as an authentication proxy in front of the Lambda function. API Gateway will check for the presence of the token. If the token is present and correct, the API Gateway will allow the Lambda function to be called. If not, the API Gateway will return a 401 Unauthorized error."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFeature requests for both summary routes and dynamic route discovery have been proposed so that new routes are automatically discovered and synchronized in order to reduce the risk of new routes being missed.\n\n## Decision\n,Programmatically making changes to a production routing table is serious business that has the potential to cause network outages.\nOnly specified routes will be synchronized to prevent unintentional changes.\n,"**Decision:** Implement both summary routes and dynamic route discovery.\n\n**Rationale:**\n\n* **Reduced risk of missed routes:** Summary routes will cover all known routes, ensuring that none are missed. Dynamic route discovery will capture new routes that are created after the initial summary is generated.\n* **Lower maintenance overhead:** Dynamic route discovery will automatically update the summary routes as new routes are added, reducing the need for manual maintenance.\n* **Improved flexibility:** Dynamic route discovery allows for more flexible route configuration, as new routes can be added or removed without requiring a manual update to the summary routes.\n* **Enhanced scalability:** As the network grows and new routes are added, dynamic route discovery will ensure that the summary routes remain up-to-date without requiring significant additional effort."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this [article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",**Decision:** To create an Architectural Decision Record (ADR) template and establish a process for documenting and reviewing architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a log rotation for the nginx webserver. As the nginx does not support log rotation out of the box, we need our own solution.\n\n## Decision\n","We implement the log rotation using the programm logrotate inside the docker container for the nginx webserver. We do not trigger the log rotation from outside of the docker container, because we want a solution that is independed from the host system.\n",Implement a custom log rotation solution using a cron job or a systemd timer to periodically rotate the logs. The solution should:\n\n- Compress the old logs into a tar.gz archive.\n- Move the archive to a separate directory for long-term storage.\n- Delete the old logs to free up disk space.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Utilize an Architectural Decision Record (ADR) framework to document and track architectural decisions.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the project a RDBMS need to be chosen and adopted.\n\n## Decision\n,- [PostrgreSQL](https://www.postgresql.org/) will be the relational database management system of choice.\n,"**Decision:** Adopt PostgreSQL as the RDBMS for the project.\n\n**Rationale:**\n\n* PostgreSQL is a powerful and open-source RDBMS with a proven track record.\n* It is well-suited for a wide range of applications, including data warehousing, enterprise resource planning (ERP), and customer relationship management (CRM).\n* PostgreSQL has strong support for data integrity, security, and concurrency control.\n* It is backed by a large and active community of developers and users.\n* PostgreSQL is compatible with a wide range of operating systems and hardware platforms.\n* It is also free and open-source, which makes it an attractive option for cost-sensitive projects."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the project a back end technology need to be chosen and adopted coupled with a unit test framework.\n\n## Decision\n,"[Nestjs](https://github.com/nestjs/nest) as a back end development framework, and [Jest](https://jestjs.io/) as a unit testing framework.\n### Pros:\n- Under the hood, Nest makes use of Express, but also, provides compatibility with a wide range of other libraries.\n- Very popular 35153 github stars.\n- Under Active development and maintenance.\n- Heavily inspired by Angular.\n### Cons:\n- None known.\n",**Decision:**\n\nAdopt **Golang** as the backend technology and **Ginkgo** as the unit test framework.\n\n**Rationale:**\n\n* **Golang:**\n    * Concurrency and high performance\n    * Strong support for unit testing and test coverage\n    * Simple and expressive syntax\n    * Extensibility through libraries and packages\n* **Ginkgo:**\n    * BDD (Behavior-Driven Development) framework\n    * Focus on readability and maintainability of tests\n    * Extensibility through custom matchers and reporters\n    * Easy integration with Golang for seamless unit testing
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the project a front end technology need to be chosen and adopted coupled with a unit test framework.\n\n## Decision\n,"[React](https://reactjs.org/) as a front end development library, and [Jest](https://jestjs.io/) as a unit testing framework.\n","**Decision:**\n\nAdopt React as the front-end technology and Jest as the unit test framework.\n\n**Reasons:**\n\n* **React:**\n    * Popular and widely used, with a large community and ecosystem.\n    * Component-based architecture for building complex UIs efficiently.\n    * High performance and scalability.\n\n* **Jest:**\n    * Fast and efficient, with a rich set of features.\n    * Seamless integration with React and provides support for various testing styles.\n    * Extensible and customizable to meet specific testing needs.\n\n**Alternative Considered:**\n\n* Vue.js and Angular with Mocha and Chai for testing.\n\n**Consequences:**\n\n* **Benefits:**\n    * Improved developer productivity and efficiency.\n    * Increased code quality and reliability.\n    * Enhanced testability and maintainability.\n* **Drawbacks:**\n    * React and Jest may have a steeper learning curve compared to some alternatives.\n    * Ongoing development and updates may require periodic adaptation."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the project a deployment and development mechanism is need to be chosen and adopted.\n\n## Decision\n,"- [Kubernetes](https://kubernetes.io/) will be the deployment technology of choice.\nThe project will be deployed into a kubernetes cluster using [helm](https://helm.sh/).\n- [Docker](https://www.docker.com/) will be the container building technology of choice.\nEvery thing will be directed through docker containers, from development to deployment. CI Pipeline will run inside a docker container as well.\n","**Decision:** To adopt a DevOps approach using a continuous integration and continuous delivery (CI/CD) pipeline.\n\n**Rationale:**\n\n* DevOps fosters collaboration between development and operations teams, streamlining the software development lifecycle.\n* CI/CD pipelines automate the build, test, and deployment processes, reducing errors and accelerating release times.\n* This approach enables rapid iteration, feedback loops, and improved software quality.\n* It aligns with the project's need for efficient deployment and development mechanisms."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the project a database structure need to be chosen and adopted.\n\n## Decision\n,Three table database will be used at this point:\n![database structure](./assets/db_structure.png)\n```\nTable book {\nisbn varchar(50) [pk]\ntitle varchar(300)\nyear char(4)\ncategory varchar(300)\nsub_category varchar(300)\n}\nTable author {\nid bigserial [pk]\nname varchar(300)\n}\nTable book_author {\nbook_isbn varchar(50) [ref: > book.isbn]\nauthor_id bigint [ref: > author.id]\n}\n```\n,"**Decision:** Adopt a relational database structure for the project.\n\n**Rationale:**\n\n* **Structured Data:** Relational databases are optimized for handling structured data, which is the type of data typically used in business systems.\n* **Data Integrity:** Relational databases enforce data integrity through constraints and referential relationships, ensuring data accuracy and consistency.\n* **Scalability:** Relational databases can handle large volumes of data and scale efficiently as the database grows.\n* **Performance:** Relational databases provide efficient data access and query capabilities, especially when using appropriate indexes and optimizations.\n* **Industry Standard:** Relational databases are widely adopted in the industry and have a large ecosystem of tools and technologies available for support.\n* **Flexibility:** Relational databases allow for flexible data modeling and can be adapted to changing business requirements relatively easily."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to choose our lint and format tools, and the code style to enforce as well.\n\n## Decision\n","Chosen option: ""Prettier-ESLint with Airbnb code style"", because\n- Airbnb code style is widely used (see [npm trends](https://www.npmtrends.com/eslint-config-airbnb-vs-eslint-config-google-vs-standard-vs-eslint-config-standard))\n- Prettier-ESLint enforce some additional code style. We like it because the more opinionated the code style is, the less debates there will be :-)\nIn addition, we use also Prettier to format json and markdown files.\n### Positive Consequences <!-- optional -->\n- Developers are encouraged to use the [Prettier ESLint](https://marketplace.visualstudio.com/items?itemName=rvest.vs-code-prettier-eslint) and [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode) VSCode extensions while developing to auto-format the files on save\n- And they are encouraged to use the [ESLint VS Code extension](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint) as well to highlight linting issues while developing\n","**Decision:**\n\nAdopt the following code linting and formatting tools and code style:\n\n* **Linting tool:** [ESLint](https://eslint.org/)\n* **Formatting tool:** [Prettier](https://prettier.io/)\n* **Code style:** [AirBnB Style Guide](https://github.com/airbnb/javascript)\n\n**Rationale:**\n\n* **ESLint:** Widely used in the industry, highly customizable, and supports a wide range of rules.\n* **Prettier:** Enforces consistent code formatting, reducing the need for manual formatting and improving code readability.\n* **AirBnB Style Guide:** Well-established, widely adopted, and aligns with industry best practices.\n\n**Consequences:**\n\n* **Improved code quality:** Lint and format tools help identify and fix potential code errors and inconsistencies.\n* **Reduced development time:** Automatic code formatting saves time and reduces the need for manual formatting tasks.\n* **Improved team collaboration:** Enforcing a consistent code style facilitates easier code review and collaboration.\n* **Adherence to industry standards:** Adopting industry-standard code styling and linting practices improves project credibility and aligns with broader development practices."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to provide a search bar to perform full-text search on ADRs.\n## Decision Drivers <!-- optional -->\n- Works in preview mode AND in the statically built version\n- Provides good fuzzy search and stemming capabilities\n- Is fast enough to be able to show results while typing\n- Does not consume too much CPU and RAM on the client-side, especially for the statically built version\n\n## Decision\n","- Works in preview mode AND in the statically built version\n- Provides good fuzzy search and stemming capabilities\n- Is fast enough to be able to show results while typing\n- Does not consume too much CPU and RAM on the client-side, especially for the statically built version\nChosen option: ""Option 2: Lunr.js"".\n",- Use ElasticSearch in the backend to perform fuzzy search and stemming\n- Use Vuelidate library to handle input validation\n- Use TailwindCSS library to handle styling
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\n- Run `npx init-log4brains`\n- Which installs locally `@log4brains/cli` and `@log4brains/web`\n- And creates custom entries in `package.json`'s `scripts` section:\n- `""log4brains-preview"": ""log4brains-web preview""`\n- `""log4brains-build"": ""log4brains-web build""`\n- `""adr"": ""log4brains adr""`\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\n## Decision Drivers <!-- optional -->\n- Simplicity of the installation procedure\n- Language agnostic\n- Initialization script works on any kind of project\n- Faster ""getting started""\n\n## Decision\n","- Simplicity of the installation procedure\n- Language agnostic\n- Initialization script works on any kind of project\n- Faster ""getting started""\nThe new installation procedure is now language agnostic and will be the following:\n```bash\nnpm install -g log4brains\nlog4brains init\n```\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\n- This global package will require the existing `@log4brains/cli` and `@log4brains/web` packages\n- `init-log4brains` will be renamed to `@log4brains/init` and required as a dependency\n### Consequences\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\nWe may implement a [xojs/xo](https://github.com/xojs/xo)-like behavior later: ""the CLI will use your local install of XO when available, even when run globally.""\n",The installation procedure for Log4Brains will be simplified and made language-agnostic through the following steps:\n\n1. Create a new `log4brains-cli` package that will contain the initialization script and the necessary dependencies.\n2. Make the `log4brains-cli` package available as a standalone installation via npm.\n3. Update the documentation to reflect the new installation procedure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe markdown body of ADRs cannot be used as is, because:\n- Links between ADRs have to be replaced with correct URLs\n- Header (status, date, deciders etc...) has to be rendered with specific components\n## Decision Drivers <!-- optional -->\n- Potential future development of a VSCode extension\n\n## Decision\n","- Potential future development of a VSCode extension\nChosen option: ""Option 2: the core API is responsible (with MDX)"".\nBecause if we develop the VSCode extension, it is better to add more business logic into the core package, and it is better tested.\n### Positive Consequences <!-- optional -->\n- The metadata in the header is simply removed\n### Negative Consequences <!-- optional -->\n- Each UI package will have to implement its own Header component\n",Provide a markdown transformer to transform the ADR markdown body to the working version.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to define the initial overall architecture of the project.\nFor now, we are sure that we want to provide these features:\n- Local preview web UI\n- Static Site Generation from the CI/CD\n- CLI to create a new ADR quickly\nIn the future, we might want to provide these features:\n- Create/edit ADRs from the local web UI\n- VSCode extension to create and maybe edit an ADR from the IDE\n- Support ADR aggregation from multiple repositories\n\n## Decision\n","Chosen option: ""Multi-packages, monorepo, with Yarn and Lerna"", because\n- We don't want a monolith because we want the core library/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\n","**Decision:**\n\nThe initial architecture of the project will consist of the following components:\n\n* **Local preview web UI:** A web UI that allows users to preview ADRs locally.\n* **Static Site Generation from the CI/CD:** A CI/CD pipeline that will generate a static site from the ADRs in the repository.\n* **CLI to create a new ADR quickly:** A command-line interface (CLI) tool that will allow users to quickly create new ADRs.\n\n**Rationale:**\n\nThese components provide the core functionality that is required for the project at this stage. They are also relatively easy to implement and maintain.\n\nThe future features that are mentioned in the context and problem statement can be added later, as needed."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe need to be able to identify uniquely an ADR, especially in these contexts:\n- Web: to build its URL\n- CLI: to identify an ADR in a command argument (example: ""edit"", or ""preview"")\n\n## Decision\n","Chosen option: ""ADR number"", because\n- It is possible to have duplicated titles\n- The filename is too long to enter without autocompletion, but we could support it as a second possible identifier for the CLI in the future\n- Other ADR tools like [adr-tools](https://github.com/npryce/adr-tools) already use the number as a unique ID\n",The ADR ID will be composed of the following formula: `YYYY-MM-DD-number` where:\n\n* `YYYY` is the year when the ADR was created\n* `MM` is the month when the ADR was created\n* `DD` is the day when the ADR was created\n* `number` is a sequential number starting from 1 for each day
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nCurrently, ADR files follow this format: `NNNN-adr-title.md`, with NNNN being an incremental number from `0000` to `9999`.\nIt causes an issue during a `git merge` when two developers have created a new ADR on their respective branch.\nThere is a conflict because [an ADR number must be unique](20200926-use-the-adr-number-as-its-unique-id.md).\n\n## Decision\n","From now on, we won't use ADR numbers anymore.\nAn ADR will be uniquely identified by its slug (ie. its filename without the extension), and its filename will have the following format: `YYYYMMDD-adr-title.md`, with `YYYYMMDD` being the date of creation of the file.\nAs a result, there won't have conflicts anymore and the files will still be correctly sorted in the IDE thanks to the date.\nFinally, the ADRs will be sorted with these rules (ordered by priority):\n1. By Date field, in the markdown file (if present)\n2. By Git creation date (does not follow renames)\n3. By file creation date if no versioned yet\n4. By slug\nThe core library is responsible for sorting.\n",Rename ADR files to `<ADR-ID>.md` with `<ADR-ID>` being a GUID/UUID.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wanted to explore Vault's capabilities to serve as a CA for services deployed on the strategic platform at HMCTS, that's to issue the certificates services may need for authentication purposes.\n\n## Decision\n","In order to discuss the CA capabilities Vault has we need first to talk about Secret Backends. Secret Backends are the components in Vault which store and generate secrets, they are part of the mount system in Vault. They behave very similarly to a virtual filesystem: any read/write/delete is sent to the secret backend, and the secret backend can choose to react to that operation however it sees fit.\nFor CA purposes Vault provides the PKI Secret Backend which generates X.509 certificates dynamically based on configured roles. This means services can get certificates needed for both client and server authentication without going through the usual manual process of generating a private key and CSR, submitting to a CA, and waiting for a verification and signing process to complete.\nVault's documentation [quick-start](https://www.vaultproject.io/docs/secrets/pki/index.html#quick-start) on this subject illustrate pretty well some of the operational concepts behind running Vault as a CA. The documentation also presents some of the [considerations](https://www.vaultproject.io/docs/secrets/pki/index.html#considerations) needed when using the PKI backend but we believe the [""Be Careful with Root CAs""](https://www.vaultproject.io/docs/secrets/pki/index.html#be-careful-with-root-cas) section require special attention and that's why we focused our spike on showing how Vault can be used to create a Root CA and an Intermediate CA for our organization.\nWith the above in mind we start by creating the Root CA for our org. We begin by mounting the PKI backend for our hmcts Root CA:\n```code\n/ # vault mount -path=hmcts -description=""HMCTS Root CA"" pki\nSuccessfully mounted 'pki' at 'hmcts'!\n/ #\n```\nNow we can create the CA certificate\n```code\n/ # vault write hmcts/root/generate/internal \\n> common_name=""HMCTS Root CA"" \\n> key_bits=4096 \\n> exclude_cn_from_sans=true\nKey             Value\n---             -----\ncertificate     -----BEGIN CERTIFICATE-----\nMIIFADCCAuigAwIBAgIUd0VBVb7dPA3fO8X36dt9LVhBpVcwDQYJKoZIhvcNAQEL\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNTU3MzRaFw0x\nNzA4MjgxNTU4MDRaMBgxFjAUBgNVBAMTDUhNQ1RTIFJvb3QgQ0EwggIiMA0GCSqG\n...\n<output truncated>\n...\n-----END CERTIFICATE-----\nexpiration      1503935884\nissuing_ca      -----BEGIN CERTIFICATE-----\nMIIFADCCAuigAwIBAgIUd0VBVb7dPA3fO8X36dt9LVhBpVcwDQYJKoZIhvcNAQEL\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNTU3MzRaFw0x\nNzA4MjgxNTU4MDRaMBgxFjAUBgNVBAMTDUhNQ1RTIFJvb3QgQ0EwggIiMA0GCSqG\n...\n<output truncated>\n...\n4QnBYzvMNlxCHyhlrmG7+TzRXRYEV2HSV9VNX0/+DfGxo2Xk2UsXYDq3BvwUPkk7\nW47N/TSaO1kFNMFhE+QNeAzddp4SFrpoWrIZ1Z1mPo12Jl8aW89hzbj8CUHtv12t\n6/U0lfBERcz13VzPl5pAfEWQs3/DJcfKJXVodkcMaD7BhDkk\n-----END CERTIFICATE-----\nserial_number   77:45:41:55:be:dd:3c:0d:df:3b:c5:f7:e9:db:7d:2d:58:41:a5:57\n/ #\n```\nUsing `curl` and `openssl` we can verify the certificate we just created:\n```code\n/ # curl -s http://127.0.0.1:1234/v1/hmcts/ca/pem | openssl x509 -text\nCertificate:\nData:\nVersion: 3 (0x2)\nSerial Number:\n77:45:41:55:be:dd:3c:0d:df:3b:c5:f7:e9:db:7d:2d:58:41:a5:57\nSignature Algorithm: sha256WithRSAEncryption\nIssuer: CN=HMCTS Root CA\nValidity\nNot Before: Jul 27 15:57:34 2017 GMT\nNot After : Aug 28 15:58:04 2017 GMT\nSubject: CN=HMCTS Root CA\nSubject Public Key Info:\nPublic Key Algorithm: rsaEncryption\nPublic-Key: (4096 bit)\nModulus:\n00:a1:d8:f3:2a:43:09:6c:39:42:4e:f1:b8:c9:86:\n82:e5:ed:4e:34:b2:92:f7:c6:d2:68:10:a2:46:ec:\n...\n<output truncated>\n```\nWe can now proceed to configure the URL's for accessing not only the CA but also the Certificate Revocation List (CRL):\n```code\n/ # vault write hmcts/config/urls issuing_certificates=""http://127.0.0.1:1234/v1/hmcts""\nSuccess! Data written to: hmcts/config/urls\n/ #\n```\nNote that we used the localhost ip address in the issuing_certificates URL only for demo purposes and this can be set to any address as appropriate. With the Root CA ready we can now create an Intermediate CA:\n```code\n/ # vault write hmcts/config/urls issuing_certificates=""http://127.0.0.1:1234/v1/hmcts""\nSuccess! Data written to: hmcts/config/urls\n/ # vault mount -path=hmcts_probate -description=""Probate Intermediate CA"" pki\nSuccessfully mounted 'pki' at 'hmcts_probate'!\n/ # vault mounts\nPath            Type       Default TTL  Max TTL  Force No Cache  Replication Behavior  Description\ncubbyhole/      cubbyhole  n/a          n/a      false           local                 per-token private secret storage\nhmcts/          pki        system       system   false           replicated            HMCTS Root CA\nhmcts_probate/  pki        system       system   false           replicated            Probate Intermediate CA\nsecret/         generic    system       system   false           replicated            generic secret storage\nsys/            system     n/a          n/a      false           replicated            system endpoints used for control, policy and debugging\n/ #\n```\nNow we proceed to generate the Intermediate Certificate Signing Request (CSR):\n```code\n/ # vault write hmcts_probate/intermediate/generate/internal \\n> common_name=""Probate Intermediate CA"" \\n> key_bits=4096 \\n> exclude_cn_from_sans=true\nKey     Value\n---     -----\ncsr     -----BEGIN CERTIFICATE REQUEST-----\nMIIEZzCCAk8CAQAwIjEgMB4GA1UEAxMXUHJvYmF0ZSBJbnRlcm1lZGlhdGUgQ0Ew\nggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQDBAzV85KEbAH3UYgxrR6F5\n...\n<output truncated>\n...\nqoaquy/+vS6o5Fg2e4XE0ZNMDpTj37visONk\n-----END CERTIFICATE REQUEST-----\n/ #\n```\nNow we need to use the CSR to get a cert signed by the Root CA. For this we save/copy the contents of the CSR onto a file (`probate.csr`) and send it to the Root CA backend:\n```code\n/ # vault write hmcts/root/sign-intermediate \\n> csr=@probate.csr \\n> common_name=""Probate Intermediate CA"" \\n> ttl=24h\nKey             Value\n---             -----\ncertificate     -----BEGIN CERTIFICATE-----\nMIIFjTCCA3WgAwIBAgIUUGSq+TcwSQnLtkFP5WjS/yk1T5kwDQYJKoZIhvcNAQEL\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNjMzMTNaFw0x\n...\n<output truncated>\n...\nIAO2K7gqK1gVV1DmXWSpmTcT4mUXFgGWtM3TP2ILV3lYRzRkkGK2PULJGEWS/0pc\nh1286BOsydSM2Ai6+bnEbV0s/6X0YL4L3WlYxQgEkNuP\n-----END CERTIFICATE-----\nexpiration      1501259623\nissuing_ca      -----BEGIN CERTIFICATE-----\nMIIFADCCAuigAwIBAgIUd0VBVb7dPA3fO8X36dt9LVhBpVcwDQYJKoZIhvcNAQEL\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNTU3MzRaFw0x\n...\n<output truncated>\n...\nW47N/TSaO1kFNMFhE+QNeAzddp4SFrpoWrIZ1Z1mPo12Jl8aW89hzbj8CUHtv12t\n6/U0lfBERcz13VzPl5pAfEWQs3/DJcfKJXVodkcMaD7BhDkk\n-----END CERTIFICATE-----\nserial_number   50:64:aa:f9:37:30:49:09:cb:b6:41:4f:e5:68:d2:ff:29:35:4f:99\n/ #\n```\nNow we need to import the Root CA signed cert we created in the previous step back to the Intermediate CA backend. For this we copy the contents to a file `probate.crt`:\n```code\n/ # vault write hmcts_probate/intermediate/set-signed \\n> certificate=@probate.crt\nSuccess! Data written to: hmcts_probate/intermediate/set-signed\n/ #\n```\nUsing `curl` and `openssl` we can verify the contents of this certificate:\n```code\n/ # curl -s http://127.0.0.1:1234/v1/hmcts_probate/ca/pem | openssl x509 -text\nCertificate:\nData:\nVersion: 3 (0x2)\nSerial Number:\n20:e8:ca:3d:49:33:e6:b0:65:32:4e:9c:9a:84:a3:20:90:0c:ae:61\nSignature Algorithm: sha256WithRSAEncryption\nIssuer: CN=HMCTS Root CA\nValidity\nNot Before: Jul 27 16:54:40 2017 GMT\nNot After : Jul 28 16:55:10 2017 GMT\nSubject: CN=Probate Intermediate CA\nSubject Public Key Info:\nPublic Key Algorithm: rsaEncryption\nPublic-Key: (4096 bit)\nModulus:\n00:c1:03:35:7c:e4:a1:1b:00:7d:d4:62:0c:6b:47:\na1:79:a0:c7:a8:7e:9e:1d:0b:b1:90:6e:6a:cd:96:\n...\n<output truncated>\n```\nJust like we did with the Root CA, we can now configure the CA and CRL URLs for our Intermediate CA:\n```code\n/ # vault write hmcts_probate/config/urls \\n> issuing_certificates=""http://127.0.0.1/v1/hmcts_probate/ca"" \\n> crl_distribution_points=""http://127.0.0.1/v1/hmcts_probate/crl""\nSuccess! Data written to: hmcts_probate/config/urls\n/ #\n```\nBefore we can request certificates we should establish some restrictions around the certificates we generate like key types, ttl, etc ... For this we use the `role` functionality provided by vault:\n```code\n/ # vault write hmcts_probate/roles/frontend \\n> key_bits=2048 \\n> max_ttl=10h \\n> allow_any_name=true\nSuccess! Data written to: hmcts_probate/roles/frontend\n/ #\n```\nand now we should be able to issue certs for this role:\n```code\n/ # vault write hmcts_probate/issue/frontend \\n> common_name=""test.probate.hmcts.net"" \\n> ip_sans=""172.16.0.1"" \\n> ttl=5h \\n> format=pem\nKey                     Value\n---                     -----\nca_chain                [-----BEGIN CERTIFICATE-----\nMIIFjTCCA3WgAwIBAgIUIOjKPUkz5rBlMk6cmoSjIJAMrmEwDQYJKoZIhvcNAQEL\nBQAwGDEWMBQGA1UEAxMNSE1DVFMgUm9vdCBDQTAeFw0xNzA3MjcxNjU0NDBaFw0x\nNzA3MjgxNjU1MTBaMCIxIDAeBgNVBAMTF1Byb2JhdGUgSW50ZXJtZWRpYXRlIENB\n...\n<output truncated>\n```\n",Adopt Vault as a CA for the strategic platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe term 'environment' is incredibly overloaded and means different things to different people\neven on the same team. In the context of CNP infrastructure and applications, one developer's environment might be\nanother's Azure subscription, or might be another's demo application location.\nIn the Jenkins library code, the single variable `environment` (or `env`, etc) is passed around with different meanings at both the calling and receiving sites. This causes confusion and makes understanding and debugging the library difficult. Also, having 3 separate concepts represented by a single variable introduces a source of potential bugs and reduces the flexibility of the overall solution. E.g. at the moment we can only have one environment per subscription.\nWhere developers have realised this limitation, other terms and variable names have been used locally but there is no consistent, shared set of terms so different names have been used to represent the same concepts.\n\n## Decision\n","Use three terms consistently to distinguish between the subscription, environment and application context.\nThe three terms are:\n| Term | Variable name | Example values | Definition |\n| ---- | ----|  ----------- | -- |\n| Subscription | `subscription`| `nonprod`, `prod` | The identifier of the Azure subscription to use for infrastructure operations. |\n| Environment | `environment` | `dev`, `aat`, `prod` | A named instance of an Azure ASE and all the supporting infrastructure including DNS, WAF, etc. Three instances named `dev`, `aat` and `prod` will exist by default. The special value `prod` will always represent environments in the `prod` subscription, all other environments will exist in the `nonprod` subscription. |\n| Application Context (TBA) | `application_context` | `demo1`, `aat`, `prod` | An application-level suffix to support deploying the same application many times in the same environment. In particular, allowing for named demo and exploratory instances of the application to deploy to the `dev` environment. The value could, for example, be the (non-`master`) branch name. The special values `aat` and `prod` are only deployable from the `master` branch and map to the `aat` and `prod` environments respectively. |\nThese terms should have a consistent meaning across all method and function calls. Method\nand function definitions should use the convention above.\n",Rename the single `environment` variable to `subscription`. Introduce a new `context` variable to represent the multi-dimensional concept of the current execution. The `context` variable will likely have a type of dictionary. The dictionary will contain the `subscription` value and other values that the library needs to know about the current execution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAdoption of the CNP brings with it the ability to change and improve the developer workflow when making application code changes.  As per [5. Application Delivery Pipelines](0005-pipeline-design.md) the pipeline is opinionated and promotes the practice of Continuous Delivery.  This proposal goes into the detail of the expected workflow of a developer making an application change in the context of CNP using Continuous Delivery.\n### What is Developer Workflow?\nDeveloper workflow is the process developers use to create application changes and prepare them for release in a reliable manner.  This includes the branching strategies, code review, and testing, all with a heavy reliance on the pipeline.\n\n## Decision\n","The following patterns are proposed in order to facilitate confident code changes with fast feedback.\n### Trunk based development using short lived feature branches\nTrunk based development is an enabler for Continuous Delivery via Continuous Integration.  Changes are frequently made to the master branch (trunk) whilst ensuring the master branch is always in a releasable state.  The releasable state is supported by a wide range of automated testing and development strategies for managing change such as feature toggles or branch by abstraction.\nA short lived feature branch is a branch created with a specific and small change as its purpose.  The branch should live no more than a couple of days, preferably under a single day, before it is merged into master and deleted.  The merging takes place via a Pull Request in the [Github Flow](https://guides.github.com/introduction/flow/) (not to be confused with the more onerous [GitFlow](http://endoflineblog.com/gitflow-considered-harmful)).  There should be no more than one short lived feature branch per developer and are not shared.  This is easily achievable when change is small, focussed and frequently merged into master.\nThe master branch of each repository should be configured as a Protected Branch, thereby preventing force pushes, deletion and requiring status checks before merging.\n![Diagram of a short lived feature branch with commits, a PR, comments and a merge to master](../../img/trunk_pr.png)\n<br />\n_Image from [TrunkBasedDevelopment.com](https://trunkbaseddevelopment.com/short-lived-feature-branches/)_\nIntegration is frequent, test are run automatically and master is kept green with failed builds fixed within ten minutes as per Martin Fowler's [Continuous Integration Certification Test](https://martinfowler.com/bliki/ContinuousIntegrationCertification.html).\n### Use Pull Requests for code review\nCode reviews are an enabler for improving code quality and knowledge sharing.  Combined with short lived feature branches, small pull requests (PR) enable fast feedback and an early opportunity to incorporate it.\nThe in-built support within Github is excellent for facilitating reviews and each repository should configured to require at least one review before merge.\nThe creation of a pull request will trigger a shortened pipeline.  The PR code will be applied to the master code locally on the Jenkins agent workspace, as such simulating a merge.  The shortened pipeline will then run the static checks (i.e. checks which do not require deployment to an environment).  This will include running unit tests and static analysis.  These checks, combined with a peer review, should provide enough confidence that a PR is ready for merge.\n![Diagram of a pull request going through static checks before merge](../../img/Pull-Request-Pipeline-Flow.png)\nOnce a PR is merged, a full pipeline will be triggered and the integrated code will once again pass through the static checks before a deployment stage and on to the Production subscription.\n### Use strategies to allow deployment to continue whilst controlling the release of changes\nIn a Continuous Delivery world, using short lived feature branches, ideally all changes will be small enough to fit into this process.  For the times that this is not the case, it is important to separate the action of ""deploying code"" from ""releasing a change"".  This is to allow the code to continue to be tested and deployed whilst avoiding big bang deployments.\nOne strategy is [Branch by Abstraction](https://martinfowler.com/bliki/BranchByAbstraction.html)"".  This is a technique for making a large-scale change to a software system in gradual way that allows you to release the system regularly while the change is still in-progress.\nAnother strategy is [Feature Toggles](https://martinfowler.com/articles/feature-toggles.html).  At their most basic, they are flags that disable or enable code paths through an application.  There are multiple categories of feature toggles allowing for different levels of change, from experimentation to controlling a release.\n### Blue/Green deployments support ""zero downtime deploys""\nTo support frequent, small deployments we will use [Blue/Green deployment mechanisms](https://martinfowler.com/bliki/BlueGreenDeployment.html) to ensure zero downtime during the process.  Production is split into two subsets of apps: one set serving live traffic and another parallel set which is not.  Deployments are sent to the non-live set and tests are run against them.  If the tests are successful a manual switch is hit and the traffic is routed to this subset making it live.\n![Diagram of a blue/green environment and traffic being routed from one to the other](../../img/BlueGreen.png)\nThis switch requires that new deployments of the applications are backwards compatible with the deployment currently receiving live traffic.  As a result, changes become small, gradual and less risky.\n### Managing database changes whilst using Blue/Green deployments\nWith a Blue/Green deployment any data store is shared between both subsets.  To support this, changes to the schema must be done in phases in tandem with application deployments. Recommended reading is the blog post [Database Migrations Done Right](http://www.brunton-spall.co.uk/post/2014/05/06/database-migrations-done-right/)\nThis is explored in [10. Developer Workflow for DB Schema Changes](0010-developer-workflow-for-db-schema-changes.md).\n",**Decision:** Adopt a Continuous Delivery workflow for application code changes using the Continuous Integration/Continuous Delivery (CI/CD) pipeline.\n\n**Benefits:**\n\n* Improved developer productivity and efficiency\n* Reduced time to market for new features and bug fixes\n* Increased reliability and stability of application changes\n* Alignment with the recommended pipeline design and Continuous Delivery practices
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to run webapps within a fully private and secure Application Service Environment we needed to come up with a DNS solution so that when web applications are put into service these should be discoverable and available for consumption by other INTERNAL services. The ""Internal"" part of the description is fundamental within this context and it defines the scope of this proposal. In this context we are also assuming that each web application will have it's own DNS name, which will be dynamically generated by convention meanind DNS records will need to be updated dinamically.\n\n## Decision\n","Although DNS offerings like BIND, unbound or dnsmasq allow us to provide name resolution services for our problem, we felt that none of them provided options good enough to update name records dynamically out of the box. As a solution for this particular challenge we have decided to start leveraging Consul's DNS interface, which allows applications to make use of service discovery without any high-touch integration with Consul.\nFor our particular case we are proposing to have DNS service lookups directed to Internal domains to be forwarded from a current/new DNS server (i.e BIND or Dnsmasq) to our Consul service. For example a host can use the DNS server directly via name lookups like `rhubarb-frontend.service.internaldomain`. This query automatically translates to a lookup of the IP that provide the rhubarb-frontend service, are located in the <internal domain> ASE. The following drawing give us a high level overview of the proposed solution:\n![Internal DNS](../../img/internal-dns-proposal.png)\nThis approach give us a number of advantages including simplification of the DNS server configuration (a single config entry can forward all queries about internaldomain to our consul service) or reusing existing DNS infrastructure (i.e. Support for BIND, Dnsmasq, Unbound, etc ...), but more importantly it allow us to update DNS records with very simple API calls as demonstrated below.\nConsider an example setup where a Dnsmasq server is running on `10.0.1.4` and for demonstration purposes the Consul server is running on the same node. Consul is listening on tcp port 8500 for app registration and tcp port 8600 for DNS queries forwarded from Dnsmasq, we can then configure dnsmasq to forward all lookup queries for domain `internal` to consul:\n```code\n[dsanabria@dnsserver ~]$ cat /etc/dnsmasq.d/10-consul\nserver=/internal/127.0.0.1#8600\n[dsanabria@dnsserver ~]$\n```\nIf a new service wants to register to our DNS system this can be achieved with a very simple API call as shown in the following snippet:\n```code\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.service.internal\nHost mojwaftest-dev.service.internal.3ipn3ped5wke5cbmc154jrnarb.zx.internal.cloudapp.net not found: 5(REFUSED)\n[dsanabria@dnsserver ~]$ cat service.json\n{\n""ID"": ""mojwaftest-dev"",\n""Name"": ""mojwaftest-dev"",\n""Tags"": [],\n""Address"": ""10.0.4.9"",\n""Port"": 443\n}\n[dsanabria@dnsserver ~]$ curl --request PUT --data @service.json http://localhost:8500/v1/agent/service/register\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.service.internal\nmojwaftest-dev.service.internal has address 10.0.4.9\n[dsanabria@dnsserver ~]$\n```\nAnother advantage of embracing consul is that it lead us to start adopting best naming practices by following specific methods for service lookups. In consul service queries support two lookup methods: standard and strict RFC 2782, for our use case we are recommending the ""standard"" service lookup method.\nThe format of a standard service lookup is:\n```code\n[tag.]<service>.service[.datacenter].<domain>\n```\nThe tag and datacenter parts are optional but we can see, however, that the service part is static and can't be changed. At first this can be seen as a limitation on how services are named at provisioning time but by leveraging Azure's webapp custom domains we can implement naming patterns driven by good industry practices.\nAll our webapp based services can easily follow these conventions since we, as operators, have good control over their settings, but the Azure Source Control Management (SCM) is an exception to this rule mainly because we have very little control over its naming conventions (i.e. Azure force us to use `<service name>.scm.<internal domain>`. In order to comply with both, Industry best practices and Azure's own conventions, we can register SCM as a service in consul and update its tags when services are registered. The following cli session illustrates how this can be accomplished:\n```code\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.scm.service.internal\nHost mojwaftest-dev.scm.service.internal.3ipn3ped5wke5cbmc154jrnarb.zx.internal.cloudapp.net not found: 5(REFUSED)\n[dsanabria@dnsserver ~]$ cat scm.json\n{\n""ID"": ""scm"",\n""Name"": ""scm"",\n""Tags"": [""mojwaftest-dev""],\n""Address"": ""10.0.4.9"",\n""Port"": 443\n}\n[dsanabria@dnsserver ~]$ curl --request PUT --data @scm.json http://localhost:8500/v1/agent/service/register\n[dsanabria@dnsserver ~]$ host mojwaftest-dev.scm.service.internal\nmojwaftest-dev.scm.service.internal has address 10.0.4.9\n[dsanabria@dnsserver ~]$\n```\n",Establish a DNS service in the internal network to resolve internal application names and provide dynamic DNS updates.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs per CNP-129: ""Ensure no data loss for Jenkins down time"", the requirements were to:\n1.  Ensure no data loss for Jenkins down time\n2.  Use a separate data drive for Jenkins\n3.  Ensure that data drive has replication enabled\n4.  Ensure that the data drive is backed up\n5.  Ensure when the Jenkins box is rebuild, it mounts the (above) drive\n6.  All data drives for IAAS should be encrypted\n7.  manually Enable recovery VAULT for Jenkins\n\n## Decision\n","#### Data Loss\nCurrently, the terraform script to create the infrastructure creates an additional volume as a jenkins ""data volume"".  This resolves point 2) and partially resolves point 1) as block devices currently support Locally Redundant Storage (LRS) with data being replicated to at least 3 places within the same data center.\nOriginally, the intention was to use Azure Files (https://azure.microsoft.com/en-gb/services/storage/files/) in order to have an NFS volume attached, which would ease migration to a Jenkins PaaS offering.  However, given the current state of that technology, Azure Files does not provide Zone-redundant storage (ZRS) and so does not meet the requirement of point 3).\n#### Encryption\nIn order to acheive the requirement for point 6), custom kernel modules are required to be installed on Linux systems to take advantage of the Azure Linux Agent (waagent) which communicates with the underlying hypervisor.  This is done during the Packer build of our base image, along with several other required packages.  As part of the bootstrap process for a Jenkins master, a script is installed and run to check for the presence of a data disk and configure it if required.  This script can be found here: https://raw.githubusercontent.com/contino/moj-module-jenkins/master/datadisk/datadisk.sh?token=AJ8ylGQJSftk3zecme9EPRtUupRalF5Kks5aAdmiwA%3D%3D\nUnfortunately, due to the way the decryption of the disk seems to take place (a virtual CDROM is mounted with the credentials required to decrypt the disk), the VM never comes back online once it has been rebooted.\n**A summary of this bahaviour has been sent to Microsoft and we are currently awating feedback.**\n#### Backup\nCreating a Recovery Vault and backup policy achieve the requirement for point 4).  Terraform currently does not support this operation natively, so a Azure Resource Manager template is used to create the vault (based on the azure-quickstart-templates).  This works successfully, however, creation of a Recovery Vault policy fails to create (both on the CLI and via the console).  Looking at the current API specification for a vault policy, it seems like much of the old functionality has been regressed.\n**A summary of this bahaviour has been sent to Microsoft and we are currently awating feedback.**\n","The decision is to implement a separate data drive for Jenkins and ensure replication and encryption for the drive. Additionally, the recovery VAULT for Jenkins should be manually enabled."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA just enough high level infrastructure design is needed to drive how we build the strategic infrastructure.\nThe key questions are;\n- How do Application Service Environments (ASEs) fit in the infrastructure?\n- What are the IP ranges?\n-  What’s the IP allocation policy?\n-  How do we handle internal DNS queries?\n-  How do the components connect to each other on the network?\n\n## Decision\n,"[High Level Design](https://www.lucidchart.com/documents/edit/69b50fa0-77e7-43b5-92ed-1933abb10a80)\n![](../../img/high-level-infra-design.png)\n### Production Applications Infrastructure VNet (applications-prod)\nProduction services will run inside a dedicated Virtual Network (VNet).\n### Management and Tooling Infrastructure VNet (management-prod)\nManagement and Tooling will be hosted inside a dedicated VNet. This hosts the CI/CD Tooling and components which support  managing and deploying applications.\n### Non-Production Applications Infrastructure VNet (applications-non-prod)\nDev, Test (QA) and Load test versions the services will run inside this VNet. The same infrastructure as code (IaC) will drive both prod and non-prod infrastructure.\n### Azure Traffic Manager\n[Azure Traffic Manager](https://azure.microsoft.com/en-gb/services/traffic-manager/) will be used to host the public DNS entries for each Citizen facing frontend.\nTraffic Manager supports configurable routing, which will support [blue/green deployments](https://martinfowler.com/bliki/BlueGreenDeployment.html).\n### Azure Application Gateway + Web Application Firewall\n[Azure Application Gateway](https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-introduction)\nThe Azure Application Gateway will be used as a gateway for HTTPS traffic. It will be used to expose internal service for public consumption.\nServices that are not exposed by the Azure App Gateway cannot be accessed externally.\nThe Web Application Firewall component of the Azure App Gateway,  comes preconfigured with CRS 3.0 by default or you can choose to use 2.2.9. CRS 3.0 offers reduced false positives over 2.2.9. The ability to customize rules to suit your needs is provided. Some of the common web vulnerabilities which web application firewall protects against includes:\n- SQL injection protection\n- Cross site scripting protection\n- Common Web Attacks Protection such as command injection, HTTP request smuggling, HTTP response splitting, and remote file inclusion attack\n- Protection against HTTP protocol violations\n- Protection against HTTP protocol anomalies such as missing host user-agent and accept headers\n- Prevention against bots, crawlers, and scanners\n- Detection of common application misconfigurations (i.e. Apache, IIS, etc.)\n### Azure Application Service Environments with Internal Load balancer\nThe Application Service Environment provides an isolated and dedicated environment for securely running apps. See [App Service Environment Documentation](https://docs.microsoft.com/en-us/azure/app-service/app-service-environment/readme)\nAn ASE hosts applications. Each application spans one or more nodes within an ASE. An application uses more nodes according\nto it’s scale.\nThe entry point to applications is via an internal load balancer  (ILB) attached to the ASE. The ILB handles the routing and SSL offload.\n### Private DNS\nA private DNS service will run inside each VNet to provide name resolution for services within the network.\nThe DNS solution is TBD.\n### TLS/SSL by default\nAll traffic flows between components will be TLS/SSL enabled.\n","**Decision**\n\nTo establish a network infrastructure that meets the requirements of the Application Service Environments (ASEs), the following decisions will be made:\n\n1. **IP Ranges:** The IP ranges for the ASES will be determined based on the projected number of ASES and the size of each ASE.\n2. **IP Allocation Policy:** The IP addresses for the ASES will be allocated dynamically from the pool of available IP addresses.\n3. **Internal DNS Queries:** Internal DNS queries will be resolved using an internal DNS server.\n4. **Network Connectivity:** The ASES will be connected to each other and to the rest of the network via a virtual private network (VPN)."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe raised [spike-575](https://contino.tpondemand.com/entity/575) because we wanted to explore the alternatives to expose outputs from the product infrastructure pipelines to the application pipelines. This is necessary because we know that application pipelines will need to consume artificats that will be dinamically generated by other external pipelines (i.e. published urls which are outputs by Product Infrastructure Pipelines).\n\n## Decision\n,"For this spike we looked at both, Azure’s KeyVault and Hashicorp’s Vault, as key/value stores for making Terraform outputs available for other applications to consume in a secure manner.\nBoth offerings are very similar however there are a couple of differences worth noting here. The first one is that KeyVault is a hosted cloud native service and key part of the Azure offering, whereas Vault will require additional (although not significant) infrastructure to be provisioned in order to run this service.\nThe second main difference is around the Terraform support currently available. Although we have Terraform providers covering both services the resource capabilities are more extensive for the Hashicorp Vault provider. Only the Hashicorp Vault provider allow us to populate the store with secrets using terraform code (`azurerm_key_vault` only allow us to create a keyvault); this pretty much reduces the scope of this spike to Hashicorp Vault. Despite the fact that it is possible to populate KeyVault with outputs, any approach will require us to perform actions outside of the current Terraform workflow adding nothing but complexity (i.e. runnint the terraform output command and parsing stdout).\nStoring output in Vault as part of any terraform workflow can be simple. The code below shows a very simple and basic example illustrating how this can be accomplished:\n```code\n.\n├── modulea\n│   └── output.tf\n├── terraform.tfstate\n├── terraform.tfstate.backup\n└── vault.tf\n1 directory, 4 files\nportable:vaultpoc dan$ cat vault.tf\nmodule ""testmodule"" {\nsource = ""./modulea""\n}\nresource ""vault_generic_secret"" ""example"" {\npath = ""secret/foo1""\ndata_json = <<EOT\n{\n""foo"":   ""${module.testmodule.test}"",\n}\nEOT\n}\nportable:vaultpoc dan$ cat modulea/output.tf\noutput  ""test"" {\nvalue = ""testing""\n}\n```\nLikewise, it is also very easy to consume the outputs since Vault (and KeyVault as the matter of fact) is a RESTful service after all, providing a good level of abstraction which means any http client can retrieve values as shown below:\n```code\nportable:vaultpoc dan$ curl -H ""X-Vault-Token: 86aa6a1f-cf36-59ff-abd1-ab2624100dec"" -X GET http://127.0.0.1:8200/v1/secret/foo1 | jq\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100   198  100   198    0     0  34226      0 --:--:-- --:--:-- --:--:-- 39600\n{\n""request_id"": ""8518b07f-bce0-dd61-f2a9-20ca8da1f212"",\n""lease_id"": """",\n""renewable"": false,\n""lease_duration"": 2764800,\n""data"": {\n""foo"": ""testing"",\n},\n""wrap_info"": null,\n""warnings"": null,\n""auth"": null\n}\nportable:vaultpoc dan$ curl -H ""X-Vault-Token: 86aa6a1f-cf36-59ff-abd1-ab2624100dec"" -X GET http://127.0.0.1:8200/v1/secret/foo1 | jq -r .data.foo\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100   198  100   198    0     0  40268      0 --:--:-- --:--:-- --:--:-- 49500\ntesting\n```\nRegarding naming conventions for secret storage there are many options available but we recommend to start by mirroring the agreed conventions for TF state detailed in the [general terraform guidelines](https://github.com/contino/moj-infrastructure-guidelines#storing-state) i.e.\n```code\n<statestorebackend>/<project>/<env>/<output key>\n```\nSimilarly there are many approaches when it comes to access policies for Vault, it is a very extensive topic and well documented by [hashicorp](https://www.vaultproject.io/docs/concepts/policies.html). A good starting point, however, would be - again - to mirror the policies the Terraform/Jenkins user have against the Azure storage containers used to store the state, which at the time of this writing are pretty much matching the sudo level in Vault. With time and when we start considering more complex use cases, Vault access policies are flexible enough and provide and very good level of granularity for us to start writing more comprehensive policies.\nFinally, it is important to understand the general guidelines when writing Terraform code but in particular, when writing Vault related resources, please do not forget the following:\n> **Important** All data provided in the resource configuration will be written in cleartext to state and plan files generated by Terraform, and will appear in the console output when Terraform runs. Protect these artifacts accordingly. See the main provider documentation for more details.\nOne might think that the above can be mitigated by setting the sensitive attribute in the output to true, but during our testing (specifically when running the terraform plan command) we were hitting the following limitation of sensitive outputs:\n> **Sensitivity is not tracked internally, so if the output is interpolated in another module into a resource, the value will be displayed.**\n","**Decision:**\n\nImplement a mechanism to enable the consumption of artifacts produced by product infrastructure pipelines in application pipelines.\n\n**Rationale:**\n\nAfter evaluating the alternatives, we believe that implementing a mechanism to directly consume artifacts from product infrastructure pipelines in application pipelines is the best solution. This will allow us to meet our requirements and avoid the limitations of other approaches.\n\n**Impact:**\n\nThis decision will impact the following areas:\n\n* **Development:** The development team will need to implement the new mechanism.\n* **Operations:** The operations team will need to manage the new mechanism.\n* **Testing:** The testing team will need to test the new mechanism.\n\n**Timeline:**\n\nWe plan to implement this decision within the next six months.\n\n**Next Steps:**\n\n* Work with the development team to implement the new mechanism.\n* Work with the operations team to manage the new mechanism.\n* Work with the testing team to test the new mechanism."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","Use an Architectural Decision Record (ADR) template to document architectural decisions. Include fields for decision title, description, rationale, consequences, and related decisions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wanted to explore Azure's KeyVault capabilities to serve as a CA for services deployed on the strategic platform at HMCTS, that's to issue the certificates services may need for authentication purposes and compare to our previous findings when .\n\n## Decision\n","The first difference that shows up when comparing Azure's KeyVault (KV) with Hashicorp's Vault (as per ADR 0004) is that the Azure offering is a self hosted service provided by Microsoft whereas Hashicorp's vault would have been an IaaS based solution hosted inside a private VNET. This is an important difference since currently the only way to interact with KV is by connecting to a public endpoint.\nAlthough KV makes its REST endpoints public (SSL encrypted) it is important to highlight that the only way to interact with this service is by first authenticating against Azure's Active Directory service which, in combination with KV's access policy capabilities, provides a fairly granular control with regards to which information and which KV's can be accessed by an app/service_principal.\nThe KV service supports a couple of methods to create certificates including self-signed as described [here](https://docs.microsoft.com/en-us/rest/api/keyvault/create-a-certificate). The KV REST API provides support for an extensive number of Certificate related operations including the most basic use cases HMCTS might need to cover in CNP currently. The following is a list of the certificate related operations:\n#### Certificate operations\nThe Azure Key Vault REST API supports the following operations on certificates.\n* Create a certificate\n* Import a certificate\n* List versions of a certificate\n* List certificates\n* Get a certificate\n* Delete a certificate\n* Update a certificate\n* Merge a certificate\n#### Certificate management operations\nThese REST operations are for the management of certificate operations associated with a Key Vault certificate.\n* Delete certificate operation\n* Get certificate operation\n* Update certificate operation\n#### Certificate policy operations\nThe following operations are available on a certificate policy:\n* Get a certificate policy\n* Update a certificate policy\n#### Soft-delete operations\nThe soft-delete feature supports these operations for deleted certificates:\n* Get deleted certificate\n* Get deleted certificates\n* Purge deleted certificate\n* Recover deleted certificate\n#### Certificate Issuers\nYou can do the following with certificate issuers in a key vault:\n* Set a certificate issuer\n* Get a certificate issuer\n* Update a certificate issuer\n* Delete a certificate issuer\n* Get certificate issuers\n#### Certificate Contacts\nYou can do the following with certificate contacts:\n* Get certificate contacts\n* Set certificate contacts\n* Delete certificate contacts\n### Interacting with the KeyVault API\nInteracting with the KeyVault REST API is a 2 step process. First one must obtain a valid authentication token that can subsequently be used to conduct any of the KeyVault operations listed above providing your service principal have the right access policies for the targeted vault. To illustrate the process here are some sample API calls to obtain a token and then List the certificates in a keyvault called `danvaultpoc`:\n* We first request an access token. For this you'll need some information about your service_principal, specifically the `TENANT_ID`, `CLIENT_ID` and `CLIENT_SECRET` values:\n```shell\n$ curl -X ""POST"" ""https://login.microsoftonline.com/$ARM_TENANT_ID/oauth2/token"" -H ""Content-Type: application/x-www-form-urlencoded"" --data-urlencode ""client_id=$ARM_CLIENT_ID"" --data-urlencode ""grant_type=client_credentials"" --data-urlencode ""client_secret=$ARM_CLIENT_SECRET"" --data-urlencode ""resource=https://vault.azure.net"" | jq -r .access_token\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100  1425  100  1279  100   146   6350    724 --:--:-- --:--:-- --:--:--  6363\neyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IjJLVmN1enFBaWRPTHFXU2FvbDd3Z0ZSR0\n< output truncated >\n```\n* Once we have obtain the token it is possible to conduct operations like those listed above against an specific vault. Here we show how to get a list of certificates stored in a vault (`danvaultpoc`):\n```shell\n$ curl -H ""Authorization: Bearer <put your token here>"" https://dankvaultpoc.vault.azure.net/certificates?api-version=2016-10-01 | jq\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100   234  100   234    0     0    253      0 --:--:-- --:--:-- --:--:--   253\n{\n""value"": [\n{\n""id"": ""https://dankvaultpoc.vault.azure.net/certificates/cert1"",\n""x5t"": ""1h6bRfbhRdsrDvJfF3S5jVU_gbk"",\n""attributes"": {\n""enabled"": true,\n""nbf"": 1507645296,\n""exp"": 1539181896,\n""created"": 1507645896,\n""updated"": 1507645896\n}\n}\n],\n""nextLink"": null\n}\n```\n",Use Azure KeyVault as the certificate authority for services deployed on the strategic platform at HMCTS.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n#### Context\nSS🐙 fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2\nvia Google). The provider may experience transient faults for any number of reasons - perhaps the\nprovider is experiencing a transient outage, or our traffic has been temporarily rate limited. We\nwanted to build SS🐙 to anticipate and handle the presence of faults during communication with\nauthentication providers, no matter the underlying cause.\n#### Decision\nWe implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design\npattern made popular by Michael T. Nygard's book, [""Release It!""](\nhttps://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through\na stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the\n`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the\nprovider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`\nstate, in which a limited number of outbound requests are allowed. If failures persist, then the\n`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait\nfor a longer interval of time before trying again. If instead the `Breaker` observes that requests\nare consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).\nThe SS🐙 [`docs`](/docs/) directory contains a [Circuit Breaker](\n/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found\nin [`breaker.go`](/internal/auth/circuit/breaker.go)\n#### Consequences\nSS🐙 now utilizes a robust and generic strategy for handling faults originating from the upstream\nauthentication provider service.\n\n## Decision\n","We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design\npattern made popular by Michael T. Nygard's book, [""Release It!""](\nhttps://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through\na stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the\n`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the\nprovider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`\nstate, in which a limited number of outbound requests are allowed. If failures persist, then the\n`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait\nfor a longer interval of time before trying again. If instead the `Breaker` observes that requests\nare consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).\nThe SS🐙 [`docs`](/docs/) directory contains a [Circuit Breaker](\n/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found\nin [`breaker.go`](/internal/auth/circuit/breaker.go)\n#### Consequences\nSS🐙 now utilizes a robust and generic strategy for handling faults originating from the upstream\nauthentication provider service.\n",Implement a generic `circuit.Breaker` type that follows the Circuit Breaker design pattern to handle transient faults during communication with authentication providers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn HTML5, `link` and `script` nodes have an attribute named `integrity`, which lets the browser check if the remote file is valid, mostly for the purpose of enhancing page security.\n\n## Decision\n","In order to replicate the browser's behavior, the program should perform integrity check the same way it does, excluding the linked asset from the final result if such check fails.\nThe `integrity` attribute should be removed from nodes, as it bears no benefit for resources embedded as data URLs.\n",We should adopt the `integrity` attribute for all external `link` and `script` nodes added to the application.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA slow network connection and overloaded server may negatively impact network response time.\n\n## Decision\n,"Make the program simulate behavior of popular web browsers and CLI tools, where the default network response timeout is most often set to 120 seconds.\nInstead of featuring retries for timed out network requests, the program should have an option to adjust the timeout length, along with making it indefinite when given ""0"" as its value.\n","Implement a caching mechanism at the client-side to store frequently accessed data, reducing the number of network requests and server load."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHTML documents may contain `meta` tags capable of automatically refreshing the page or redirecting to another location.\n\n## Decision\n,"Since the resulting document is saved to disk and generally not intended to be served over the network, it only makes sense to remove `meta` tags that have `http-equiv` attribute equal to ""Refresh"" or ""Location"", in order to prevent them from reloading the page or redirecting to another location.\n",**Decision:** Enforce a policy to disallow `<meta>` tags that automatically refresh the page or redirect to another location.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHTML documents may contain `base` tag, which influences resolution of anchor links and relative URLs as well as dynamically loaded resources.\nSometimes, in order to make certain saved documents function closer to how they operate while being served from a remote server, the `base` tag specifying the source page's URL may need to be added to the document.\nThere can be only one such tag. If multiple `base` tags are present, only the first encountered tag ends up being used.\n\n## Decision\n","Adding the `base` tag should be optional — saved documents should not contain the `base` tag unless it was specified by the user, or the document originally had the `base` tag in it.\nExisting `href` attribute's value of the original `base` tag should be used for resolving the document's relative links instead of document's own URL (precisely the way browsers do it).\n","**Decision:**\n\nIn order to add a `<base>` tag to HTML documents that do not have one, a preprocessor should be used.\n\n**Justification:**\n\nA preprocessor can be used to add the `<base>` tag to all HTML documents during the build process, ensuring that all documents have a consistent base URL. This approach is more efficient and reliable than manually adding the tag to each document by hand, and it allows for the use of a single, customizable script to generate the correct URL."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHTML pages can contain `noscript` nodes, which reveal their contents only in case when JavaScript is not available. Most of the time they contain hidden messages that inform about certain JavaScript-dependent features not being operational, however sometimes can also feature media assets or even iframes.\n\n## Decision\n","When the document is being saved with or without JavaScript, each `noscript` node should be preserved while its children need to be processed exactly the same way as the rest of the document. This approach will ensure that even hidden remote assets are embedded — since those hidden elements may have to be displayed later in a browser that has JavaScript turned off. An option should be available to ""unwrap"" all `noscript` nodes in order to make their contents always visible in the document, complimenting the ""disable JS"" function of the program.\n","**Decision:** Remove noscript elements.\n\n**Rationale:**\n\n* **Improved user experience:** Noscript elements can disrupt the user experience by displaying unexpected content or breaking the page layout.\n* **Improved security:** Noscript elements can be exploited by attackers to bypass JavaScript restrictions and execute malicious code.\n* **Simplified codebase:** Removing noscript elements simplifies the codebase and reduces maintenance overhead.\n* **Consistent presentation:** Browsers handle noscript elements differently, leading to inconsistent presentation across different devices and browsers.\n* **Alternative solutions:** For cases where essential information needs to be provided to non-JavaScript users, alternative solutions such as server-side rendering or progressive enhancement can be used."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nServers may return information with HTTP response codes other than `200`, however those responses may still contain useful data.\n\n## Decision\n","Fail by default, notifying of the network error. Add option to continue retrieving assets by treating all response codes as `200`.\n","If the server returns an HTTP response with a status code other than 200, parse the body as JSON and, if the response body is a JSON object with a `""data""` field, use that field as the response data. Otherwise, return an error."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt may look like a good idea to make monolith compress retrieved assets while saving the page for the purpose of reducing the resulting document's file size.\n\n## Decision\n,"Given that the main purpose of this program is to save pages in a convenient to store and share manner — it's mostly an archiving tool, aside from being able to tell monolith to exclude certain types of asests (e.g. images, CSS, JavaScript), it would be outside of scope of this program to implement code for compressing assets. Minimizing files before embedding them does not reduce the amount of data that needs to be transferred either. A separate tool can be used later to compress and minimize pages saved by monolith, if needed.\n","Do not compress retrieved assets while saving the page.\n\n**Rationale:**\n\n* Compressing retrieved assets can introduce additional latency and complexity in the page saving process.\n* Compression algorithms can be computationally intensive, potentially impacting the performance of the monolith.\n* The benefits of reducing the resulting document's file size may be minimal, especially if the assets are already compressed in a lossless format.\n* Relying on external compression tools or services introduces additional dependencies and potential maintenance overhead."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use the Architectural Decision Record (ADR) format to document all architectural decisions made on this project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nAzure Cosmos DB is available in two different capacity modes: provisioned throughput and server less.\nChoosing the right model will help optimize the cost and performance\n\n## Decision\n,"The recommended approach is to use Server less mode\nThe decision is based on the guidance provided by Microsoft here: https://docs.microsoft.com/en-us/azure/cosmos-db/throughput-serverless\nThe approach is based on the following assumptions\n* The application had no organizational constraints to use preview technology.\n* The application does not have requirements for being geo-redundant.\n* Maximum throughput is less than 5,000 RU/s\n* Maximum storage is less than 50 GB\n","**Use server less capacity mode for Azure Cosmos DB.**\n\n## Rationale\n\nServer less capacity mode is a cost-effective option that automatically scales provisioned throughput units (PTUs) based on demand. This eliminates the need to manually provision PTUs, which can be time-consuming and error-prone. Server less mode also offers a consistent performance experience, even during traffic spikes.\n\n## Pros\n\n* Cost-effective: Only pay for the resources you use.\n* Automatic scaling: No need to manually provision PTUs.\n* Consistent performance: Even during traffic spikes.\n\n## Cons\n\n* Not suitable for applications with highly predictable workloads.\n* May not be able to achieve the same level of performance as provisioned throughput mode.\n\n## Alternatives\n\n* **Provisioned throughput mode:** Offers a higher level of performance and control, but requires manual provisioning of PTUs.\n* **Hybrid mode:** Combines provisioned throughput and server less capacity modes, offering a balance of cost and performance."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\n\n## Decision\n,The application will be hosted on Microsoft Azure Cloud datacenter.\n,"**Decision:** Host the application on a managed cloud datacenter.\n\n**Rationale:**\n\n* **Scalability:** Cloud datacenters offer on-demand scalability, allowing the application to easily handle increased traffic or demand.\n* **Cost:** Managed cloud datacenters provide cost-effective infrastructure, as businesses only pay for the resources they use. This can result in lower overall costs compared to private datacenters.\n* **Technology choices:** Cloud datacenters support a wide range of cutting-edge technologies and services, such as serverless computing, containers, and managed databases. This enables developers to leverage the latest technologies and best practices.\n* **Reliability:** Managed cloud datacenters are designed for high availability and redundancy, ensuring minimal downtime and data loss.\n* **Security:** Cloud datacenters implement robust security measures and compliance certifications, providing a secure environment for applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nAzure Cosmos DB provides different APIs to access and interact with the data it stores.\n* Core(SQL) API\n* Mongo DB API\n* Cassandra API\n* Azure Table\n* Gremlin (graph) API\nChoosing the right API model will play a role in integration complexity and data access performance.\n\n## Decision\n,The recommended approach is to use Core(SQL) APIs.\nThe decision is based on the guidance provided by Microsoft here: https://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/3-analyze-the-decision-criteria\n,"Use the Core(SQL) API for its flexibility, wide range of data types, and rich query capabilities. It provides a consistent experience across all Azure services and supports advanced features like stored procedures, user-defined functions, and materialized views."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nThe Web APIs can be implemented as Synchronous or Asynchronous.\n\n## Decision\n,"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\nThe decision is based on the guidance provided by Microsoft here: https://azure.microsoft.com/mediahandler/files/resourcefiles/api-design/Azure_API-Design_Guide_eBook.pdf\n",**Implement Web APIs asynchronously using JavaScript Promises.**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nThe application APIs have the following requirements around the interfaces\n* Should support a rich developer experience\n* Should support Gatekeeper pattern for security\nThe following options for service are considered for this:\n* App Gateway\n* API Management\nChoosing the right service to implement the gatekeeper patter will help optimize the development experience and operational cost\n\n## Decision\n,The recommended approach is to use Azure API Management considering the following points:\n* Support for a developer portal\n* Ability to offload security\n,"The decision is to use **API Management** for implementing the gatekeeper pattern.\n\n**Justification:**\n\nAPI Management offers a more comprehensive set of features and capabilities that better align with the requirements of the application APIs. It provides a richer developer experience, including a developer portal, documentation generation, and API mocking capabilities. Additionally, API Management seamlessly integrates with Azure Active Directory, enabling the implementation of the gatekeeper pattern for secure access control."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the serving tier:\n* Should support exposing Web APIs\n* Should be able to connect to backend Cosmos DB\n* Should support .net core framework\n* APIs hosted on this service will need to be secured\n* Would prefer a managed service\n* Does not have portability requirements\n* Should support CD from Github\nThe following options for compute are considered for this service:\n* App Services\n* Functions\n* Container Instances\n* Service Fabric\n* AKS\nChoosing the right compute model will help optimize the development experience and operations\n\n## Decision\n,The recommended approach is to use Azure App Services considering the following points:\n* Supports CD from Github\n* Supports development slots for updates to reduce downtime\n* Can be integrated with App Gateway and VNET for higher security\n* Natively supports AD integration\n* Supports .net core framework and is well integrated with Visual Studio development experience\n* Can leverage cosmos SDK (nuget) to connect back to the data store\n* Is a fully managed PaaS service\n* No overheads related to cluster management or infrastructure provisioning.\nThe decision is based on the guidance provided by Microsoft here: https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree\n,"**Decision:** App Services\n\n**Rationale:**\n\n* **Supports Web APIs:** App Services is designed to host web applications and provides built-in support for exposing Web APIs.\n* **Connects to Cosmos DB:** App Services can easily connect to Azure Cosmos DB, meeting the requirement for backend connectivity.\n* **Supports .NET Core framework:** App Services supports the .NET Core framework.\n* **Security:** App Services provides built-in security features, including authentication and authorization, to secure APIs.\n* **Managed service:** App Services is a fully managed service, allowing developers to focus on building and running their applications without managing infrastructure.\n* **No portability requirements:** As the requirement does not specify portability, App Services meets this criterion by being exclusive to Azure.\n* **Supports CI/CD from GitHub:** App Services integrates with Azure Pipelines, enabling continuous delivery from GitHub."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nFollowing Architecture styles were considered for the application\n1. n-tier\n1. Web-Queue-Worker\n1. Microservices\n1. Big data/compute\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\n\n## Decision\n,A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https://martinfowler.com/bliki/CommandQuerySeparation.html\nThe decision is based on the guidance provided by Microsoft here: https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/\n,"The best architectural style for the application is Microservices.\n\nMicroservices is an architectural style that decomposes an application into a set of small, independent services. Each service is responsible for a single function, and can be deployed and scaled independently. This makes microservices applications more flexible, scalable, and easier to maintain than traditional monolithic applications.\n\nIn the case of the application, microservices would be a good choice because it would allow the team to decompose the application into a set of smaller, more manageable services. This would make the application more flexible, scalable, and easier to maintain. Additionally, microservices would allow the team to take advantage of the cloud-native features of AWS, such as auto-scaling and load balancing."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## :dart: Context\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\n* Should support scheduling\n* Should support running in background\n* Should be able to connect to backend Cosmos DB\n* Should support .net core framework\n* Service will only run once a day\n* Would prefer a managed service\n* Does not have portability requirements\nThe following options for compute are considered for this service:\n* App Services\n* Functions\n* Container Instances\n* Service Fabric\n* AKS\nChoosing the right compute model will help optimize the development experience and operations\n\n## Decision\n,The recommended approach is to use Azure Functions considering the following points:\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\n* Supports .net core framework and is well integrated with Visual Studio development experience\n* Can leverage cosmos SDK (nuget) to connect back to the data store\n* Has built-in scheduling capabilities\n* Is a fully managed PaaS service\n* No overheads related to cluster management or infrastructure provisioning.\nThe decision is based on the guidance provided by Microsoft here: https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree\n,"The best compute option for the ingestion sub-system is **Azure Functions**. Azure Functions meets all the requirements:\n\n* **Supports scheduling:** Azure Functions can be scheduled to run on a recurring basis.\n* **Supports running in background:** Azure Functions can run in the background without requiring a user interface.\n* **Can connect to backend Cosmos DB:** Azure Functions can access data in Cosmos DB using the Cosmos DB SDK for .NET.\n* **Supports .NET Core framework:** Azure Functions supports .NET Core 3.1 and later.\n* **Service will only run once a day:** Azure Functions can be configured to run on a specific schedule, such as once a day.\n* **Managed service:** Azure Functions is a fully managed service, so you don't have to worry about managing the underlying infrastructure.\n* **No portability requirements:** The ingestion sub-system does not have any portability requirements, so Azure Functions is a good choice."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\n\n## Decision\n",Build a SPA.\n,Use a pop-up player for audio playback.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nThe registration of a node requires an ID to be given.\nThis ID is then used internally in LRUD for retrieving/manipulating the specific Node.\nIDs are also ""surfaced"" and used as part of the API, in functions such as `getNode(<node id>)`.\nQuestions have been raised around whether or not LRUD should support _duplicate_ IDs.\n**Is it technically feasible?**\nYes. The actual ""internal"" ID of a node could be the combination of its own ID and all its parents. This means we could handle duplicate IDs as long as no 2 IDs were both duplicates _and_ siblings.\nOR, we could make it so all IDs that are registered are actually registered as the ID concatonated with a UUID, etc.\nSo the question is, should we?\n**Pros**\n- the data that is used to build a navigation tree can be even dumber, in that it can allow internal IDs in itself. For example, 2 footers on the same page but under different parents would be valid, e.g\n```js\nnav.registerNode('footer', { parent: 'left_column' })\nnav.registerNode('footer', { parent: 'right_column' })\n```\n**Cons**\n- Implicit confusion. By allowing duplicate IDs, the tree is now filled with multiple of the same ID at different levels. This will add confusion to any process attempting to parse the tree (both human and machine process)\n- Complicates the surface of the API. For example, the user can currently call `getNode(<node id>)`. If duplicate IDs exist in the tree, these methods must handle this.\n- Complicates service-land code that makes use of LRUD. Lots of code exists in services that do something along the lines of ""register a node with ID x and then later on, use the ID of X to get that node back out"". This is done using calls such as `getNode()` (as explained above). This means service land will now need to start keeping track of which nodes live under which parents, etc. which immediately becomes a huge headache.\n- Complicates the internal state and processes of LRUD. It means that the internal pathing mechanisms and state of LRUD must complicate to handle duplicate IDs across different parents.\n### Decision\nWe have decided that until the need arrises, we will _not_ allow duplicate IDs.\nIf `registerNode()` is called with an ID that has already been registered against the navigation instance, an exception will be thrown.\n### Status\nApproved\n### Consequences\nAs discussed above, it means the registration data and processes of registering must ensure that no duplicate IDs are used.\n\n## Decision\n","We have decided that until the need arrises, we will _not_ allow duplicate IDs.\nIf `registerNode()` is called with an ID that has already been registered against the navigation instance, an exception will be thrown.\n### Status\nApproved\n### Consequences\nAs discussed above, it means the registration data and processes of registering must ensure that no duplicate IDs are used.\n","We have decided that until the need arises, we will not allow duplicate IDs.\n\nIf `registerNode()` is called with an ID that has already been registered against the navigation instance, an exception will be thrown."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nLRUD navigation trees already work very well when every component you need to render is included inside the same tree. e.g if you can render an entire page and all elements are inside the same tree, eventually pointing to the same root container.\nThe current SSR request/response system has to build multiple LRUD trees, however.\nThis is because the Partial Page Update System (PPUS) inside our mountains relys on a ""Region"" system (where each Region defines both its markup and LRUD navigation tree) and the ability to ""hot swap"" these Regions in and out of a page, along with their navigation nodes.\nWhen new Regions are loaded in, there is a ""primary tree"" that is rebuilt from all the navigation nodes of each loaded-in Region.\nBecause this ""primary tree"" is made up purely from the concatonated nodes of the Region trees, none of the Regions have information about how they relate to _each other_. For example, no 2 Regions can currently be horizontally aligned next to each other.\nAlongside this, there are _also_ situations arising where desired journeys around the page don't align exactly with the layout. For example, any given page may want to land on node ""X"" from node ""Y"" when the user presses ""UP"", even through in the navigation tree those 2 nodes are horizontally aligned next to each other (product/UX requirements, etc.)\n### Decision\nWe want to implement an `overrides` system into an LRUD instance.\nThe overrides can live alongside the `navigation` object.\nIt will be an array of objects, each representing an override for a direction from a node to another node.\nFor example, a given override may represent ""when on node 'X', and the user presses 'UP', go to node 'Y'""\nBecause the overrides live as a separate data item on an instance, and are checked at run time, they can be updated/added/removed as and when needed, based on app state.\n### Status\nApproved\n### Consequences\n- LRUD now ""handles"" more information than just a navigation tree. This is extra complexity, and as it is an extra data item, any LRUD implementation that currently moves data around will also have to move around the `overrides`\n- naive overrides can cause unexpected behaviour in LRUD itself. For example, setting an override target to `X` will _actually_ cause the final focus to end up on the first focusable child of `X`. While this does make sense, it can be somewhat unintuitive at first\n\n## Decision\n","We want to implement an `overrides` system into an LRUD instance.\nThe overrides can live alongside the `navigation` object.\nIt will be an array of objects, each representing an override for a direction from a node to another node.\nFor example, a given override may represent ""when on node 'X', and the user presses 'UP', go to node 'Y'""\nBecause the overrides live as a separate data item on an instance, and are checked at run time, they can be updated/added/removed as and when needed, based on app state.\n### Status\nApproved\n### Consequences\n- LRUD now ""handles"" more information than just a navigation tree. This is extra complexity, and as it is an extra data item, any LRUD implementation that currently moves data around will also have to move around the `overrides`\n- naive overrides can cause unexpected behaviour in LRUD itself. For example, setting an override target to `X` will _actually_ cause the final focus to end up on the first focusable child of `X`. While this does make sense, it can be somewhat unintuitive at first\n","Implement an `overrides` system into an LRUD instance, where overrides can live alongside the `navigation` object, allowing for more complex navigation paths and customizations."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nThere has been interest from other teams using LRUD that the ability to _cancel_ a movement as it's happening would be useful.\nFor example, focussing on a node and pressing down would ordinarily take you to the specified node, but perhaps the developer wants to run some business logic at that point that would mean that _actually we don't want the move to happen._\n### Decision\nWe have decided to implement this feature in LRUD. It will be useful for the specific team that requested the feature, and as further discussions have happened, all interested parties agree that there are valid use cases in many different scenarios for such a feature.\n### Status\nApproved\n### Consequences\nIt makes LRUD internally more complex (and alongside that makes the final bundle larger too). However, it is only marginally increasing the bundle size, and we feel the complexity is managable and well understood.\n### Further Reading\n- [Github issue discussing topic of cancellable movement](https://github.com/bbc/lrud/issues/25)\n\n## Decision\n","We have decided to implement this feature in LRUD. It will be useful for the specific team that requested the feature, and as further discussions have happened, all interested parties agree that there are valid use cases in many different scenarios for such a feature.\n### Status\nApproved\n### Consequences\nIt makes LRUD internally more complex (and alongside that makes the final bundle larger too). However, it is only marginally increasing the bundle size, and we feel the complexity is managable and well understood.\n### Further Reading\n- [Github issue discussing topic of cancellable movement](https://github.com/bbc/lrud/issues/25)\n",Implement a feature in LRUD that allows users to cancel a movement as it's happening.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nAfter much usage of LRUD V2, we (Lovely Horse 🐴) have ended up with a laundry list of things we want LRUD to do that V2 doesn't support. We also have a desire for a more maintainable codebase that utilises more up-to-date terminology for the expected behaviour of LRUD.\nThe list of desired functionality currently sits at;\n- a real tree structure\n- cleaner/easier to understand ""grid"" functionality\n- supporting a concept of ""column span""/""column width""\n- real definition of which node is the “root” node\n- all focusable nodes to maintain an ""index"" for easier understanding or sorting\n- better handling of unregistering\n### Decision\nWe have decided to re-write LRUD from the ground up, maintaining many of the concepts from V2, while addressing the list of desired functionality.\nThis will also give us an opportunity to re-write LRUD into Typescript, further increasing the maintainability of the codebase in future.\n### Status\nApproved\n### Consequences\n- user land usages of LRUD will need to update their code in order to make use of the new version. We are planning to keep breaking changes to a minimum, but some changes will be necessary.\n- slightly increased library size, affecting response payload sizes. The increase in size is small enough (an increase of 2.6kb when minified) that we deem this acceptable. Furthermore, the changes mean that current ""workaround"" code in service land can be removed, reducing payload size in other areas.\n- slightly increased runtime computation. Usage of a real tree in memory requires extra computation. Dedicated testing will take place to ensure LRUD is still performant enough on low powered devices, but initial testing of 92 test cases in 2.4s suggests this is well within limits.\n### Further Reading\n- [Paper Doc discussing what LRUD is and why we want to change some things](https://paper.dropbox.com/doc/SSR-Controller-Module-LRUD-V3--Aca6ZBsM4Uv8zEN44j5o4TsvAg-y0v9YqarEOXNP7R2151RK)\n\n## Decision\n","We have decided to re-write LRUD from the ground up, maintaining many of the concepts from V2, while addressing the list of desired functionality.\nThis will also give us an opportunity to re-write LRUD into Typescript, further increasing the maintainability of the codebase in future.\n### Status\nApproved\n### Consequences\n- user land usages of LRUD will need to update their code in order to make use of the new version. We are planning to keep breaking changes to a minimum, but some changes will be necessary.\n- slightly increased library size, affecting response payload sizes. The increase in size is small enough (an increase of 2.6kb when minified) that we deem this acceptable. Furthermore, the changes mean that current ""workaround"" code in service land can be removed, reducing payload size in other areas.\n- slightly increased runtime computation. Usage of a real tree in memory requires extra computation. Dedicated testing will take place to ensure LRUD is still performant enough on low powered devices, but initial testing of 92 test cases in 2.4s suggests this is well within limits.\n### Further Reading\n- [Paper Doc discussing what LRUD is and why we want to change some things](https://paper.dropbox.com/doc/SSR-Controller-Module-LRUD-V3--Aca6ZBsM4Uv8zEN44j5o4TsvAg-y0v9YqarEOXNP7R2151RK)\n","To rewrite LRUD from the ground up, maintaining many of the concepts from V2, while addressing the desired functionality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nWe want to use Lrud as a `tap-static` module, which requires converting to `amd` format. To do so we use `esm-2-amd`, which means we need to distribute lrud as ESM. Currently it's not possible as it's only built to CJS.\n### Decision\nThe distribution folder structure will change with two subfolders, `cjs` and `esm`. The type definitions will also be distributed in another subfolder, `types`. Rollup will still be used to create the CJS and ESM format.\n### Status\nApproved\n### Consequences\nThe final distribution size will increase.\nThis change won't affect how current users consume lrud, as the package `main` is updated to point to the CJS min file.\n\n## Decision\n","The distribution folder structure will change with two subfolders, `cjs` and `esm`. The type definitions will also be distributed in another subfolder, `types`. Rollup will still be used to create the CJS and ESM format.\n### Status\nApproved\n### Consequences\nThe final distribution size will increase.\nThis change won't affect how current users consume lrud, as the package `main` is updated to point to the CJS min file.\n","Update the distribution folder structure to include `cjs`, `esm`, and `types` subfolders. Continue using Rollup to create CJS and ESM formats."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFacilitate organizing shell customizations into smaller, more readable, more understandable, more maintainable blocks and to provide a clear and common patter for extensibility.\n\n## Decision\n",Create a $ZDOTDEEDIR directory where extension files can be stored. Add a rooutine to .zprofile or .zshrc which sources all files found in this directory in lexagraphical order by file name.\n,Create a modular shell configuration system that allows users to define their own custom shell functions and aliases.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** Implement an Architectural Decision Record (ADR) process.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way of documenting and communicating architectural decisions.\n* They facilitate collaboration and knowledge sharing among team members.\n* ADRs help in tracking the evolution of the architecture and its rationale, which is crucial for future maintenance and updates.\n* By capturing the context, decision, rationale, and consequences of architectural choices, ADRs enable informed decision-making and reduce the risk of architectural drift.\n\n**Benefits:**\n\n* Improved transparency and accountability in the decision-making process.\n* Reduced time spent on duplicating discussions or searching for past decisions.\n* Enhanced collaboration and knowledge transfer within and across teams.\n* Better alignment between architectural decisions and project goals.\n* Simplified maintenance and evolution of the architecture.\n\n**Next Steps:**\n\n* Define the ADR template and review process.\n* Establish clear ownership and responsibilities for writing, reviewing, and approving ADRs.\n* Educate the team on the benefits and usage of ADRs.\n* Create a central repository or documentation system for storing and managing ADRs.\n\n**Alternatives Considered:**\n\n* Not using ADRs: This option would lead to fragmented and undocumented architectural decisions, making it difficult to maintain and evolve the architecture.\n* Using a custom documentation system: This would require significant effort to create and maintain, potentially leading to inconsistent documentation practices and additional complexity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe manage many different versions of applications, services and libraries across multiple environments.\nAs we think about how we want to identify our artifacts with version numbers, we have the following goals:\n* As much as possible we want to follow existing industry conventions so that\n* it's easier for new developers to understand\n* it's easier for external parties to adopt and understand any APIs we expose\n* we can take advantage of any tooling that relies upon those conventions\n* We want our ""full"" version number to uniquely identify an artifact.\nIf a commit is built a second time, it should have a different version number because\nthe resulting artifact may have different bits.\n* We want to be able to easily identify the latest artifact,\ni.e. version numbers should have a chronological order.\n* Every build should create a potentially releasable artifact.\nThat is to say, we shouldn't need to change anything about the artifact to deploy it to production,\nwhich includes not changing the version number that is embedded in the artifact.\n* We'd like to be able to easily identify which artifacts are still a work-in-progress\nvs which are ""released"" (have passed QA and are deemed stable).\n* Marking an artifact as ""released"" should not change its contents in any way.\ni.e. It should be exactly what QA tested.\n* We'd like to have a ""release"" version number (e.g. 1.2.3) that we can anticipate in advance\nto identify a release for use in planning, Pivotal labels, HEAT, etc.\n* Backward-incompatible changes should be easy to identify\nso that dependent projects don't pull in breaking changes by accident.\n* For the purposes of capitalizing our time, we want to be able to distinguish between new feature work\nand bug fixes to existing releases.\nFrom an accounting standpoint, there are some versions that will be considered assets to be depreciated\nand others that will be treated as expenses.\n* We would like to be able to easily trace back to the commit from which an artifact was built.\nIt's also helpful to keep in mind that dependencies between our components take a couple of different forms:\n* End-user applications that have no other components that depend on them.\nTherefore there's really no such thing as a backward-incompatible change.\n* Services that expose APIs provide their updates automatically to consumers.\nThat is to say that dependent components don't need to change anything in order to get the latest bug fixes.\nThat being said, backward incompatible changes (i.e. new major versions) require establishing a new endpoint\nso that dependents are forced to make an explicit change to consume the breaking changes.\n* For shared libraries that are compile-time dependencies,\nthe dependent components need to rebuild to consume the new changes.\nUsually the dependent components will need to make a change to their dependency reference\n(e.g. change the maven dependency coordinates from 1.2.3 to 1.2.4).\nEven though the uses of version numbers are a little dependent on the type of component being produced,\nwe would like our version numbers to be as consistent as possible to reduce confusion.\nDatabase migrations are an exception since they have a very different set of concerns.\n\n## Decision\n","We will use [Semantic Versioning](http://semver.org/) as a basis for all of our applications, services and libraries,\nwith some TCS-specific interpretations.\nGiven a version number MAJOR.MINOR.PATCH-BUILD-COMMIT:\n* Increment the MAJOR version when you make incompatible API changes.\nFor applications that have no dependencies,\nincrement the major version when the team deems that it is a significant enough change, but this is quite rare.\n* Increment the MINOR version when you add functionality in a backwards-compatible manner.\n* Increment the PATCH version when you make backwards-compatible bug fixes.\nThe PATCH version number should only be incremented once for each release\nso that the MAJOR.MINOR.PATCH can be known in advance for release planning purposes.\n* Increment the BUILD version when you build the artifact.\nThis doesn't need to start at 0, and it's ok if it has gaps (e.g. jump from 23 to 26),\nbut it does need to increase with each subsequent build within a MAJOR.MINOR.PATCH.\n* Include the 7-character git COMMIT sha.\nBecause this comes after the build version, the commit won't be material when ordering versions\nor even for uniquely identifying the version (the previous four numbers can serve as a unique identifier),\nbut it serves as a useful piece of metadata for jumping right to the last commit included in the build.\n#### Where versions are recorded\n* MAJOR, MINOR and PATCH are under the developer's control and should be tracked in version control.\n* BUILD and COMMIT are added by the build system and should not be tracked in version control.\nSome frameworks have an established place to record the version information\n(e.g. Maven `pom.xml`, RubyGems `.gemspec`, Node `package.json`) and we should follow those conventions.\nIn cases where we are developing software and there is no established standard for where to store the\nversion information, we will create a `.version` file in the application's root directory that will be tracked in git.\nThe contents of the file will be one line of the form\n```\n[MAJOR].[MINOR].[PATCH]\n```\nIn some cases (e.g. Gradle) the contents of the `.version` file may include a `-SNAPSHOT` suffix, e.g.\n```\n1.2.4-SNAPSHOT\n```\nbut that `-SNAPSHOT` suffix should be ignored when extracting the MAJOR.MINOR.PATCH.\nWhen the CI server builds the application, it will create a `build.properties` file\nin the application's root directory that includes the full version (with build number and commit)\nin a line like\n```\nbuild.version=3.38.2-4-ace34d5\n```\nThe build.properties file will only exist in the artifact and will not be committed to git.\n#### Releasing\nAfter an artifact has QA's approval, it can be ""released.""\nThis means that it is the most stable and final artifact for that MAJOR.MINOR.PATCH.\nReleasing an artifact generally involves:\n* Identifying the appropriate MAJOR.MINOR.PATCH release label\n* Tagging the commit in git with that release label\n* Incrementing the PATCH version in git so that no other builds will use this same MAJOR.MINOR.PATCH\n* Publishing the artifact wherever appropriate under that release label.\nNote that in the Java world with Maven dependencies, the artifact will need to be published in Nexus\nunder the full version number, not an abbreviated release label.\n#### A note on semver and our build numbers\nThe semver specification refers to [build information being appended with + sign](http://semver.org/#spec-item-10),\nhowever the specification also states that ""Build metadata SHOULD be ignored when determining version precedence.""\nSince our build numbers do serve as a meaningful indicator of precedence, we will not use the `+` suffix for them.\nThe specification does define a precedence for\n[pre-release versions that are denoted by a hyphen](http://semver.org/#spec-item-9),\nso we have chosen to put our build number after the `-`.\n","We will use the following versioning scheme:\n\nWe will use a three-part version number:\n* Major (X) is for backward-incompatible changes to our public API.\n* Minor (Y) is for new features that are backward compatible.\n* Patch (Z) is for bug fixes.\n\nFor example, version 1.0.0 is the first release of the software. Version 1.1.0 is a new release with new features that are backward compatible with version 1.0.0. Version 1.0.1 is a new release with a bug fix that is not backward compatible with version 1.0.0.\n\nWe will use a pre-release identifier to indicate that a version is not ready for production use. For example, version 1.0.0-beta.1 is a pre-release version of version 1.0.0.\n\nWe will use a build identifier to indicate the specific build of a version. For example, version 1.0.0+20180101 is the build of version 1.0.0 that was built on January 1, 2018.\n\nWe will use a commit identifier to indicate the specific commit that a version was built from. For example, version 1.0.0+20180101@abcdef123456 is the build of version 1.0.0 that was built from commit abcdef123456.\n\nWe will use the following rules to determine the version number of a new release:\n\n* If the release contains backward-incompatible changes, the major version number will be incremented.\n* If the release contains new features that are backward compatible, the minor version number will be incremented.\n* If the release contains bug fixes, the patch version number will be incremented.\n\nWe will use the following rules to determine the pre-release identifier of a new release:\n\n* If the release is not ready for production use, the pre-release identifier will be set to a non-empty string.\n* If the release is ready for production use, the pre-release identifier will be set to an empty string.\n\nWe will use the following rules to determine the build identifier of a new release:\n\n* The build identifier will be set to the date of the build.\n* The build identifier will be appended to the version number with a plus sign (+).\n\nWe will use the following rules to determine the commit identifier of a new release:\n\n* The commit identifier will be set to the SHA-1 hash of the commit that the release was built from.\n* The commit identifier will be appended to the version number with an at sign (@)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we increase our adoption of [Vault](https://www.vaultproject.io/), we need to establish a convention of our paths to secrets in the [generic backend](https://www.vaultproject.io/docs/secrets/generic/).\n* The convention should support a variety of secrets - passwords, TLS certificates, SSH keys, etc.\n* The convention should allow for a variety of granularity. It should allow for allowing a client read access to a single secret (e.g. app user's database password), as well as allowing a client read/write access to a entire group of secrets (e.g. r/w access to all database password for a DBA).\n\n## Decision\n",Secrets added to Vault's generic backend should use the following path convention:\n`/usage/environment/scope/key`\nWhere:\n* `usage` - What is the usage of this secret\n* `environment` - In what environment will this secret be used\n* `scope` - The scope of the secret\n* `key` - The key used to look up the secret\nBelow are some examples:\n### Database passwords\nThe password for the `example` user in the FOO databases would be located at:\n* `/database/development/foo/example`\n* `/database/test/foo/example`\n* `/database/production/foo/example`\nDevelopers would have read only access to `/database/development/*`.\nDBAs would have read/write access to `/database/*`\n### TLS certificates\nThe wild card TLS certificates would be located at:\n* `/certificate/development/www/cert`\n* `/certificate/test/www/key`\n* `/certificate/production/www/key`\nThe platforms team would have read/write access to `/certificate/*`.\n,"**Convention:**\n\n**Path Structure:**\n\n```\n/secrets/<environment>/<secret-type>/<group>[/<id>]\n```\n\n**Components:**\n\n* **Environment:** The deployment environment (e.g., dev, qa, prod)\n* **Secret Type:** The type of secret (e.g., password, tls, ssh)\n* **Group:** The logical grouping for secrets (e.g., database, application, system)\n* **ID (optional):** A unique identifier for a specific secret within a group\n\n**Example Paths:**\n\n* `/secrets/dev/password/database/production-db` - A production database password\n* `/secrets/qa/tls/web/certificate` - A TLS certificate for a web server\n* `/secrets/prod/ssh/jump-server` - An SSH key for a jump server\n* `/secrets/prod/password/application/api` - All API user passwords\n\n**Considerations:**\n\n* **Granularity:** The convention allows for fine-grained control of secret access. Clients can be granted read or write access to specific secrets or entire groups of secrets.\n* **Consistency:** The consistent path structure simplifies management and automation of secret access.\n* **Scalability:** The convention supports a large number of secrets without creating unwieldy paths."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\nHere are the notes from the original discussion:\n\n## Decision\n","We will use the following definitions for the different types of schemas in our databases:\n### System Schemas\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\n### DBA User Schemas\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\nDBA User Schemas should follow this naming convention:\n```\n{}_DBA\n```\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\n### Application User Schemas\nApplication user schemas will exist for each application that needs to access data in our databases.\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\nApplication User Schemas should follow this naming convention:\n```\n{}_APP\n```\nWhere {} is the application name (i.e. `CALENDAR_APP`).\n### Individual User Schemas\nIndividual user schemas represent unique individual people that need access to database systems.\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\nIndividual users should have object creation permissions only for their own user schema.\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\n### Domain Schemas\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\nDomain schemas should have object creation permissions only for their own schema.\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\nDomain Schemas should be named logically so their purpose can at least be guessed at.\n### Replicated Schemas\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the ""parent"" schema.\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\nReplicated Schemas should follow this naming convention:\n```\n{}_REPL\n```\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\n","**Definitions**\n\n* **Application schema** - A schema that contains the objects (tables, views, procedures, functions, etc.) that are used by a specific application.\n* **Common schema** - A schema that contains objects that are shared by multiple applications.\n* **Data warehouse schema** - A schema that contains objects that are used for data warehousing purposes.\n* **Development schema** - A schema that is used for development purposes.\n* **Production schema** - A schema that contains the objects that are used in production.\n* **System schema** - A schema that contains objects that are used by the database system itself.\n* **User schema** - A schema that is owned by a specific database user."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe decision records must be stored in a plain text format:\n* This works well with version control systems.\n* It allows the tool to modify the status of records and insert\nhyperlinks when one decision supercedes another.\n* Decisions can be read in the terminal, IDE, version control\nbrowser, etc.\nPeople will want to use some formatting: lists, code examples,\nand so on.\nPeople will want to view the decision records in a more readable\nformat than plain text, and maybe print them out.\n\n## Decision\n","browser, etc.\nPeople will want to use some formatting: lists, code examples,\nand so on.\nPeople will want to view the decision records in a more readable\nformat than plain text, and maybe print them out.\nRecord architecture decisions in [Markdown format](https://daringfireball.net/projects/markdown/).\nDecisions will be formatted nicely and hyperlinked by the\nbrowsers of project hosting sites like GitHub and Bitbucket.\nTools like [Pandoc](http://pandoc.org/) can be used to convert\nthe decision records into HTML or PDF.\n",Store the decisions in plain text format with Markdown support to allow formatting.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\n## Decision Drivers\n- Should be performant, with code splitting, caching and minimal runtime overhead.\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\n- Should be type-safe to catch issues when refactoring.\n- Reusable components should be closed, not accepting arbitrary styles/classes.\n- We want a pattern for responsive props with atomic layout components.\n\n## Decision\n","- Should be performant, with code splitting, caching and minimal runtime overhead.\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\n- Should be type-safe to catch issues when refactoring.\n- Reusable components should be closed, not accepting arbitrary styles/classes.\n- We want a pattern for responsive props with atomic layout components.\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\nExample:\n```typescript\n// Good:\n<Box padding""small"" />\n<Box padding={{xs: 'small', md: 'medium'}} />\n<Input large />\n<Text preset=""heading3"" as=""p"" />\n```\n```typescript\n// Bad:\n<Box className={customLayout} />\n<Input style={{ height: 50, padding: 16 }} />\n<Text className={styles.heading} />\n```\n### Positive Consequences\n- Treat is statically extracted at build time, so it has minimal runtime.\n- Styles load in parallel with JS, also when code splitting.\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\n- We can pull in responsive layout component patterns from Braid, which gives us a good base to lay out components and pages.\n### Negative Consequences\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\n",Use CSS modules with Sass and TypeScript.\n\nCSS modules is a module system for CSS where each CSS file is scoped to a single JavaScript module. This can help you to avoid style conflicts between different components.\n\nSass is a CSS preprocessor that can help you to write more maintainable and reusable CSS code.\n\nTypeScript is a superset of JavaScript that can help you to write type-safe code.\n\nThis combination of technologies can help you to meet all of your decision drivers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nHow do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?\n## Decision Drivers\n- We need to have confidence in our releases.\n- We want more structured releases while we're still getting our footing in a shared monorepo.\n- We need simplicity and clear [takt time](https://kanbanize.com/continuous-flow/takt-time) so different teams can plan for what is going out the door from them.\n- It should work well with our agile work environment.\n\n## Decision\n,"- We need to have confidence in our releases.\n- We want more structured releases while we're still getting our footing in a shared monorepo.\n- We need simplicity and clear [takt time](https://kanbanize.com/continuous-flow/takt-time) so different teams can plan for what is going out the door from them.\n- It should work well with our agile work environment.\nChosen option: ""OneFlow"" because it provides a single eternal branch with well structured releases.\nWe'll implement OneFlow with these details:\n- Release branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.\n- Release and quality managers from each team are responsible for reviewing and approving releases.\n- Releases apply to all apps in the monorepo.\n- Releases are versioned like this: `{cycle}.{sprint}.{hotfix}`. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.\n- Feature branches are merged using ""Squash and merge"", so they can be easily reverted.\n- There are two ways to build larger features.\n- If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.\n- If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.\n- If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.\n### Future strategy\nWith time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.\n### Hosting environments\nWe'll set up continuous delivery to different hosting environments:\n| Environment | Git source            | Databases/services | Features |\n| ----------- | --------------------- | ------------------ | -------- |\n| sandbox     | feature branch        | Test               | All      |\n| dev         | main                  | Test               | All      |\n| staging     | main                  | Prod               | All      |\n| pre-prod    | release/hotfix branch | Prod               | Finished |\n| prod        | latest release tag    | Prod               | Finished |\nWe'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage.\n",**Decision:**\n\nWe will use a GitFlow branching model with the following branches:\n\n* **main:** Production branch\n* **develop:** Integration branch\n* **feature:** Branch for each feature being developed\n* **release:** Branch for each release\n* **hotfix:** Branch for urgent fixes\n\n**Release Process:**\n\n1. Create a feature branch from develop.\n2. Implement the feature.\n3. Merge the feature branch into develop.\n4. Create a release branch from develop.\n5. Test the release branch in QA.\n6. Merge the release branch into main.\n7. Deploy the main branch to production.\n\n**Hotfix Process:**\n\n1. Create a hotfix branch from main.\n2. Implement the hotfix.\n3. Merge the hotfix branch into main.\n4. Deploy the main branch to production.\n\n**Continuous Deployment for QA:**\n\nEach feature branch will be deployed to a separate QA environment. This will allow us to continuously test our changes and ensure that they are ready for release.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\nThere are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\n## Decision Drivers\n- The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\n- It is important to build on the experience of similar government led inititives in other countries.\n- Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\n- It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\n\n## Decision\n","- The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\n- It is important to build on the experience of similar government led inititives in other countries.\n- Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\n- It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\nThe MIT license was chosen, for the following reasons:\n- It is the least restrictive of the licenses.\n- It is very consise, simple and easy to understand and therefore should be clear to users and developers.\n- Digital Iceland does not require protection of patents or existing intelletual property.\n- Well known government lead initiatives like uk.gov and X-Road use the MIT license.\n- The MIT license is the best known and most widely used free and open-source license in the world.\n","The decision is to use a permissive open source license, such as the MIT, BSD, or Apache 2.0 license that grants users the freedom to use, modify, distribute, and study the software without any restrictions. This type of license aligns with the goals of Digital Iceland to encourage collaboration, transparency, and reuse, while promoting open development and minimizing complications for all participants."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.\n## Decision Drivers\n- Reliability\n- Feature-richness\n- Cost\n- Maintenance\n- Development\n- Vendor lock level\n\n## Decision\n","- Reliability\n- Feature-richness\n- Cost\n- Maintenance\n- Development\n- Vendor lock level\nSaaS solution (DataDog) is a clear winner due to its hastle-free service usage, continuous improvement as well as its competitive pricing.\n### Positive Consequences\n- Easy access to observability for devs and devops\n- Well known integration target for third-party services\n- Monitoring setup is well known in the industry\n- No special knowledge about setup of the plaftorm\n### Negative Consequences\nNone.\n",Use a SaaS provider like DataDog.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nViskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?\n## Decision Drivers\n- Should use NodeJS and React as outlined in [SÍ technical direction](../../technical-direction.md)\n- Should be able to support markdown content rendered to HTML\n- Should be open source\n- Should be customizable to island.is UI design\n\n## Decision\n,"- Should use NodeJS and React as outlined in [SÍ technical direction](../../technical-direction.md)\n- Should be able to support markdown content rendered to HTML\n- Should be open source\n- Should be customizable to island.is UI design\nChosen option: NextJS + NestJS\nNextJS is the chosen web framework for all island.is websites needing server side rendering. As Viskuausan will probably be merged with island.is main website, creating it using same frameworks makes it easy to merge later on. It is easier to reuse Island UI components using NextJS over Docusaurus. Docusaurus main advantage over Next is out-of-the-box markdown support but it is easy to add markdown support in NextJS using [Remark](https://github.com/remarkjs/remark) library.\nNestJS is used to create backend services and Viskuausan needs few backend services related to the X-Road and API GW integrations. Provides functionalities like ORM, dependency injection, unit testing.\n","**Decision:** Use NextJS.\n\n**Rationale:**\n\nNextJS is a popular React framework that meets all of the decision drivers:\n\n- It is built on NodeJS and React.\n- It supports markdown content rendering to HTML through the MDX plugin.\n- It is open source.\n- It allows for customization to the island.is UI design using custom components and CSS.\n\n**Consequences:**\n\n- NextJS is a more complex framework than some other options, but it provides a wider range of features and customization options.\n- The use of NextJS will require additional learning and development time for the team.\n- NextJS will add a dependency to the project, which could potentially introduce security risks or maintenance issues.\n\n**Non-Goals:**\n\n- This decision does not address the specific technical requirements of the Viskuausan platform.\n- This decision does not guarantee that the Viskuausan platform will be successful."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.\n## Decision Drivers\n- Provide consistency when navigating the codebase\n- The earlier we decide on this, the better\n\n## Decision\n","- Provide consistency when navigating the codebase\n- The earlier we decide on this, the better\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the file name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid using kebab-case and snake_case and make sure the name follows the default export of the file.\nNaming directories should follow these guidelines: Only use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`: `import { Box } from '@island.is/island-ui/core'`\nUse PascalCase for directories only containing React components:\n```text\ncomponents/CtaButton/index.ts\nimport 'components/CtaButton'\n```\nor:\n```text\ncomponents/CtaButton/CtaButton.tsx\nimport 'components/CtaButton/CtaButton'\n```\nrather than\n```text\ncomponents/cta-button/CtaButton.tsx\n```\nIn all other cases, use camelCase.\n### Positive Consequences\n- Easier to navigate the codebase\n- File names are more readable, and developers know what to expect\n- This approach is the most common practice, and something most JS and TS developers are familiar with.\n","**Enforce consistent naming conventions:**\\n- Define and enforce naming conventions for files and directories throughout the monorepo, ensuring consistency across different schematic types and adhering to best practices."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIs there available tool that is compliant to requirements? Can the tool provide functionality for API Gateway? Can the tool provide functionality for API Development Portal?\n### Requirements\nSince the API Gateway is intended for students and startups to gather open government data, the following requirements need to be fulfilled. The student / startup is defined as Consumer of service. The organization that deliver the service as open service is defined as Provider of service. The open service is hosted on organizations X-Road server. The API Gateway must provide functionality for\n- Registration of services with rate limit\n- Self-service portal\n- API key for Consumer\n- Rate limit for Consumer\n#### Provider\n- Register open service in API gateway.\n- Set rate limit on open service in API gateway.\n#### Consumer\n- Register as service user.\n- Register application intended to use API (Consideration).\n- Get API key for that application or consumer.\n- Register what API to use in the application.\n- Ability to test the API from API console with application API key (Consideration).\n#### Considerations\nAPI Keys / Rate Limits Is it sufficient to have only One API key for each Consumer, or is it required to define different Consumer applications with different API key. If a typical Consumer has created application and has valid API key, it is likely that he will not bother to register new application and get new API key. He could reuse the already given one. Consumer could also use the ability to register another application and get new API key with fresh rate limits.\nConsumer registration What will be used to validate / approve students or startups for access on services. Should it be registered by SSN or some unique id, or only by email, with ability to reregister with new email again and again.\n## Decision Drivers\n- Vendor lock in for runtime\n- Open source or not\n- Installation options\n- Functional ability\n- Market presence\n- Pricing\nPricing model for API management solutions are complex. Usually based on transaction count, or CPU instances. Sometimes pricing is variation of annual fee and transactional fee. All prices that are exposed in this documentation are estimates, needed to be negotiated with vendor.\nThere is consideration that most API management providers are aiming customers in hosted solutions, instead of on-prem installation, that would provide lock in for that vendor.\n### Functional ability for decision\nFunctional ability to consider when evaluating the tool. The following list was taken from [wikipedia page for API Management](https://en.wikipedia.org/wiki/API_management).\n- Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.\n- Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.\n- Developer portal/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.\n- Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.\n- Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.\n\n## Decision\n","- Vendor lock in for runtime\n- Open source or not\n- Installation options\n- Functional ability\n- Market presence\n- Pricing\nPricing model for API management solutions are complex. Usually based on transaction count, or CPU instances. Sometimes pricing is variation of annual fee and transactional fee. All prices that are exposed in this documentation are estimates, needed to be negotiated with vendor.\nThere is consideration that most API management providers are aiming customers in hosted solutions, instead of on-prem installation, that would provide lock in for that vendor.\n### Functional ability for decision\nFunctional ability to consider when evaluating the tool. The following list was taken from [wikipedia page for API Management](https://en.wikipedia.org/wiki/API_management).\n- Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.\n- Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.\n- Developer portal/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.\n- Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.\n- Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.\nWe recommend using hosted solution from **AWS** for API gateway. The reason is that if we look at the requirements and other architectural decisions in the project, the AWS solution does both fit in the architecture and pricing based on usage is cheaper than in other options. We made a pricing estimate for five years. Based on 100 million API calls per month the price of using AWS is one third of bought enterprise or homemade solutions. If the usage is 20 million API calls per month, we are looking at one fifth of the enterprise solutions. Note that there is also cost in using the management API and storage cost of logging, but it seems to be fraction of the total cost of using the product. Since the requirement is to host X-Road services that are defined as open and hosted at organization, we need to access the service through open X-Road server. Currently there will be installed X-Road server on AWS environment to use by island.is, that server can also be used by API gateway. Downside is that all request needs to go to the AWS environment, and then go back to Iceland. According to vendor lock in, then the investment cost of this option is not in the range that it will stop us to change to other solution. That could happen if requirement changes or the usage will be more than expected. This decision is based on the requirements and intended usage. If those requirements change, for example we would use the API portal as portal for Viskuausan, or more intense usage is expected, other options could be more relevant.\nFollowing is the decision phases used to get this conclusion.\n### Second phase of decision\nFor Open Source tool, we recommend usage of **Kong Community Edition**, with custom made API Developer Portal and Analytics. The analytics part could be based on ELK stack through plugin. The Kong community is large, and there are some Developers Portals available in the Open Source community. There is also some plugin available for logging and monitoring available. Kong API Gateway provides rest interface that can be used for customizing API Portal, and ability to create custom plugins for custom implementations. This decision provides more custom code to be developed and we need to rely on that the community can provide plugins. We also rely on that the Kong product will remain open source, but lot of plugins are only available for Enterprise edition.\n**WSO2** could be considered, since the whole suite is Open Source, so API Developer Portal and Admin UI is part of their Open Source offering. It is not recommend to use it without paid support plan.\nFor Vendor specific tool we recommend **Software AG API Management**. It is fully functional with customizable Developer Portal, and analysis tool. It has both partner and customers in Iceland. Current pricing model is based on transaction count and same applies for On-Prem vs. Hosted implementations. There are no cloud provider or runtime lock in. Implementation is that the tool needs to be installed and configured. The Developer Portal needs to be customized. For starter it is also option to host the installation at Advania for further evaluation. Analytics are fully integrated to ELK stack.\nFor hosted solutions, we recommend **AWS**, since it best fits the architectural decisions made for island.is\n### First phase of decision\nIn the first decision phase the following tools were initially pinpointed for further analysis. That was based on the option to run the API Management tool on premise. In decision outcome above, the tools have been narrowed to two options.\nIf open source options are not a requirement, it is suggested to evaluate the following tools.\n- [Software AG API Management](https://softwareaggov.com/products/integration/webmethods-api-management/)\n- [IBM API Connect](https://www.ibm.com/cloud/api-connect)\n- [Google Apigee Edge](https://apigee.com/about/cp/open-source-api-management)\n- [Mulesoft Anypoint](https://www.mulesoft.com/platform/api-management)\n- [Axway Ampify](https://www.axway.com/en/products/api-management)\nThese are the tools that are most mature, and do not provide lock in for runtime platform. They need to be evaluated based on pricing and technical ability.\nIf true open source is required, it is suggested to evaluate the following tools.\n- [WSO2](https://wso2.com/api-management/)\n- [Kong](https://konghq.com/)\nThese tools provide open source offering, but in most cases the features for Api Developer Portal and Publishing tools are only part of enterprise offering with subscription. Evaluation is needed for validating if the open source offering of the tool contains what is needed for implementation. We need to evaluate pricing of enterprise offering against the price of creating/implementing required pieces, like custom Api Developer Portal.\nFor hosted solution, the following options is considered\n- [AWS](https://aws.amazon.com/api-gateway/api-management/)\n- [Microsoft Azure API Management](https://azure.microsoft.com/en-us/services/api-management/)\n- [Google Apigee Edge](https://apigee.com/about/cp/open-source-api-management)\nDownside is that they are all platform dependent, to the owners proposed platform with more limited on-premise options. These are all top of the line tools according to capabilities.\nFor all considered tools we need to check what underlying software components are required. For example, data storage, queuing, and logging capacity. We need to take into consideration effort and ability to build Developer portal, compared to customizing the tool offering.\nOther tools that we looked at did in our opinion lack functionality or other ability for further considered, even though many of them could be considered. Note that the list provided is not all existing Api Management tools, so other options might apply.\n### Positive Consequences\n- Api Management tools are listed and grouped based on if they have open source option or not.\n### Negative Consequences\n- All considered options are vendor lock in.\n","To develop the API Gateway, use Tyk.\n\n**Tyk has the following advantages:**\n\n- It is open source and free to use.\n- It can be installed on-premises or in the cloud.\n- It provides all of the functionality required for the API Gateway, including:\n  - Registration of services with rate limit\n  - Self-service portal\n  - API key for Consumer\n  - Rate limit for Consumer\n  - Gateway\n  - Publishing tools\n  - Developer portal/API store\n  - Reporting and analytics\n  - Monetization"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhat protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.\n## Decision Drivers\n- Secure\n- Well defined and well reviewed standard\n- Easy to implement by client and resource systems\n- Support for non web client systems i.e. mobile devices\n\n## Decision\n,"- Secure\n- Well defined and well reviewed standard\n- Easy to implement by client and resource systems\n- Support for non web client systems i.e. mobile devices\nChosen option: ""OAuth 2.0 + OpenID Connect"", because it is secure and well examined and and has support libraries for our tech stack.\n","Implement OAuth2.0 as the standard protocol for authentication and authorization. OAuth2.0 is a widely adopted, well-defined, and well-reviewed standard. It is easy to implement in both client and resource systems and supports non-web client systems such as mobile devices. OAuth2.0 also provides strong security features such as encryption and token-based authentication."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want to be able to roll out new features gradually, perform A/B testing and target individual groups with a new feature. Also, we want to be able to flip a switch to turn features on or off for everyone.\n## Decision Drivers\n- Ease of setup\n- Ease of maintenance\n- Cost\n- Developer experience\n- Usability/UX\n- Operational concerns\n- Handling of PII\n\n## Decision\n","- Ease of setup\n- Ease of maintenance\n- Cost\n- Developer experience\n- Usability/UX\n- Operational concerns\n- Handling of PII\nChosen option: ""ConfigCat"", because:\n- We can probably get away with using it for very low cost\n- We can start using it almost right away with little configuration\nIf we decide later that we would like some of the features of LaunchDarkly, we want to be able to quickly swap. Thus, it is vital that we write some kind of service-agnostic wrapper.\n### Positive Consequences\n- We can start using feature flags across our stack.\n### Negative Consequences\n- Complexity of applications will increase\n",Implement Feature Flags
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.\n## Decision Drivers\n- Low complexity and overhead in development.\n- Fit for our stack.\n- Optimize CI/CD with dependency graphs and/or caching.\n- Flexible.\n\n## Decision\n","- Low complexity and overhead in development.\n- Fit for our stack.\n- Optimize CI/CD with dependency graphs and/or caching.\n- Flexible.\nChosen option: ""Nx"", because:\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\n- It's relatively easy to learn with focused documentation.\n- It has schematics to generate apps, libraries and components that includes all of our tools.\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\n","**Decision:** Use Nx.\n\n**Rationale:**\n\n* Nx met all the decision drivers:\n    * Low complexity and overhead: Nx is a simple and lightweight tool that does not add significant overhead to development.\n    * Fit for our stack: Nx is compatible with our existing stack and can be easily integrated with our existing tools.\n    * Optimize CI/CD: Nx provides dependency graphs and caching mechanisms that can help to optimize CI/CD pipelines.\n    * Flexible: Nx is a flexible tool that can be customized to meet the specific needs of our team and projects.\n\n**Alternatives Considered:**\n\n* Bazel: Bazel is a build tool that is more complex than Nx, but offers more features.\n* Rush: Rush is a monorepo tool that is similar to Nx, but does not offer as many features.\n\n**Additional Considerations:**\n\n* We should evaluate the cost and licensing implications of using Nx.\n* We should consider the learning curve for Nx and provide training and support to our team.\n* We should monitor the performance of Nx and make adjustments as needed."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nisland.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\nWhich CMS system would best suit the needs of island.is?\n## Decision Drivers\n- Content needs to be editable by non technical users\n- Content needs to be accessible across multiple domains and platforms\n- Setup should be simple for developers new to the project\n- The system should manage flexible content structures to limit systems impact on design\n- The system should be user friendly and easy to use for a non technical person\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\n\n## Decision\n",- Content needs to be editable by non technical users\n- Content needs to be accessible across multiple domains and platforms\n- Setup should be simple for developers new to the project\n- The system should manage flexible content structures to limit systems impact on design\n- The system should be user friendly and easy to use for a non technical person\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\nDevs narrowed the choice down to two options Contentful and Contentstack.\nBoth systems meet the required featureset.\nA decision from management was made to use Contentful.\nContentful is deemed to have a larger presence in the Icelandic dev community.\nContentful is also believed to have a stronger funding base.\nContentful is already implemented in some of our projects.\n,"**Decision:** Use Drupal as the CMS system for island.is.\n\n**Rationale:**\n\nDrupal meets all of the decision drivers outlined in the problem statement:\n\n- **Content editable by non-technical users:** Drupal has a user-friendly interface and drag-and-drop functionality that makes it easy for non-technical users to create and edit content.\n- **Content accessible across multiple domains and platforms:** Drupal can be used to create multi-site networks, which allows agencies and institutions to publish content on their own domains while still being managed centrally.\n- **Simple setup for developers:** Drupal has a well-documented API and a large community of developers who can provide support. This makes it easy for developers new to the project to get started quickly.\n- **Flexible content structures:** Drupal allows for the creation of custom content types and fields, which gives agencies and institutions the flexibility to manage content in a way that meets their specific needs.\n- **User-friendly and easy to use:** Drupal has a clean and intuitive user interface that makes it easy for non-technical users to find and manage content.\n- **Suitable workflow option:** Drupal has a built-in workflow system that can be used to manage the review and approval of content before it is published. This helps to ensure that content is accurate and of high quality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\n## Decision Drivers\n- Meet all requirements listed above\n- API quality\n- Pricing\n- Bundle size\n- Typescript support\n\n## Decision\n","- Meet all requirements listed above\n- API quality\n- Pricing\n- Bundle size\n- Typescript support\nChosen option: ""Recharts"", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\n### Positive Consequences\n- We can start implementing charts and graphs as needed in island.is\n### Negative Consequences\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\n","**Decision:** Chart.js is the recommended library for creating charts and graphs in the repository.\n\n**Rationale:**\n\n* **Meets all requirements:** Chart.js provides a comprehensive set of chart types and customization options, ensuring it can meet the needs of all island.is projects.\n* **High API quality:** Chart.js has a well-documented and intuitive API, making it easy to use and integrate.\n* **Free and open source:** Chart.js is available for free under the MIT license, making it accessible to all island.is projects.\n* **Lightweight bundle size:** Chart.js has a relatively small bundle size, minimizing the impact on page performance.\n* **Excellent Typescript support:** Chart.js offers excellent support for Typescript, allowing developers to take advantage of static typing and improved code maintainability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a [monorepo](../monorepo.md) we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.\n### Terms\n- `code integration` - this is a process that checks the integrity/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.\n- `artefact building` - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after `code integration` process finishes successfully.\n- `continuous integration` - the practice of running the `code integration process` triggered by events such as\n- pushing new revisions of the code to the code repository\n- opening a Pull Request\n- fixed schedule\n- etc.\nWe also run the `artefact building` process after a successful `code integration` process to have artefacts ready for deployment at all times.\n- `continuous integration platform` (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your `continuous integration` and publish the results\n## Decision Drivers (Policy)\n- The code integration process needs to be independent from CI platform integration\n- Benefits\n- Easier troubleshooting and development of the process\n- Easier migration to a different CI platform\n- Easier experimentation\n- Easier to run as part of the development process\n- Drawbacks\n- Needs more knowledge and experience\n- We use Docker as much as possible to implement the steps in the integration process\n- Benefits\n- Platform independence\n- Repeatability\n- Security\n- Drawbacks:\n- Needs expertise in Dockerfile and Docker in general\n- We only build the code affected by the change but re-tag all unchanged code artefacts\n- Benefits\n- Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hash\n- Supports the monorepo benefits and ideology\n- Drawbacks\n- Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)\n- We support only Linux as a target operating system when we cannot use Docker\n- Benefits\n- Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differences\n- We minimize the effort and complexity of supporting different operating systems\n- Linux tooling works on all major other OSs today - macOS and Windows\n- Drawbacks:\n- Devs that use non-Linux OS might need to install additional software and customizations\n\n## Decision\n","- The code integration process needs to be independent from CI platform integration\n- Benefits\n- Easier troubleshooting and development of the process\n- Easier migration to a different CI platform\n- Easier experimentation\n- Easier to run as part of the development process\n- Drawbacks\n- Needs more knowledge and experience\n- We use Docker as much as possible to implement the steps in the integration process\n- Benefits\n- Platform independence\n- Repeatability\n- Security\n- Drawbacks:\n- Needs expertise in Dockerfile and Docker in general\n- We only build the code affected by the change but re-tag all unchanged code artefacts\n- Benefits\n- Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hash\n- Supports the monorepo benefits and ideology\n- Drawbacks\n- Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)\n- We support only Linux as a target operating system when we cannot use Docker\n- Benefits\n- Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differences\n- We minimize the effort and complexity of supporting different operating systems\n- Linux tooling works on all major other OSs today - macOS and Windows\n- Drawbacks:\n- Devs that use non-Linux OS might need to install additional software and customizations\nGitHub Actions\n- Number 1 CI platform on GitHub at the time of this writing\n- Easy customization of which parts of the CI process to run depending on branching patterns and pull requests\n- Good integration of code health with the pull request process\n- As a GitHub open-source project, we have an unlimited number of ""compute""-minutes that come as a part of the package\n- Supports parallelisation of the process which can be pretty important in the context of monorepo\n- Support using own runners which can be helpful to maximize speed, minimize costs and increase security.\n",We will implement code integration in the form of local scripts organized in a single repository. This repository will be independent of any CI platform integrations and Docker will be used to implement the steps in the integration process. We will only build the code affected by the change but re-tag all unchanged code artefacts. We will only support Linux as a target operating system for code integration.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nKnow it before they do! We need a tool to discover, triage, and prioritize errors in real-time.\n\n## Decision\n","Chosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\n",**Decision:** Implement an error monitoring and reporting system that provides real-time visibility into application errors.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https://api.data.amsterdam.nl/bag/), [Basisregistratie kadaster (BRK)](https://api.data.amsterdam.nl/brk/) and [Handelsregister (HR)](https://api.data.amsterdam.nl/handelsregister). All required data is readily available.\n\n## Decision\n","The project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https://api.data.amsterdam.nl).\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\n","**Implement a RESTful API endpoint to expose the combined data from the specified APIs.**\n\n**Rationale:**\n\n* A RESTful API endpoint provides a consistent and flexible interface for accessing data from multiple sources.\n* It allows for selective retrieval of specific data items or subsets based on address or verblijfsobject.\n* This approach ensures data integrity and eliminates the need for direct integration with individual APIs, simplifying maintenance and reducing dependencies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContainer components can inject sagas and have them registered by a specific key. This key doesn't have to be unique. This, however, leads to unexpected problems when running on a production build. In development it's not an issue, because of hot-reloading, but in production, having more than one saga be registered by the same key, fails silently.\nWhat happens is that the already registered saga is overwritten by another saga. The first sagas functionality will no longer be available to the application.\n\n## Decision\n","Injecting more than one saga with a key that has already been used, will throw an error and will prevent the application from running.\n",Enforce uniqueness of saga registry keys in production builds.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo allow this project to be used by other parties (mostly non-Dutch speaking), it will be necessary to be able to translate all content in the application that is not coming from the APIs.\n\n## Decision\n",The application's content will be stored in translation files. The code will make use of [React-Intl](https://github.com/formatjs/react-intl). Also see the [i18n documentation](https://github.com/react-boilerplate/react-boilerplate/blob/master/docs/js/i18n.md) in the [react-boilerplate repository](https://github.com/react-boilerplate/react-boilerplate).\n,The decision is to use a translation service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSASS is used in most of the [Amsterdam](https://github.com/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\nAn alternative to global scope CSS is [styled components](https://www.styled-components.com/). This approach is also used by the [Amsterdam reusable component library](https://amsterdam.github.io/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\n\n## Decision\n","SASS will not be used in the Registraties project. Instead, styled components will be used.\n",Use [styled components](https://www.styled-components.com/) to define CSS rules for this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe application loads data from a JSON file into a mongodb database. Along with the raw data the mongo database\nmay also need to create indexes to improve search performance or provide geolocation searches.\nThe configuration of a database such as database name, collection name, indexes and ID key remain the same\nacross environments.\n\n## Decision\n","As the index may be fundamental to the operation of the consuming application, such as the geolocation search in the pharmacy database,\nthese configurations should be stored in version control.\nGiven the small number of databases (currently only Pharmacy, and GP Profiles data are held in mongodb) it is pragmatic to co-locate the configuration files\nalongside the `mongodb-updater` code, rather than creating a new repository and file hosting for each database configuration files.\n","Use an infrastructure-as-code tool, such as Terraform or CloudFormation, to define and manage the database configuration in a declarative manner. This approach provides the following benefits:\n\n- **Consistency:** The database configuration is defined in a single, central location, ensuring consistency across environments.\n- **Version control:** The configuration is stored in a version control system, allowing for easy tracking of changes and rollback if necessary.\n- **Automation:** Infrastructure-as-code tools can automate the process of creating and managing the database, reducing the risk of human error.\n- **Scalability:** The configuration can be easily scaled to multiple environments and clusters as needed."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTwo different MongoDBs need to be updated on a daily basis.\n\n## Decision\n,"Given the small number of databases to be updated both services will be hosted in the same stack, rather than manually create a stack for each database updater.\n",Use a synchronization mechanism to keep both MongoDBs in sync.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** Use an Architectural Decision Record (ADR) template to document and track architectural decisions throughout the project lifecycle.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent format for capturing and sharing architectural decisions.\n* They facilitate collaboration and knowledge transfer between team members.\n* They allow for easy searching and referencing of decisions made, promoting transparency and traceability.\n* They help evaluate and communicate the rationale behind architectural choices, fostering informed discussions and reducing technical debt."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWe have been using `@Named` annotation to provide dependencies that have same return type.\n```kotlin\n// Providing dependency\n@Provides\n@Named(""full_date"")\nfun provideDateFormatterForFullDate(locale: Locale): DateTimeFormatter = DateTimeFormatter.ofPattern(""d-MMM-yyyy"", locale)\n@Provides\n@Named(""date_for_user_input"")\nfun provideDateFormatterForUserInput(locale: Locale): DateTimeFormatter = DateTimeFormatter.ofPattern(""dd/MM/yyyy"", locale)\n// Injecting dependency\n@Inject\n@Named(""full_date"")\nprivate lateinit var dateFormatter: DateTimeFormatter\n```\nThis is problematic because this is not a type-safe approach and prone to developer error.\nIt is possible to have a typo in `@Named` value when providing and injecting the dependency, we can find the issue after building the project when\nDagger graph is generated.\n\n## Decision\n","In order to be more type safe when we are defining our dependencies we need to move to using `@Qualifier` from `javax.inject` package.\nWe can now define `Qualifier`s for different dependencies that have same return type.\n```kotlin\n@Qualifier\n@Retention(AnnotationRetention.SOURCE)\nannotation class FullDate\n```\nThen we can use that `Qualifier` while providing and injecting the dependencies.\n```kotlin\n@Provides\n@FullDate\nfun provideDateFormatterForFullDate(locale: Locale): DateTimeFormatter = DateTimeFormatter.ofPattern(""d-MMM-yyyy"", locale)\n@field:[Inject FullDate]\nlateinit var dateFormatter: DateTimeFormatter\n```\nThis approach allows us to be more type-safe when providing and injecting our dependencies. Since `@Named` itself is a `@Qualifier`  they both behave\nsame way internally and there will be no additional overhead on build.\n",Use Dagger's `@Qualifier` annotation for dependency injection. This annotation is used to provide a unique identifier for each dependency that has the same return type.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRESTful APIs tend to return enumerated values to represent properties with a fixed number of well-defined values. These are usually represented\nas `enum` classes on the client. When new business use cases are introduced, there could be additions to these values. If these values are consumed on\nthe fly and are not persisted on the client, they could simply be treated as a special case (unknown) or ignored all together.\nSimple is an ""offline-first"" application and some of these values have to be persisted to the database. Hence, these enumerated values cannot be\nrepresented using a Kotlin `enum` class. Offline-first means, there are some unique constraints that we have to deal with.\nApps on an older app version should be able to handle new enum values added to a type even though they don't know how to interpret the new value. This\ncould be solved by adding an UNKNOWN fallback value which the server adds as well. Except,\n* Records are shared between different users and thus, between different app versions. A newer app version that understands the new enum value could\ncreate a record with the new value and upload it, but an older app version that does not know the new value will overwrite it with UNKNOWN and this\nvalue will overwrite the actual value on the server as well when it syncs.\n* An app on an older version could have a degraded, fallback experience for enum values it does not understand, but as soon as it updates, the app\nshould be able to interpret the new values as if it was synced afresh.\n\n## Decision\n","To maintain forward-compatibility with enum values, we have to roll out our own infrastructure. To accommodate, we use sealed classes to represent\nenumerated values so that,\n- We could save these values to the database.\n- Use these values in newer versions of the app.\n- Send the same enum values back to the server during sync even though the current version of the app cannot interpret these values during runtime.\nIn order or achieve this, we require 4 classes.\n1. A sealed class hierarchy that is used to represent the enumerated values. Which also includes an `Unknown(val actualValue: String)` data class\nwhich is used to capture and handle newly introduced values.\n2. An implementation of the `SafeEnumTypeAdapter` class.\n3. A `RoomTypeConverter` to safely store and retrieve newly added enum values.\n4. A `MoshiTypeAdapter` to serialize and deserialize newly added enum values.\nThe interesting part is the `SafeEnumTypAdapter` class definition,\n```kotlin\nopen class SafeEnumTypeAdapter<T>(\nval knownMappings: Map<T, String>,\nprivate val unknownEnumToStringConverter: (T) -> String,\nprivate val unknownStringToEnumConverter: (String) -> T\n) {\n// …\n}\n```\nThe `knownMappings` field takes in a `Map` that can convert to and from enumerations that the current version of the application understands.\nThe `unknownEnumToStringConverter` converts unknown enum values to their actual string representation (usually used when pushing data to the server).\nThe `unknownStringToEnumConverter` can wrap the newly introduced value into the `Unknown` sealed data class defined earlier.\n",**Decision**: Introduce a column type in the database that knows about the set of possible values for an enum.\n\n**Benefits**: \n\n* Enables offline-first support for enum values.\n* Ensures data integrity by preventing unknown values from overwriting known values.\n* Provides a consistent experience for users on different app versions.\n\n**Drawbacks**: \n\n* Requires additional development effort to create and maintain the column type.\n* May not be supported by all database systems.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWe have been using `Theme.Design.*` theme from the start, while that worked perfectly it did not allow us to use new material components because those\nare dependent on `Theme.MaterialComponents.*` app theme. Using new material components allows us to have much more control over component theming like\ntext styles, icon & icon gravity, strokes, corners, etc.\n- Right now we are using 3 different kind of buttons to satisfy various design requirements `Button`, `OmegaCenterIconButton` & `PrimarySolidButton`.\n- Since all our Figma designs are made using material specs/material components, the design specs for certain components didn’t translate properly to\nappcompat variants or there are no components available in appcompat at all. Moving to MDC will make the design to layout/style much easier.\nYou can find more information on setting up MDC Android [here](https://material.io/develop/android/docs/getting-started/).\n\n## Decision\n","Moving to MDC will allow us to use `MaterialButton` & other material components if needed. You can find more material components supported by Android\nand how to theme them [here](https://material.io/components).\nWith `MaterialButton` we can finally use icon, icon gravity, icon tint. This migration will also open better theming support for material components\nsuch as corner radius, strokes, shapes etc., Since we no longer need 3 types of buttons we will remove\n`PrimarySolidButton` & `OmegaCenterIconButton`, we will be using `MaterialButton` in place of `Button`.\nThese are the changes that occur due to this migration.\n### Button tag change:\n- We will start using `com.google.android.material.button.MaterialButton` in views instead of `Button`.\n- In order to use icon in a `MaterialButton` we will use the `icon` attribute instead of `drawableStart` or `drawableEnd` attr.\n- We will use `iconGravity` (`textStart`, `viewStart`, `textEnd`, `viewEnd`) for setting the icon position, `iconTint` for setting the icon color.\n### Style change:\n- `theme_material.xml` will house the main app theme which now extends `Theme.MaterialComponents.Light.NoActionBar`, some of the new attrs we set in\napp theme.\n```\n<style name=""Clinic.V2.Theme"" parent=""Theme.MaterialComponents.Light.NoActionBar"">\n<item name=""materialButtonStyle"">@style/Clinic.V2.MaterialButton</item>\n<item name=""materialButtonOutlinedStyle"">@style/Clinic.V2.OutlineButton</item>\n<item name=""borderlessButtonStyle"">@style/Clinic.V2.TextButton</item>\n<item name=""toolbarStyle"">@style/Clinic.V2.ToolbarStyle</item>\n</style>\n```\nThese attrs can be directly referenced in the view styles instead of using the entire style tag in the view.\n- We have 4 primary button styles that we use in app\n- `Clinic.V2.MaterialButton`\n- `Clinic.V2.TextButton`\n- `Clinic.V2.OutlineButton`\n- `Clinic.Button.Flat`\n- You can then style `MaterialButton` using any of these styles or attr.\n- Filled Button : `?attr/materialButtonStyle` or `@style/Clinic.V2.MaterialButton`\n- Text Button: `attr/borderlessButtonStyle` or `@style/Clinic.V2.TextButton`\n- Outline Button: `?attr/materialButtonOutlinedStyle` or `Clinic.V2.OutlineButton`\n- Un-Elevated/Flat Button: `@style/Clinic.Button.Flat`\n- You can extend any of those primary button styles to override certain attrs like color, for example:\n```\n<style name=""Clinic.V2.MaterialButton.Green3"">\n<item name=""backgroundTint"">@color/green3</item>\n<item name=""android:textColor"">@color/green1</item>\n<item name=""iconTint"">@color/green1</item>\n</style>\n```\n","**Decision:** Migrate the app's theme to `Theme.MaterialComponents.*`.\n\n**Rationale:**\n\n* Using `Theme.MaterialComponents.*` allows the app to utilize new material components with enhanced theming capabilities.\n* The current use of `Theme.Design.*` limits the app's ability to implement material design specifications accurately.\n* Migrating to MDC simplifies the design-to-layout/style process, ensuring adherence to Figma's material design specs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur [current screen architecture](001-screen-controllers.md) has a bunch of problems:\n- We recreate application state from the UI state, which depends on some hidden behaviour:\n- RxBinding should have initial value observables that emit the current event immediately.\n- All widgets on the screen should save and restore state properly.\n- The entire event stream is replayed because the system is setup in a way that they begin emission of events like ScreenCreated before the entire\nevent handling loop is setup.\n- The controller handles both, the business logic and the view logic.\nAll of these come together to make it hard to test and maintain screens. In addition, screens which perform a lot of business logic in memory require\na lot of working around the fact that the architecture depends on the state being saved either in persistence or by the platform mechanisms.\n\n## Decision\n","### Goals\n- Separate presentation and business logic so that they can tested independently of each other.\n- Make UI state explicit and save/restore it manually instead of depending on hidden behaviour.\n- Make it easy to migrate from the v1 architecture to the v2 for existing screens.\n### Implementation\nWe split the controller into two discrete pieces, each with their own responsibility.\n- `UiStateProducer`: This will be responsible for performing the function of the controller related to the business logic.\n- `UiChangeProducer`: This will be responsible for performing the function of the controller related to the presentation logic.\nIn addition, we introduce a helper class, `ViewControllerBinding`, which is used to tie the state producer and consumer together with the event\nstream.\n#### Reference implementation\nA reference implementation of the complete architecture can be found\nat [this commit](https://github.com/simpledotorg/simple-android/blob/9e8412259e034e555fa40c2b07810a98d736df95/app/src/main/java/org/simple/clinic/shortcodesearchresult/ShortCodeSearchResultScreen.kt)\n.\n#### Terminology\n- `Event`: a generated event. This will typically be events generated by the user interface, but might also include events generated by the platform\nlike sensors, camera, etc. These will generally be represent as Kotlin data classes that implement the `UiEvent` interface.\n- `UiState`: a Kotlin data class that represents everything needed to render the content of a given screen.\n- `Ui`: an interface which represents all the functionality that the actual screen needs to provide to the controller.\n- `UiChange`: a Kotlin lambda that has the signature `(Ui) -> Unit`.\n#### Screen Setup Process\n- Define a `UiState` for whatever view/screen is being built.\n- Create a `UiStateProducer` which is an `ObservableTransformer<Event, UiState>`. This class will be responsible for business logic by transforming a\nstream of events into a stream of UI states.\n- Create a `UiChangeProducer` which is an `ObservableTransformer<UiState, UiChange>`. This is responsible for presentation logic by transforming a\nstream of UI states into a stream of lambdas which will executed on the `Ui`.\n- Create a controller which is an `ObservableTransformer<UiEvent, UiChange>` which will compose `UiStateProducer` and `UiChangeProducer` internally.\n- Use the `ViewControllerBinding` to tie the event stream to the view.\n##### Sample\n```kotlin\n@Inject\nlateinit var uiStateProducer: ShortCodeSearchResultStateProducer\n@Inject\nlateinit var uiChangeProducer: ShortCodeSearchResultUiChangeProducer\nlateinit var binding: ViewControllerBinding<UiEvent, ShortCodeSearchResultState, ShortCodeSearchResultUi>\noverride fun onFinishInflate() {\n// Inject the screen\nbinding = ViewControllerBinding.bindToView(this, uiStateProducer, uiChangeProducer)\nnewPatientButton.setOnClickListener { binding.onEvent(SearchPatient) }\n}\n```\n### Goals Review\n#### Separation of concerns and testability\nSince the business logic and presentation logic are separated into two discrete components, testing them is simpler since they can be tested\nindependently of each other. In addition, we can test the behaviour of the business logic using value testing (asserting the generated states) which\nis more readable than verifying behaviours using mocks.\n#### UI state saving/restoration\nSince the state of a screen is represented as a data class, saving and restoring the state is easy, and has just two steps:\n1. Make the `UiState` class implement the `Parcelable` interface.\n2. The `ViewControllerBinding` exposes two functions, `latestState()` and `restoreSavedState()`, which can be used in conjunction with the platform\nlifecycle methods.\n#### Migrating from the older architecture\nOne of the major concerns about moving from the v1 architecture to v2 is breaking existing features while moving code around. Since both the v1 and v2\narchitectures expose the same overall interface (`ObservableTransformer<Event, UiChange>`), we can replace the controller in tests for the older\nscreens with a composition of the state producer and state consumer and verify that no behaviours were changed when migrating.\n","**Switch to a unidirectional data flow architecture that clearly separates concerns between business logic, presentation logic, and UI state.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe don’t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\nthe [MVI architecture](https://medium.com/@ragunathjawahar/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https://medium.com/@ragunathjawahar/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\n.\n\n## Decision\n","Every screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\nto the screen.\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\nRxJava streams.\n```kotlin\n// Create the UsernameTextChanged event by listening to the EditText\nRxTextView\n.textChanges(usernameEditText)\n.map { text -> UsernameTextChanged(text) }\n// Event\ndata class UsernameTextChanged(text: String) : UiEvent\n```\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\nuni-directional. To merge multiple streams into one, RxJava’s `merge()`  operator is used.\n```kotlin\n// Login screen\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\n.compose(controller)\n.takeUntil(screenDestroy)\n.subscribe { uiChange -> uiChange(this) }\n```\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\n```kotlin\ntypealias Ui = LoginScreen\ntypealias UiChange = (LoginScreen) -> Unit\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\nevents.ofType<UsernameTextChanged>\n.map { isValidUsername(it.text) }\n.map { isValid ->\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } // UiChange lambda!\n}\n}\n}\n```\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\n```kotlin\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\nval replayedEvents = events.replay().refCount()\nreturn Observable.merge(\nenableSubmitButton(replayedEvents),\nloginUser(replayedEvents))\n}\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\n}\n```\n![data flow from the Ui to the controller](images/diagram_screen_controller.png)\n([diagram source](https://docs.google.com/drawings/d/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4/edit?usp=sharing))\n",Use the MVI architecture pattern to separate screens and controllers and keep business logic outside Android framework classes for unit testability and fast feedback loops.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur [intermediate architecture](./007-screen-architecture-v2.md) was a step above the [v1 screen architecture](./001-screen-controllers.md). However,\nit still has problems of its own:\n- The `UiStateProducer` is still some sort of a **God Class** since it is reponsible for both business logic (what needs to happen) and effects (\nchanges to the system). This makes testing it cumbersome since the tests are more of integration tests which end up verifying both business logic\nand implementation details.\n- The architecture is heavily dependent on [RxJava](https://github.com/ReactiveX/RxJava/). While `RxJava` is good for a lot of things, the current\narchitecture encourages us to use it for everything and this eventually leads to the tests and production code becoming increasingly harder to\nmaintain and refactor.\n- In addition, `RxJava` has a steep learning curve which requires significant onboarding effort before a new contributor is able to reach an\nacceptable level of productivity with the current architecture.\nWe need a scalable framework to build screens that lets us manage business logic and effects, and which is receptive to change.\n\n## Decision\n","### Goals\n- Separate business logic, presentation logic, and effects so that they can tested independently of each other.\n- Make UI state explicit and save/restore it manually instead of depending on hidden behaviour.\n- Restrict `RxJava` to managing events and effects, and let the business logic be implemented as pure functions.\n### Choosing a framework\nWe evaluated many patterns and frameworks that are common in the industry, including but not limited to:\n##### [Android recommended architecture](https://developer.android.com/jetpack/docs/guide)\nThe Android recommended architecture depends on a few architecture components to work together:\n- [`ViewModel`](https://developer.android.com/topic/libraries/architecture/viewmodel): To retain the state across configuration changes.\n- [`LiveData`](https://developer.android.com/reference/androidx/lifecycle/LiveData): To provide lifecycle-aware reactive notifications to the UI.\n- [`Lifecyle`](https://developer.android.com/reference/androidx/lifecycle/Lifecycle): To automatically manage subscriptions to the reactive\nnotifications from the `LiveData` instances provided via the `ViewModel`.\nThis architecture makes sense for new codebases, but this is a codebase with an already existing architecture which does not lend itself well to this\nspecific setup. Some of the problems are:\n- The `Lifecycle` component is designed to work with screens that are built on top of `Activity` or `Fragment` classes. However, our current\narchitecture uses a single `Activity` setup, where the individual screens are implemented using `View` subclasses. The `LiveData` component will not\ngive us much benefit unless we transition all the `View` based screens to be `Fragment` instances.\n- We already use `RxJava` for reactive notifications. Using `LiveData` would mean that we would either have to replace usages of `RxJava`\nwith `LiveData` which is neither feasible not desirable at this point.\n##### [MvRx, by Airbnb](https://github.com/airbnb/MvRx)\n`MvRx` is a library that is built on top of `RxJava` and the Android `ViewModel` architecture component. While this is a good architecture, it has a\ncouple of limitations which stopped us from choosing this:\n- Its core model is based on `RxJava`. We already have issues because of an overuse of `RxJava` across the app and part of the goals of this new\narchitecture is to restrict the usage of `RxJava` to limited sections of the codebase.\n- It does not support custom views and is designed to be used with `Fragment` based screens. This suffers from the same problem as the Android\nrecommended architecture.\n##### [MVI](http://hannesdorfmann.com/android/mosby3-mvi-1)\n`MVI` (Model-View-Intent) was one of the more promising architectures that we reviewed. The problem with `MVI` however, is that it is generally a set\nof principles more than an architecture. This means that there are many implementations of `MVI` in the industry, and they are all implemented\ndifferently based on the needs of the project. There is no ""one-way"" to implement it at all.\nWe took a look at the core principles which most `MVI` implementations are based on, which are similar\nto [`Redux`](https://redux.js.org/introduction/three-principles).\n- Single source of truth\n- State is read-only\n- Changes are made with pure functions\nWe decided to look for libraries/frameworks which are based on these principles and build our new screen architecture based on them.\n### Result\nLooking at the frameworks available in the Android world that are built on the [Redux principles](https://redux.js.org/introduction/three-principles),\nwe found [Mobius](https://github.com/spotify/mobius), a reactive framework for managing state and side-effects, by Spotify.\nThe [objectives](https://github.com/spotify/mobius/wiki/Objectives) of the framework also aligned very well with what we need from the architecture.\nThus, we decided to use it as the basis for the v3 screen architecture.\nBasing our new screen architecture on this framework lets us satisfy the following goals:\n#### Separation of concerns\nThis is satisfied by Mobius since it enables us to separate concerns at an even more granular level than before.\nWe have three core components:\n- [`Update`](../mobius/implementing-a-new-screen.md#update): Responsible for deciding the business logic.\n- [`EffectHandler`](../mobius/implementing-a-new-screen.md#effecthandler): Responsible for making changes to the system (or the *Real World*).\n- [`UiRenderer`](../mobius/implementing-a-new-screen.md#uirenderer): Responsible for updating the UI in response to changes to the state.\nThese components are responsible for discrete parts of the system. These are smaller and more focused, thereby making them easier to test and\nmaintain.\n#### Making UI state explicit and support state restoration\nThis is handled by the `MobiusDelegate` classes for us and enforced by it since the class expects the `Model` to be `Parcelable` by default.\n#### Restricting `RxJava` usage\nThe `Update` component here is solely responsible for the business logic and is implemented only as pure functions. The usage of `RxJava` will be\nlimited to:\n- Setting up event sources to feed into the Mobius loop.\n- Perform asynchronous operations in the `EffectHandler` component\n### Usage\nThere are two things to consider when we look at this architecture from an implementation perspective:\n##### How do we create a new screen\nMobius has its own core components, and we have created our components on top of them in order to build out our screen architecture. These\ncomponents (both Mobius' and our own) have been documented on [this page](../mobius/implementing-a-new-screen.md).\n##### How do we migrate an older screen to the new architecture\nMigrating to the newer architecture is an involved process that requires us to follow deliberate, measured steps. The migration process is detailed\nin [this document](../mobius/migrating-to-mobius.md)\n##### Reference\nA reference implementation of the complete architecture can be found\nat [this commit](https://github.com/simpledotorg/simple-android/tree/6da548b36c3cceb3e3db344c09a0f5ae588fc2c0/app/src/main/java/org/simple/clinic/settings/changelanguage)\n.\n","We will adopt a **Redux-like** pattern for screen development.\n\n**Redux-like** is a pattern that provides a unidirectional data flow in applications. It consists of three main components:\n\n1. **Actions**: Actions are events that describe what happened in the application.\n2. **State**: The state is the current state of the application and is influenced by the actions.\n3. **Reducer**: The reducer is a function that takes the current state and an action and returns the new state.\n\nIn this pattern, the screen is responsible for creating actions and managing its state. \n**Actions** are the only way to modify the state, and they are always dispatched to a **reducer**. \nThe **reducer** is a pure function that takes the current state and an action and returns the new state. \nThe **state** is the only source of truth for the screen, and it is never modified directly.\n\nThis pattern has several advantages:\n\n1. **Simplicity**: The pattern is simple to understand and implement.\n2. **Testability**: The pattern makes it easy to test the screen logic.\n3. **Maintainability**: The pattern makes it easy to maintain the screen code.\n4. **Extensibility**: The pattern makes it easy to extend the screen with new features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nStrong references between SQL tables using foreign keys are useful for maintaining data integrity. They introduce safeguards against orphaned records\nby,\n- making it impossible to create data records that do not fulfill relation.\n- deleting referenced records in a cascading fashion when the root dependent record is deleted.\nIn the context of Simple, a good example would be the `Patient` and `BloodPressureMeasurement` tables. In real world, it’s impossible to record a\nblood pressure for a non-existent patient. Likewise, maintaining a strong reference between these two tables in SQL would have ensured that blood\npressures do not get created either unless the associated patient already exists in the database.\nAdditionally, if the patient ever gets deleted, SQLite would have handled deletion of all their blood pressures automatically.\nUnfortunately, strong references work only when the data storage is centralized. Because Simple’s data is distributed across many devices, it’s\nimpossible to guarantee the existence of all data records at the same time due to its offline by default nature. For instance, it’s easy to imagine a\nscenario where the app receives some blood pressures from the server, but fails to receive their associated patients because of bad network\nconnectivity. This is expected behavior and we do not want the app to fail because of a foreign key integrity failure.\n\n## Decision\n",SQL tables in Simple will not keep strong references using foreign keys unless they can be synced **together** in the same network call with the\nserver.\n,Do not enforce referential integrity in SQL.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUnlike an online-only app that can depend on the server to create unique record IDs, Simple is designed to work completely offline. It needs to create\ndata locally and perform two-way syncing of data with the server whenever possible. This brings two kinds of challenges,\n1. Resolving conflicts when merging data from the server with its local copy.\n2. Generating IDs that are universally unique.\n\n## Decision\n","Simple is primarily an app-only service, therefore the client app is considered as the source of truth for\nall [Protected Health Information (PHI)](https://en.wikipedia.org/wiki/Protected_health_information). If a conflict arises when merging a record\nreceived from the server with a local copy of the record with the same ID, the server copy is discarded if the local copy is dirty (i.e., pending a\nsync with the server). Otherwise, the server copy is saved.\nFor ensuring that the IDs are unique, [UUIDs](https://developer.android.com/reference/java/util/UUID.html) (universally unique identifier) are used.\nWhile the probability that a UUID will be duplicated is not zero, it is close enough to zero to be negligible. There are multiple versions of UUID,\nbut we’re using **v4** because it is the most convenient to use and doesn’t require any seed value.\n```kotlin\nUUID.randomUUID()\n```\nIt would have been nice to let our database handle generation of IDs, but SQLite only supports generation of auto-incrementing integer IDs. Other\nalternatives like using `Random.nextLong` were also discarded because their space isn’t large enough to guarantee near uniqueness.\nBoth the client app and the backend can create a UUID and use it to identify something with near certainty that the identifier does not duplicate one\nthat has already been, or will be, created to identify something else.\nAccording to [Wikipedia](https://en.wikipedia.org/wiki/Universally_unique_identifier#Collisions),\n> the number of random version-4 UUIDs which need to be generated in order to have a 50% probability of at least one collision is 2.71 quintillion.\n",**Decision:** Use UUIDs for data IDs and implement conflict resolution based on timestamps.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFragments were introduced in Android API 11, as a way to model behavior or a portion of the user interface. Fragments can be shared between multiple\nActivities, and can be replaced by other Fragments without affecting the parent Activity.\nHowever, Fragments have a lifecycle that, although part of the Activity lifecycle, is slightly different and the two are not sychronous. This has\ncreated a class of bugs, of which many are common in Android apps; e.g.:\n- Fragments are usually created after `Activity#onCreate()` has been called. But during state restoration, Fragments get created\nin `Activity#onCreate()` when the Activity’s properties may not have been initialised.\n- When exchanging data between Activities, the `Activity#onActivityResult()` gets called before the Activity is resumed, and creating a Fragment when\na result is received will crash the app.\n- Fragments are not resumed when `Activity#onResume()` is called. The documentation recommends using `Activty#onResumeFragments()` instead.\n- Fragment lifecycle events are not synchronised with Activity lifecycle, and trying to do this introduces an extra layer of complexity. Race\nconditions are difficult to avoid with an asynchronous lifecycle\ncausing [pain, suffering and misery](https://www.google.com/search?q=illegal+state+exception+fragment+android).\n\n## Decision\n","We will use a single `Activity` to hold all our “screens”, and entirely avoid using `Fragment` classes. To switch between “screens”, we will\nuse [Flow](https://github.com/square/flow) — a library that enables navigating between different UI states.\nFlow makes it easy to construct screens using just `View` classes. Views are inflated synchronously, which drastically reduces complexity and\neliminates race conditions. When we inflate a layout XML using `LayoutInflater#inflate()`, we are guaranteed that the View will be ready for making\nany modifications. A single `Activity` also allows for easy animations, because inter-activity transitions are harder to get right.\nSince we are using `View` classes as screens, we also lose the ability to use\nthe [Back Stack](https://developer.android.com/guide/components/activities/tasks-and-back-stack). Flow also helps with this by providing a `View`\n-based backstack.\n","**Decision:** Use View Binding for Fragments\n\n**Rationale:**\n\nView Binding solves the problems associated with Fragment lifecycle by providing a way to bind views to fragments in a type-safe manner. This eliminate the need for manually finding and setting views, which can lead to bugs and crashes. View Binding also ensures that views are bound to fragments only after the fragment has been created, avoiding the issues that can arise from binding views too early.\n\n**Consequences:**\n\n* **Improved code safety:** View Binding eliminates the potential for bugs and crashes caused by manually finding and setting views.\n* **Simplified development:** View Binding reduces the boilerplate code required to bind views to fragments, making development easier.\n* **Increased performance:** View Binding improves performance by caching views, which reduces the number of findViewById calls required."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCreating and holding a timer object can introduce errors and noise in the code that's using it. It's also hard to remove for release code to optimize performance.\nExample:\n```c++\n#include <time_diagnostics.h>\nTimeDiagnostics td<2>;\nstatic const TIMER_ID = 0;\n{\nTimer t = td.get_timer(TIMER_ID);\n// ... user code\n}\n```\n\n## Decision\n,"Provide a macro that can be used to start a new timer and can be disabled during recompile.\nExample:\n```c++\n// uncomment this to remove timers for release code\n// #define DISABLE_TIME_DIAGNOSTICS\n#include <time_diagnostics.h>\nTimeDiagnostics td<2>;\nstatic const TIMER_ID = 0;\n{\nSTART_TIMER(td, TIMER_ID);\n// ... user code\n}\n```\n","Use an RAII (Resource Acquisition Is Initialization) class for the timer object.\n\nThe RAII class would automatically create the timer object in its constructor and destroy it in its destructor. This would ensure that the timer object is always properly destroyed, even if the user code throws an exception.\n\nExample:\n\n```cpp\n#include <time_diagnostics.h>\n\nclass TimerObject {\n  public:\n    TimerObject(TimeDiagnostics &td, size_t timerId) : td_(td), timerId_(timerId) {\n      timer_ = td.get_timer(timerId);\n    }\n\n    ~TimerObject() {\n      if (timer_) {\n        td_.release_timer(timer_);\n      }\n    }\n\n    Timer &get_timer() { return *timer_; }\n\n  private:\n    TimeDiagnostics &td_;\n    size_t timerId_;\n    std::optional<Timer> timer_;\n};\n\n...\n\nTimerObject timerObject(td, TIMER_ID);\ntimerObject.get_timer() // ... user code\n```"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTimers need a way of being uniquely identified in order to be processed after several are recorded. The ID should be something easy for the user to work with.\n\n## Decision\n,"Integers will be used over the alternatives of strings, enumerations, enum classes, or custom object.\n","**Use a UUID to uniquely identify timers.**\n\n**Rationale:**\n\n* UUIDs are universally unique identifiers that are easy to generate and work with.\n* They are also more secure than sequential IDs, as they are not predictable.\n* UUIDs are supported by most programming languages and databases."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use architectural decision records (ADRs) to document architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Problem\n1. We want to provide a good design out-of-the-box, so that users of the library do not feel obligated to override it\n2. Nobody in the core team is a professional designer\n3. Implementing a design is expensive (especially accounting for browser compatibility)\n4. We want to make it as easy as possible to override the design (for users of the library) and to use it (for contributors)\n### Options\n1. Find a designer, implement their design\n- This is expensive (in designer time and dev time)\n- Any future requirement has to involve a designer\n2. Use a component library\n- Cheaper (no designer time, no design implementation time)\n- Hard to find a library that everyone agrees on\n- Potential bloat\n- Opinionated libraries won't make it easy to override the design for users of nestjs-admin\n### Component library options\n1. Bootstrap 4\n**Pros:**\n- Well-known by devs\n- Easy for contributors\n- Easy to extend for nestjs-admin users\n- Good accessibility\n- Good browser compatibility\n- Lots of UI widgets available\n- We probably won't use them, but nestjs-admin users might (when we support custom widget for form input)\n- Easy customization through SCSS variables\n- High-level components (list group, navs, page header...)\n- Well maintained\n**Cons:**\n- Not liked a lot by devs\n- A bit bulkly (68kb, 12kb gzipped)\n- Significant DOM bloat (`.row` and `.col-*`)\n- Utils classes (""CSS-in-classes"" such as `.d-flex`, `.border`, `h-100`...) make it harder for nestjs-admin users to override the design without touching HTML\n2. Bulma\n**Pros:**\n- Readable syntax\n- Fontawesome support\n- Light (73kb, 10kb gzipped)\n- Generally liked by devs\n- Well maintained\n**Cons:**\n- Lack of high-level components\n- Accessibility (from bulma.io: ""90% of Bulma works in IE11"")\n3. Semantic UI\n**Pros:**\n- Fairly well-known\n- Looks sleek\n- Good collection of UI widgets\n- Nice to read, semantic classes\n**Cons:**\n- Complex installation\n- Very heavy (146kb, 20kb gzipped)\n- Not clear how to customize it as a nestjs-admin user\n- Poorly maintained\n4. Tailwind\nThe utility class focus makes it a non-starter for a lib that wants to be customizable.\n\n## Decision\n","After a hesitation with Bulma, we decided to go with Bootstrap 4, for the following reasons:\n- Though it seems to not be the most liked library by the dev community, most front-end devs know the basics\n- This makes it very easy to contribute to and extend\n- Better browser compatibility\n- Better accessibility\n- Reasonable size, we can import only the elements we need\n- Easy install => easy to get rid of if we want to change later\n","Use Bootstrap 4 as the component library for NestJS-Admin.\n\nBootstrap 4 is a well-known and widely used component library that offers a good balance of features and ease of use. It is relatively easy to customize, and it provides a wide range of UI widgets that can be used to build complex applications. Additionally, Bootstrap 4 is well-maintained and has a large community of users and contributors."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe admin interface obviously cannot be public for the vast majority of websites. This means that we need to provide a way to handle authentication and authorization.\n### Characteristics\nWe want this authentication system to be:\n- Easy to set up for nestjs-admin developers (or better: working out-of-the-box)\n- Extensible to handle a wide range of use cases\n### What does django-admin do?\nDjango-admin uses the builtin ""user"" model, that has a username and password. It allows them to rely on these fields for authentication, in addition to `is_staff` and `is_superuser` fields.\n### Challenges\n1. NestJS, as opposed to frameworks like Django, has no built-in concept of ""user"" with a username and password. We cannot rely on this.\n2. Users come in all shapes and form: identified through username, email, id, using a password or not... There's no invariant we can rely on.\n3. We need to make sure whatever solution we choose doesn't lock everybody out of the admin interface by default (we need to manually be able to create an admin, like django-admin with `./manage.py createsuperuser`)\n### Options\n1. Have the developer extend the Admin controller, which will allow them to define their own guard and authentication logic\n- Very flexible\n- Nothing to do for us but documentation\n- Lot of friction to start using nestjs-admin, it makes the lib not usable out-of-the-box\n2. Add some config for the admin module to specify the User entity, which needs to implement some AdminUserInterface (getAdminUsername, getAdminPassword, isAdmin). We can then have a guard to implement authentication/authorization.\n- Need to implement a login page\n- Not very flexible, we'll have to make assumptions (user has a username and a password)\n3. Provide an AdminUser entity out-of-the-box, that has a username and password.\n- Need to implement a login page\n- Not very flexible. Admin entities are distinct from User entities, which is annoying (how to create admins? Choose their password?).\n- Works entirely out-of-the-box\n\n## Decision\n","The N.1 concern at the time is to make this library self-contained, usable out-of-the-box. For this reason, we'll go for option 3.\nThe downsides of this option (flexibility, Users distinct from Admins) can be mitigated:\n- For more flexibility, we could also provide a AdminUserInterface, that can be implemented by another entity (a User entity very probably) and allow configuring the AdminModule to use this entity (option 2)\n- We can still let the developer extend the admin controller to implement their own guard (option 1., need to document how)\n","**Option 2:** Add some config for the admin module to specify the User entity, which needs to implement some AdminUserInterface (getAdminUsername, getAdminPassword, isAdmin). We can then have a guard to implement authentication/authorization.\n\n**Rationale:**\n\n* **Extensibility:** This option provides flexibility by allowing developers to define their own User entity and authentication logic.\n* **Out-of-the-box functionality:** The default AdminUserInterface implementation can provide basic authentication and authorization, making the admin interface usable without additional configuration.\n* **Reduced friction:** Developers can use the admin interface without extending the Admin controller, which reduces the learning curve and makes the library more user-friendly.\n* **Simplicity:** Implementing a login page is a straightforward task, and it avoids the complexity of creating an AdminUser entity that is separate from the User entity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n",The change that we're proposing or have agreed to implement.\n,"## Decision\n\n**Use service discovery and load balancing.**\n\n**Rationale**\n\n**Advantages:**\n\n* Improved reliability: By distributing traffic across multiple instances, the application can continue to function even if one instance fails.\n* Increased scalability: As the application grows, new instances can be added or removed without affecting the availability of the application.\n* Simplified management: Service discovery and load balancing can be managed centrally, making it easier to maintain the application.\n\n**Disadvantages:**\n\n* Added complexity: Service discovery and load balancing can add complexity to the application.\n* Performance overhead: Service discovery and load balancing can introduce some performance overhead.\n\n**Other options considered:**\n\n* **Use a single instance of the application.** This would be the simplest option to implement, but it would also be the least reliable and scalable.\n* **Use a manual load balancing solution.** This would be a more complex solution to implement, but it would give the application more control over how traffic is balanced."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n",The change that we're proposing or have agreed to implement.\n,"**Decision:** The provided context does not contain any information about a decision that was made. Therefore, I cannot provide a decision."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n",The change that we're proposing or have agreed to implement.\n,The decision made based on the provided context cannot be provided since the context information is missing from the provided text. Please provide the context to make a decision.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n",The change that we're proposing or have agreed to implement.\n,"The provided context does not contain any information regarding a decision to be made, so I cannot provide a decision based on the provided text."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",To use an Architectural Decision Record (ADR) template to record architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe built-in deserializers included in JMS are sufficient for most cases, but sometimes the provided payloads are not compatible with them. Situations like this include strings in plain text, where no deserialization is expected. Feeding this type of data to JMS raises an exception that stops the execution of the application.\nIn these cases, the serialization needs to be bypassed in a clean and transparent way that will allow the calling class to continue its execution.\n\n## Decision\n","We decided to implement a custom deserializer **that accepts plain text**.\nThis deserializer is plugged **as a custom deserializer** in JMS' configuration and called whenever the format type ""plain_text"" is passed to the Serializer class.\n",Disable the deserialization process by implementing a custom `MessageListener` and providing an empty implementation for the `onMessage()` method. This will prevent any deserialization attempts and allow the application to continue its execution without encountering exceptions.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe ability to update/upgrade our servers for security or testing purposes is\nkey. We will have at least two virtual machines sharing a mounted directory on\neach that will contain files that all servers can view. The biggest benefit will\nbe the Web Application Firewall that will be applied to all traffic that is\ntargeted to the `library.princeton.edu`. Using deep packet inspection malicious\npatterns will be diverted away.\n\n## Decision\n,We will create\n* a smb/cifs and/or nfs mount that will be present on all nodes behind the\nloadbalancer\n* we will create a service (Kemp terminology) called `library.princeton.edu`\n* we will create nodes that will mount the aforementioned drives\n* we will implement SolrCloud on a remote endpoint that all nodes can read and\nwrite to\n* we will implement remote database (mariadb) that all nodes can read/write to\n,Implement a load balancing solution using virtual machines and a mounted directory to facilitate server updates and security enhancements. This setup will allow for seamless updates without interrupting service availability and ensure all servers benefit from the added security measures.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","To record the architectural decisions made on this project, we will use an Architectural Decision Record (ADR) template. The ADR template will include fields for the following information:\n\n* Decision title\n* Decision statement\n* Date\n* Author\n* Context\n* Decision\n* Consequences\n* Alternatives considered\n* References"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\n\n## Decision\n",- We will build a library of atomic ui components that will be used throughout the applications.\n- Developers should be able to easily import only the components they need and use them in their applications.\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\n,"**Decision:** Create a monorepo with a separate package for the shared Angular components library.\n\n**Rationale:**\n\n* **Centralized management:** Maintains all components in a single repository, enabling easy collaboration and version control.\n* **Consistent design language:** Ensures that all shared components adhere to a consistent design language, enhancing the user experience.\n* **Reusability:** Allows components to be easily reused across multiple applications, promoting efficiency and reducing development time.\n* **Modularity:** Separating the shared components into a separate package allows for greater flexibility in updating and maintaining them without affecting the individual applications.\n* **Documentation and testing:** Facilitates centralized documentation and testing of the shared components, ensuring their reliability and maintainability.\n\n**Consequences:**\n\n* **Increased repository complexity:** The monorepo will contain multiple projects, potentially increasing the complexity of the repository structure.\n* **Dependency management:** Proper dependency management is crucial to ensure compatibility between the shared components and the individual applications.\n* **Potential performance issues:** Depending on the size and complexity of the monorepo, performance issues may arise, particularly during builds.\n* **Communication and collaboration:** Collaborative development efforts may require clear communication and coordination among teams working on different applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nComponents should be well documented so that the users of the library will be able to quickly start using the component.\n\n## Decision\n,"For now, we will create a README.md file inside each component folder, adjacent to the code for the component. The readme file should have the same structure and highlight some use cases, code examples and have api reference.\n","**Decision:**\n\nTo create comprehensive documentation for components that includes the following elements:\n\n* **Purpose and Overview:** A clear explanation of the component's purpose, functionality, and target audience.\n* **Installation Instructions:** Detailed steps on how to install and configure the component.\n* **Usage Examples:** Code snippets and walkthroughs demonstrating how to use the component in different scenarios.\n* **API Reference:** A technical description of the component's API, including method signatures, parameters, and return values.\n* **Best Practices and Guidelines:** Recommendations for using the component effectively and avoiding common pitfalls.\n* **Version History:** A changelog tracking major changes, updates, and bug fixes to the component."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a versioning scheme for our project.\n\n## Decision\n,- We will use [semver](https://semver.org/) which is the most prevalent versioning scheme for node.js libraries and is easy to use.\n- We will follow the rules of updating `MAJOR.MINOR.PATCH` version numbers and their semantic meaning.\n,"We will use **semantic versioning**. Semantic versioning is a popular versioning scheme that uses three numbers to denote the version of a project: the major version, the minor version, and the patch version. The major version is incremented when there is a backward-incompatible change to the project. The minor version is incremented when there is a new feature added to the project. The patch version is incremented when there is a bug fix or other minor change to the project.\n\nSemantic versioning is a good choice for our project because it is simple to understand and use, and it provides a clear way to track the changes that have been made to the project over time."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince we do not have the expertise and experience needed to build ui library projects and since we cannot fully anticipate the future needs of such library it's best to follow the path of other libraries and implement it in a similar way.\n\n## Decision\n,"- We will copy the (api) design of Angular Material - Angular Material is created and maintained by the Angular team and is a very popular library. It is well tested and very well designed and most importantly has been used by many other teams already. It makes sense to copy their api design and features.\n- We will try to stay as close as possible to the same design approach chosen by Angular Material for each component.\n- We will only copy features that we need, we will not implement features we do not need like the ripple effects or support for bidirectionality.\n- We will not copy paste the code as is without understanding it, we will first study it carefully and then borrow what we need from it.\n- We will use the CDK as much as possible.\n",Adopt an existing UI library that has been successfully used in similar projects.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have some issues with the versioning because sometimes the code updated regardless of the fix version in the once-ui library. So for this we used the beta versioning mechanism.\n\n## Decision\n,- In beta versioning mechanism we publish the prerelease version only for the team environment for testing purpose that would be from the once-ui QA branch and all the release version will be updated in the Master branch of once-ui library.\n,Use semantic versioning for the once-ui library to ensure that breaking changes are only introduced in major version releases. This will help to avoid issues with code being updated regardless of the fix version.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNeed to find a way for developers to import this library into their own projects.\n\n## Decision\n,We will use npm to publish the library privately in [MyGet](https://www.myget.org) (a 3rd party npm compatible registry)\n,Use the `go get` command to install the library into the user's `$GOPATH`.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn addition to writing tests for each component, we need to run these tests continuously.\n\n## Decision\n",We will use Jenkins to run all the tests in the project and also run linter and Prettier.\n,"We will use a continuous integration (CI) tool, such as Jenkins, to automatically run tests when code is committed to the repository."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince many developers are going to use this library for many different features, we need to have some rules of thumb for what goes into the component and what does not go.\n\n## Decision\n","- We will choose to keep the api surface as minimal as possible, adding only the most necessary features in.\n- We will prefer leaving some features to the user of the library.\n- If we are not sure about something, it's better to leave it out and wait before we make the decision to add it.\n","The library should contain components that meet the following criteria:\n\n- **Reusable:** The component should be able to be used in multiple different contexts and applications.\n- **Scalable:** The component should be able to handle a high volume of requests and data without compromising performance.\n- **Maintainable:** The component should be easy to understand, maintain, and update.\n- **Extensible:** The component should be designed in a way that makes it easy to add new features and functionality in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMultiple applications are going to use the ui components, each one with slightly different theme (color). We need the library to be flexible enough to support such themes.\n\n## Decision\n",- We'll create a theming system based exactly on how Angular Material does themes.\n- Users will be able to override theme for specific component instance if really needed.\n,"**Decision:** Implement the UI components library using a component styling system that supports theming.\n\n**Justification:**\n\nThis decision aligns with the requirement for flexibility in supporting different themes. A component styling system, such as CSS-in-JS or Sass modules, provides the following benefits:\n\n* **Themeability:** Allows the creation of different themes by modifying a small set of variables.\n* **Isolation:** Encapsulates styles within each component, ensuring that changes in one component do not affect others.\n* **Reusability:** Promotes code reuse and consistency by sharing common styles across multiple components.\n* **Maintainability:** Makes it easier to manage and update styles over time, as the styles are集中在一个地方.\n\n**Alternative Considered:**\n\n* **Inline Styling:** Using inline styles for each component would be less flexible and would not allow for easy switching between themes.\n* **Global Stylesheet:** A single global stylesheet would not provide the level of customization and isolation required for multiple applications with different themes.\n\n**Consequences:**\n\n* **Increased Development Complexity:** Implementing a component styling system may require additional effort and technical expertise.\n* **Improved Maintainability:** In the long run, the use of a component styling system will improve the maintainability and extensibility of the UI components library."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have to provide the accessibility feature in the application. Right now most of the places we are using the once-ui components so we won't be able to access the application through the keyboard. So for this we need to enhance the accessibility in the application.\n\n## Decision\n,- We are adding the cdk focus monitor in all the components to enhances the accessibility which it self managing all the focus states.\n,"**Decision:** Enhance accessibility in the application by:\n\n* **Utilizing ARIA (Accessible Rich Internet Applications) attributes:** Clearly define roles, states, and properties of UI elements to enable assistive technologies to provide appropriate guidance and support.\n* **Improving keyboard navigation:** Ensure all essential application functionality can be accessed using the keyboard, allowing users to tab through elements, activate buttons, and use keyboard shortcuts.\n* **Adhering to Web Content Accessibility Guidelines (WCAG) standards:** Implement accessibility best practices as defined by WCAG 2.1 or higher, focusing on areas such as color contrast, text readability, and cognitive support.\n* **Testing for accessibility:** Conduct regular automated and manual accessibility testing using assistive technologies and screen readers to identify and address any accessibility barriers.\n* **Considering user feedback:** Gather user feedback from people with disabilities to continuously improve the accessibility features and ensure they meet their needs."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** Implement an Architectural Decision Record (ADR) Framework.\n\n**Rationale:**\n\n* ADRs provide a structured way to document and track architectural decisions.\n* They enable transparency, communication, and accountability within the development team.\n* They facilitate knowledge transfer and decision making for future projects.\n* By establishing a framework, we can ensure consistency in ADR creation and maintenance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are a lot of tools available for documenting APIs. These tools generate easy to read documentation that can be used by frontend engineers and even third party developers. Setting up a documentation workflow will allow those engineers to integrate with the TTAHUB API faster and with more confidence. In addition, these API tools open a path for further testing and automation in the future, since the definition of the API is machine readable.\n\n## Decision\n",We will document our API with [OpenAPI specification 3](https://swagger.io/specification/) (formerly known as swagger).\n,**Decision:** Invest in an API documentation tool to improve developer experience and enable future testing and automation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are a few different options for application logging.\n1. Logging to file(s) used to be the norm. Logging to a file has several issues, however. You must be on the host to view the log file, or setup a system to ship the log file off the host. You must rotate log files or your host will eventually run out of disk space.\n2. Logging to stdout and letting a different system handle the logging is a more modern approach to logging. In our case cloud.gov takes care of gathering logs sent to stdout/stderr. Logs can be viewed with a cloud.gov cli tool.\n\n## Decision\n","We will log to stdout/stderr. On development machines logs will be presented as human readable strings. In deployed environments (dev, staging and prod) logs will be formatted in JSON.\n",Use logging to stdout/stderr
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to satisfy the [RA-5](https://nvd.nist.gov/800-53/Rev4/control/RA-5)\ncontrol around vulnerability scanning, we wish to scan all user-uploaded files\nwith a malware detection service. These files are user-supplied, and thus\ncannot fit into our other security controls built into the CI/CD pipeline.\n\n## Decision\n","We will run a ClamAV daemon, fronted by a REST API that we will pass all user-uploaded\nfiles through as part of the upload process. The ClamAV app is maintained in a\n[separate government-controlled repository](https://github.com/HHS/Head-Start-clamav-api-cg-app)\n",Implement a centralized malware scanning service that scans all user-uploaded files before they are stored or processed by the application. The service should be integrated with the application's file upload process and should scan files using a reputable malware detection engine. The results of the scan should be stored in a secure location and should be used to determine whether the file should be allowed to be stored or processed by the application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to capture information about user behaviors and task completion on the website. We should use previously approved systems for capturing this information. This excludes services that capture and track web analytics in external systems, e.g. Google Analytics.\n\n## Decision\n","We will use New Relic to capture and track web analytics. New Relic provides [browser monitoring](https://docs.newrelic.com/docs/browser/) that can capture essential metrics such as page views, and offers both [agent and SPA API](https://docs.newrelic.com/docs/browser/new-relic-browser/browser-agent-spa-api/) and [APM](https://developer.newrelic.com/collect-data/custom-attributes) methods for capturing custom data.\n",Use the xyz analytics service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBehavior-driven development (BDD) allows for a broader team to collaborate on software development. The business stakeholders have insight into how well the software meets the requirements. Through automated tests they have a way to validate the functionality in a user friendly way. Cucumber or in our case cucumber-js is a top tool to provide a friendly language for all team members. To implement cucumber-js, additional tools are needed to provide a way to run automated browser tests. Here, Puppeteer as well as selenium webdriver both of which are popular tools to enable browser automation were evalueated.\n\n## Decision\n","To go along with cucumber-js Puppeteer was selected. Puppeteer offers more control over Chromium based browsers, very fast, headless by default, run mode, and it can take screenshots of the pages, both in an image and PDF formats. Additionally, it measures rendering and load times by Chrome Performance Analysis tool and it is easier to set up than selenium webdriver. The drawback to using Puppeteer is it's lack of full cross-browser support, which is offered by the selenium webdriver, since Puppeteer is specialized for Chromium.\n","As a team, we have decided to use the Selenium Webdriver for browser automation in our Behavior-Driven Development (BDD) process. The Cucumber-js framework will be used for BDD, and Selenium WebDriver will be used to automate browser testing.  Selenium WebDriver was chosen over Puppeteer due to its extensive support for various browsers, higher stability and its wide adoption within the industry, making it an industry-standard tool for browser automation.."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\n\n## Decision\n","While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN/GIST indexing accelerating full-text searches.\n",Decision: To use PostgreSQL as the relational database management system for the tta smarthub platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to satisfy the [RA-5](https://nvd.nist.gov/800-53/Rev4/control/RA-5)\ncontrol around vulnerability scanning, we wish to scan all user-uploaded files\nwith a malware detection service. We want to satisfy the following requirements.\n1. Scanning can be done asyncronously so as not to negatively impact the user experience.\n2. Scanning should be loosely coupled to main application to allow for more resiliance and fault tolerance.\n3. Scanning should be retried if malware detection service is unavailable.\n4. Scanning should run on a seperate instance to prevent a negative impact to the user experience.\n\n## Decision\n","We will use redis as a queue and build a worker node which will take jobs from the queue, send them to the malware detection service and then update the database with the scan results.\n","Implement a separate microservice that handles file scanning asynchronously, using a queueing system to decouple the scanning process from the main application. The microservice should have a retry mechanism in place to handle temporary unavailability of the malware detection service. The microservice should run on a dedicated instance to prevent performance degradation of the main application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Office of Head Start is expanding data sharing across internal systems. To support this, TTA Hub would like\nto make documented APIs available to external systems in reliable, consistent, and secure ways.\n### Options Considered\nTo achieve this, two paths were considered.\n#### Expose existing internal APIs\nTTA Hub has a robust set of APIs already implemented to support the React frontend.\nPros:\n* already implemented\n* well documented\nCons:\n* API is highly coupled to TTA Hub UI needs, will become more difficult to change rapidly\n* authentication is based on sessions stored in browser cookies\n* authorization fully limited to existing TTA Hub roles\n#### Create a new API layer for external partners\nCreating a new API layer would de-couple the data needs from the TTA Hub UI needs.\nPros:\n* existing API can continue to be optimized for UI considerations\n* new API can be more rigidly backwards-compatible\n* authentication schemes can be easily customized for each use\n* authorization can be scope-based and integrated into the full HSES OAuth implementation\nCons:\n* two API endpoints will need to be maintained through data model changes\n\n## Decision\n","A new API layer has been chosen as the better set of trade offs.\nTo distinguish between the API flavors, the external API will be namespaced under the `/api/v1` route path.\n### Authentication\nThe initial API client also utilizes HSES as a single-sign-on solution, so API authentication will be done\nby passing an HSES token via the `Authentication` HTTP header. TTA Hub will then validate that token with HSES\nto verify the user.\n### JSON Format\nTo promote consistency, the external facing API will conform to [the JSON:API schema](https://jsonapi.org/)\nas much as possible. The first endpoints will include objects in `attributes` that may eventually migrate\nto `relationships`. This trade-off has been made due to the exponential growth of required API endpoints when each\nobject is represented in the `relationships` section.\n","**Decision:** Create a new API layer for external partners.\n\n**Rationale:**\n\nThe benefits of creating a new API layer for external partners outweigh the concerns. The new API:\n\n* Will be more flexible and customizable to meet the needs of external partners.\n* Will allow us to maintain the existing API for internal use without having to worry about changes breaking external systems.\n* Will provide better security and authorization controls.\n\nWe acknowledge the additional maintenance burden of having to maintain two API endpoints, but we believe the benefits of the new API layer outweigh this concern."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo integrate with the HSES oauth2 a package providing that functionality can be used to help the development. Two packages were considered for this purpose:\n1. passport - one of the more popular and extensive Javascript packages covering various authorization flows with multiple strategies. Passport provides a lot of functionality of out the box and was initially explored.\n2. client-oauth2 - a smaller package allowing a straight-forward implementation.\n\n## Decision\n,"Even though passport was originally considered, client-oauth2 will be used.\n",The decision was made to use the `client-oauth2` package.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\n\n## Decision\n,"TTA Smart Hub will utilize New Relic for Monitoring.\n* Integrates easily with front end and backend code\n* Track performance metrics and errors\n* FedRAMP approved\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\n","**Decision:** Implement a monitoring solution using Prometheus and Grafana.\n\n**Justification:**\n\n* Prometheus is a widely-adopted open-source monitoring and alerting system known for its high scalability and flexibility.\n* Grafana is a popular data visualization and dashboarding tool that complements Prometheus, allowing for real-time monitoring and historical analysis.\n* The Prometheus/Grafana combination provides a comprehensive monitoring solution that meets the requirements of TTA Smart Hub in terms of:\n    * Uptime monitoring: Prometheus can collect metrics from various sources (e.g., servers, databases, applications) and track their availability and performance over time.\n    * Error resolution: Grafana dashboards can visualize key metrics and alert on potential issues, enabling quick identification and resolution of errors.\n    * Scalability: Prometheus and Grafana are designed to handle large-scale environments and can be easily scaled to meet the growing monitoring needs of TTA Smart Hub.\n    * Open source: Both Prometheus and Grafana are open-source tools, eliminating licensing costs and providing flexibility in customization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCertain things we need to document, such as our data model and boundary diagrams are\nbetter represented as diagrams than text. Choosing a single service will make it easier to\nlearn the system and keep diagrams up to date.\n\n## Decision\n","We will use https://www.plantuml.com to generate diagrams with the following checks:\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\n1. We will store each type of diagram in its own markdown file in the /docs folder\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\nwe must keep the UML source in our source control.\n",Use PlantUML for diagrams.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\n\n## Decision\n",We will use environment variables to configure applications.\n,"Use environment variables to store configuration. Environment variables are a common way to store configuration because they are:\n\n- **Easy to use**. Environment variables can be set and retrieved using simple shell commands.\n- **Portable**. Environment variables are available to all processes on a system, regardless of the language or framework that the process is using.\n- **Secure**. Environment variables are not stored in any files, so they are not accessible to other users on the system.\n\nTo use environment variables to store configuration, you can create a configuration file that contains the environment variables that you need. Then, you can source the configuration file in your application. For example, if you have a configuration file named `config.env`, you can source it in your application using the following command:\n\n```\nsource config.env\n```\n\nAfter you have sourced the configuration file, you can access the environment variables in your application using the `os.environ` object. For example, the following code gets the value of the `FOO` environment variable:\n\n```\nfoo = os.environ.get('FOO')\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n**Static Code Analysis**\nThe code we write should be routinely and automatically scanned for programming and stylistic errors to ensure our code is highly readable, maintainable, and free of bugs. This scan should be performed in addition to developer code review. It will be most performant when run frequently and with little developer involvement.\n**Static Security Audit of Package Dependencies**\nEnsuring our node modules do not have security vulnerabilities prior to deployment gives us the opportunity to ""find and fix known vulnerabilities in dependencies that could cause data loss, service outages, unauthorized access to sensitive information, or other issues"" [ref NPM documentation](https://docs.npmjs.com/auditing-package-dependencies-for-security-vulnerabilities). This audit is most helpful when performed at frequent intervals and with little developer involvement.\n**Dynamic Security Scan**\nA dynamic security scan is needed to identify application features that could leave website data vulnerable to loss, corruption, or unauthorized access by malicious actors. This scan is most valuable when performed at frequent intervals and with little developer involvement.\n\n## Decision\n","**Static Code Analysis with ESLint**\nWe will use ESLint for static code analysis. This tool is easy to integrate into most text editors and implements some fixes automatically, which means errors are fixed before code is committed to our version control system. The tool can also run as part of the Continuous Integration (CI) pipeline. The pipeline job will fail when ESLint identifies errors. Code can only be deployed if pipeline jobs pass. This analysis combined with the gate on failing jobs prevents bad code from being deployed to users.\n**Static Security Audit of Package Dependencies with Yarn Audit**\nWe will use Yarn's `audit` command for static security audits. This tool checks for known security vulnerabilities in node modules and is frequently updated to detect new vulnerabilities. As we are already using Yarn for node module package management, using Yarn `audit` lets us avoid additional project complexity. Our CI pipeline will be configured to run this scan automatically each time code is committed to our version control system. The pipeline job will fail when vulnerabilities of a `MODERATE` or higher severity level are found, preventing node modules with known security vulnerabilities from being used in a deployed application.\n**Dynamic Security Scan with Open Web Application Security Project Zed Attack Proxy (OWASP ZAP)**\nWe will use OWASP ZAP's baseline scan to test application security. The tool is free, open-source, and updated and released weekly. Our CI pipeline will be configured to run this scan automatically each time code is committed to our version control system. The pipeline job will fail when the scanner finds vulnerabilities marked as `FAIL` in the ZAP configuration file. This pipeline failure prevents vulnerable code from being deployed.\n",Integrate static code analysis and security audit tools for continuous scanning of code as part of your CI/CD pipeline.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are two purposed ways to setup the frontend and backend of the TTADP system.\n1. The backend hosts the static frontend directly; there is one app. The single app will require one Authority to Operate (ATO). Some tooling can easily be shared between the frontend and backend. Also synchronized deploys will be easier to accomplish with a single app.\n2. The backend and frontend are two separate apps, the backend a node express app and the frontend static HTML, javascript and CSS. Two apps allow for greater separation between the two components. The ATO will be more complicated to document. Tooling will be harder to share when the apps are split into two.\n\n## Decision\n",We will have a single unified app that combines the frontend and backend.\n,"**Decision:** Option 2: The backend and frontend are two separate apps\n\n**Rationale:**\n\n* Greater separation between frontend and backend components, improving maintainability and scalability.\n* Reduced complexity in Authorizations to Operate (ATO) documentation by separating frontend and backend.\n* Easier leveraging of specialized tooling for each component when the apps are split into two.\n\nWhile Option 1 offers the advantages of reduced development effort and easier synchronized deployments, the benefits of separation between frontend and backend, simplified ATO, and enhanced tooling outweigh these considerations."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** Use the Architectural Decision Record (ADR) template to document architectural decisions.\n\n**Rationale:** The ADR template provides a structured and consistent way to capture the context, discussion, decision, and consequences of architectural decisions. It helps ensure that decisions are well-documented and easily accessible to stakeholders.\n\n**Consequences:**\n\n* **Benefits:**\n    * Improved decision-making transparency\n    * Reduced risk of miscommunication\n    * Facilitated knowledge transfer\n* **Drawbacks:**\n    * Additional documentation overhead\n    * Potential for overlooking important details"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe frontend side of the Digital Citizenship project consists of a mobile\napplication named [italia-app](https://github.com/teamdigitale/italia-app).\nThe `italia-app` application relies on a backend application ([italia-backend](https://github.com/teamdigitale/italia-backend))\nfor intermediating the interaction with external services (e.g. the Digital\nCitizenship APIs) and for coordinating the SPID authentication process.\n\n## Decision\n,"The backend application (`app_backend`) is deployed as a Docker container\ninside a Kubernetes cluster, provisioned by Terraform.\n![img](0013-1.svg)\n[source of diagram](https://www.planttext.com/?text=XP7FIiGm4CRlynJp0cK5zo2oo887tSkoO0-2I4ZJsJPicyH9ilW7tzsaQbjwKCWXpFpovPlvjZv83h7l08oj2NMGdPP2EWTmPDaJolV899WQRGr-_60kLfrMGe_KALR4XW9veRhe0_78QjCmUIyyyLah6dMT4vLK9pArSBOUyLaTuFFtm6GCDqLHt4mMy1glBbRtPNdu6rglBmVg0M0gRpArSFWjMDuU_kUoPWPXsHhusIWpasdCGCYwQGFQ8pujdZu3xkzs-qTaEXFxj6kshs-0QQMzDZ9j68SfK5bZI8KK2o3Rc1jCBv5ym4fwgZ7brLeVLaw65hA7_370DbrJu5y0)\nRequests coming from the app to the backend get routed through a few components:\n1. An Azure public IP with a firewall configured to listen on ports 80 and 443\n1. A [K8S Service](https://kubernetes.io/docs/concepts/services-networking/service/) that routes the ports 80 and 443 to the Ingress\n1. A [K8S Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) that terminates the HTTPS connection and routes the request based on the HTTP `Host` and path\n1. The `italia-backend` app.\nFor details about the Service and the Ingress configuration, see\n[ingress.yml](https://github.com/teamdigitale/digital-citizenship/blob/master/infrastructure/kubernetes/ingress.yml).\n",The frontend side of the Digital Citizenship project will use the `italia-backend` application as a backend for intermediating the interaction with external services and for coordinating the SPID authentication process.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Requirements\nAs building blocks for the services we're about to develop, we're going to need the following PaaS components:\n*   A structured data store with automated distribution of data across multiple shards and regions and multi-master capabilities.\n*   A message bus that supports partitioning, transactions and non destructive reads (peek).\n*   A framework for building _serverless_ applications and APIs.\n*   A service for managing push notifications via mobile (both iOS and Android), and browsers.\n*   A service for delivering emails and text messages.\n*   Facilities for application log and telemetry aggregation, analysis and monitoring.\nAll the above components should be fully managed and should\nprovide elastic and horizontal scalability, latency guarantees,\n99.99% or greater availability, and a competitive TCO.\nMoreover, the cloud provider should guarantee that the data\nstored, transferred to and from the cloud services will never\nleave the borders of the European Union.\n### Market analysis\nHere's a comparison of the players in the public cloud market against the above criteria:\n| | Amazon Web Services | Google Cloud Platform | Microsoft Azure |\n|------------------------|-----|-----|-----|\n| EU data guarantees     | Yes | No  | Yes |\n| Geo replicated db      | No  | [Spanner](https://cloud.google.com/spanner/) | [CosmosDB](https://docs.microsoft.com/en-us/azure/cosmos-db/)  |\n| Messages / events      | [Kinesis](https://aws.amazon.com/kinesis/) / [SQS](https://aws.amazon.com/sqs/) | [Pub/Sub](https://cloud.google.com/pubsub/) | [Queue Storage](https://azure.microsoft.com/en-us/services/storage/queues/) / [Service Bus](https://azure.microsoft.com/en-us/services/service-bus/) / [Event Hubs](https://azure.microsoft.com/en-us/services/event-hubs/) |\n| Serverless framework   | [Lambda](https://aws.amazon.com/lambda/) | [Functions](https://cloud.google.com/functions/) | [Functions](https://azure.microsoft.com/en-us/services/functions/) |\n| Push notifications     | [SNS](https://aws.amazon.com/sns/) | [Firebase Cloud Messaging](https://firebase.google.com/docs/cloud-messaging/) | [Notification Hubs](https://azure.microsoft.com/it-it/services/notification-hubs/) |\n| Email and SMS delivery | [SNS](https://aws.amazon.com/sns/) | Sendgrid / Twilio | Sendgrid / Twilio |\n| Logging and telemetry  | [CloudWatch](https://aws.amazon.com/cloudwatch/) | [StackDriver](https://cloud.google.com/monitoring/) | [Application Insights](https://azure.microsoft.com/en-us/services/application-insights/) / [Log Analytics](https://azure.microsoft.com/en-us/services/log-analytics/) |\nAll the major players stack equally in terms of the richness and\ncost effectiveness of the cloud offering and in terms of\nelasticity, scalability and resiliency of the cloud platform.\nAlmost all the required PaaS services get offered by the major\nplayers. A notably exception is the geographically distributed\ndatabase with multi-master capabilities, missing in the AWS\noffering and recently launched by Google and Microsoft.\nFinally, all major players guarantee the storage of user data\nin EU regions but only Google cannot guarantee that the data\nnever crosses the EU border while getting transferred\nthrough its global private network.\n\n## Decision\n",We decide to select Microsoft Azure as our primary cloud provider.\n,"**Decision:** Use Google Cloud Platform as the primary cloud provider for this project.\n\n**Rationale:**\n\n* **EU data guarantees:** Google guarantees that user data will never leave the EU border, meeting the security and compliance requirements.\n* **Geographically distributed database:** Google's Spanner database provides the required multi-master capabilities and geographical distribution.\n* **Strong PaaS offering:** Google offers all the required PaaS components, including a message bus, serverless framework, push notification service, and logging and telemetry aggregation.\n* **Elasticity, scalability, and resiliency:** Google Cloud Platform meets the requirements for elasticity, scalability, and resiliency.\n* **Cost effectiveness:** Google Cloud Platform offers a competitive TCO.\n\n**Alternatives Considered:**\n\n* **Amazon Web Services:** Does not offer a geo-replicated database.\n* **Microsoft Azure:** Offers a geo-replicated database but may not guarantee data privacy within the EU."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[CosmosDB](https://docs.microsoft.com/en-us/azure/cosmos-db/) provide two document-oriented APIs:\n* [DocumentDB API](http://azure.github.io/azure-documentdb-node/DocumentClient.html)\n* [MongoDB API](http://mongodb.github.io/node-mongodb-native/2.0/)\nFor the purpose of the Digital Citizenship project we will need the following capabilities:\n* Single document CRUD operations\n* Query capabilities on secondary indexes (with filtering, result limiting and ordering)\n* Conflict detection during concurrency writes\nWe also want to minimize the coupling with proprietary technologies and protocols.\nComparison of the CosmosDB APIs:\n|                    | DocumentDB API | MongoDB API |\n| ------------------ | -------------- | ----------- |\n| CRUD operations    | Yes            | Yes         |\n| Query capabilities | Yes            | Yes         |\n| Conflict detection | Yes            | Yes (1)     |\n| Lock-in            | High           | Low         |\n(1) Conflict detection is not supported by the MongoDB API but can be implemented at the application level via [MVCC](https://en.wikipedia.org/wiki/Multiversion_concurrency_control) with the addition of a `version` field to the documents.\n\n## Decision\n",We decide to implement all database operations via the MongoDB API.\n,"**Adopt the CosmosDB DocumentDB API.**\n\n**Justification:**\n\n* **Sufficient Capabilities:** The DocumentDB API provides all the required capabilities, including single document CRUD operations, query capabilities with filtering, result limiting, and ordering, and conflict detection during concurrency writes.\n* **Minimal Lock-in:** Unlike the MongoDB API, the DocumentDB API is less proprietary, reducing the risk of vendor lock-in.\n* **Ease of Integration:** The DocumentDB API is more tightly integrated with Azure services, making it easier to use other Azure components within the application."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to define the API specifications of the services we're going to implement.\n\n## Decision\n,We use the [OpenAPI 2.0](https://swagger.io/specification/) specification (aka Swagger spec) as standard for our REST API definitions.\n,Create an OpenAPI specification for each service.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR #9](0009-we-choose-a-cosmosdb-api.md) we decided to use the MongoDB API\nfor CosmosDB but as soon as we implemented the first version of the DAL logic\nwe realized that the MongoDB support in CosmosDB was flawed: a serious [bug](https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/19361521-fix-bug-that-destroys-nodejs-mongodb-connection-po)\nthat destroyed the connection pool every ~10s made the MongoDB API impossible\nto use in a production environment.\n\n## Decision\n,"Since at the time we were still in the early phase of development (around\nAugust 2017), we decided to implement the DAL on top of the more mature\nDocumentDB API (see [this commit](https://github.com/teamdigitale/digital-citizenship-functions/commit/c72b95ebb5ed038cdf62f43dc1adacbde9668d4e)).\nNote that the bug has been recently fixed (October 2017), so it may be worth to\nplan a migration of the DAL to the MongoDB API.\n",We decided to switch to the SQL API for CosmosDB.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like ""serverless"" computing.\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\nDecision criteria will also include:\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\n\n## Decision\n","* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\n","**Decision:**\n\nTo leverage the benefits of serverless computing by utilizing cloud-native services for the Digital Citizenship platform.\n\n**Key considerations:**\n\n* Serverless computing can provide elasticity and resilience, enabling the platform to handle varying workloads effectively.\n* Pay-as-you-go pricing model aligns with the platform's dynamic usage patterns, reducing infrastructure costs.\n* Cloud-native services offer economies of scale and access to the latest security advancements.\n* Utilizing existing cloud components reduces development time and maintenance overhead.\n* The platform will be API-driven, facilitating integration with external systems and supporting a flexible, distributed infrastructure."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are no formal standards for describing or managing most types of cloud resources. This is because the cloud hosting market moves so rapidly. Organisations are very quick to adopt new cloud products that offer significant opportunities, but this reduces the opportunities to work with standards.\nFor this reason, when choosing to develop a product on top of a non-standard cloud component there's a risk of lock-in.\n\n## Decision\n",We should take steps to avoid lock-in with cloud hosting providers.\n,"To minimise the risk of lock-in, adopt a modular architecture that clearly identifies the non-standard cloud dependencies. This will make it easier to migrate to a different cloud provider or to replace the non-standard cloud component with a standard one in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMost components of the Digital Citizenship platform are designed on the\nserverless runtime model, specifically they are developed for the Azure\nFunctions platform.\nSome components don't fit into this model, specifically the integration with\nthe SPID authentication service requires the deployment of an instance of the\n[Shibboleth](https://wiki.shibboleth.net/confluence/display/SHIB2) SAML\nService Provider that sits in front of the application that needs to be secured\nby SPID authentication.\nThis kind of deployment requires a VM/container IaaS service.\n\n## Decision\n","Microsoft Azure provides two application hosting services that suits our needs:\n| Service                                                                                      | Pros | Cons |\n| -------------------------------------------------------------------------------------------- | ---- | ---- |\n| [Web App for Containers](https://azure.microsoft.com/en-us/services/app-service/containers/) | Flexible and portable (it's basically a DCOS or Kubernetes platform) | High configuration/management effort |\n| [Web Apps](https://azure.microsoft.com/en-us/services/app-service/web/)                      | Low configuration/management effort | Deployment mechanism specific to the service, harder to port to other platforms |\nGiven that:\n1.  We already have a working Docker container that provides a SPID/Shibboleth\nsetup.\n2.  We want to minimize the coupling to non open-source technologies\nWe decide that we will use *Web App for Containers* to setup a *Kubernetes*\ncluster.\n",**Decision:** Deploy Shibboleth on an Azure VM.\n\n**Rationale:**\n\n* Shibboleth requires a VM/container IaaS service for deployment.\n* Azure VM provides a cost-effective and scalable solution for this requirement.\n* The Azure Functions model is not suitable for components that require a VM/container IaaS service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMost of the business logic for the backend services (HTTP APIs, async event pocessing) will be deployed on [Azure Functions](https://azure.microsoft.com/en-us/services/functions/) (Azure's serverless, reactive framework).\nAzure Functions [provides 1st class support](https://docs.microsoft.com/en-us/azure/azure-functions/functions-compare-logic-apps-ms-flow-webjobs) for the .NET/C#/F# and the Node/Javascript runtimes.\n|                  | .NET/C#/F# | Node/Javascript |\n| ---------------- | ---------- | --------------- |\n| Support on Azure | 1st        | 1st             |\n| Open Source      | No (1)     | Yes             |\n| Typesafe         | Yes        | Yes (2)         |\n| Performance      | High       | Medium          |\n| Traction         | Medium     | High            |\n| Tools            | Best       | Good            |\n| Ecosystem        | Good       | Best            |\n*   (1) Only the core .NET runtime is open source (MONO)\n*   (2) Type safety can be achieved with Typescript or Flow\n\n## Decision\n","We decide to implement the backend logic using [TypeScript](https://www.typescriptlang.org/), a typed superset of Javascript that compiles to Javascript.\nWe favor TypeScript due to its superior tooling and workflow, compared to other alternatives (see for instance a [comparison of TypeScript against Flow](https://github.com/niieani/typescript-vs-flowtype)).\n","**Decision:** Use the Node/Javascript runtime for Azure Functions.\n\n**Rationale:**\n\nThe following factors influenced the decision:\n\n* **Traction:** Node/Javascript has a higher traction than .NET/C#/F# in the Azure Functions ecosystem, which suggests a larger community and more resources available.\n* **Ecosystem:** Node/Javascript has a more comprehensive ecosystem than .NET/C#/F#, providing a wider range of libraries and tools.\n* **Performance:** While .NET/C#/F# has a performance advantage, the medium performance of Node/Javascript is still adequate for the expected workload.\n* **Open Source:** Node/Javascript is fully open source, making it more accessible and flexible for development.\n\nAdditionally, the Node/Javascript runtime provides type safety with Typescript or Flow, addressing the concern of type safety while maintaining the benefits of the Node/Javascript ecosystem."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\n\n## Decision\n","We decide to deploy our services on the [West Europe](https://azure.microsoft.com/en-us/regions/) region, located in the Netherlands.\n","All data will be stored, processed, and transferred within the European Union."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are going to associate data to citizens, thus we need to\ndefine a primary key for identifying citizens in the data model.\nItalian citizens have a ""natural"" identification key: the Fiscal Code (FC).\nFor natural persons, the [FC is made of 16 alphanumeric characters](https://en.wikipedia.org/wiki/Italian_fiscal_code_card#Fiscal_code_generation).\nThe FC is supposed to be unique and immutable but these properties are not always true: a person may have his FC revoked or changed (e.g. after a change of name or because his FC is not unique).\n\n## Decision\n","The citizen's FC will be used when defining a primary key for a citizen entity.\nIn general, no component handling citizen entities should assume that the primary key is actually a FC. This would prevent extending the system to include citizens with other kind of keys (e.g. foreign citizens).\n","Use the Fiscal Code (FC) as the primary key for identifying citizens in the data model.\n\n**Rationale:**\n\n* The FC is a widely used and accepted identification key for Italian citizens.\n* It is a unique identifier for most citizens.\n* It is immutable for the majority of citizens.\n* Even though the FC is not always unique or immutable, it is the best available identifier for Italian citizens.\n* The data model should be able to handle the cases where the FC is not unique or immutable."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo perform asynchronous operations we use [Azure Storage Queues](https://azure.microsoft.com/en-us/services/storage/queues/):\nenqueued messages are processed by the methods exposed by Azure Functions using [triggers](https://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings).\nIn case some error occurs in the message handler, it is necessary to re-schedule the message in the queue\nto be processed again at a later time.\nOur goal is to re-schedule the processing at increasing intervals, using an exponential back off strategy for retries.\nThe default behavior is specified in the [triggers documentation](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue#trigger---poison-messages):\n> When a queue trigger function fails, the Azure functions retry the function up to five times for a given queued message, including the first attempt.\n> If all five attempts fail, the runtime> functions adds a message to a queue called <originalqueuename> -poison.\nCurrently the Function SDK does not provide an *output binding* to delay the processing of an enqueued message.\nIt is therefore necessary to implement a custom retry policy to schedule messages over a longer period of time.\nThe following possibilities, to delay message processing, have been considered:\n1. Switching from Storage Queues to [ServiceBus](https://docs.microsoft.com/en-us/azure/service-bus/):\nthat allows to schedule messages at specified point in time bue [there are some known issues](https://github.com/Azure/Azure-Functions/issues/454#issuecomment-324785962) that suggest to avoid this approach. Moreover the ServiceBus is more expensive\nand has less storage capability than Storage Queues.\n1. Using [Durable Functions](https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview#language-support): this is [only supported\nin .NET environment](https://github.com/Azure/azure-functions-durable-extension/issues/13#issuecomment-359448481).\n1. Using a [Custom Queue Processor](https://stackoverflow.com/questions/26937782/queuetrigger-attribute-visibility-timeout): this is only supported in .NET environments and is not documented.\n1. Update the *visibilityTimeout* parameter in enqueued messages (https://github.com/Azure/azure-webjobs-sdk/issues/1040)\nAt the time of this writing Functions don't provide an [output binding to set the `visibilityTimeout`](https://github.com/Azure/azure-webjobs-sdk/issues/1040)\non one individual message, but only a global setting in the [host.json file](https://docs.microsoft.com/en-us/azure/azure-functions/functions-host-json).\n\n## Decision\n","To manage processing errors, we decided to update `visibilityTimeout` value using the methods provided by the\n[Azure Storage SDK](https://github.com/Azure/azure-storage-node) and re-enqueue the messages that caused the error.\nThe implemented strategy, that uses the [SDK method `updateMessage()`](http://azure.github.io/azure-storage-node/QueueService.html#updateMessage__anchor), lets us re-schedule messages with an exponential back off and a maximum time to live of 7 days.\n","**Use the `visibilityTimeout` parameter to delay message processing.**\n\nThis is a supported and documented approach that provides the necessary flexibility to control the delay interval for each message. By updating the `visibilityTimeout` property in the enqueued messages, we can specify the amount of time before the message becomes visible again for processing. This allows us to implement an exponential back-off strategy for retries, ensuring that messages are reprocessed at increasing intervals until they are successfully handled."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",We will use the following process to record architectural decisions for this project:\n\n1. Identify the decision that needs to be made.\n2. Gather input from the relevant stakeholders.\n3. Evaluate the options available.\n4. Make a decision and document the rationale behind it.\n5. Implement the decision.\n6. Monitor the decision and make adjustments as necessary.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\nThe project can either use `npm` or `yarn` as the package manager. This should be an either/or situation but a number of issues have been encountered when using `yarn` including:\n- `yarn login` still prompts for a password at release time which doesn't work with `np`\n- publishing with `yarn` leaves only the `index.js` file in the `lib` directory\n\n## Decision\n,"<!-- What is the change that we're proposing and/or doing? -->\nUse npm with the script pattern.\nAny common tasks (e.g. `lint`, `test`, `build`) should have a script defined in the `script` directory. Any scripts run by CI should also be stored in this directory.\n",The project will use `npm` as the package manager.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\nThe AWS CDK provides components with two levels of abstraction. CfnComponents map directly to cloudformation resources whilst constructs are a higher abstraction which may create multiple related resources. For example, the `AutoScalingGroup` construct will also create `LaunchConfig` and `SecurityGroup` resources. This is a useful abstraction as it allows you to define all the components required for a particular concept in one place.\nFurther to this, various patterns are available - both AWS supported and open source. These patterns define an entire stack, based around common template. For example, a `EC2App` pattern might provide an `AutoScalingGroup`, `LoadBalancer` and the required `Roles` and `SecurityGroups`. These patterns are likely composed of multiple constrcuts (rather than CfnComponents) under-the-hood.\nThis library aims to standardise and simplify the process of setting up Guardian stacks by providing reusable components but what level(s) of abstraction should be provided?\n\n## Decision\n","<!-- What is the change that we're proposing and/or doing? -->\nThis library should define a number of Guardian flavoured constructs which extend those provided by the AWS CDK library with Guardian defaults baked in. For example, a `GuAutoScalingGroup` and `GuApplicationLoadBalancer`.\nWhere those constructs are used in multiple ways, it should provide utlity classes for any common usages. For example, for the `Policy` constructs: `GuSSMPolicy`, `GuLogShippingPolicy` and `GuGetS3ObjectPolicy`\nBuilt on top of those, it should also provide a number of patterns to cover common Guardian stack architectures. For example, `GuEC2App` and `GuLambdaApp` patterns. These patterns should be the encouraged entry point to the library, with the constructs only used outside of standard cases.\n","This library should provide CfnComponents and Constructs. Providing both levels of abstraction will allow the library to accommodate a wider range of use cases. CfnComponents will be useful for users who need fine-grained control over their stack configuration, while Constructs will be useful for users who prefer a higher level of abstraction.\n\nIn addition, the library should also provide a set of patterns that are composed of Constructs. These patterns will provide users with a starting point for creating stacks that implement common architectural patterns."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\nOne of the benefits of writing our infrastructure in a fully fledged programming language is the ability to write reusable components which can be tested. This library defines those components, both in the form of constructs and patterns (see [001-constructs-and-patterns](./001-constructs-and-patterns.md) for more details).\nThere are two main strategies that can be used to unit test these components.\nSnapshot tests synthesise a given stack and compare the output to a previous version that has been verified by the user and checked into the repository. If there are any changes, the test fails and the user is displayed the diff to either fix or update the stored snapshot.\n[+] Quick and easy to write\n[+] Pick up any changes to the output cloudformation (particularly useful for unintended side effects)\n[-] A change to the component may cause many unrelated tests to also fail\n[-] When testing different permutations of a component, a number of snapshots will be created, each of which contains a certain amount of information which is irrelevant to the particular test. This adds some extra effort to understand what is relevant to the test which you're looking at\n[-] Snapshots are easy to update which, especially when multiple are affected by a change, makes it easy to accidentally update them incorrectly. Further to this, as the snapshot (which is essentially the assertion) is stored in a different file, it's not immediately obvious if the assertions are valid\nDirect Assertions use the different assertions provided by the test framework to test specific functionality. For example, asserting that a property exists or that an array contains a particular value.\n[+] Each test only contains the relevant assertions for the test, making it easier to understand the consequence of settings certain props\n[+] Changes will only fail tests that cover that particular area\n[-] More complex and time consuming to write\n\n## Decision\n",<!-- What is the change that we're proposing and/or doing? -->\nUse direct assertions for constructs and snapshots for patterns (and stacks)\n_This decision is a recommendation for the general approach. There may be some cases where using a different approach is more applicable for a given test._\n,"Use snapshot tests to provide a quick and easy way to test for unexpected or otherwise undesirable changes in the output CloudFormation, and use direct assertions to test for specific functionality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\n\n## Decision\n","<!-- What is the change that we're proposing and/or doing? -->\nConstructors should follow the following rules for consistency.\n1. The first parameter should be a `scope` of type `GuStack`:\n:white_check_mark: Valid\n```ts\nclass MyConstruct {\nconstructor(scope: GuStack) {\n...\n}\n}\n```\n:x: Invalid\n```ts\nclass MyConstruct {\nconstructor(scope: Stack) {\n...\n}\n}\n```\nThe construct/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\n2. They can also take a `props` object which should be correctly typed:\n:white_check_mark: Valid\n```ts\nclass MyConstruct {\nconstructor(scope: GuStack, props: MyConstructProps) {\n...\n}\n}\n```\n:x: Invalid\n```ts\nclass MyConstruct {\nconstructor(scope: Stack, props: object) {\n...\n}\n}\n```\nThe construct/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\n:white_check_mark: Valid\n```ts\ninterface MyConstructProps {...}\nclass MyConstruct {\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\n...\n}\n}\n```\n:x: Invalid\n```ts\nclass MyConstruct {\nconstructor(scope: GuStack, id: any, props: object) {\n...\n}\n}\n```\n4. Where all `props` are optional, the `props` object should be optional as a whole\n:white_check_mark: Valid\n```ts\ninterface MyConstructProps {\nprop1?: string;\nprop2?: string\n}\nclass MyConstruct {\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\n...\n}\n}\n```\n:x: Invalid\n```ts\ninterface MyConstructProps {\nprop1?: string;\nprop2?: string\n}\nclass MyConstruct {\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\n...\n}\n}\n```\n","**Decision:**\n\nCreate a unified interface for the different constructs and patterns, abstracting away the underlying implementation details and enabling more intuitive usage."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\nThis project defines a library of components build on top of the AWS CDK and aiming to improve the user experience for managing infrastructure at the Guardian. As the library continues to grow, it is important that the library is structured sensibly, both for developers maintaining the library and those using it.\n\n## Decision\n","<!-- What is the change that we're proposing and/or doing? -->\nThe top level directories with the `constructs` directory should mirror the AWS CDK library names.\nEach directory should contain an `index.ts` file which exports all of the classes within it.\nFiles within these directories can either be at the top level or nested within directories. Where nested directories exist, they should only be used for grouping multiple implementations of the same underlying construct. For example, `GuLogShippingPolicy`, `GuSSMRunCommandPolicy`, `GuGetS3ObjectPolicy` could all be in seperate files within the `constrcuts/iam/policies` directory. These directories should also export all memebers in an `index.ts` file.\nPatterns can all be defined at the top level within the `patterns` directory. They should all be exported in the `index.ts` file so that they can all be imported from `@guardian/cdk`\n",**Decision:** \\nThe library will be organised such that components are grouped into modules by their general purpose. Each module will contain one or more components sharing a clearly defined purpose.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\n### Options\n#### NHS Digital BloomReach\nWe looked at the implementation of document management functionality in BloomReach delivered as part of the work for NHS Digital - https://github.com/NHS-digital-website/hippo. This project provides a good view of what native BloomReach document management and publishing looks like, it delivers a flexible content model for publishing to HTML documents.\n#### Office 365\nIn addition we should look at the current usage of Office 365 to determine whether the platform would be suitable for integration with BloomReach.\n#### FutureNHS\nFinally looked at the FutureNHS collaboration platform to see whether it would be suitable for integration with BloomReach. Whilst the platform in its current state would be a good candidate for integration, we discovered that at the time of writing, this platform is due to be rewritten.\n\n## Decision\n","We believe that Microsoft Office 365 and Sharepoint provide a good basis for the platforms document management capabilities and we have proved that it can be successfully integrated.\nHaving looked at the requirements for LKS staff, particularly around the ability to support a range of document types such as spreadsheets and presentations, our belief is that we would be better placed to integrate an external document management system. The NHS Digital publishing platform provides an excellent HTML publishing model and workflow, however extending it to support a broader range of document types would be complex.\nFutureNHS may provide a good candidate for integration in the future, however at the time of writing it is difficult to recommend as the product is in the process of being rewritten.\n",Integrate with Office 365.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\n\n## Decision\n","Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\n","**Decision:** Use **Azure DevOps** as the platform for automating the build, test, and deployment of the HEE National Website Platform.\n\n**Rationale:**\n\n* **Mature platform:** Azure DevOps is a well-established and mature platform with a proven track record in automating software development processes.\n* **End-to-end automation:** It provides a comprehensive suite of tools that cover the entire software lifecycle, from requirements management to deployment.\n* **Integration with Azure services:** Azure DevOps seamlessly integrates with other Azure services, such as Azure Pipelines, Azure Artifacts, and Azure Boards, enabling efficient collaboration and data sharing.\n* **Scalability and flexibility:** Azure DevOps supports large-scale projects and can be customized to meet specific needs and workflows.\n* **Proven success:** Similar healthcare organizations have successfully used Azure DevOps for automating their website platform development processes.\n* **Cost-effective:** Azure DevOps offers various pricing options to suit different budget requirements.\n* **Technical expertise:** The HEE team has experience and expertise in using Azure DevOps, which will facilitate smooth implementation and adoption.\n* **Supports industry best practices:** Azure DevOps adheres to industry best practices for software development, including continuous integration and continuous deployment (CI/CD).\n* **Cloud-based platform:** Azure DevOps is a cloud-based platform, providing ease of access, scalability, and disaster recovery capabilities.\n* **Collaboration and communication:** Azure DevOps fosters collaboration between developers, testers, and stakeholders through its integrated communication and issue tracking tools."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe sought to identify a single templating language that would provide a source of truth across our frontend & content management platforms. This aim seeks to reduce the effort in developing on the content management platform by mitigating the burden of markup integration. We also aimed to find a templating language that allowed us to utilise the components already created as part of the NHSUK frontend framework.\n### Options\nWe identified three main candidates to use as our templating language:\n#### Freemarker:\nFreemarker - https://freemarker.apache.org/ is the native templating language used by BloomReach and although there are projects that provide the ability to parse Freemarker templates with Javascript - this would be a prerequisite to using a tool like StoryBook to manage the frontend components - choosing Freemarker and utlising the prexisting work of the NHSUK Frontend project would require implementing and maintaining Freemarker versions of those components.\n#### Nunjucks:\nNunjucks - https://mozilla.github.io/nunjucks/ is the native templating language used by the NHSUK Frontend project. It's syntax is similar to the Jinja templating language which is widely used in Python application development. Choosing Nunjucks would require implementing a custom integration with BloomReach using that platforms SPA/headless functionality. Integrating with StoryBook would require the creation of a custom StoryBook renderer.\n#### React:\nReact - https://reactjs.org/ is a fuller javascript framework for building UIs. BloomReach provides an out of the box, if perhaps somewhat nascent integration with React through its SPA/headless functionality. StoryBook provides full support for React and NHS Digital have developed and maintain an implementation of the NHSUK Frontend in React\n- https://github.com/NHSDigital/nhsuk-react-components\n\n## Decision\n",We will use React to develop the HEE Frontend platform.\n,"We decided to use Nunjucks as our templating language.\n\n**Reasoning:**\n\n* Nunjucks is the native templating language used by the NHSUK Frontend project. This will reduce the effort in developing on the content management platform as we can utilize the prexisting work of the NHSUK Frontend project.\n* Nunjucks has a syntax that is similar to the Jinja templating language which is widely used in Python application development. This will make it easier for developers to learn and use.\n* There is a custom integration available between Nunjucks and BloomReach using that platforms SPA/headless functionality. This will allow us to use Nunjucks with BloomReach.\n* Integrating with StoryBook will require the creation of a custom StoryBook renderer. However, this is a one-time effort and will be worth the investment in the long run."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving determined to use Gherkin and Selenide to manage the generation of our automated tests, we need to determine the platform on which these tests will run. The platform should be open source, if at all possible and portable between hosting platforms.\n\n## Decision\n",We have determined to use the testcontainers.org project to manage the infrastructure for our automated tests. This platform provides a way of generically describing the containers that we use to run our tests and can be executed on platforms including Github Actions.\n,"**Decision:** Use Docker as the platform for running our automated tests.\n\n**Rationale:**\n\n* **Open source:** Docker is an open-source platform, meeting the stated requirement.\n* **Portability:** Docker containers are portable across hosting platforms, allowing for easy deployment and testing in different environments.\n* **Ease of setup and management:** Docker provides a standardized and automated approach to setting up and managing test environments, reducing the overhead of maintaining the test infrastructure.\n* **Flexibility:** Docker allows for customization and extension of the test environment, enabling the team to tailor it to specific needs.\n* **Isolation:** Each test run is isolated in its own Docker container, ensuring the consistency and reliability of test results."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of efforts to create a separation between the development of our content management and content delivery tiers and in order to support the development of a frontend framework that could be used across multiple delivery platforms, we sought to identify a tool to support a component based development workflow. We reviewed the following options:\n### Options\n#### Styleguidist - https://github.com/styleguidist/react-styleguidist\n- **Pros:**\n- Provides great styleguide documentation\n- **Cons:**\n- Is restricted to React or Vue (for which there is an alternative project)\n#### StoryBook.js - https://storybook.js.org/\n- **Pros:**\n- Has support for a wide range of templating languages including HTML and Web Components as well as React and Vue\n- **Cons:**\n- Isn't really setup for serving associated documentation (ie less of a Design System tool)\n#### PatternLab - https://patternlab.io/\n- **Pros:**\n- Provides out of the box support for PHP templating such as Twig and Javascript templates such as Handlebars\n- **Cons:**\n- Has taken a very long time to move from version 2 to version 3 and still doesn't appear to be stable\n\n## Decision\n",We have chosen StoryBook.js as the component development tool to use for the HEE National Website platform. This choice was guided by the following:\n- StoryBook's current usage across other NHS Digital projects\n- StoryBook's support for a wide range of templating languages\n- StoryBook's ability to provide automated testing for concerns such as accessibility\n,"Based on the evaluation of the available options, we have decided to utilize StoryBook.js as the tool to support our component-based development workflow.\n\nStoryBook.js offers several advantages that make it the most suitable choice for our needs:\n\n* **Extensive Template Support:** StoryBook.js supports a wide range of templating languages, including HTML, Web Components, React, and Vue. This flexibility allows us to work with various technologies and integrate our components into different platforms.\n\n* **Comprehensive Features:** StoryBook.js provides robust features for developing, testing, and documenting our components. It enables us to create reusable components, explore different states and scenarios, and generate documentation automatically.\n\n* **Active Community Support:** StoryBook.js boasts a vibrant community and extensive documentation, making it easy for our team to adopt and leverage the tool effectively.\n\n* **Proven Stability:** StoryBook.js has a proven track record of stability and reliability, ensuring that our team can focus on building and maintaining our components without significant interruptions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe sought to determine whether the native out of the box solution provided by the platform nativly would be fit for purpose, or if an external service would be required.\nFor the purpose of the decision, the following were considered as requirements:\n* Flexible facet management\n* Synonym configuration\n* Query suggestions\n* Configurable misspelling tolerance\n* Natual language query support\n* Ability to integrate with external sources\n### Options\n#### Native Bloomreach Lucene Search\nThe OOTB search provided by the platform nativly is provided by Apache Lucene. This native search option provides standard search options, including free text search and faceted filters. Common query types are supported, such as wildcard searches.\nWhilst the native search functionality is fine for simple content websites, it lacks some of the enhanced functionality and configurability that the final solution will be asked to provide, including synonym configuration.\n#### Bloomreach Search & Merchandising (brSM)\nBloomreach offer an enhanced managed search service, called 'Search & Merchandising'\nhttps://www.bloomreach.com/en/products/search-merchandising\nThis tool provides several additional pieces of functionality including semantic understanding and personalisation. The product however is geared in the first instance to product and merchansing and many of the features are directed towards that usecase.\nNHS digital are undertaking some work to prove out the use of brSM for this scenario. This is one of the first POCs using this search tool outside of a comemerce functionality.\n#### Algolia\nAlgolia is a decided search-as-a-service product that provides much of the functionality needed out of the box, including synonym support and filters and facets. It is also highly customisable through the UI, allowing for non-developers to configure, adjust and maintain the search offering.\nAlgolia also offers prebuilt configurable front end components, which make implementing the search experience quick and easily.\n#### Azure Cognative Search\nMicrosoft offer a cloud search service called Azure Cognative Search. This is a scalable search-as-a-service product, with a focus on machine learning powered capabilities such as optical character regognition\n#### Amazon Kendra/ Elasticsearch\nAmazon has long provided a well regarded open source search solution called Elasticsearch, which can be run on premises or on an EC2 or managed search instance. Amazon also offered Kendra, which is a machine learning based search service.\n\n## Decision\n","We believe that an external search service will be required to provide all of the capabailities that will ultimatly be needed to meet the complex user experience needs. Further to that, using an external search service will better suit the service based architectural model, where the search service will likely need to ultimatly take data from a variety of services such as the LKS document and colberation platform, and in future potentially other services such as Oriel and TIS.\nUsing a managed service such as Algolia provides a good balance between powerful and user friendly functionality and implementation complexity - Algolia was chosen as the basis for the POC in part owing to its comprehensive service offering combined with its prebuilt react component library offering fast and efficient implementation.\nBloomreach Search and Merchandising is an interesting option that provides advantages being tightly integrated into the core CMS project, however using it outside of commerce is somewhat unproven in the market.\nSearch as a service options such as Elastic or Azure Cognative search would also be viable candidates (assuming the organisational goal of aligning more functionality to MS's offerings, Azure would likely be recommended ahead of Elasticsearch), and the cost models of these offer likely cost savings, the trade off is more complexity to implement and maintain.\n","**Decision: Integrate Algolia**\n\n**Rationale:**\n\nAlgolia meets all the requirements specified for the search solution:\n\n* **Flexible facet management:** Supports both dynamic and static facets, allowing for easy configuration and filtering.\n* **Synonym configuration:** Provides robust synonym management capabilities, ensuring improved search accuracy.\n* **Query suggestions:** Offers autocomplete and fuzzy search suggestions, enhancing user experience.\n* **Configurable misspelling tolerance:** Allows adjustment of misspelling detection sensitivity, optimizing search relevancy.\n* **Natural language query support:** Uses semantic analysis to improve search understanding and retrieve relevant results.\n* **Ability to integrate with external sources:** Supports integration with other data sources, such as a CMS, to enrich search results.\n\nCompared to the other options, Algolia stands out due to its:\n\n* **Out-of-the-box functionality:** Provides a comprehensive set of features that meet our requirements without the need for extensive customization.\n* **Ease of configuration:** Has a user-friendly UI that enables non-developers to configure and maintain the search system.\n* **Scalability:** Delivers a cloud-based service that can handle high traffic and data volume.\n* **Prebuilt front-end components:** Offers ready-made search widgets, simplifying implementation and improving UX.\n\nTherefore, Algolia is the most suitable solution for our search requirements, offering a comprehensive and flexible search experience that meets the needs of our end users."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe should determine how to develop and document our accptance tests. This decision describes the choice around the langauge (DSL) that we use to describe them.\n\n## Decision\n,We have determined to describe our acceptance tests using Gherkin.\n,"We will use Cucumber as the DSL for our acceptance tests. Cucumber is a popular and well-supported DSL that is easy to use and understand. It is also well-suited for testing web applications, which is our primary focus."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\n\n## Decision\n,We have chosen to store our packages on NPM.\n,"**Use NPM as the package repository for front end code.**\n\n**Reasoning:**\n\n* NPM is a widely adopted package manager for JavaScript applications.\n* It is used by many NHS UK and NHS Digital projects, ensuring consistency across projects.\n* It provides a centralized repository for managing and distributing packages.\n* It simplifies dependency management and allows for easy updates to packages.\n* It supports various security features to ensure the integrity and authenticity of packages."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to determine where to store and manage the source code for the HEE National Website Platform. HEE currently have a number of projects managed on GitHub so already have a presence on the platform.\n\n## Decision\n,We will store the platform source code on Github\n,Store the source code for the HEE National Website Platform on GitHub.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving determined to use Gherkin as the way that we describe our acceptance tests, we need to choose a Java based (because BloomReach is Java based) framework to write our acceptance tests in.\n\n## Decision\n",We have determined to use Selenide as the framework to implement our automated tests.\n,Use TestNG as the Java-based framework for writing acceptance tests.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\nAs part of this, considerations were made around:\n* Ease of use for creating new sites\n* Ability to share components\n* Ability to segregate content for specific channels (sites)\n* Ability to share content up and down the stack where needed and appropriate\n* Permissions model required to support this model\n\n## Decision\n","Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\n","Integrate an external document management platform for the following reasons:\n\n* **Increased functionality:** An external document management platform will provide more robust functionality for managing and organizing documents, including features such as version control, document collaboration, and document security.\n* **Reduced development time:** Integrating an external document management platform will reduce the amount of time and effort required to develop the document management capabilities for the platform.\n* **Lower cost:** Integrating an external document management platform will be less expensive than developing the functionality in-house.\n\nBloomreach's 'Channel' concept will be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights. The 'Channel' concept will allow us to:\n\n* Create new sites easily\n* Share components between sites\n* Segregate content for specific channels (sites)\n* Share content up and down the stack where needed and appropriate\n* Implement the required permissions model to support this model"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement Architectural Decision Records (ADRs) using a well-defined template.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to describe an approach to authentication that supports the use of multiple authentication providers. Azure AD provides the ability to configure direct federation with a range of authentication providers and includes the ability to provide managed authorisation for users without the ability to authenticate with Azure AD\n- https://docs.microsoft.com/en-us/azure/active-directory/b2b/delegate-invitations\n\n## Decision\n,We will choose an authentication provider/platform that supports the federation of authentication.\n,"**Decision:**\n\nImplement authentication using Azure Active Directory (Azure AD) with support for multiple authentication providers via direct federation.\n\n**Rationale:**\n\nAzure AD provides a comprehensive solution for authentication that supports:\n\n* **Direct federation:** Allows users to authenticate using various external identity providers (e.g., Google, Facebook, Okta).\n* **Managed authorization:** Enables organizations to grant access to users who cannot authenticate with Azure AD, such as contractors or partners.\n\nBy leveraging Azure AD's capabilities, the application can provide a seamless authentication experience for users while ensuring security and flexibility."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe've adopted a microservices architecture for A2 in order to attain certain technical characteristics (scalability, performance, evolvability) which translate into competitive advantage for Chef (time-to-market, business agility). Small, single-purpose services which can be developed and deployed independently are the cornerstone of this architectural style; in order to reap its benefits, our services must collaborate without introducing counterproductive dependencies.\nEvent-driven computing (a.k.a. event-driven architecture, or EDA) allows services to communicate while remaining loosely coupled. In an event-driven system, event producers generate events in response to significant changes in state. Event consumers listen for these events and react by doing some additional processing. A dedicated intermediate component receives the events generated by producers and publishes them asynchronously to consumers which have registered interest. Producers are unaware of the consumers which react to their changes in state, and consumers just listen for events without knowing the details of where and how the events are produced. The only dependency introduced is that of producers and consumers on the event service which ingests and publishes events. Any service can be a producer, a consumer, or both.\n![eda diagram](../diagrams/eda.png)\nUse cases for an event-driven model include:\n- situations in which multiple components must be notified of a change in system state\n- scenarios which demand a near real-time response to changes in system state\n- cases where distributed transactions must be coordinated as a single logical unit of work, and two-phase commit is not feasible due to scalability and performance concerns\n- complex event processing scenarios in which a consumer analyzes a series of events and reacts when conditions in aggregate meet prescribed thresholds\n- the ability to recreate system state on demand by replaying actions that have occurred in the system (event sourcing)\nBenefits\n- High scalability, performance\n- Facilitates parallelized operations\n- Loose coupling between services\n- No point-to-point integration; easy to add/remove/modify consumers\n- Near real-time response to system events\n- Lays the groundwork for event sourcing, should that become a direction we want to take\nChallenges\n- Asynchronous processing makes the system harder to test and reason about\n- Guaranteed delivery requires additional infrastructure (pub/sub messaging, event streaming, etc.)\n- Processing events in order or exactly once requires extra thought and effort\n- Dealing with failure scenarios is more complex (retries, compensating events, etc.)\nA2 Events: Current Flow Example\n![diagram: current events flow](../diagrams/events-current.png)\nCurrently, A2's compliance service fires events when a profile is created or deleted and when a scan job is created, updated, or deleted. The profiles component calls the service's internal events component via its gRPC interface to publish an event. The events component receives events on a channel and processes them one by one. During processing, the event component looks up the correct handler for the event (FeedServiceClient in the example above) and uses a new goroutine to make a gRPC call to HandleEvent on the feed component. The feed component then creates a new feed entry document in Elasticsearch. Feed entries are visible in the GUI'S event feed timeline.\nEvents are not persisted, and there are no retries or other compensating strategies in the event of a failure.\n\n## Decision\n","Enable event-driven capabilities for all of A2 by exposing compliance service's events component as an independent service. The new event service will abstract the mechanics of event publishing from the rest of the system, acting as a generic interface over an event publishing and distribution mechanism which can evolve or be replaced over time.\nA2 Events: Proposed Flow Example\n![diagram: proposed events flow](../diagrams/events-proposed.png)\nThe initial version of the new service will not persist events or perform retries. We will iterate on the event service as development use cases require new capabilities (message durability, guaranteed delivery, exactly-once and in-order processing, failure handling, etc.). We will consider third party solutions and frameworks (e.g., pub/sub messaging, event streaming) to underpin the event service abstraction as advanced features become necessary.\n","Adopt an event-driven architecture for A2 to facilitate loose coupling between services, high scalability, performance, and near real-time response to system events."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Background on Compliance Profile Storage and API\nCompliance profiles are stored and accessed using a base name that serves as a namespace and a profile name, and, optionally, a version. Here's an example of how a profile is specified using the [audit coobkook](https://github.com/chef-cookbooks/audit):\n```ruby\ndefault['audit']['profiles'].push(\n# Profile from Chef Compliance\n{\n'name': 'linux',\n'compliance': 'base/linux'\n},\n# Profile from Chef Compliance at a particular version\n{\n'name': 'linux-baseline',\n'compliance': 'user/linux-baseline',\n'version': '2.1.0'\n})\n```\nIn the example above, two profiles are specified. The first is named `linux` and is stored in the `base` namespace. The second is named `linux-baseline` and stored in a namespace called `user`.\nIn A2, the API supports creating, reading, and deleting profiles (profiles are immutable, so there is no update).\nHere's the read API from the gateway:\n```proto\nrpc Read (ProfileDetails) returns (Profile) {\noption (google.api.http) = {\nget: ""/compliance/profiles/read/{owner}/{name}/version/{version}""\n};\noption (chef.automate.api.policy) = {\nresource: ""compliance:profiles:storage:{owner}""\naction: ""read""\n};\noption (chef.automate.api.iam.policy) = {\nresource: ""compliance:profiles:{owner}""\naction: ""compliance:profiles:get""\n};\n};\n```\nThe delete REST API follows the same URL path.\n### Current Usage and Authorization for Compliance Profiles\nIn A2 today, each user has an implicit compliance profile namespace mapped to their A2 username. Users can create and delete profiles within their own username-based namespace. Default authorization policy allows users to read any profile. The username being used in profile URLs does not include the authentication connector (e.g. local, LDAP, SAML). As a result, the username-based namespace is not unique across auth connectors and the system is unable to distinguish between a local user ""olivia"" and an LDAP user ""olivia"".\nWhen we tested an improvement to the username-based namespace approach with SAML users, we identified a bug in the system in which the auth system and UI have a disagreement about which field to use as ""username"". For SAML users, this prevented the username-based namespace for compliance profiles to function properly.\n\n## Decision\n","We will treat the base name of compliance profiles as a generic namespace.\nA2 admins will create compliance profile namespaces and corresponding IAM policies that will determine which users are allowed to create, read, and delete profiles in a given namespace. In future, we can extend the system to support user creation and implicit ownership of profile namespaces, but this will not be supported initially.\nWe will represent compliance profile namespaces in the system (not just implicitly via IAM policies). This will allow listing profile namespaces and copying a profile from one namespace to another (note that this is a reference copy as internally profiles are stored by content SHA256).\nWe will address migration of current deployments through default policy behavior, documentation, and suggested workflow for A2 admins. The outline for migration is:\n* On upgrade, all users can READ profiles in any namespace.\n* On upgrade, only admin users can CREATE and DELETE profiles (in any namespace). All non-admin users have read-only access -- even for profiles that they previously created.\n* Documentation will walk admin users through the process of creating compliance profile namespaces along with IAM policies to control access to those namespaces.\n* Documentation will walk admin users through the process of consolidating existing username-based namespaces via copy operation.\n* Documentation will walk admin users through process of using existing username-based namespaces and restoring access by adding IAM policies (not recommended approach). This is only relevant for customers who maintain a strong desire to maintain the existing behavior and layout for profile management.\n",Move compliance profiles from a username-based namespace to a new compliance organization namespace.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPreviously, we did not have any rules for governing Automate UI route\nlifecycle. In the past, we likely broke backwards compatibility when\nre-naming or moving routes.\nAdditionally, the top nav highlighting in Automate UI was driven off the\nroot route (e.g. everything under the `Settings` tab must live under the `/settings`\nroute or it will not be highlighted properly for pages that live under it).\nThis means that if you want to move a page to a new tab, it will require a new route.\n\n## Decision\n","Every route that originally linked to a page that is still in use in the webapp\nmust still work and backwards compatibility must be maintained. Only when the webpage\nthat the original route linked to is being removed from the app can a route be removed.\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\ncustomers use internally will continue to work until they no longer make sense at all\nand the original resource no longer exists.\nTo maintain backwards compatibility, the original route must redirect to the new route\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\n[this commit](https://github.com/chef/automate/commit/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\ntests should look like, and how a page can move between topnav tabs.\n","**Decision:** To establish a set of rules for governing the Automate UI route lifecycle.\n\n**Consequences:**\n\n* **Positive:**\n    * Improved backwards compatibility by preventing route changes that break existing functionality.\n    * Enhanced consistency and usability by ensuring that top nav highlighting correctly reflects the page location.\n    * Reduced development overhead by providing clear guidance on how to manage routes.\n* **Negative:**\n    * Potential for increased complexity in the route structure.\n    * Additional effort required to maintain the rules over time.\n\n**Alternatives Considered:**\n\n* **Do nothing:** Continue without any formal rules for managing routes, potentially leading to future compatibility issues.\n* **Establish a naming convention:** Define a specific naming scheme for routes to prevent conflicts and ensure consistency.\n* **Use a route management system:** Implement a centralized system to manage and track all routes, providing a more structured approach.\n\n**Chosen Alternative:** Establish a set of rules for governing the Automate UI route lifecycle. This decision balances the need for backwards compatibility and usability with the potential complexity of managing the route structure."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to ensure that the Data resilience (this is what we are calling the guarantee that data once sent to the ingest pipeline will be ingested into A2) in our system is well understood. We also want to make sure that this is captured here for future reference.\n\n## Decision\n,"1) Data resilience / durability - our current approach on leaving retries to the edges (Chef Infra Client, Chef Infra Server, Chef InSpec) is still valid and we will not change that at this time. However, there is no retry logic built into Chef Infra Server or Chef InSpec. We will add that to those products' backlog and triage according to those teams' priorities.\n2) For Notifications right now, this is built in a way where notifications might get triggered but the data has not yet been fully ingested. We will change that behavior so that notifications only get triggered upon successful ingestion of data\n3) There is an event service in compliance right now that we all agreed can be extracted and enhanced to use across the system. However, the right time to look at this is when there is a new use-case for it. While the approach is agreed upon, we will await a use-case before proceeding\n","We will introduce a new service that ensures that all data sent to the ingest pipeline will be ingested into A2. This service will be responsible for monitoring the ingest pipeline and taking action if any data is lost. We will use a combination of techniques to ensure data resilience, including data replication, data validation, and data recovery."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we continue to improve and evolve Chef’s product architecture, we are in a position where there is a need to evolve the process itself of how architectural decisions get made and to expand the participants and seek input from a wider team.\nThe important goals we are trying to accomplish via this process are:\n1. More transparency and participation from everyone - including:\n1. Proposing new architectural changes / patterns\n2. An opportunity for anyone interested to chime in with their thoughts\n3. Having this decision be known to all parts of the organization for future reference\n2. A more streamlined decision making process and tracking\n3. Clarity on who makes decisions\nBelow is the process we are going to follow going forward.\n\n## Decision\n","The following is the process:\n1. <Optional step> If you have a topic that should be considered for an architecture discussion, write a google document describing the need for the decision, the options evaluated, the recommended decision, etc\n* The purpose of this document is to give enough context and information to proceed with the discussion.\n* Note that there is no prescribed format for this document at this time. Just make sure that all relevant information that you can think of is captured\n* While we want to get as much information written down as possible, please don't get blocked by trying to document everything you can think of. This is really meant to get the conversation started\n2. Once some initial discussions have happened and you are ready to bring this to the architecture team, follow this [ADR process][1] and create a new ADR with status Proposed\n3. Create a PR for the topic in the A2 repository.\n* We are limiting this process to A2 for the time being.\n4. Drop a link to the PR in the #platform-architecture slack channel so that everyone becomes aware\nWe will automate this soon\n5. The architecture team will then discuss all topics that have come in over the period of the week in the next architecture meeting\n* An agenda for each week’s topics will be posted a day before so people can come prepared\n6. The person requesting a review will be invited to the meeting to present their topic and discuss.\n* This will be an open meeting and anyone who has input and  wants to join are welcome to the meeting\n7. The architecture team and others are expected to come in having reviewed the content and ready with their questions and thoughts\n8. We will discuss the topic and hopefully reach a decision right in that meeting.\n9. If further discussions are required, we will indicate that as next steps and reconvene at the earliest for follow ups\n10. If there is no clear decision or if there are differing opinions, the engineering directors (Christoph Hartmann, Seth Falcon, Sudhir Reddy) will be responsible for resolving these and helping the team with a decision\n11. NOTE that if a particular topic does not align with company or engineering strategy or is something not even worth considering, it is the Directors’ responsibility to indicate and articulate this as early in the process as feasible.\n12. Here is a link to the [ADR process][1] set forth to document decisions\n13. There are a lot of good practices documented here on [decision making][2] which we’ll use as appropriate over time.\n[1]: https://github.com/chef/automate/blob/master/dev-docs/adr/adr-2018-08-15.md\n[2]: https://medium.com/@barmstrong/how-we-make-decisions-at-coinbase-cd6c630322e9\n","Establish an ADR (Architectural Decision Record) process to document and track architectural decisions, encourage broader participation, and streamline decision-making."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn our current IAM implementation, some services identify entities using human-readable names and other services identify entities using UUIDs (which we'll refer to as IDs in the following discussion).\nUsing IDs makes ensuring entity uniqueness easier. With UUIDs we can allow independent and concurrent creation of identifiers across multiple instances of a given service. IDs have performance advantages for use in data stores since comparisons are faster and indices are smaller (in general) than those that use a natural key.\nAPIs that can be accessed using human-readable names tend to provide a more self-documenting protocol whereas those that deal only in IDs result in message payloads that require additional tooling to map into a human readable form. A named-based approach requires verifying uniqueness within a data store to maintain data consistency.\nWhen users are the one to select a name, they often want the ability to change their mind and change the name. Rename functionality improves usability by letting users reflect their updated understanding into the system. Renames also introduce implementation challenges as one needs to decide when and how to cascade renames throughout the system.\nWhen we consider building aggregate functionality combinging the capabilities of multiple services, we want to be able to share an entity identifier across the bounded context in which the entity resides. Once this occurs, changing the identifier (rename) becomes more complicated and, definitionally, will be outside of a transaction. Strategies to handle this distributed cascade problem include event driven architecture where such updates are published to the event bus and providing long-lived aliases such that previous identifiers continue to work (e.g. github repo rename behavior).\nLooking across the A2 API, today most messages use an `id` field as the name for the entity's unique identifier. This does not align with Google's API design guidance (https://cloud.google.com/apis/design/resource_names) the requires the field to be called `name`. The proposed decision is to make use of `name` following Google's API design guide for new APIs and to commit to evolving existing APIs to support `name` as well. The alternative is to agree that our identifiers are called `id` regardless of their form. Either way, a goal of this decision is to define a standard that will result in increased consistency across the A2 API.\n\n## Decision\n","For IAMv2 we will use human-readable resource identifiers for the user visible protocol. This means that policies, roles, scopes, teams, and users will all be identified using unique ""names"" in the public GRPC API and resulting HTTP/JSON API that are intended to be human readable ""friendly"" names.\nWe will disallow renames to an entity's identifying name so that the names can be shared outside of the IAMv2 system.\nWe will support a ""display name"" field for entities in IAMv2 that users can use to describe the item and that we can use for UI display. The display name can be renamed. The UX is undecided, but we can consider flows where resource name is derrived from display name or vice versa to reduce the number of inputs a user needs to provide when creating things. The GCP projects API serves as a good example. In general, uniqueness should be enforced for `display_name` since the intention is to provide a human readable and changable label for distinct items.\nIn the IAMv2 data store (postgresql), we will use UUIDs and the core IAM services will handle UUID/name mapping and uniqueness constraint enforcement.\nWe will use the `id` field to store the unique, unchanging identifier in our GRPC protocol message definitions. We will use `display_name` for the can-be-edited name. For now, we are deciding not to adopt `name` as the field name for an entity's unique identifier as suggested in Google's [design guidance](https://cloud.google.com/apis/design/resource_names). While we'd like to follow those guidelines, the change would cause more disruption than benefit at this time. We can reconsider in future.\nWhen using names instead of UUIDs, special consideraton needs to be given to handling delete operations for entities that may be referenced outside of their originating bounded context. The system cannot rely on cascading delete behavior when a given identifier is stored across different bounded contexts in the system. In these situations, we have to consider what happens when an item is deleted and then a new item is created with the same id. If the system allows this re-use of id after delete, then it must ensure that all references are removed or invalidated. Alternatively, the system can implement a delete marker (aka ""tombstone"") approach and disallow deleted identifiers from ever being reused.\n","The decision is to use `name` as the field name for the entity's unique identifier following Google's API design guide. This will provide increased consistency across the API and align with industry best practices. The decision to use `name` also aligns with the goal of making the API more self-documenting, as it is more intuitive for users to understand the purpose of a field named `name` than a field named `id`.\n\n**Pros:**\n\n* Increased consistency across the API\n* Alignment with Google's API design guide\n* More self-documenting\n\n**Cons:**\n\n* May require changes to existing APIs"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to achieve high availability of Chef Automate. HA support for Postgresql & Elasticsearch would require considerable time and effort and likely be very difficult to tailor to individual customer needs. Rather than focus on building out HA support for our backend data services, which do not add much value to the product, we would instead prefer to add support for external data sources. This allows us to focus on building value into the product and allow customers to use existing cloud-based data services or in-house clusters.\n\n## Decision\n","### Requirements\n* Initial deployment of Chef Automate with external data stores should require minimal user configuration.\n* Migrations from a single-node install to external data stores should require minimal downtime and operator intervention.\n* Backup and Restore operations must work when external data stores are in use.\n### External Elasticsearch\n#### Deployment\nDeployment with external Elasticsearch should remain the same as the existing deployment options, only additional configuration in the initial configuration will be required. Domain services that access Elasticsearch will continue to so by binding to the `automate-elasticsearch` package. When their habitat configuration files are rendered they include the IP address and port. To support external Elasticsearch, we keep all of the existing logic the same, however, when external mode is configured the `automate-elasticsearch` package will not start an instance of Elasticsearch, it’ll create an instance `nginx` which listens on the same address and port but proxies to the upstream external Elasticsearch nodes. This approach has a few properties that are desirable:\n* Existing domain applications do not have to care if the Elasticsearch cluster is local, external, has many nodes or one, or if it requires special signing or security.\n* If the external cluster requires xpack security we can use the configuration they’ve provided, along with `nginx`, to transparently perform mutual TLS with the xpack cert and key.\n* If the external cluster is AWS Elasticsearch Service, we can use the `nginx` proxy to transparently sign outgoing requests with AWS access key and secret key that are provided (or resolved).\n#### Migration\nThere are two ways a user might want to migrate their data from a single instance deployment of Chef Automate to an external cluster.\nIn cases where the user has control over the cluster configuration, they can join the `automate-elasticsearch` service to their external cluster and configure the automate-elasticsearch node for no shards. As soon a shard relocation has completed the user could stop their Chef Automate installation, remove the `automate-elasticsearch` service from the cluster, and patch their Chef Automate config with the external cluster configuration.\nFor clusters in AWS's Elasticsearch Service (or Elastic’s Elastic Cloud service), the user would likely need to:\n* Configure their Chef Automate cluster to use S3 backups\n* Create a full backup (which will create S3 snapshot repositories in their desired bucket)\n* Create a new Elasticsearch domain in the cloud providers service\n* Configure their Elasticsearch domain to use the same S3 bucket as the Chef Automate cluster\n* Use the `_snapshot` API of their Elasticsearch domain to restore the latest snapshot of each Chef Automate snapshot repository\n* Configure the Chef Automate install to use their AWS Elasticsearch domain\n#### Backup\nBackups should more or less work transparently if the user provides the backup repository type that the `es-sidecar-service` will use when creating the snapshot repositories. If they’re using the AWS Elasticsearch Service we’ll also need the `role_arn` that the repository plugin will assume to take snapshots.\n#### Restore\nRestoration should work the same, though we might need to add more s3 flags to support the `role_arn`.\n### External Postgresql\n#### Deployment\nDeployment with external Postgresql should remain the same as the existing deployment options. Additional configuration in the initial configuration will be required. Domain services would be accessed in a similar manner to Elasticsearch, using the bind information from the `automate-postgresql` service. When external Postgresql is configured, services could still bind the `automate-postgresql` service, but instead of actually running `postgresql` it would a `postgresql` proxy to then external Postgresql FQDN's. We would configure the proxy to handle any Postgresql authn/authz transparently and require mTLS to the proxy from the upstream clients.\nNote: We haven't yet determined what software we'd like to use as the Postgresql proxy, but we've identified a few options that we'll investigate further during implementation and planning.\n#### Migration\nMigration from an existing Chef Automate install to an external Postgresql cluster can be achieved in several ways depending on target cluster:\n* The user could create a backup and manually restore the database .sql files that are contained in the backup repository.\n* We could build a utility that restores backup databases to a remote Postgresql cluster. We’d have the user create a backup and then restore it to their remote cluster as part of the migration.\n* It’s possible that a user could even configure a remote cluster to replicate from the `automate-postgresql` service. The user could wait for replication to catch up, stop Chef Automate, promote a read replica in their cluster to primary, and then update the configuration for external Postgresql.\n#### Backup\nTo support on-premises Postgresql clusters it makes sense to keep the default backup strategy consistent with the current backup implementation, which involves taking a dump of each database and storing the SQL file in a backup repository. RDS/Aurora support creating their own snapshots. We could eventually integrate with this feature for these customers.\n#### Restore\nRestoration of a database dump would work mostly the same. We’d restore the database SQL against the remote database. If we integrated with RDS/Aurora we could leverage their snapshot restoration ability. RDS supports restoring to an exact point in time. As our backup ID’s are an exact point in time we might be able to utilize such a feature if we decided to build that integration.\n",**Decision:**\n\nIntegrate with external data sources for Chef Automate instead of building in-house HA support for PostgreSQL and Elasticsearch.\n\n**Rationale:**\n\n* Focus on building value-added features into the product.\n* Allow customers to leverage their own cloud-based or in-house data services for HA.\n* Reduce engineering time and effort required for HA implementation.\n* Ease of customization and tailoring to individual customer needs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn A2, we use protobufs to describe our APIs and config. Services talk to each other using clients generated from these protobuf files. The `deployment-service` uses the protobufs for config to allow users to configure A2.\nWhile this works, it has a few flaws. First, there is no clean separation between a service's implementation and its API. This means that when certain services update a service, but not the API, dependent services must be rebuilt. A similar thing is true for config, where `deployment-service` ends up depending on all services, and chaining any of them causes `deployment-service` to be rebuilt.\n\n## Decision\n","We will define APIs and config in our top-level `api` directory. This top-level `api` directory will have 3 subdirectories:\n* external - Our public facing API, which is currently exposed by `automate-gateway`\n* internal - API definitions for each of our services\n* config - The config definitions for the services and the global config definition\nServices that want to communicate with each other should do so by using the clients that are in the `internal` API subdirectory. When taking such a dependency, the service must update the `.expeditor/config.yml` file with this information. Components which are services should not have dependencies to other components which are services in the `.expeditor/config.yml` file.\n","Enforce a clean separation between service code and its API, and between service implementation and config."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\n\n## Decision\n","### Lifetime of a single resource\n```mermaid\ngraph TD\nA[Create resource] -->|Auto-assign to creator| B\nB[State: Open] --> |Do work|B\nB --> |Assign to somebody who can close| C\nC[State: Open & Ready to close] --> |Close| D\nD[Resource closed]\n```\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\n#### Example: 4-Eyes Principle\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\nPrereqs: Closing right is given to an administrative person and not to the people signing off\nApproach:\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\n### Impact to child resources\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\nIf a resource has no children, there is no impact on other resources.\nIf a resource has children (e.g. subprojects / workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\n```mermaid\ngraph TD;\nProject-->Subproject_A;\nProject-->Subproject_B;\nSubproject_A-->Workflowitem_A;\nSubproject_A-->Workflowitem_B;\nSubproject_B-->Workflowitem_C;\nSubproject_B-->Workflowitem_D;\n```\n","**Define a resource lifecycle abstraction and implement it in the platform.**\n\nThe resource lifecycle abstraction should include the following concepts:\n\n* **Resource states:** The different states that a resource can be in throughout its lifetime.\n* **State transitions:** The allowed transitions between resource states.\n* **Lifecycle events:** The events that can trigger state transitions.\n* **Lifecycle policies:** The rules that govern how resources transition between states.\n\nThe platform should implement the resource lifecycle abstraction in a way that allows it to be used by different use-cases. For example, the platform could provide a default lifecycle policy that is suitable for most use-cases, but also allow users to define their own custom lifecycle policies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, branches are used to draft releases. Going forward, the branching model needs to satisfy the following criteria:\n- Releases need to be tagged to ensure that, given a release name, the commit sha can be inferred.\n- We need to support multiple networks (chains), but they are not necessarily running the same version of TruBudget.\n- Each network should have its own deployment URL that does not change on non-breaking version updates.\n- Conversely, in case of breaking changes, the deployment URL should be changed.\n\n## Decision\n","- `main` continues to be the main development branch.\n- Releases are tagged using an [annotated Git tag](https://git-scm.com/book/en/v2/Git-Basics-Tagging) (optionally [signing the tag](https://git-scm.com/book/en/v2/Git-Tools-Signing-Your-Work)) like this:\n```bash\ngit tag -a ""v2.0.0-beta.1"" -m ""version 2.0.0-beta.1""\n```\n- For each network, there is a Git branch prefixed by the major version the network is running on - **major version upgrades are assumed to break compatibility with the network**. For example, branch names may look like this:\n```plain\n1.x.x_my-example-network\n1.x.x_my-other-example-network\n2.x.x_network-running-a-newer-version\n```\nNote that a branch `1.x.x_ACMECorp-UmbrellaCorp` that is running tag `v1.0.0` may upgrade to a newer minor version like `v1.1.0`, but cannot upgrade to a `2.x` release. If the ACMECorp and the UmbrellaCorp decide to upgrade to the `2.x` series of TruBudget, they need to migrate to a new branch called `2.x.x_ACMECorp-UmbrellaCorp`. The definition of the necessary processes is not in scope of this ADR.\nAdditionally, there is one release branch for each major version that tracks the most current release (tag) for that major version.\nThe steps required for making a new release are:\n1. Increase the version number (see #33).\n1. Update the CHANGELOG.md file to reflect the release.\n1. Tag the commit on the main branch (see above).\n1. Checkout the release branch of the current major version (e.g. `release_1.x.x`) and `git merge main`.\nAs soon as a network is ready to be upgraded to the new version, its branch may be updated:\n1. Checkout the network branch, e.g. `1.x.x_my-example-network`.\n1. Merge-in the release branch (or a specific tag), e.g. `git merge release_1.x.x`.\n","Use a trunk-based development model with multiple deployment targets.\n\n**Pros:**\n- Satisfies all of the criteria in the context.\n- Eliminates the need for branches, which can be confusing and error-prone.\n- Makes it easier to track changes and roll back if necessary.\n\n**Cons:**\n- Requires a more disciplined development process.\n- Can be more difficult to manage if there are multiple developers working on the same project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to decline node requests. The current api logic saves an event on the `nodes` stream for all nodes registered on the network. These nodes appear in the ui's nodes table as requests. So far, the requests that were not accepted would pile up and could not be deleted. When accepted, the nodes receive permissions from the MultiChain and can successfully connect to the system. Basic nodes receive the connect, send, receive, issue and create permissions, and admin nodes also have the mine, activate and admin permissions. For more details on these permissions check out the [Multichain permissions management](https://www.multichain.com/developers/permissions-management/) documentation. The opposite of the approval of nodes (which is actually a granting of permissions) would be to revoke these permissions. However, this would not lead to the desired outcome because the nodes would again appear in the list of requests as trying to connect to the network.\n\n## Decision\n","As MultiChain doesn't offer a possibility to decline nodes, we want to save an event on the `nodes` stream that mentions that a node was declined and by what organization. This way, when listing the nodes, the ones that appear in the stream as declined will be hidden from all nodes belonging to the decliner organization.\nIn order to promote transparency, every node in the network should see if a node has been declined by another orga.\n",Create a new workflow for processing declined node requests. This workflow would:\n\n1. Delete the event from the `nodes` stream.\n2. Remove the node from the list of pending requests in the UI.\n3. Send a notification to the requester informing them that their request has been declined.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to define our approach to access-control/authorization.\n\n## Decision\n,"Since our users' organizations differ a lot in terms of structures and policies, we\nneed to have a very versatile access control mechanism in place. In the industry, the\nmost used technique employed is role-based access control (RBAC). We base our\nmandatory access control (MAC) mechanism on RBAC, but use the notion of _intent_\ninstead of role (an intent can be executed by exactly one role, thus effectively\nreplacing the role concept).\n![Intent + Resource = Permission](./img/0002-access-control-model.png)\n### Intents\nAn intent is what the user is trying to achieve, for example, ""add a workflow to a\nsubproject"". By using intents rather than roles, we side-step the problem found in\nmany projects, where over time developers create similar roles on-the-fly, as from\ntheir point of view the implications of re-using a role are not always clear.\nConversely, intents are always specific to a use case (examples: ""create project"" or\n""list a project's subprojects"" rather than ""admin"" or ""user"").\nWhile roles _could_ be used to bundle intent-execution rights (e.g. have one role\nthat is allowed to execute all ""view"" intents), we think that those roles would have\nto be managed by organizations themselves (as it will depend on their structure).\nSince in most cases this would mean a 1:1 mapping from role to user group, we skip\nroles altogether.\n### User Groups\nOrganizations group their users into _user groups_. For any given projects, each\n(resource-specific) intent has a list of user groups assigned to it; all users in the\nassigned groups are then allowed to execute the respective intent.\n```plain\n+-------+               +---------------------+                +---------------+\n| User: | is member of  | Group:              | is allowed to  | Intent:       |\n| Alice +-------------->+ Project Maintainers +--------------->+ Add workflow  |\n|       |               |                     |                | to subproject |\n+-------+               +---------------------+                +---------------+\n```\n### Implementation Pattern\nThe goal is to enable us to follow a clear pattern for our access control needs:\n- HTTP controllers call the domain modules and the authorization module (perhaps using a\nmiddleware), but do not deal with intents or groups.\n- Domain modules may interact with the chain to fetch domain objects, and/or prepare\nclosures to be authorized and executed later on. They deal with intents, but not with\nusers or groups.\n- Finally, the authorization module ensures that the _user_ executing the _intent_\nbelongs to a _group_ that is allowed to do that. In order to decide that, the module\nhas to fetch resource-specific ACLs from the chain.\nModifying ACLs is done in the same way: each resource's ACL specifies the groups that\nmay execute the ""change this ACL"" intent (to be renamed). This hints at the necessity\nto provide _defaults_ for ACLs when creating resources.\n### Resource-specific Access Control Lists (ACLs)\nWith each resource/stream on the chain, an ACL stream-item is stored that lists for\neach intent the groups allowed to execute that intent:\n```json\n{\n""acl"": {\n""view project"": [""all users""],\n},\n...\n}\n```\n","**Enforce authorization at the API Gateway layer.**\n\n**Rationale:**\n\n* **Centralized control:** Enforcing authorization at the API Gateway allows for a single point of enforcement, ensuring consistent authorization policies across all APIs.\n* **Scalability:** The API Gateway acts as a centralized hub, reducing the overhead of managing authorization logic within individual microservices.\n* **Flexibility:** The API Gateway can easily handle complex authorization policies, such as role-based access control, OAuth 2.0, or custom authorization schemes.\n* **Offloading authorization:** Moving authorization responsibility to the API Gateway frees up microservices from having to implement their own authorization logic, reducing their complexity and improving performance.\n* **Security:** By handling authorization at the edge, the API Gateway acts as a protective layer, preventing unauthorized access to the microservices behind it."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to display more info regarding the status of the nodes connected to the network. A useful feature would be to display if a node is still connected or not and also to see a ""last seen"" timestamp of the nodes.\n\n## Decision\n","In order for the connection status to be displayed, an endpoint checking the multichain's [getpeerinfo](https://www.multichain.com/developers/json-rpc-api/) function would be called. This returns the nodes connected at the moment. However, since MultiChain doesn't provide information about when a node was last seen we want to implement this feature in the api. A new stream should be created called `network_log` and every 24h an event would be saved on the stream displaying the addresses of the nodes connected at that moment. All nodes in the network perform the check but only saves it on the stream if there is no event saved for that day. This way, the check is being recorded even if one of the nodes is disconnected but it is also prevents duplicate entries.\n","**Decision:** Implement a mechanism to track the status of connected nodes and display the ""last seen"" timestamp.\n\n**Rationale:** This decision provides the desired functionality of displaying the status and ""last seen"" timestamp of connected nodes. It allows users to quickly identify if a node is still connected and when it was last seen, which can be valuable for troubleshooting and monitoring purposes.\n\n**Consequences:**\n\n* **Positive:** Provides users with better visibility into the status of their network.\n* **Negative:** May require additional development effort and maintenance overhead."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEven though Multichain (like Bitcoin) has to be bootstrapped from a single node, we're aiming at a system design that doesn't rely on a single node being involved in most administrative tasks, such as adding additional nodes to an existing deployment.\nThis ADR discusses terminology and setup of the TruBudget network.\n\n## Decision\n","### Terminology\nEach **node** is associated to a single _organization_ (e.g. ACMECorp, UmbrellaCorp).\nThe **origin node** refers to the one node, which mines the first block in the whole network. Nodes that belong to the same organization are called _internal peers_, nodes that belong to other organizations are called _external peers_.\nThe **organization's address** is used to identify the organization in the network.\nThe **private key** is used to authenticate members of the organization, it is encrypted using the ORGANIZATION*VAULT_SECRET.\nThe **ORGANIZATION_VAULT_SECRET** is a shared key between internal peers used to en-/decrypt the private key. It is saved in an environment variable on the API host of a node.\nA **user** is a member of an organization. To access the Trubudget API, the user is given a username and password. When performing actions (which are described by their \_intent*, e.g. ""project.create"" or ""global.addUser"") the API writes this action in form of an _event_ to the blockchain.\nAn **event** describes a performed action like creating a project or adding users and holds meta data like timestamp or creator.\nExample for creating a group ""MyGroup"" with users ""user1"" and ""user2"":\n```json\n{\n""key"": ""123"",\n""intent"": ""global.createGroup"",\n""createdBy"": ""root"",\n""createdAt"": ""2018-12-17T14:52:11.511Z"",\n""dataVersion"": 1,\n""data"": {\n""group"": {\n""groupId"": ""123"",\n""displayName"": ""MyGroup"",\n""users"": [""user1"", ""user2""]\n}\n}\n}\n```\nThe first data added to the blockchain is the organization's address and (encrypted) private key:\n```json\n{\n""publishers"": [""1bNueyVy4j7V6yRSa6SDyHhkfVBHyFJ4QoAiD4""],\n""keys"": [""address""],\n""offchain"": false,\n""available"": true,\n""data"": {\n""json"": {\n""address"": ""1bNueyVy4j7V6yRSa6SDyHhkfVBHyFJ4QoAiD4"",\n""privkey"": ""151fca5bbb689321a410c5646cc81a582c7bdade7365a806c994b530b9f28689e67b29af306996f0e95ea47a0ec66fc0bca9c78b7ce7510f71e95111a476d0cb6799b431b249d5a632ddc45aa1984f8fe8c2bcbd903bc6d9c6b8ba8458efb5b5""\n}\n},\n""confirmations"": 8,\n""blocktime"": 1545054793,\n""txid"": ""c3ce3a37669d7ce005c7813a1e9a97152fac873cc9149ad81eadea5f78c6718e""\n}\n```\n### Distributing secret keys\nIn order to prevent other organizations from using the private key, it is encrypted.\nThe encryption key is a shared secret known to all internal peers, called the **organization vault secret** (OVS).\nEach internal peer uses the address and private key of the first node of its organization after [joining the network](#joining-as-new-node-of-an-existing-organization).\nEach action of any user is published to the multichain using the corresponding organization's private key ,no matter through which internal peer he/she is connected.\nSidenote:\nEach user has their own pair of address and private key which is held by a streamitem on the _users_ stream, which currently is not in use.\n(_In the future_ whenever a user logs in, the user's address should be read from the _users_ stream. Then this keypair is used to publish any data using command _publishfrom_)\n### Creating a network\nTo create a new network the origin node creates the genesis block (i.e. the first block of a blockchain) and the multichain creates the organization's address and private key. The new organization creates its own stream (e.g. org:ACMECorp) and writes its address and encrypted private key to the first streamitem ([see above](# Terminology)).\n### Joining the network\n#### Joining as new organization\nIn this case, a new organization wants to join an already existing network. Example: ACMECorp already has one note in the network and UmbrellaCorp wants to join.\nA = Orga A (e.g. ACMECorp)\nB = Orga B (new organization, e.g. UmbrellaCorp)\n```mermaid\nsequenceDiagram\nparticipant A as Orga A\nparticipant B as Orga B\nB->>A:connect\nA->>A:Multichain: B has no connect permission\nA->>A:Add connection attempt to dashboard\nA-->>B:connection denied!\nA->>A:Admin validates the address. If more than 50% of all organizations in the network validates the address, the new organization is approved.\nB->>A:connect\nA->>A:Multichain: B has connect permission\nA-->>B:connection established!\nB->>B:Initialize own organization stream\nB->>B:Encrypt own private key with *organization vault secret* and save it to its organization stream\n```\n#### Joining as new node of an existing organization\nIn this case a new node from an organization that already has a node in the network wants to join. Example: ACMECorp has one node in the network and a new ACMECorp node wants to join the network.\nThe main difference here is, that you only need the approval of one admin user from another node in the network.\nImportant: Even though the new node of the organization has its own address, only the address and private key of the already existing node is used for future transactions. It is therefore absolutely necessary that both nodes have the same ORGANIZATION_VAULT_SECRET.\nNode A = first node of Orga A (e.g. ACMECorp)\nNode A2 = second node of Orga A\n```mermaid\nsequenceDiagram\nparticipant A as Node A\nparticipant B as Node A2\nA->>A: creates organization stream with organization key-pair\nB->>A: connect with A2 node address\nA->>A: Multichain: A2 has no connect permission\nA->>A: Add connection attempt to dashboard\nA-->>B: connection denied!\nA->>A: *One* admin of any organization in the network approves the new node.\nB->>A: connect\nB->>B: read organization key-pair from stream and import into local wallet\n```\n",The decision is not provided in the given context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWorkflowitems are sorted by their creation time by default, but there needs to be some mechanism that allows for manual sorting as well (mainly relevant for the UI). Previously, each workflowitem would hold a pointer to the previous item in the list. However, this approach cannot prevent an inconsistent state if there is a data race between two concurrent requests: it may happen that two workflowitems share the same pointer (turning the list into a tree).\n\n## Decision\n","We solve this by maintaining the ordering as a list, stored with the subproject the workflowitems belong to:\n```plain\nsubproject stream:\nstream item ""workflowitem_ordering"" => { data: [id1, id2, ...], log: [], permissions: {}}\n```\nNote that we use the resource structure here simply to be able to treat the record like any other, but `log` and `permissions` have no meaning at the time of writing.\nSince Multichain doesn't offer transactions for stream operations, we cannot guarantee that a newly created workflowitem would always be recorded in the list, so we apply the following trick when computing the ordering:\n- Workflowitems that are included in the workflowitem-ordering are included in the result exactly in that ordering;\n- all remaining workflowitems are sorted by their creation time and appended to the result.\nBecause workflowitems are sorted by their creation time by default, newly created items _do not_ have to be added to the ordering, so no inconsistencies can occur.\n",Introduce a separate sequencing table that contains pairs of workflowitem IDs and their order in the sequence.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEven though Multichain (like Bitcoin) has to be bootstrapped from a single node, we're aiming at a system design that doesn't rely on a single party being involved in most administrative tasks, such as adding additional nodes to an existing deployment.\nThis ADR discusses terminology and setup of the TruBudget network.\n\n## Decision\n","### Terminology\nEach _node_ is associated to a single _organization_ (e.g. ACMECorp, UmbrellaCorp), including the special _origin node_, which mines the first block in the network. Nodes that belong to the same organization are called _internal peers_; nodes that belong to other organizations are called _external peers_.\n### Blockchain Addresses\nUser authenticate themselves using username and password, but internally every user is assigned a blockchain address, along with a public and a secret key. On the chain, each action is then associated with the user's key and the associated permissions, which makes operations secure and auditing reliable.\nWith Multichain, each node owns a unique address, generated when started the first time (its wallet address). In day-to-day operations, those addresses are not used and only the address of the first node of an organization is put on the chain (and thus propagated among nodes).\nBy mining the genesis block (that is, the first block in the network), the origin node gets _full_ permissions on _everything_, which includes exclusive access to the `root` stream. The permissions are bound to the origin-node's address; we call the associated key _origin root key_ (ORK). The ORK is not put on the chain and should be backed up by the administrator; it is also not used in day-to-day operations. Instead, the ORK is used to create another node address that holds necessary permissions; that key is called _organization key_.\n### Distributing secret keys\nThe organization key is put on the chain, into a stream-item on the organization's stream, called the _vault_. The vault holds the private keys of all (technical and non-technical) users of the organization. In order to prevent other organizations from using them, the vault needs to be encrypted. The encryption key is a shared secret known to all internal peers, called the _organization vault secret_ (OVS). Whenever ~~the vault is updated, all internal peers import new keys into their wallet (using a walletnotify script)~~ a user logs in, the user's private key is read from the vault and imported into the node's wallet, which allows users to move freely between their organization's nodes.\n### Joining the network\nThere are two ways an organization can join the network:\n1. The organization maintains the origin node, in which case the ORK is used to create the organization key (see above).\n1. For all other organizations, the first node needs to connect to any node of an organization that has _trusted access_ (see below).\n#### Trusted vs. managed access\nIn the TruBudget network, there are two types of organizational access to the network: _admin_ and _basic_. The main difference is that with admin the node can also _mine_ and write to so-called ""admin"" streams. In TruBudget, each organization should have exactly one address (the _organization address_) with admin permissions; all other node addresses should have basic access only. This ensures that a single organization cannot manipulate the network without other organizations noticing.\nIn other words, connecting as a new organization **B** to an existing TruBudget network of organization **A** works as follows:\n1. **A** gives **B** the API URL of any one of their nodes.\n1. **B** runs TruBudget for the first time, pointing it to the given URL. Since the instance address is unknown to **A**'s node, the connection fails; instead, the administrator of **A** will see the connection request in TruBudget's connection dashboard. Meanwhile, the node of **B** continues its connection attempts.\n1. The administrator of **A** verifies the address by calling **B**, asking them for confirmation. The administrator then chooses ""trusted access"" for the connection request. The address of the node of **B** is now **B**'s organization key.\n1. Now that the `connect` permission has been granted, **B**'s node is finally able to establish a connection, joining the network and fetching all existing data in the process. Since the organization stream for **B** is not yet present, the node infers it's **B**'s first node; consequently, the node creates the organization stream and initializes the ""vault"" stream-item with its encrypted private key (= organization key).\nWith managed access, the connecting organization cannot execute the last step itself.\n```mermaid\nsequenceDiagram\nparticipant A as Orga A (trusted access)\nparticipant B as Orga B, node 1\nparticipant B2 as Orga B, node 2\nB->>A:connect\nA->>A:Multichain: B has no connect permission\nA->>A:Add connection attempt to dashboard\nA-->>B:connection denied!\nA->>A:Admin validates the address, approves ""trusted access""\nB->>A:connect\nA->>A:Multichain: B has connect permission\nA-->>B:connection established!\nB->>B:Initialize own organization stream\nB->>B:Encrypt own private key with vault key and set as vault\nB2->>B:connect\nnote over B,B2:same procedure, except node 2 uses existing organization stream and doesn't add own private key to vault (it imports the existing one instead).\n```\n### User Management\n`N Users <-> 1 Organization`\nAs mentioned above, each organization's users have their addresses and private keys stored in the organization's vault, which is encrypted using the organization vault secret (OVS). The OVS is initialized using the `VAULT_SECRET` environment variable.\nConsequently, all client-related operations are only possible on nodes that have the right OVS set in their environment (typically equal to the nodes that belong to the same organization as the requesting user):\n- user creation, as it directly modifies the vault\n- authentication, since it causes the user's private key to be imported in the node's wallet, which in turn requires read access to the vault\n- all other API calls, as the token is always augmented with the user's private key, which requires read access to the vault\nTo ease testing (and because it doesn't affect production), a node always tries to decrypt the vault that belongs to the user's organization, without checking first whether it belongs to that organization itself. A node always belongs to a single organization, but using the same OVSs for all organizations enables a node to cater all API requests (not to be done in production, because then all nodes have all keys of all users of all organizations in their wallet, basically circumventing all other security mechanisms).\n",The decision is not provided in the context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR discusses how to handle breaking schema changes of the data-model in the context of a distributed application (TruBudget).\n### Explanation of the current implementation\nData is currently stored in streams. A stream contains none, 1, or multiple items. These items can either represent the complete data-set at time of creation or data change (for example an event). In streams that represent the complete data in every item, the last item represents the current state. In streams where items only contain data changes, the items are sourced from the first to the latest in order to get the current state of the data (event-sourcing).\nItems which represent complete data-sets are called `Ressource`. Items which contain only data changes are called `Event`.\nIn order to be able to change the interface of `Event` a `Event.dataVersion` field can be defined describing the interface version of the `Event`.\nAn item of type `Ressource` currently doesn't contain a `dataVersion` field. Furthermore it is generally using `HEX` as a dataformat, instead of `JSON` which is used as dataformat for `EVENTS`\nFor items of type `Event` the dataVersion is only checked when performing a read requests. When performing a write/publish request to streams containing items from type `Event` no check is performed.\n### Subject for change\nBefore the upcoming release we want to unify the dataformats. More precisly the plan is to get rid of the `HEX` dataformat and only use `JSON`. Hence, we have to introduce a breaking change regarding in our items of type `Ressource`. Items created by a node running the new version of Trubudget will be in `JSON` and can't be read by Nodes running an older version of TruBudget (which is expecting `HEX`).\nFor changing the dataformat in the current case there is no technical solution, how to gracefully migrate. Hence, we will do it on an organisational level (updating all versions simultanously).\nBut for future updates we want to be better prepared handling version conflicts between different distributed nodes in a TruBudget network.\n### Options\nIn general we need to distinguish on how data is read from the stream.\nFor streams containing items of type `Event` and doing event-sourcing the newest version needs to be able to read items of older version. If an older version of a node tries to read a stream which contains items of a newer version, it is not able to do so and returns an error.\nFor streams containing items of type `Ressource` only the last element is read. It is pretty much the same: New versions can read stream with items of older versions. But not vice-versa. The only difference is: Since only the last item is read from the stream, old data versions could be migrated resulting in removing source-code which is needed to read the older versions of the item.\nThings get more complicated for write operations:\nFor streams containing items of type `Event` it makes no sense for a node with the old data-version to write, since it might not be able to read it upfront or afterwards.\nFor streams containing items of type `Ressource` it is pretty much the same.\n\n## Decision\n","Since read and write operation might return errors for nodes running an older dataVersion, these nodes need to be forced to upgrade to a newer version if they still want to participate in the network.\nThis means:\n* Nodes with older dataVersion are not allowed to write to streams\n* Nodes with older dataVersion are allowed to read, but might not be able to get the most recent data\n* Items of type `Event` or `Ressource` extend a DataVersion interface\n* The network is aware of all nodes and it's versions (in order to inform them if they run on an old version).\n### TODOs\n#### Change Hex to JSON format\nEvery item, which is written to a stream has `JSON` as datatype and should extend the `DataVersion` interface (e.g. `Ressource`, `Event`, `Vault`)\nUse a DataVersion interface for all three interfaces (Event, Resource, Vault)\n````\ninterface DataVersion {\ndataVersion: number\n}\ninterface Resource extends DataVersion {\n....\n}\n````\n#### Set new version in network\nWe create a multichain stream named ""versions"" where all dataversion should be saved. On startup every node checks its version and updates the stream if it is higher.\nstreamname : versions\n````\n{\nkeys: [""1""]\ndata: {\njson: {\ndataVersion: 1,\nany: [this is a example of the data-format in this version]\n}\n}\n}\n````\n#### Track versions in network\nWe currently track all nodes that have been registered in the network. For every node an item is saved to the `nodes` stream. We also have to save an item if the node's dataVersion increases to track versions of every node in the network.\nstreamname: nodes\nnode-address: ""1axuNPTudeCHHBwhJD5tYtRaaGWGpkTon5qd""\n````\n{\n""keys"" : [\n""1axuNPTudeCHHBwhJD5tYtRaaGWGpkTon5qd""\n],\n""data"" : {\n""json"" : {\n""key"" : ""1axuNPTudeCHHBwhJD5tYtRaaGWGpkTon5qd"",\n""intent"" : ""network.registerNode"",\n""createdBy"" : ""<system>"",\n""createdAt"" : ""2018-11-12T09:57:19.104Z"",\n""dataVersion"" : 1,\n""data"" : {\n""address"" : ""1axuNPTudeCHHBwhJD5tYtRaaGWGpkTon5qd"",\n""organization"" : ""ACMECorp""\n}\n}\n}\n}\n````\n#### Prohibit writing with older version when newer version is set\nBefore every write operation, the version needs to be checked. The most efficient way to do so is to query the streamkeys with `liststreamkeys` of the `versions` stream.\n","Introduce a `dataVersion` to items of type `Ressource`. When reading from streams containing items of type `Ressource`, check the `dataVersion` of the newest item and if it is not recognized, an error is returned.\n\nWhen writing to streams containing items of type `Ressource`, compare the `dataVersion` of the newest item with the `dataVersion` of the item to be written. If the `dataVersion` of the newest item is higher than the `dataVersion` of the item to be written, an error is returned."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to define how to save and query project, subproject and workflow data efficiently.\n### Definition of terms\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\n- Workflowitems: Are a number of steps which need to be executed in a certain order\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\n### Relations\n```mermaid\ngraph TD;\nProject-->Subproject_A;\nProject-->Subproject_B;\nSubproject_A-->Workflowitem_A;\nSubproject_A-->Workflowitem_B;\nSubproject_B-->Workflowitem_C;\nSubproject_B-->Workflowitem_D;\n```\n### Options\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage / memory footprint **Con:** None\n\n## Decision\n","Since we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage / memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\n","**Option 3: Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource.**\n\n**Rationale:**\n\n* **Easy to query:** With key-items in the project stream, specific resources can be queried with a single query.\n* **Low storage / memory footprint:** Only the affected resource needs to be copied when updates are made, reducing storage and memory usage.\n* **Upgradeable:** This option allows for future upgrades to the data model without affecting the existing data.\n* **Scalable:** The use of multiple key-items in the project stream ensures that the data model can scale well with a growing number of resources.\n* **Reduces complexity:** Compared to other options, this option introduces less complexity in terms of data management and retrieval."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","We will use an Architectural Decision Record (ADR) template to document all architectural decisions made on this project. The ADR template will include the following sections:\n\n* **Title**: A brief and descriptive title for the decision.\n* **Context**: A brief explanation of the problem or opportunity that led to the decision.\n* **Decision**: A clear and concise statement of the decision that was made.\n* **Consequences**: A discussion of the potential consequences of the decision, both positive and negative.\n* **Trade-offs**: A discussion of the trade-offs that were considered when making the decision.\n* **Alternatives**: A list of alternative solutions that were considered and rejected, along with the reasons for their rejection.\n* **Authors**: A list of the people who made the decision.\n* **Date**: The date on which the decision was made.\n\nWe will store ADRs in a central repository, such as a wiki or document management system. This will ensure that all ADRs are easily accessible to all project stakeholders."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need an overview for the execution architecture\n\n## Decision\n,TruBudget can be deployed either locally or in the cloud using Docker. Interfaces are exposed via web interface and via blockchain connection.\n![Overview of architecture](./img/0007-execution-architecture-overview.png)\n,"**Decision:**\n\nAdopt a microservices architecture for the execution architecture.\n\n**Rationale:**\n\n* **Improved scalability and resilience:** Microservices can be scaled independently, allowing us to handle increased load or accommodate different performance requirements.\n* **Reduced code coupling:** Microservices are small and self-contained, minimizing dependencies between components and making it easier to maintain and evolve the system.\n* **Enhanced flexibility:** Microservices can be easily replaced or updated, allowing us to respond quickly to changing business requirements or technological advancements.\n* **Improved developer productivity:** Microservices enable parallel development and deployment, reducing the turnaround time for new features or bug fixes.\n* **Support for distributed computing:** Microservices align well with distributed computing environments, where components can be deployed across multiple servers or cloud platforms."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nHow to guarantee reproducibility of Jupyter Notebooks?\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\n## Decision Drivers <!-- optional -->\n* user prospective\n* reproducibility\n* traecability\n\n## Decision\n","* user prospective\n* reproducibility\n* traecability\nThe option selected is 3. because:\n* enforce reproducibility\n* enforce traceability between notebook\n### Positive Consequences <!-- optional -->\n* Satisfy reproducibility, traecability, shareability.\n* Notebooks are coupled with dependencies in their metadata.\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\n","**Adopt a container-based approach for Jupyter Notebooks.**\n\n**Rationale:**\n\nThis approach addresses the decision drivers as follows:\n\n* **User perspective:** Containers provide a convenient and consistent way for users to run Jupyter Notebooks, regardless of their environment or dependencies.\n* **Reproducibility:** Containers guarantee that all dependencies required to run a notebook are included in the container image, ensuring reproducibility across different environments.\n* **Traceability:** Containers enable tracking of the dependencies and versions used in each notebook, providing traceability for auditing and troubleshooting purposes.\n\n**Details:**\n\n* Create container images for each Jupyter Notebook, including all necessary dependencies.\n* Store the container images in a repository accessible to users.\n* Provide tools or instructions for users to build and run the containers for the notebooks.\n\n**Benefits:**\n\n* **Improved reproducibility and consistency:** Notebooks can be run with identical behavior across different environments.\n* **Enhanced security:** Containers provide isolation and dependency management, reducing security risks.\n* **Transparency and traceability:** The container images provide a clear record of all dependencies used in a notebook.\n* **Collaboration and reuse:** Notebooks can be easily shared and reused by others, as users can simply build and run the container images.\n\n**Considerations:**\n\n* **Container size:** Containers can be bulky, so it's important to optimize the dependencies and use lightweight base images.\n* **Resource consumption:** Containers require additional resources to run, so it's important to consider the resource requirements of the notebooks.\n* **Maintenance:** Containers need to be maintained and updated periodically to ensure they remain functional."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a consistent way to add menus within fragments.\n\n## Decision\n,Every fragment handles its menu independently. This is achieved through calling setHasOptionsMenu(true)\nand overriding the specific methods needed for creating the menu and handling selection events.\n,"Use the **`NavigationUI`** class and the **`setupWithNavController()`** method.\n\n**Example:**\n\n```kotlin\nimport androidx.navigation.NavController\nimport androidx.navigation.ui.NavigationUI\n\nfun setupBottomNavMenu(navController: NavController) {\n    val bottomNavigationView = findViewById<BottomNavigationView>(R.id.bottom_nav)\n    NavigationUI.setupWithNavController(bottomNavigationView, navController)\n}\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOpen Source Software should be honored and apart from our obligation to show the used open source libraries, we think\nthe great people have to be named to honor their contribution.\n\n## Decision\n",We need a new view showing all relevant open source libraries we are using including a link to the project-page.\nFor simplicity we should use a simple webview showing a pre created HTML page.\n,"**Decision:** Create an ""In Memory Of"" section in the application that lists the names of the open source contributors who have passed away."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\nafter returning to a fragment from another navigation path. Errors or special events like automatic\nnavigation should only be executed once.\n\n## Decision\n,A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\n,"Use a `SingleEventLiveData` instead of a regular `LiveData`. `SingleEventLiveData` ensures that the event is only delivered once to the observer. After the observer has been notified, the event is considered consumed. This prevents the event from being delivered again after a configuration change or when the observer is re-attached."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a consistent way to access the Application Bar to update the Caption or change the Hamburger menu to homeAsUp\n\n## Decision\n,"We create an interface which will be implemented by the Activity. Due to the single activity architecture, the base\nfragment will cast the context to which the fragment is attached to the interface and set a member variable. Also some\nmethods will be exposed by the fragment so that extended fragments can make use of these methods.\n","We should use the `Window.FEATURE_OPTIONS_PANEL` flag to enable the Options Panel, which will provide a consistent way to access the Application Bar."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe ViewModel is responsible for the data, so i tried an attempt to notify the ViewModel from within the view that the\nuser clicked the ""edit"" menu item. The ViewModel exposed a LiveData object which was observed by the view to trigger the\nnavigation to a edit fragment.\nAfter editing the workout and clicked saved, the edit fragment was opened again instantly. This was caused by the\nViewModel which triggered the edit event again.\n\n## Decision\n","The view (DetailviewWorkoutFragment) no longer notifies the ViewModel about the edit action but directly calls the\nnavigation component to open the EditWorkoutFragment, and therefore decides what should be done after the edit action\nwas clicked (normally the view should not make this kind of decision).\n","To solve the issue, the ViewModel should expose an event instead of a LiveData object. The event will be triggered only once when the ""edit"" menu item is clicked, and it will not be triggered again when the edit fragment is saved.\n\nThe view can observe the event and trigger the navigation to the edit fragment. After the edit fragment is saved, the view can call a method on the ViewModel to reset the event, so that it will not be triggered again when the view is recreated."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOnboarding new engineers on a software project should be fast. Also our future self should be back on track on a project fast.\nConsistent and logical naming can be a benefit here.\n\n## Decision\n,"All ""event""-methods within a ViewModel class should have a prefix ""on"". Events are methods which are called by the view.\n(ie. onClickFavorite, onSearchChanged).\n","**Decision:**\n\nUtilize a naming convention that is:\n  - **Consistent:** Establish a set of clear and concise naming rules for all code elements (e.g., variables, functions, classes).\n  - **Logical:** Ensure that names accurately reflect the purpose and behavior of the corresponding code elements.\n  - **Documented:** Clearly document the naming convention and make it easily accessible to all team members.\n\n**Justification:**\n\nBy adhering to a consistent and logical naming convention, we can:\n\n- Reduce the cognitive load for new engineers, making it easier for them to understand and navigate the codebase.\n- Improve the readability and maintainability of code, allowing our future selves and other team members to quickly grasp its intent.\n- Facilitate error detection and debugging by ensuring that names match the actual functionality of code elements.\n- Enhance team collaboration by establishing a shared understanding of naming conventions."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI think exposing the whole state from the ViewModel to the fragment moves some logic to the fragment and is error prone\nin my opinion.\n\n## Decision\n,"Instead of exposing whole state objects, we expose only single properties which can individually handled by the fragment\n",**Decision:** Encapsulate the ViewModel state in a livedata object and expose only the livedata object to the fragment.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to unit test the ViewModel classes but the ViewModel class itself creates a specific UseCase and is using hardwired Rx Schedulers.\nThis makes it impossible (or unnecessary hard) to mock the dependencies used by the ViewModel.\n\n## Decision\n,"The creation of a UseCase instance is moved to a factory class. The factory itself is passed to every ViewModel class (using an interface for inversion of control).\nFor unit testing we simply can mock the factory.\nAs the factory only holds the creation of a specific UseCase, it should be enough to only create a single factory for all UseCases.\nIn the future it could be necessary to create additional factories if a single one is polluted to much.\nThe Schedulers are also passed as parameter to be able to mock them too.\n","**Decision:** Introduce a custom UseCaseProvider that allows us to inject a mock.\n\n**Rationale:**\n\n* This approach allows us to cleanly separate the unit testing setup from the production code.\n* It provides a consistent and flexible way to mock dependencies for testing purposes.\n* It minimizes the need to modify production code or create convoluted mocks to accommodate unit testing.\n* By using a custom UseCaseProvider, we can inject mocks into the ViewModel during testing, allowing us to isolate and test the ViewModel's behavior without worrying about the actual use cases."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter saw the results of ADR 0002 (update stream mess) and read some blog posts regarding ""redux like"" Android apps,\nwe should move back to exposing whole state objects from the ViewModel.\n\n## Decision\n",ViewModels exposing a single state object (sealed class) which is handled by the fragment.\n,"**Decision:** Expose whole state objects from the ViewModel.\n\n**Rationale:**\n\n* The results of ADR 0002 (update stream mess) showed that the current approach of exposing individual state properties from the ViewModel is not optimal for managing complex state.\n* ""Redux like"" Android apps provide a more efficient and maintainable way to manage state by exposing whole state objects.\n* Exposing whole state objects from the ViewModel allows for a more concise and declarative way to update and observe state changes."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose main language for the app.\n\n## Decision\n,"We use Kotlin because it is the main language in JetBrains. When the summer practice is ended, this will simplify maintenance and contributions from other JetBrains employees.\n","**Decision:** Adopt Java as the main language for the app.\n\n**Rationale:**\n\n* **Java is widely used and well-supported:** Java is a mature language with a large community, extensive libraries, and tool support, making it a reliable choice for building scalable and complex applications.\n* **Cross-platform compatibility:** Java's ""write once, run anywhere"" principle allows code to be run on a wide range of platforms, providing flexibility and portability for the app.\n* **Strong performance:** Java's Just-In-Time compiler (JIT) optimizes code during execution, resulting in good performance and responsiveness for the app.\n* **Large talent pool:** Java has a substantial pool of developers, making it easier to find and retain qualified engineers to maintain and extend the app.\n* **Compatibility with existing infrastructure:** Java integrates well with commonly used frameworks and tools, such as Spring, Hibernate, and AWS services, reducing complexity and ensuring seamless integration with the app's existing ecosystem."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose how we will deploy our app.\n\n## Decision\n,We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\n,"Deploy the application using AWS EC2 (Elastic Compute Cloud), a web service that provides secure and resizable compute capacity in the cloud.\n\n## Rationale\nWe chose EC2 because it offers several advantages for our application:\n\n- **Scalability:** EC2 allows us to scale our application up or down as needed, ensuring that it can handle fluctuating traffic and demand without compromising performance.\n- **Reliability:** EC2 provides high availability and fault tolerance, ensuring that our application remains online and accessible even in the event of hardware failures or network issues.\n- **Security:** EC2 offers robust security features, such as firewalls, access control lists, and encryption, to protect our application and data from unauthorized access and cyber threats.\n- **Cost-effectiveness:** EC2 offers a pay-as-you-go pricing model, which allows us to optimize our cloud spending by only paying for the resources we consume.\n- **Easy integration:** EC2 seamlessly integrates with other AWS services, such as Amazon S3 (Simple Storage Service), Amazon RDS (Relational Database Service), and Amazon CloudWatch, enabling us to build a comprehensive and scalable infrastructure for our application."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose a frontend framework.\n\n## Decision\n,"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\n","**Decision:** Use React as the frontend framework.\n\n**Rationale:**\n\n* **Popularity and Community Support:** React is one of the most popular and widely used frontend frameworks, with a large and active community. This provides access to a vast ecosystem of libraries, resources, and support.\n* **Component-Based Architecture:** React's component-based architecture allows for easy maintainability, reusability, and code organization.\n* **Virtual DOM:** React optimizes rendering performance through its virtual DOM, which creates a virtual representation of the UI and efficiently updates it only when necessary.\n* **Performance and Scalability:** React's lightweight and efficient design contributes to high performance and scalability, even for complex applications.\n* **Developer Experience:** React offers a developer-friendly experience with its declarative syntax, hot module reloading, and extensive tooling support.\n* **Alignment with Business Goals:** React aligns well with the business requirements for a performant, user-friendly, and maintainable frontend. It enables rapid development and iterative improvements."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose test framework.\n\n## Decision\n,"We use JUnit because it is a standard framework for JVM, and we don't have any advanced requirements yet.\n","We will use the Pytest framework for testing. Pytest is a widely adopted and well-maintained framework in the Python community, providing a comprehensive set of features and a flexible architecture to cater to various testing needs. It offers clear and concise test syntax, support for fixtures and parametrization, seamless integration with the built-in unittest module, and a vast ecosystem of plugins and extensions to enhance testing capabilities. Pytest's focus on code coverage and rich reporting tools enables efficient debugging and maintenance. Its active community and extensive documentation provide valuable resources for developers. By leveraging Pytest, we can ensure a robust and maintainable testing framework that aligns with best practices and allows us to effectively test our codebase."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose how we will build our app.\n\n## Decision\n,"We use Gradle because it is very flexible, has some plugins for Docker integration and @mkuzmin know this technology very well.\n",Use a microservices architecture.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose a web framework for the app.\n\n## Decision\n,"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\n","**Decision:** Use Django as the web framework for the app.\n\n**Rationale:**\n\n* Django is a popular and well-established web framework with a large community and extensive documentation.\n* It provides a full-stack development framework that includes features such as a built-in ORM, authentication and authorization, and template engine.\n* Django is known for its ease of use and rapid development capabilities, making it suitable for quick and efficient development of web applications.\n* The framework offers strong security features and follows industry best practices, ensuring the security of the application.\n* Django's modular architecture allows for easy customization and integration with external services and components.\n* It has a large ecosystem of third-party libraries and extensions, providing additional functionality and flexibility.\n\n**Alternatives considered:**\n\n* Flask: A lightweight and flexible microframework suitable for small and simple web applications.\n* Node.js + Express: A popular JavaScript-based framework for building fast and scalable web applications.\n* Spring Boot: A Java-based framework used for building robust and enterprise-grade web applications.\n\n**Additional Considerations:**\n\n* The team's familiarity with Django and its features.\n* The scale and complexity of the planned web application.\n* The need for specific features or integrations supported by Django."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision: Implement an Architectural Decision Record (ADR) Framework**\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to document and share architectural decisions.\n* They improve communication and understanding among stakeholders, especially when working on complex or distributed systems.\n* ADRs allow teams to track and refer back to architectural choices, reducing the risk of inconsistent or uninformed decisions in the future.\n\n**Implementation Plan:**\n\n* Establish a template and process for creating ADRs.\n* Define a workflow for reviewing, approving, and publishing ADRs.\n* Integrate ADRs into the project's documentation and knowledge management systems.\n* Provide training and support to team members on the use of ADRs.\n\n**Consequences:**\n\n* **Positive:**\n    * Improved decision-making and communication.\n    * Reduced ambiguity and inconsistency in architectural design.\n    * Enhanced ability to track and justify design choices.\n* **Negative:**\n    * Potential overhead in creating and maintaining ADRs.\n    * Requires discipline and commitment from team members to consistently document decisions."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose view representation\n\n## Decision\n,We use Thymeleaf because it provides:\n- clean HTML view\n- templates that can be run without server\n,"**Decision:** To use the **Model-View-ViewModel (MVVM)** view representation for the application.\n\n**Rationale:**\n\n* **Separation of Concerns:** MVVM cleanly separates the data model, user interface, and view logic, making the codebase more maintainable and testable.\n* **Data Binding:** MVVM supports data binding, which simplifies data updates in the UI and reduces boilerplate code.\n* **Testability:** MVVM enables unit testing of view models and view logic independently from the user interface.\n* **Extensibility:** MVVM provides a flexible framework that can be extended to support custom view types and features.\n* **Compatibility:** MVVM is widely supported by popular development frameworks and tools, ensuring availability of resources and community support."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n\n## Decision\n,Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\n,Implement automated tools to detect and report inefficient database queries.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* Case management system requires secure cloud storage to store case related documents.\n* GOV.UK  Ruby applications often use [Amazon S3 storage and Rails Active Storage](https://docs.publishing.service.gov.uk/manual/conventions-for-rails-applications.html#header) together and as it is preferred technical choice.\n* A private aws-s3-bucket would be provisioned on Gov PaaS and only accessible via rails app  bind with the service.\n\n## Decision\n,* Use Amazon S3 storage service on Gov PaaS and Rails Active Storage\n,**Decision**: Use Amazon S3 storage and Rails Active Storage to store case related documents securely.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDocumenting changes for a release can be challenging. It often involves reading\nback through commit messages and PRs, looking for and classifying changes, which\nis a time consuming and error prone process.\n\n## Decision\n","We will use a changelog (`CHANGELOG.md`) in the\n[Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be\nupdated when code changes happen, rather than at release time.\n",**Decision:**\n\nUse a release management tool with a changelog feature to automate the process of documenting changes for a release.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\n\n## Decision\n",We will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\n,Use a linter to enforce a consistent coding style throughout the project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe early beta was originally hosted on dxw's Heroku to get delivering quickly whilst access to GPaaS could be set up.\nAccess to GPaaS with approved billing has now been confirmed so we are migrating the service to its longer term home.\n\n## Decision\n,- Move the service from Heroku to GPaaS for all environments except the ephemeral pull request review environments.\n- Use Terraform to define the infrastructure as code.\n,"**Migrate the service to GPaaS.**\n\n**Rationale:**\n\n* GPaaS is the approved and preferred hosting solution for government services.\n* GPaaS provides a more secure and reliable hosting environment.\n* GPaaS offers greater scalability and flexibility.\n* Migrating the service to GPaaS will allow it to benefit from these advantages.\n\n**Consequences:**\n\n* The service will be unavailable during the migration process.\n* The migration will require some downtime for the service.\n* The migration will require some additional configuration and testing.\n\n**Alternatives:**\n\n* **Continue hosting the service on Heroku.** This would allow the service to continue running without any downtime. However, it would not provide the same level of security, reliability, scalability, and flexibility as GPaaS.\n* **Host the service on an alternative cloud provider.** This would provide a similar level of security, reliability, scalability, and flexibility as GPaaS. However, it would require more configuration and testing, and it may not be as cost-effective as GPaaS."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\n\n## Decision\n,Use Rollbar to collect and manage our application errors.\n,"Continue using Rollbar for error capture and management. Monitor the usage of other error capture tools and evaluate their potential benefits against the costs of maintaining multiple tools. Ensure that error capture tool configuration (e.g., ROLLBAR_ENV) is explicit and well-documented to avoid confusion."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\nWe want a tool that provides an open and modern security standard.\n\n## Decision\n,We are going to use DSI as our single sign-on provider using the OIDC standard.\n,Use DfE Sign-in (DSI).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCodeClimate is an automated code review tool that looks at test coverage, code smell and tech debt.\n\n## Decision\n",We will use CodeClimate as an additional level of quality assurance on our pull requests.\n,Use CodeClimate to improve code quality and reduce tech debt.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAccessing ENV directly without a wrapper is limited and can introduce problems.\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https://github.com/laserlemon/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(""1234"")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\n\n## Decision\n",Use DotEnv to load our environment variables.\n,"Use dotenv for loading environment variables from a YAML file. dotenv will check that all required environment variables are set, and will raise an error if any are missing. dotenv also provides a way to stub environment variables in tests, using the `dotenv-rails` gem."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\n\n## Decision\n,We will use [accessible-autocomplete](https://github.com/alphagov/accessible-autocomplete) to provide the autocomplete capability in our pages.\nThis package has been chosen because accessibility has been carefully considered when developing the package.\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\nand not be jarring to the user.\n,"Implement an autocomplete feature using a third-party library, such as Algolia or ElasticSearch."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGov.uk projects should maintain consistent [code formatting](https://gds-way.cloudapps.digital/manuals/programming-languages/ruby.html#code-formatting)\n\n## Decision\n,"Change from [Standard.rb](https://github.com/testdouble/standard), which is a wrapper around [Rubocop](),\nto the [GovUK maintained version](https://github.com/alphagov/rubocop-govuk).\n",Use the Code Climate gem for automated code formatting.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n- We need to keep a record of activities taken by users so that we can gather quantitative research data\n- We need to avoid third-party tracking services, since these carry with them privacy concerns, lack of clarity around retention rules, additional user consent, and the need for additional approvals\n\n## Decision\n",Store all records of activities taken by users in-app.\n,Implement a first-party Activity Tracking System using an open-source tool like Plausible Analytics or Matomo.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to keep our test coverage as high as possible without having to run\nmanual checks as these take time and are easy to forget.\n\n## Decision\n,Use Simplecov with RSpec to monitor coverage changes on every test run\n,"**Automate test coverage monitoring.**\n\nThis decision ensures that test coverage is monitored continuously, without the need for manual checks. By using an automated tool, we can set a target coverage percentage and receive alerts when coverage falls below that threshold. This helps us to quickly identify and address any potential gaps in our test coverage."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe service needs a way to build resilience against the external Contentful API becoming unexpectedly unavailable. This service is a single point of failure.\ndxw use often use Rails and Redis together and recommend it as a technical choice.\n\n## Decision\n,Add and use Redis for a read cache.\n,"**Use Rails and Redis together to implement a caching layer.**\n\nThis solution is recommended by dxw because it is a well-established and reliable combination for building resilient systems. Rails can be used to handle requests and manage the caching logic, while Redis can be used as a fast and reliable data store for cached content.\n\nThis solution will allow the service to continue to serve content even if the Contentful API becomes unavailable, which will improve the user experience and reduce the risk of outages or data loss."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way for the service to automatically and regularly run a task.\nWe already have Redis available as our caching layer and Sidekiq works great with it.\nAn alternative could be to use Cron for scheduled tasks and a postgres backed asynchronous jobs, perhaps even run inline. We know how to get Sidekiq running with Docker and reusing Redis (rather than Postgres) for job data that is ephemeral feels a better fit given we already have Redis.\n\n## Decision\n",Use Sidekiq for processing asynchronous tasks.\n,"**Decision:** Use Sidekiq with Redis for scheduled tasks.\n\n**Rationale:**\n\n* Sidekiq integrates well with Redis, which is already available as the caching layer.\n* Sidekiq is easy to set up and manage with Docker.\n* Storing job data in Redis (which is ephemeral) is a better fit than using Postgres, as it aligns with the transient nature of the jobs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consquences of a series of changes.\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.\n\n## Decision\n",Include a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\n,"The team has decided to adopt a template for GitHub pull requests. This template will provide a consistent structure for pull request descriptions, which will make them easier to review. The template will include the following information:\n\n- A summary of the pull request\n- A description of the changes made\n- A list of the benefits of the changes\n- A list of the risks of the changes\n- A list of the alternatives considered\n- A list of the references used"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt is necessary for the service to confirm various actions with users via email messages.\n\n## Decision\n,"Gov.UK Notify is the recommended solution, using the `notifications-ruby-client` library.\n",The service will use Amazon Simple Email Service (SES) to send email messages.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\n\n## Decision\n,Use the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test\n,Integrate a commercial Static Application Security Testing (SAST) tool into the CI/CD pipeline to automatically scan code for vulnerabilities during the build process.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* Several content-only pages are served by our `Pages` table;\n* `Pages` within the app mirror `Pages` within Contenful;\n* This allows content designers (via Contentful) the ability to edit, add and\ndelete Pages without help from developers.\n\n## Decision\n",* Remove [high_voltage](https://github.com/thoughtbot/high_voltage);\n* Rely on a final generic route that directs to `PagesController`;\n* Use the slug to retrieve the Page from `t.pages`.\n,"**Decision:** Introduce a new `PageContent` table to hold the content of pages.\n\n**Rationale:**\n\n* Separates the content of pages from the structural information stored in the `Pages` table.\n* Allows for easier management and editing of page content by content designers through Contentful.\n* Reduces the dependency on developers for content updates, enabling greater flexibility and responsiveness.\n* Improves scalability and performance by storing content in a separate table, optimizing database operations and reducing data retrieval time."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt is necessary for breadcrumb navigation to be supported and configured in a consistent way across the application\n\n## Decision\n,The recommended solution is to use the `loaf` library - which is already used in several other DfE repositories as the go-to solution for breadcrumb management:\n- https://github.com/DFE-Digital/academy-transfers-frontend\n- https://github.com/DFE-Digital/get-into-teaching-app\n- https://github.com/DFE-Digital/npd-find-and-explore\n,Use the breadcrumb component from the design system. It provides a consistent and accessible way to implement breadcrumb navigation. The component should be used according to the guidelines provided in the design system documentation.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n- Application logs needs to be aggregated and presented back to the dev team to assist in monitoring and debugging of the live service.\n- [DfE Digital have technical guidance expressing a preference for Logit](https://github.com/DFE-Digital/technical-guidance/blob/8380ad9dbfeefaeece081cace9f13e4c36200cd0/source/documentation/guides/default-technology-stack.html.md.erb#L93)\n- We were prompted for a Logit account by DfE when setting up our GPaaS account\n- GPaaS does provide access to logs but it is clumsy to access each environment over by using the CLI\n- dxw have used other logging aggregators in the past such as Papertrail but as DfE have expressed a preference it makes sense to align our technical tooling\n\n## Decision\n,Use Logit\n,To use Logit for application logging for this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* Procurement Operations team uses a shared mailbox hosted in DfE's Exchange Online to communicate with School Buying Professionals.\n* A key requirement from case workers is to record all the email interactions regarding a case in the case management system , so there is a full view of a case and all associated interactions\n\n## Decision\n",* Use Microsoft Graph for to integrate with Microsoft Exchange Online.\n* Use Microsoft Identity [OAuth 2.0 client credentials grant flow](https://github.com/microsoftgraph/microsoft-graph-docs/blob/main/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow) to set up application's permissions for using MS Graph API with restricted permissions only to access shared mailbox.\n* Microsoft Graph is recommended by Microsoft and [Outlook API has been deprecated](https://docs.microsoft.com/en-us/previous-versions/office/office-365-api/api/version-2.0/use-outlook-rest-api)\n,Implement an Outlook Add-in that allows case workers to automatically copy all emails in the shared mailbox to the case management system.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to add new functionality to the project - the ability to create custom NRPE checks.\nThis means having templated configuration for adding service-specific plugins for the purposes of monitoring.\nThe `base` module already contains NRPE specific code.\nIt installs NRPE and ensures it is running. It also configures a set of default monitoring plugins.\n\n## Decision\n,"We considered three possible approaches to adding the new NRPE functionality:\n- Add it to the `base::nrpe`\n- Create a separate `nrpe_custom_checks` module and keep `base` the way it is\n- Create a separate `nrpe_custom_checks` module and refactor `base` to use it for check creation\nRather than extending the base NRPE class, we chose to create a standalone NRPE custom checks module and keep `base` independent.\n","**Decision:** Enhance the `base` module to support custom NRPE checks.\n\n**Rationale:**\n\n* The `base` module already handles NRPE-related configuration and plugins.\n* Moving the custom NRPE check functionality to a separate module would add unnecessary complexity and reduce maintainability.\n* Enhancing the `base` module allows for centralized management and updates of all NRPE-related functionality, including custom checks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","To use an Architectural Decision Record (ADR) template to document architectural decisions. ADRs should include the decision, the context in which it was made, the alternatives considered, and the rationale for the decision."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\nIn order to do that it needs keys and initialization vectors (IV).\nThe problem is how to retrieve or derivate those keys in a safe way and how\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\nat the time of writing).\n\n## Decision\n,"### General Approach\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\nThe PBKDF2 needs an iteration number and a salt in order to work.\nThose values will be stored per Key as MetaKey.\n### Implementation Details\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config/masterChallenge`.\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\n",**Decision:** Use Key derivation function which takes Key or KeySet and returns a proper key or IV\n\n**Rationale:**\n- Compliant with application and key management design\n- Future proof: Supports additional key types\n- Key can be securely deleted after it’s used\n- Supports FIPS compliance
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\nrepresentation?\n\n## Decision\n,Remove files on empty KeySet.\n,The correct persistent representation is an empty string.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nBoth code comments and assertions are unfortunately not very popular.\nA quite efficient way to still get some documentation about the code\nare logging statements. In Elektra they are currently inconsistent\nand unusable. Thus there is an urge for this decision.\n\n## Decision\n,"Provide a Macro\n```c\nELEKTRA_LOG (int module, const char *msg, ...);\n```\nthat calls\n```c\nelektraLog ([as above], const char * function, const char * file,\nconst int line, ...);\n```\nand adds current function, file and line to `elektraLog`'s arguments.\n`elektraLog` is implemented in a separate `log.c` file. If someone\nneeds filtering, logging to different sources or similar, he/she\nsimply modifies `log.c`.\n### Severity\nThe severity passed to `ELEKTRA_LOG_` should be as in syslog's priority,\nexcept the error conditions which are not needed (asserts should be used\nin these situations).\nSo we would have:\n- `ELEKTRA_LOG_WARNING`: warning conditions\n- `ELEKTRA_LOG_NOTICE`: normal, but significant, condition\n- `ELEKTRA_LOG_INFO`: informational message\n- `ELEKTRA_LOG_DEBUG`: debug-level message\n### Modules\nTo add a new module, one simply adds his/her module to `elektramodules.h` via\n`#define`:\n```c\n#define ELEKTRA_MODULE_<NAME> <SEQNUMBER>\n```\nThe module name `<NAME>` shall be consistent with module names used in\n`module:` of `src/error/specification`.\n- assertions\n",**Use structured logging where possible and for structured logging use the existing trace macros.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nValidation plugins operate as independent blackboxes.\nFor every backend each mounted validation plugin iterates\nover the whole keyset, checks every key for its trigger metakey,\nand validates the key.\nCurrently all needed validation plugins need to be specified at\nmount-time - if additional validation is required, the backend\nhas to be remounted with the required plugins and plugin\nconfiguration.\nIf validation of a key fails, each plugin decides on its own\nhow to handle the issue and proceed in ways that might be\ndifferent from what is expected or desired.\n\n## Decision\n",Use a wrapper plugin to iterate over the keyset and delegate the validation\nof each key to the corresponding validation plugin.\n,Architectural decision record to implement a unified validation framework for the backend.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\n\n## Decision\n,"Ignore missing `require`d keys (in help mode), but fail for every other error.\n",Improve error handling in `kdbOpen` and `kdbGet` to allow printing the help message.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nTo develop a [Web UI](https://github.com/ElektraInitiative/libelektra/issues/252),\nwe need to be able to remotely configure Elektra via a network socket.\nThe idea is to use a Pub/Sub concept to synchronize actions which describe\nchanges in the Elektra state.\n\n## Decision\n",REST Api is used.\n,Publish and subscribe over a network socket
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nThe internal iterator inside KeySets seems to cause more problems than it solves.\n\n## Decision\n,- remove all functions related to the internal iterator:\n- ksRewind\n- ksNext\n- ksCurrent\n- ksGetCursor\n- ksSetCursor\n- ksHead\n- ksTail\n- keyRewindMeta\n- keyNextMeta\n- keyCurrentMeta\n- change `ksAtCursor` to `ksAt`\n- add implementation / documentation / tests for the external iterator\n- start using external iterators in new code\n- remove docu about internal cursor from all functions.\n,Eliminate the internal iterator inside KeySets
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently, you can easily come into wrong assumptions\nthat something would work in the specification.\nWe need to find minimal requirements to implement a sane spec plugin.\n\n## Decision\n",- no defaults for `sw/_/key` specifications\n(default will not work for `ksLookup(/sw/sthg/key)`)\n- plugins are not allowed to create keys (may change in future; depends on plugin positions)\nThe spec plugin should yield errors when it detects such situations.\n,"**Decision:** Define a minimal set of requirements for a sane spec plugin.\n\n**Rationale:**\n\n* By defining a minimal set of requirements, we can ensure that all spec plugins implement the same basic functionality.\n* This will help to prevent misunderstandings and errors when using spec plugins.\n* The minimal requirements should include the following:\n    * The plugin must be able to parse a specification file.\n    * The plugin must be able to generate code from a specification file.\n    * The plugin must be able to validate a specification file.\n* By defining a minimal set of requirements, we can ensure that all spec plugins are interoperable.\n* This will make it easier to use multiple spec plugins together."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nTo make storage-plugins suitable for `spec` they need to be able to store\nall the metadata as specified in [METADATA.ini](/doc/METADATA.ini).\nMost file formats do not have support for that.\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\nmetadata from `spec:` might end up in `user:`.\n\n## Decision\n","Do not store metadata unrelated to the configuration file structure in any namespace except in `spec:/`.\n- Trying to store any other metadata in any other namespace leads to an error.\nE.g. `kdb set-meta user:/data metadata_not_suitable_for_storage_plugins something` would fail\n(validated by `spec` plugin).\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\nE.g. `comment` or `order`, might be stored in any namespace.\nSometimes, the same metadata can be used in several namespaces but with different meanings\nand ways of serialization, e.g. `type` and `array`:\n- In `spec:/` the metadata `array=` (empty value) means ""this is an array"".\nIf you give it a value e.g. `array=#4` it means ""this is an array with default size X"" (e.g. `#4` = size 5).\n- In any other namespace `array=` means ""this is an empty array"" and e.g. `array=#4` means ""this is an array with max index #4"".\n`array=#4` is not stored literally but inferred.\n- Either the storage plugin does not support arrays, then the metadata will be discarded\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\n- Or, if the storage plugin supports arrays, the data will be serialized as array\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\nfrom the storage plugin to be validated by `spec`.\nUse different storage plugins, or plugins with different configurations,\nfor the `spec:/` namespace:\n- `ni`\n- TOML with `meta` configuration\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:/`.\n- [Spec Expressiveness](spec_expressiveness.md)\n- [Arrays](array.md)\n",1. We will implement a unified metadata format so that all storage plugins can store and retrieve metadata in the same way.\n2. We will create a metadata service that will be responsible for managing metadata for all storage plugins.\n3. We will use a key-value store to store metadata.\n4. We will use a namespace to organize metadata.\n5. We will use a versioning system to track changes to metadata.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nThe compilation variants of plugins blow up the number of plugins.\nAdditionally there is the concept of default storage + resolver that is\nneeded for bootstrapping plugins.\n\n## Decision\n,Rejected: keep default plugins as-is\n,"* All compile variants of a plugin should be represented by a single class instead of one per variant.\n* A plugin id must be unique, just like a route, thus a plugin cannot have 2 variants that share the same id.\n* The plugin metadata needs to be accessed via deferred lookups.\n* Plugins that can be used without configuration should be able to register themselves.\n* A plugin registration should be a simple method that does not involve any configuration."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nIt can get quite cumbersome to find out about key interrelations like arrays.\n\n## Decision\n,"Do not encode any semantics into the key names.\nAll semantic must be in metadata.\nNevertheless, there are guidelines (without any checks in `keySetBaseName`):\n- `#` is used to indicate that array numbers follow.\n- `®` is used to indicate that some information was encoded in the key name.\nThis is usually only needed internally in storage plugins.\n- The UTF-8 sequence `®elektra` (i.e. the 9-byte sequence `C2 AE 65 6C 65 6B 74 72 61`) is reserved,\nsee key name docu.\nThere are, however, rules and conventions which syntax to use for specific semantics.\nThe `spec` plugin guards these rules.\n- [Arrays](array.md)\n- [Base Names](base_name.md)\n","**Decision:** Implement a feature to allow users to explore key interrelations, such as arrays, in a more user-friendly manner."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nIn some situations a single mountpoint refers to more than one file per namespace:\n- For XDG in the `system` namespace may contain several files (XDG_CONFIG_DIRS).\n- A fallback file if some data cannot be stored in some format (Idea from @kodebach:\nwriting the same content to several files, merging when reading)\n\n## Decision\n","Multiple File Backends are not supported for now in the case of writing files.\nMultiple sources in one namespace only work, if the fallback KeySet is\npart of the mountpoint config. That way any change to the fallback\nKeySet would essentially make the whole thing a different mountpoint\nand thereby invalidate all guarantees.\n",Single mounts that reference multiple files are not allowed.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nAfter deciding how to remotely manage instances and groups of instances\n(clusters) with Elektra Web, there is still the issue of recursively nested\nclusters (clusters of clusters).\n\n## Decision\n",Managing the hierarchy in a single clusterd instance.\n- [Elektra Web Structure decision](elektra_web.md)\n,**Decision:** Only support flat clustering layer.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently we store both the escaped and unescaped name in a Key and in\na way that there cannot be an optimization not to store it (there are\nfunctions handling out the pointer).\n\n## Decision\n,"Remove the escaped name from `struct _Key` and use it only when necessary.\nClarify and reduce [terminology](/doc/help/elektra-glossary.md).\nAPI Changes:\n- `keyName` returns the unescaped name\n(temporary some other name for PR: `keyUnescapedName(Size)`)\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\n- reverse terminology: with ""key name"" we will refer to the unescaped (base) name,\nthe escaped name will be explicitly called ""escaped key name"".\n- escaped name will be only present in\n- `keyNew` (+ arguments for adding key names) [unclear: maybe not needed]\n- `elektraEscapeName` (operating on chars)\n- rename `keyAddName`, e.g. to `keyAddEscapedName`\n- `keyDup(.., int)` with options to filter which parts are copied\n(to allow copy of keys where only the key name is copied)\n",**Decision:** Store the name in the Key as escape-free.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\n- Notification does not happen once after final commit, but for every\nplugin\n- Problems in spec plugin\nThese problems can be traced back to the placement of the plugins.\nWe need to clean up and simplify the placement.\n\n## Decision\n","Have hooks and API specific to the list of global plugins in assumptions.\nThese hooks are not shared, so no `list` plugin is needed.\nInstalled plugins will be used.\n- [Array](array.md)\n- [Ensure](ensure.md)\n","- Move the plugins to the post commit section - this will cause them to run only once after the final commit, instead of for every plugin.\n- Fix the problems in the spec plugin."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nThere is a different behavior of various plugins whether their name is\nabsolute or relative, including:\n1. mounting the same file somewhere else does not work\n2. importing somewhere else (other than from where it was exported) does not work\n(See [here](https://github.com/ElektraInitiative/libelektra/issues/51))\n\n## Decision\n",Key names shall be relative to parent Key name\nNone\n,"All plugins must use an absolute name, with a leading `/`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nOnly libelektra-core is supposed to access private data but this contradicts the goal to keep the library minimal.\n`kdbprivate.h` was too generic, it contained many other parts next to the struct definitions of Key/KeySet.\n\n## Decision\n","Also allow `libelektra-operations` library to access private Key/KeySet.\nPut struct definitions of Key/KeySet in a separate header file, which gets\nincluded by parts that need it\n- none\n",Move the public definitions of the Key/KeySet structs to `kdb.h` and keep the private ones in `kdbprivate.h`.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nVendors (distributors, administrators) might want to modify the specification.\ngsettings has a similar feature.\n\n## Decision\n","As found out during implementation of [specload](/src/plugins/specload), only a very limited subset can be modified safely, e.g.:\n- add/edit/remove `description`, `opt/help` and `comment`\n",Allow vendors to specify an override gsettings override via the registry in a similar way to how default gsettings overrides work.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nSome plugins need to communicate more data than is possible to do with metadata.\nThis can limit the functionality of plugins, which need to exchange binary or very\ncomplex data.\n\n## Decision\n","To make the communication between plugins easier, plugins will additionally\nget a handle to a global keyset via `elektraPluginGetGlobalKeySet()`.\nThe global keyset is tied to a KDB handle, initialized on `kdbOpen()`\nand deleted on `kdbClose()`.\nThe global keyset handle is initialized and accessible for all plugins except\nmanually created plugins (by calling e.g. `elektraPluginOpen()`).\nThis decision removes the need to exchange information between plugins\nvia the parentKey.\n",Create a thread safe shared memory area for communication between plugins.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nWhen doing kdbGet() possible more keys are returned which might be\nconfusing. When doing a second kdbGet() with a new keyset\nno keys might be returned, because it is up-to-date.\nWhen doing kdbSet() a deep duplication is needed.\nIdea: keep a duplicated keyset internally. Return (non-deep?)\nduplications exactly of the parts as requested.\n\n## Decision\n",Not yet decided.\n- [Global Validation](global_validation.md)\n,"Maintain a global keyset internally. When doing kdbGet() and kdbSet(), the given keyset is used directly. When doing kdbGet() with a new keyset, the global keyset is updated with the union of the global keyset and the new keyset. The result is an exact duplicate of the parts in the global keyset as requested."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nSome plugins are generic in the sense that they cannot fully\ndefine their contract statically.\n(Note: if they can decide it statically, you should prefer\n[compilation variants](/doc/tutorials/compilation-variants.md).)\nInstead their contract is based on their\nconfiguration. We will call every combination of plugins+configuration\nwhere we get a different contract **plugin variant**.\nThe current issue is that there is no way to enumerate\npossible plugin variants as needed to list all functionality\nof Elektra.\ncan enumerate all possible essential configurations. Problems here\nare:\n- Plugins might need to self-reference (a validation plugin\nmight have plugin variants, too)\n- Elektra’s specification language was not complete nor consistent at point\nof writing\n- Does not fit with the `checkconf` ([see here](https://git.libelektra.org/issues/559))\napproach.\n\n## Decision\n","Implementation delayed after 1.0.\nBest implementation candidate was:\n1. Provide a function `int genconf (KeySet * ks, Key * errorKey)` where `ks`\nis filled with a list of all variants with the essential configuration (subkeys `config`)\nand the changed parts of the contract (subkeys `infos`).\n2. Keys defined in `system:/elektra/plugins/<plugin>/variants/<variantname>` have the same content,\nbut take precedence. If a variant with the same name is defined, only `config` or `infos`\nfrom `genconf` are considered if they are not mentioned in `system:/elektra/plugins/variants`.\n3. If the bool key `override` (for a plugin or a variant) is true, it will be overwritten (content\nof `genconf` ignored, but instead a plugin or variant as given is created).\n4. If the bool key `disable` (for a plugin or a variant) is true the plugin or a variant of the\nplugin will not be available.\n5. Empty `config` and `infos` mean:\n- `config`: The ""default variant"" (without any parameter) should be also available\n(has useful functionality)\n- `infos`: It is not possible to determine the changes of the contract,\nthe user need to instantiate the plugin and enquiry the contract.\n### Example\n`genconf` for augeas yields:\n```\nsystem:/access\nsystem:/access/config\nsystem:/access/config/lens = Access.lns\nsystem:/access/infos\nsystem:/access/infos/provides = storage/access\nsystem:/aliases\nsystem:/aliases/config\nsystem:/aliases/config/lens = Aliases.lns\nsystem:/aliases/infos\nsystem:/aliases/infos/provides = storage/aliases\n```\n`genconf` for python might yield:\n```\nuser:/configparser/config\nuser:/configparser/config/script = python_configparser.py\n```\nThe user:/admin specifies:\n```\nsystem:/elektra/plugins/jni/disable = 1\nsystem:/elektra/plugins/augeas/variants/access\nsystem:/elektra/plugins/augeas/variants/access/disable = 1\nsystem:/elektra/plugins/augeas/variants/aliases\nsystem:/elektra/plugins/augeas/variants/aliases/infos\nsystem:/elektra/plugins/augeas/variants/aliases/infos/status = 10000\nsystem:/elektra/plugins/python/variants/configparser\nsystem:/elektra/plugins/python/variants/configparser/override = 1\nsystem:/elektra/plugins/python/variants/configparser/config\nsystem:/elektra/plugins/python/variants/configparser/config/script = mybetter_configparser.py\n```\nAs result we get:\n1. `access` of plugin `augeas` is not available\n2. `aliases` of plugin `augeas` as defined from `genconf`, but with changes in contract (`infos/status`)\n3. `configparser` of plugin `python` is completely redefined (result from `genconf` will not be considered)\nbut it will be considered as specified.\n4. the plugin `jni` will not be available\nTo have a space-separated simpleini one would use:\n```\nsystem:/elektra/plugins/simpleini/variants/spacesep\nsystem:/elektra/plugins/simpleini/variants/spacesep/config\nsystem:/elektra/plugins/simpleini/variants/spacesep/config/format = ""% %""\n```\n- [CMake Plugins](cmake_plugins.md)\n","We allow to specify a series of plugin variants in a structured\nformat: a plugin spec.\nThe plugin spec is a structured description of a plugin variant that\ncontains:\n- The plugin path\n- The required configuration\n- The inferred contract\nThe plugin spec is stored as a JSON file.\nTo list all supported plugin variants of an installation, you\ncan issue the `list` command in the plugin spec repository."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nMemory Consumption in Elektra is quite high as the key names\nare long and stored twice in memory.\n\n## Decision\n,"Only store the unescaped key name, suitable for comparing/searching/iterating over name, i.e.:\n- Remove the escaped name from `struct _Key` and use it only when necessary.\n- Clarify and reduce [terminology](/doc/help/elektra-glossary.md).\n- API Changes:\n- `keyNew (const char*, size_t)`\n- `keyName` returns the unescaped name\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\n- reverse terminology: with ""key name"" we will refer to the unescaped (base) name,\nthe escaped name will be explicitly called ""escaped key name"".\n- escaped name will be outside the core for tooling\n- `keyDup(.., int)` with options to filter which parts are copied\n(to allow copy of keys where only the key name is copied)\n",**Decision:** Remove duplicate key names from memory.\n\n**Rationale:**\n\n* Storing key names twice in memory is unnecessary and inefficient.\n* Removing the duplicate names will significantly reduce the memory consumption of Elektra.\n* This change will not affect the functionality of Elektra.\n\n**Consequences:**\n\n* Memory consumption in Elektra will be reduced.\n* Elektra will be more efficient.\n* There will be no functional impact on Elektra.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nThere are ambiguous cases where the same return value can refer to multiple problems:\n- name modifications which can be either invalid name or locking the key name\n- getting values of (non-)binary keys\n\n## Decision\n,"- Update documentation in `doc/dev/error-*` and link to them in the documentation\nfor the module `kdb`\n- Add second channel for getting information about errors\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\n- Harmonize return values from all functions and move error reporting to second channel\n- Binary metadata vs flag #4194\n",Use a `Status` object to return a code and a message.\n\nThe code should be an enum with a value for each possible error.\n\nThe message should be a human-readable description of the error.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nInconsistent use of booleans in various parts of Elektra.\n\n## Decision\n,"Only the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\nEverything else should lead to errors in checkers (in `kdbSet`).\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\nExample for an implementation in C in an application:\n```c\nif (k != NULL && strcmp(keyString(k), ""1"") == 0) {/*true*/} else {/*false*/}\n```\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\nThe type checker plugin should allow\n- non-presence\n- the string ""0""\n- the string ""1""\n","Use a new data type for booleans, such as `bool` or `tribool`, that can represent three states: true, false, and undefined."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nWriting portable shell code for testing command-line tools is difficult.\n\n## Decision\n,Develop [shell recorder](/tests/shell/shell_recorder) and [tutorial wrapper](/tests/shell/shell_recorder/tutorial_wrapper).\n,Use a cross-platform wrapper for bindings.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nWhen using keyString() on empty / binary values the return values are the literal strings (null) / (binary). This seems very awkward and unintuitive from a user's perspective.\n\n## Decision\n,"- `key == NULL` return 0, error code via second channel\n- `key->value == NULL` return 0, error code via second channel\n- `key == <binary>` return 0, error code via second channel\n- everything else as is\n","**Decision:** Introduce a helper function `emptyString()` which returns a more informative string when used on empty values, change `keyString()` return value to `null` for empty values, and deprecate `keyString()` for binary values."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently, the default backend (default.ecf) will also be used for bootstrapping. There are two problems with this approach:\n1. Thus the default backend first will be read with parentKey `system:/elektra` and later with parentKey `system:/`, it needs to store absolute paths and thus won't work with most of the plugins (except dump).\n2. When `system:` is large without mount points, everything is reread twice during bootstrapping.\n\n## Decision\n",Split up the concepts of default (`default.ecf`) and bootstrap (`elektra.ecf`) backend.\nDuring bootstrap only `elektra.ecf` is read.\nThe default backend reading `default.ecf` is only relevant as long as no root backend is mounted.\nAlgorithm:\n1. try to get system:/elektra using the file elektra.ecf (KDB_DB_INIT)\n2. mount the init backend to system:/elektra\n,"**Decision:**\n\n**Introduce a dedicated bootstrap backend that is used during bootstrapping only.**\n\n**Rationale:**\n\n* **Reduced overhead:** By introducing a dedicated bootstrap backend, we avoid the need to reread the default backend multiple times during bootstrapping, reducing overhead.\n* **Compatibility with plugins:** The dedicated bootstrap backend can store absolute paths, allowing it to be compatible with most plugins (except dump).\n* **Reduced complexity:** Separating the bootstrap process from the normal operation of the default backend simplifies the codebase and reduces the risk of errors."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nA key name is made out of a sequence of key part names, and can be constructed with `keyAddBaseName/keySetBaseName`.\nBoth applications and configuration file formats might need arbitrary strings to be encoded within a key name part.\nFor example:\n- an application uses names of internal components as sections within the configuration.\n- a parser reads an empty string, to be encoded as base name.\n\n## Decision\n","`keyAddBaseName/keySetBaseName` never fail with any argument, so any character sequence can be escaped except of NULL bytes.\nThe argument goes unmodified to the unescaped key name.\nFor arrays there is no escaping needed because an array is only an array if the metadata `array` is appended to the direct parent key.\nSee [array](array.md).\n- [Array](array.md)\n","**Encode arbitrary strings as hex-encoded base 64 strings.**\n\nThis should work for any arbitrary string character set, including empty strings."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nA standard way of describing REST APIs offered by tools and plugins for Elektra is required to ease development for and usage of these. Because many good standards for describing APIs are out there already, an existing one shall be used.\n\n## Decision\n",The decision is to use [API blueprints](https://apiblueprint.org/) together with additional tools from its ecosystem.\n,"**Preferred Solution:**\n \nThe OpenAPI Specification (OAS), previously known as Swagger, should be adopted for describing REST APIs of Elektra tools and plugins.\n\n**Alternatives Considered:**\n \n* **RAML:** RAML is another popular API description language. However, OAS is more widely used and has a larger community of support.\n* **API Blueprint:** API Blueprint is a relatively new API description language that is gaining popularity. However, it is not as well-established as OAS.\n* **Custom Solution:** We could develop our own custom API description language. However, this would be a significant undertaking and would not provide the same level of compatibility with other tools and frameworks as using an existing standard.\n\n**Impact of Decision:** \n\nAdopting the OAS will have the following benefits:\n\n* It will provide a consistent way of describing REST APIs for Elektra tools and plugins.\n* It will make it easier to develop and use these APIs.\n* It will improve the compatibility of these APIs with other tools and frameworks.\n\n**Risks:** \n\nThere are some risks associated with adopting the OAS, including:\n\n* The OAS is a complex specification and it may take some time to learn how to use it effectively.\n* The OAS is not as well-suited for describing some types of APIs, such as those that use a message-based architecture.\n\n**Mitigation:** \n\nThe risks associated with adopting the OAS can be mitigated by:\n\n* Providing training and documentation on how to use the OAS.\n* Using tools and frameworks that support the OAS.\n* Avoiding the use of the OAS for APIs that are not well-suited for it."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nToo verbose error message\nCurrently for every error, 9 lines are shown in which most of them are not relevant to end users/administrators. One goal\nis to reduce the verbosity of such messages and let users/administrators see only information they need.\n\n## Decision\n","The error message has the current format:\n```\nThe command kdb set failed while accessing the key database with the info:\nSorry, the error (#121) occurred ;(\nDescription: validation failed\nReason: Validation of key ""<key>"" with string ""<value>"" failed.\nIngroup: plugin\nModule: enum\nAt: ....../src/plugins/enum/enum.c:218\nMountpoint: <parentKey>\nConfigfile: ...../<file>.25676:1549919217.284067.tmp\n```\nThe new default message will look like this:\n```\nSorry, module `MODULE` issued [error|warning] `NR`:\n`ERROR_CODE_DESCRIPTION`: Validation of key ""<key>"" with string ""<value>"" failed.\n```\nThe `NR` will be the color red in case of an error or yellow in case of a warning\nwhile `MODULE` will be the color blue.\nOptionally a third line indicating a solution can be added. Eg. for a permission related error there would be a third line:\n```\nPossible Solution: Retry the command as sudo (sudo !!)\n```\nTo avoid losing information, the user can use the command line argument `-v` (verbose) to show\n`Mountpoint`, `Configfile` in addition to the current error message.\nFurthermore a developer can use the command line argument `-d` (debug)\nto show `At` for debugging purposes.\n- [Error Codes](error_codes.md)\nShows how the new error codes are meant to be\n",Implement a new error handling mechanism that includes the following features:\n- Reduce the number of lines shown in the error message to only the relevant information.\n- Provide a clear and concise description of the error.\n- Include any relevant context information that may be helpful for troubleshooting.\n- Make the error messages consistent across all components of the system.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nWhen `kdbSet()` is called, plugins implementing the commit role need to\ntrack their state to distinguish between carrying out that role and\ncarrying out potential other roles (commit and setresolver for the\nresolver plugin, for example). This limits the possibilities of plugin\nreuse and the ways plugins can be combined.\n\n## Decision\n","Committing will no longer be done by `kdbSet()`. Instead, the functionality\nwill be implemented by its own function, `kdbCommit()`.\n",**Decision:** Plugins should only have a single role.\n\n**Consequences:**\n\n* **Pros:**\n    * Simplifies plugin implementation by removing the need to track state.\n    * Allows for more flexible plugin reuse and combination.\n* **Cons:**\n    * May require some existing plugins to be refactored.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\n- KeySet might get modified on access (hash rebuilds)\n- Expectation that already all keys are there after `kdbGet()`\n- No default value calculation\n\n## Decision\n,- spec-plugin does a lookup for values (Maybe also resolving missing fallback/override links?)\n,"Use KeySet without default value calculation, but check if the passed KeySet is empty."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nWhile we have a classification of errors and warnings, it remains\nunclear when plugins actually should emit errors and warnings.\n\n## Decision\n","Provide guidelines in the form as tutorials, covering:\n- prefer errors to warnings\n- that any not understood metadata (e.g. types), should lead to an error\n- that wrong specifications, like `kdb meta-set /tests/ipaddr/ipv4 check/ipaddr ipv8` should be rejected\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\n(e.g. only `0` or `1` as boolean)\n- anything else that is beyond the capabilities of a plugin (not implemented),\nshould lead to an error\nViolations against these guidelines can be reported as bug and then either:\n- the bug gets fixed\n- the plugin get a worse `infos/status` but still get shipped with 1.0\n- the plugin gets removed\n- [Metadata in Spec Namespace](spec_metadata.md)\n- [Capabilities](capabilities.md)\n- [Boolean](boolean.md)\n","**Decision:** Create a consistent guideline for plugin authors to determine when to emit errors and warnings.\n\n**Rationale:**\n\n* A clear guideline ensures consistency across plugins and avoids confusion for developers.\n* By defining specific criteria for errors and warnings, plugins can provide more actionable feedback to users.\n* Establish a severity level for each type of message to help users prioritize and address issues effectively."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently multiple warnings are saved in an elektra non-conforming array\nnotation which is limited to 100 entries. The notation of `#00` is against\nthe design [decision made](array.md).\n\n## Decision\n,"The format should be aligned with the correct array notation,\nstarting with `#0`. The maximum number of warnings will stay at\n100 entries (`#0` - `#_99`).\n- [Array](array.md)\n",Change elektra array type to a read only const array and move writeable array to plugin config
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nThe previous unit testing framework started as hack to have a bit more\nthan simple asserts. It is not easy to use (needs explicit enumeration\nof all test cases) and lacks really important features (e.g. output of\nthe assertion that failed).\n\n## Decision\n,"- Keep C framework for C tests and ABI tests\n- Google Unit testing framework `gtest` with code downloaded by CMake for\nsystems where no source is packaged (Debian Wheezy, Arch Linux,\nFedora,...) for C++ tests\n- [Script Testing](script_testing.md)\n",**Decision:** Implement a new unit testing framework.\n\n**Rationale:**\n\nThe current unit testing framework is not easy to use and lacks important features. Implementing a new framework will address these issues and provide a more robust and user-friendly solution.\n\n**Consequences:**\n\n* **Positive:**\n    * Improved testing capabilities and reliability\n    * Easier to write and maintain tests\n* **Negative:**\n    * Additional development time required to implement the new framework
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nApplications want to ensure that some functionality (=global plugin)\nis present in Elektra.\n\n## Decision\n,"Integrate `kdbEnsure` in `kdbOpen(Key *errorKey, KeySet *contract)` but only allow global plugins.\n- [Global Plugins](global_plugins.md)\n",Introduce a simple driver that loads the global plugin into any application that uses it.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\nof instances (clusters). The remote configuration of a single instance is\nsimple. However, to manage multiple instances, we need to store the information\nto access the daemons, as well as information about the grouping (clusters) of\ndaemons.\n\n## Decision\n",Use one cluster daemon (clusterd) to manage all clusters and instances.\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\n,"**Implement a way to remotely manage instances and groups of instances (clusters) by building a generic cluster management component that can be used for any daemon. This component will store the information to access the daemons, as well as information about the grouping (clusters) of daemons.**"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nConfig files ideally do not copy any structure if they only want to\nset a single key.\n\n## Decision\n,"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\nIf the underlying format does not support it and there is also not an obvious\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\nvalues in non-leaves can be supported with key names starting with ®elektra.\n",Create a separate config file when overriding a single key.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently, an error or warning message in elektra causes the following line to be shown:\n```\nIngroup: <group>\n```\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\n\n## Decision\n",The `ingroup` message will be removed as it does not yield any notable benefit.\nSee [Error concept](error_codes.md)\n,Remove the message.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\n- locking is not reset when ref counting again gets 0 (adding to keyset and\npop again) #2202\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\nfor KeySets (also suggested in #1332)\n\n## Decision\n,"- add second counter to Key\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\n- return error on reference count overflow\n- no error on underflow (decrement when zero), instead stay at zero\n- use fixed sized types (`uint16_t`) for reference counters\n- increment/decrement references before/after passing instances to plugins\n","- Introduce `HasKeys()` method to KeySet and refactor KeySet#Add() and Remove() to call this method (to fix the locking issue).\n- Keep KeySet using reference counting, use Handle() method in C++ API."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nOn structures like maps or [arrays](array.md) there are different\npossibilities which keys are looked up in the KeySet and which\nare simply iterated.\nWithout any guidelines, applications would provide arbitrary inconsistent\nbehavior.\n\n## Decision\n","Every key that an application wants to use, must be looked up\nwith `ksLookup` using a cascading lookup key.\n- [Arrays](array.md)\n",### Decision\n<li>Provide a general way how to configure the lookup/iteration behavior on a per-keyset basis.</li>\n<li>Define the KeySet interface with a `lookup` and an `iterate` method.</li>\n<li>Introduce a `KeySetMapper` that maps the results of the `iterate` method to an array.\n<li>Introduce `KeySets` class which is a collection of keysets and provides a single `iterate`\nand `lookup` method. These iterate over/lookup in the underlying key set and call the\n`iterate` or `lookup` method on the `Mapper`.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\n- Plugin names and plugin folders not always exactly\nmatch (`resolver_*`, `crypto_*`)\n- plugin should be able to register new variants\n- there should be only one place to define a new plugin\n- Multiple categories should be possible per plugin,\ndefined in README.md\n- On some OS some plugins won't work (simpleini)\n- Some unit tests depend on bindings\n\n## Decision\n","Introduce a CMake process where all plugins are processed three times.\nFollowing CMake variables are used for the phases:\n- `COLLECTION_PHASE` .. collect all `add_plugins`\n- `DEPENDENCY_PHASE` .. resolve all dependencies, do `add_plugins` again\n- `ADDTESTING_PHASE` .. (reserve for potential 3rd phase)\n1. Collection phase (`COLLECTION_PHASE` is `ON`),\nadd_plugin internally builds up:\n- `ADDED_PLUGINS`\n- `REMOVED_PLUGINS`\n- `ADDED_DIRECTORIES`\n2. assemble dependency phase (`DEPENDENCY_PHASE` is `ON`, only considering `ADDED_DIRECTORIES`),\nwith:\n- `find_libraries`, actually search for libraries on the system\n(only relevant libraries of plugins that are considered for inclusion)\n- `add_plugin`, with _actually adding_ the plugins\n3. assemble all unit tests (`ADDTESTING_PHASE` is `ON`), either\n- with `ADD_TEST` in `add_plugin`, or\n- with `add_plugintest` (for unittests that have dependencies to bindings)\n- [Plugin Variants](plugin_variants.md)\n","Define plugin definition classes, such as ResolversPlugin, and put definitions in the function that defines the category:\n\n\n```python\nfrom . import util\nfrom ..util import registry\nfrom ._resolvers import *\n\nutil.registry.PluginCategories.FORMATS.register(ResolversPlugin)\n```"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nProjects usually do not want to use low-level APIs.\n`KDB` and `KeySet` is useful for plugins and to\nimplement APIs but not to be directly used in applications.\n\n## Decision\n,"We provide 3 high-level C APIs:\n1. libelektra-highlevel (generic key-value getter/setter)\n2. libelektra-hierarchy (generic hierarchical getter/setter in a tree)\n3. code generator (specified key-value getter/setter with function names,\nKeySets, or strings from specifications)\nFurthermore, we will:\n- have as goal that no errors in specified keys with default can occur\n- if you use `elektraGetType` before getting a value, no error can occur when getting it later\n- enforce that every key has a type\n- use `elektraError` as extra parameter (for prototyping and examples you can pass 0)\n",**Decision**: Use a service registry/discovery component for self-registration of services
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently the way functions like keyGetName() work is by passing a buffer with\na maxSize and if the buffer is large enough, the value gets copied into the\nbuffer. This leads to the user having to write a lot of surrounding boilerplate\ncode, checking for the size of every value / name they want to copy into a buffer.\n\n## Decision\n","- Remove Functions:\n- keyGetName()\n- keyGetUnescapedName()\n- keyGetBaseName()\n- keyGetString()\n- keyGetBinary()\n- add documentation in API docu about life-time and add in release notes that you should use strncpy() / memcpy() instead:\n```c\n// str values\nstrncpy(..., keyName (k), ...)\n// binary values\nmemcpy(..., keyValue (k), ...)\n```\n","**Use Pass-By-Reference Semantics for Key Information Retrieval**\n\n**Rationale:**\n\n* **Reduces boilerplate code:** Eliminates the need for users to manage the buffer size and copy values manually.\n* **Improves performance:** Avoids unnecessary memory allocation and copying, resulting in better performance.\n* **Simplifies the API:** Provides a more straightforward and concise API for key information retrieval.\n* **Ensures data safety:** Prevents accidental data corruption or truncation by managing the buffer internally."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently it is inefficient to detect the length of an array and\nit is impossible to know if a key (without subkeys) should be\nan array or not.\n\n## Decision\n,"Store length in metadata `array` of key, or keep metadata `array` empty if empty array.\nOnly children that have `#` syntax are allowed in a valid array.\nThe index start with `#0`.\nBoth `keyAddName(""#12"")` or `keyAddBaseName(""#_12"")` is allowed to add the 13th index.\nFor example (`ni syntax`, sections used for metadata):\n```\nmyarray/#0 = value0\nmyarray/#1 = value1\nmyarray/#2 = value2\nmyarray/#3 = value3\nmyarray/#4 = value4\nmyarray/#5 = value5\n[myarray]\narray = #5\n```\nIt is not allowed to have anything else than `#5` in the metadata `array`,\ne.g. even `#4` would be a malformed array.\nWith the metadata `array` in `spec:/` the `spec` plugin will add the\n`array` marker with the correct length.\nThis needs to be happening early, so that plugins can rely on having\ncorrect arrays.\nFor example:\n```\nspec:/myarray      # <- has array marker\nuser:/myarray      # <- either has correct array marker or no array marker\nuser:/myarray/#0\nuser:/myarray/#1\n```\nHere, the `spec` plugin would add `array=#1` to `user:/myarray` if it was not there before.\nTo look up an array, first do `ksLookupByName (ks, ""/myarray"", 0)` on the parent.\nWith the last index you get from its metadata `array`, iterate over the children.\nA cascading lookup on every individual child is also needed to make sure that overrides on individual\nelements are respected.\nFor example:\n```\nspec:/myarray    # <- contains the specification for the array\nspec:/myarray/#  # <- contains the specification for the array elements\ndir:/myarray/#0  # <- not an array, just an override for user:/myarray/#\nuser:/myarray    # <- with metadata array=#0, this would be the array we get\nuser:/myarray/#0\nsystem:/myarray  # <- not found in cascading lookup, as user:/myarray exists\n```\nThe `spec` plugin should check if it is a valid array, i.e.:\n- that the parent key always contains the metadata `array`,\n- that the correct length is in `array`,\n- that the array only contains `#` children, and\n- that the children are numbered from `#0` to `#n`, without holes.\n- [Global Plugins](global_plugins.md)\n- [Global Validation](global_validation.md)\n- [Base Names](base_name.md)\n- [Metadata in Spec Namespace](spec_metadata.md)\n- [Spec Expressiveness](spec_expressiveness.md)\n","Make the key associated with the length of the array a subkey named length, whose value is the length of the array. Keys without length subkeys will be considered to be non-arrays (scalars)."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nLinks and validation constraints might point to keys not loaded\nwith the respective `kdbGet`.\n\n## Decision\n,"Not supported, admins/maintainers need to stay with their spec within what applications request by `kdbGet`.\n- [Internal Cache](internal_cache.md)\n",Use a prefix match filter that only returns results with keys that also exist in the kdb to be read.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nIn the previous error concept it was very useful to generate macros as we often added new errors.\nThe code generation itself, however, was not ideal for cross compilation.\n\n## Decision\n","Write down the few macros manually, and also manually write down exceptions for the language bindings (and also the mappings from Elektra's internal errors to nice errors specific for the languages)\nSince error codes and crucial parts Elektra's core implementation will not often change this is the best approach with minimal effort.\nThe existing code will be refactored so that error macros directly call macros.\nWhen adding a new error code, language bindings have to be adapted accordingly such as the Rust or Java Binding.\n",Create a macro generating Python script.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nCurrently all functions do proper argument checking which might degrade\nperformance.\n\n## Decision\n,Rejected (keep checks) due to time constraints\n,"**Decision:**\n\nImplement a pre-invocation function that checks the arguments, and throws an exception if any of them are invalid. This allows for stricter argument validation without impacting performance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\n(as they are designed to do so). Limitations of other storage plugins are\ne.g., that not every structure of configuration is allowed.\nSome of these limitations were documented `infos/status`, others were not.\n\n## Decision\n","Add `infos/features/storage` to document limitations of storage plugins.\nIdeally, storage plugins should throw an error in `kdbSet` for\nunrepresentable KeySets.\nElektra cannot guarantee that any configuration file format can\nbe mounted anywhere.\nDevelopers, maintainers and administrators are responsible for what\nthey mount. They need to test the setup.\n- [Base Name](base_name.md)\n","Create an API to provide other Plugins with the ability to represent different types of KeySets as well, which is also used by `dump` and `quickdump`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problem\nThe current error concept has disadvantages in following regards:\n- A lot of redundant errors\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\ngroup similar errors together so that there is one coherent and consistent state again.\n- Hard to manage specification file\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\n- No senseful way for application developers to use error codes from elektra\nIf developers of plugins/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\nsensefully grouped together and are hierarchically structured.\n\n## Decision\n","All ""fatal"" errors will be converted to ""errors"" as the distinction is not relevant.\nUnused errors will be removed from the specification.\nErrors will be categorized into logical groups with subgroups.\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\nis the abbreviation for ""Code"".\n- Permanent errors C01000\n- Resource C01100\n- Out of Memory C01110\n- Installation C01200\n- Logical C01300\n- Internal C01310\n- Interface C01320\n- Plugin Misbehavior C01330\n- Conflicting State C02000\n- Validation C03000\n- Syntactic C03100\n- Semantic C03200\nTo see an explanation of the categories along with a guideline on how to categorize please\nsee the [Error Codes Guideline](../dev/error-categorization.md)\n- [Error Message Format](error_message_format.md)\nShows the new format of the error message\n- [Error Codes Guideline](../dev/error-categorization.md)\nShows how to categorize errors\n",Centralize error handling. Introduce a new generic error handling system for Elektra. The system could consist of two major parts:\n1. An error handling service that is used as a library inside Elektra and simplifies error handling for keysets\n2. A CLI to advertise error codes and generate error handling code for the specific plugin
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI see found both `yarn.lock` and `package-lock.json` in the repository.\n\n## Decision\n,Sticking to `yarn` for `web` for now.\n,"Keep `yarn.lock` and discard `package-lock.json`.\n\nRationale: Yarn is a more modern package manager than npm, and its lockfile format is more reliable. Additionally, Yarn is more popular than npm, so there is a larger community of support available for it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Record architectural decisions in a central repository**\n\n**Rationale**\n\n* Provides a single source of truth for all architectural decisions\n* Facilitates collaboration and communication among team members\n* Enables traceability of decisions back to the requirements and design\n* Supports decision-making for future system changes
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\n### Background\nHistorically front-end web frameworks, including Angular 1.x, used a variety of\nMVC-based patterns for structuring the user interface part of an application\n, which of course is much of the code for a web app.\nMore recently (especially since React in 2013), web frameworks have generally\nmoved to a simpler model where an application is structured as a tree of\ncomponents with several key properties:\n* Each component's internals are hidden from other components.\n* Data is explicitly passed from one component to another and in one direction.\n* Components have an explicitly declared API.\n* Communication from child to parent happens via callback inputs.\n* Components have a standard set of lifecycle hooks for creation, input changes\nand destruction.\nA further pattern that emerged on top of this was to split components into those\nwhich purely reflect their inputs (""presentational"" components) and those which\nare connected to services and state in the application (""container"" or\n""connected"" components).\nThis pattern made it easier to reason about, re-use and change pieces of the\napplication in isolation, as well as enabling important optimizations and\nsimpler framework implementations.\nIn Angular JS prior to v1.5, this pattern could be achieved by using a\ncombination of features (element directives, isolate scope, bind-to-controller,\ncontrollerAs). Angular JS 1.5x introduced [explicit\nsupport](https://docs.angularjs.org/guide/component) for this architecture via\n`.component()`.\n### Components in the Client\nThe client historically used traditional Angular 1.x methods of passing data\nbetween parts of the UI - properties on the scope inherited by child scopes and\nevents. As the app grew larger it became harder to reason about where data in\ntemplates came from and who could change it (""$scope soup""). Newer parts of the\nUI have used element directives with isolate scopes to implement a\ncomponent-based architecture, thus avoiding this problem. However:\n- This requires a bunch of boilerplate for each directive.\n- It isn't clear that we use this pattern from looking at the entry point to the\nsidebar.\n- Important parts of the top level of the UI (the `*-controller.js` files and\n`viewer.html`) do not use this pattern and suffer from being hard to\nunderstand in isolation. Additionally they use a different mechanism\n(`ngRouter`) to control what is displayed than the rest of the UI, where we use\n`ng-if` guards.\n- This lack of consistency makes it difficult to understand how the top level of\nthe UI works.\nDecision\n--------\n* We will convert all element directives in `src/sidebar/directive` to\ncomponents (ie. change them to use `angular.component()` and any refactoring\nthis implies) and move them to `src/sidebar/components`. This change will be\nsimple in most cases and will require some moderate refactoring for others.\n* The top-level of the application will also be converted to a set of components\nusing the same pattern and the router will be removed.\nStatus\n------\nIn discussion\nConsequences\n------------\nIn the short term this should make it easier to understand the sidebar app by\nimproving consistency.\nIn the medium term, this brings the architecture of the client more into\nalignment with how it would be structured in other frameworks and gives us the\nopportunity to incrementally migrate to a more actively developed (and\npotentially smaller, faster, simpler) library for building our UI in future.\nPresentational components can be potentially be extracted into their own\npackages for re-use in other parts of Hypothesis, though this is not an active\npriority.\n\n## Decision\n","--------\n* We will convert all element directives in `src/sidebar/directive` to\ncomponents (ie. change them to use `angular.component()` and any refactoring\nthis implies) and move them to `src/sidebar/components`. This change will be\nsimple in most cases and will require some moderate refactoring for others.\n* The top-level of the application will also be converted to a set of components\nusing the same pattern and the router will be removed.\nStatus\n------\nIn discussion\nConsequences\n------------\nIn the short term this should make it easier to understand the sidebar app by\nimproving consistency.\nIn the medium term, this brings the architecture of the client more into\nalignment with how it would be structured in other frameworks and gives us the\nopportunity to incrementally migrate to a more actively developed (and\npotentially smaller, faster, simpler) library for building our UI in future.\nPresentational components can be potentially be extracted into their own\npackages for re-use in other parts of Hypothesis, though this is not an active\npriority.\n",Convert all element directives in `src/sidebar/directive` to components (ie. change them to use `angular.component()` and any refactoring this implies) and move them to `src/sidebar/components`. The top-level of the application will also be converted to a set of components using the same pattern and the router will be removed.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\n### Background\nBefore now, the client had not had the necessary pieces to support the standard ePub\nbook format. The primary obstacles that had to be traversed were 1) supporting iframe annotation, 2) document equivalency, and 3) scrolling to annotations.\nDecision\n--------\n* Standard ePubs viewers (including Readium and ePubJS) use iframes as a way to embed the different book pages and render them inside of their viewer. Hypothesis, before now, did not support going into iframes and allowing annotation. We modified our client to be able to watch for same origin iframes (cross origin not supported yet) on the page. When an iframe is encountered, we (if we have access to it) inject the Hypothesis embed code inside. In addition to the embed, we mirror the embed configuration values that were set. The last part of this injection is adding a “subFrameIdentifier” field to the configuration that is a random string that does two things 1) identifies the sub frame so the top frame has a point of unique reference and 2) the existence of this field tells the injected client that it should only load the Guest and not the full sidebar UI. So to recap, the client watches for iframes then injects the client and configuration into new frames. The Guest only clients inside of the frames will now be able to make and load annotations for their respective iframe locations. The client uses cross frame library to communicate what data to load - so subframes have those requests bubble up to the top frame. The sidebar stores an array of frames that it loads data for and does another cross frame call when data is returned. With all of that, the client is now able to support annotating content inside of an iframe.\n* For document equivalency, we support for documents to set two new dc-* meta tags to indicate 1) what book we are in and 2) what chapter are we in. Together, this allows cross domain document equivalency down to the chapter level of an ePub. The tags are: “dc.identifier” for the chapter and “dc.relation.isPartOf” for the unique book identifier.\n* Since ePubs use iframes for their presentation, those frames have content in various layouts/locations and move the visible area of the frame as the user is navigating to the next page (similar to how image sprites are used to show a single icon from a large image that has many icons). There is no standard event in web standards or ePub that we can use to navigate to the proper section in the frame and have it properly align the frame to include the page contents in the same manner that it does when you manually navigate to the page. That is, we could use “scrollTo” functions but those functions will just bring the section into view but makes no attempt to properly snap to the correct vertical and horizontal spacing that make up a whole page. This meant that unless we fixed it, users who select an annotation attempting to navigate to it could end up in scenarios where the highlighted section is visible but you cut the book's visible page in half. To fix this, we introduced an implementation agnostic “scrollToRange” event that we attempt to use before falling back to the traditional scrollTo event. That is, if the site registers a listener for “scrollToRange” and preventsDefault() we assume that they have taken the range and applied the proper scrolling behavior needed to get the range into view correctly.\nStatus\n------\nDeployed\n\n## Decision\n","--------\n* Standard ePubs viewers (including Readium and ePubJS) use iframes as a way to embed the different book pages and render them inside of their viewer. Hypothesis, before now, did not support going into iframes and allowing annotation. We modified our client to be able to watch for same origin iframes (cross origin not supported yet) on the page. When an iframe is encountered, we (if we have access to it) inject the Hypothesis embed code inside. In addition to the embed, we mirror the embed configuration values that were set. The last part of this injection is adding a “subFrameIdentifier” field to the configuration that is a random string that does two things 1) identifies the sub frame so the top frame has a point of unique reference and 2) the existence of this field tells the injected client that it should only load the Guest and not the full sidebar UI. So to recap, the client watches for iframes then injects the client and configuration into new frames. The Guest only clients inside of the frames will now be able to make and load annotations for their respective iframe locations. The client uses cross frame library to communicate what data to load - so subframes have those requests bubble up to the top frame. The sidebar stores an array of frames that it loads data for and does another cross frame call when data is returned. With all of that, the client is now able to support annotating content inside of an iframe.\n* For document equivalency, we support for documents to set two new dc-* meta tags to indicate 1) what book we are in and 2) what chapter are we in. Together, this allows cross domain document equivalency down to the chapter level of an ePub. The tags are: “dc.identifier” for the chapter and “dc.relation.isPartOf” for the unique book identifier.\n* Since ePubs use iframes for their presentation, those frames have content in various layouts/locations and move the visible area of the frame as the user is navigating to the next page (similar to how image sprites are used to show a single icon from a large image that has many icons). There is no standard event in web standards or ePub that we can use to navigate to the proper section in the frame and have it properly align the frame to include the page contents in the same manner that it does when you manually navigate to the page. That is, we could use “scrollTo” functions but those functions will just bring the section into view but makes no attempt to properly snap to the correct vertical and horizontal spacing that make up a whole page. This meant that unless we fixed it, users who select an annotation attempting to navigate to it could end up in scenarios where the highlighted section is visible but you cut the book's visible page in half. To fix this, we introduced an implementation agnostic “scrollToRange” event that we attempt to use before falling back to the traditional scrollTo event. That is, if the site registers a listener for “scrollToRange” and preventsDefault() we assume that they have taken the range and applied the proper scrolling behavior needed to get the range into view correctly.\nStatus\n------\nDeployed\n","Implement iframe support, document equivalency, and a scrollToRange event to support standard ePub book format."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the template there is a need to provide a base configuration for Rspec. Ideally this would be performed using the `rails generate rspec:install` command. However, on testing the Thor command `generate(""rspec:install)""` it froze when it got to install rspec. This is likely due to [spring running as described in this error](https://stackoverflow.com/questions/33189016/how-to-solve-rails-generate-commands-hanging).\nFor now, `hmu-rails` just uses template files to configure rspec. An option to explore in the future would be to test whether calling `spring stop` as part of a `generate_rspec` method would resolve this issue.\n\n## Decision\n",For now `hmu-rails` will use template files for configuring rspec for speed but revisit the need to run the install command in the future.\n,"We will use template files to configure Rspec for now. We will investigate the possibility of using `rails generate rspec:install` in the future, after exploring the option of resolving the issue by calling `spring stop` as part of a `generate_rspec` method."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to authorise users in the MOLGENIS ""Armadillo"" service to use data. We want this to be as straightforward as possible.\nOther techniques are ACL's for fine grained permissions on resources and ID-based.\n**ACL's**\nIt is more time consuming to implement ACL's. With roles the you are less flexible, but we believe this is sufficient to use DataSHIELD.\n**ID-based**\nThe ID-based approach was suggested as a first solution to make the service secure. A big disadvantage is that anyone with the link can\nshare the data. That means that there has to be a lot of trust between the data manager and the researcher. In practices, this is not feasible.\n\n## Decision\n","We will use roles as a basic principle to give users permission on the data they need to have access to.\nWithin the MOLGENIS ""Armadillo"" service there are at least these 2 types of roles:\n- Researcher\n*Implicit permissions*\n- READ --> on data in the shared folder(s) to which they have access\n- ADMIN --> on data in their own (user)folder\n- Data manager\n*Implicit permissions*\n- ADMIN --> on all that lives\nEach researcher role will have corresponding folder. The data manager is able to do anything anywhere.\n","Use role-based access control (RBAC) for authorization in the MOLGENIS ""Armadillo"" service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to make the MOLGENIS ""Armadillo"" service data provider agnostic. There are a couple of reasons why we are doing this\n- the service is usable for other parties as well\n- the service can still integrate with MOLGENIS.\n- the release cycle of the service is data provider independent\n- the service can be developed by other parties as well\n\n## Decision\n","We implement an endpoint to upload and load RData files in the MOLGENIS ""Armadillo"" service to manage data for the use in DataSHIELD.\n",Use a separate data provider connector to connect to different types of data providers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nClients can have operations running on multiple DataSHIELD servers concurrently.\n\n## Decision\n,We need to support asynchronous requests in the R client. We implemented it using completable futures.\nWe must keep the last execution result for each R session until it gets retrieved or until a new execution is started.\n,"Clients can have operations running on multiple DataSHIELD servers concurrently provided, each server has a copy of the data."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n","**Decision:** To use an Architectural Decision Record (ADR) template to document all architectural decisions made on the project.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture and communicate architectural decisions.\n* They facilitate collaboration and knowledge sharing among team members.\n* They help maintain a clear and concise record of architectural decisions over time.\n* They can be used to track decision evolution and rationale, promoting transparency and accountability.\n* Using a consistent template ensures that all ADRs are well-structured and easily searchable."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBecause of the diversity in landscape we need to be able to deploy on different environments.\n\n## Decision\n,We are going to create deployments for virtual machines based on CentOS (>=8) using Ansible and Vagrant.\nBesides that we are going to create a chart which allows you to deploy on Kubernetes.\n,Use a multi-cloud architecture to deploy on different environments.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to be able to switch between profiles.\nBut many singleton beans have profile information in them.\n\n## Decision\n,"Create a custom `@ProfileScope` to store the different versions of the profiles and inject proxies\nto look up the correct instance.\nWe already did this for the ArmadilloSession, a `@SessionScope` bean, to store the connection to\nthe R server which is different for each user session.\n","**Decision:** Use a Custom Property Source to dynamically load properties from a profile-specific file.\n\n**Rationale:**\n\nThis approach allows us to maintain the singleton nature of beans while still loading profile-specific configuration dynamically. By leveraging a custom property source, we can override existing properties based on the active profile. This provides flexibility and modularity, enabling us to easily switch between profiles without the need for extensive bean reconfiguration.\n\n**Consequences:**\n\n* **Separation of concerns:** The profile-specific configuration is kept separate from the bean definitions, making it easier to manage and maintain.\n* **Improved performance:** Singleton beans are typically initialized only once, leading to better performance compared to recreating beans for each profile.\n* **Extensibility:** The custom property source can be easily extended to support additional profile-specific configuration requirements in the future.\n* **Potential complexity:** Implementing a custom property source can introduce some additional complexity to the application, but it provides a robust and flexible solution."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to help users authentication in our system\n\n## Decision\n,"We implement openid in our application so we use ID-providers to authenticate in our services.\nWe intent to use JWTs which are machine-readable bearer tokens, to integrate external services.\n","**Decision:** Use OAuth 2.0 with multiple identity providers.\n\n**Justification:**\n\n* OAuth 2.0 is a widely adopted open standard for authentication and authorization.\n* It allows users to authenticate with a third-party provider (e.g., Google, Facebook, Twitter) without having to create a separate account for our system.\n* Supporting multiple identity providers gives users flexibility and convenience.\n* It reduces the risk of password theft and phishing attacks.\n* It simplifies account management for users."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need some way of being sure that there is no malicious script execution on the DataSHIELD environment.\n\n## Decision\n,We implement datashield4j to prevent users to execute method and parameters other than the DataSHIELD provided methods.\nWe looked at OCAP ([R native object capabilities](https://docs.google.com/document/d/1Yx10Xw8Uige3hK-6YwzM8RhrtqSogDTGAZlcF218U2U/edit)) as well but came to the conclusion\nthat the client that OCAP is using needs to be developed in JAVA. The datashield4j library is already available and tested which makes it easier to implement.\n,Use a whitelisting approach to allow only trusted scripts to execute on the DataSHIELD environment.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to give researchers as much freedom as possible when they're selecting and loading data.\n\n## Decision\n,The `/load-tables` endpoint supports loading multiple .RData files at once. These files can be in different folders.\n,Provide researchers with a library of pre-built data connectors that can be used to load data from a variety of sources. This will give researchers a starting point for their work and will help to ensure that the data is loaded in a consistent and reliable way.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAstarte Channels are a way to receive events of specific Astarte entities via\nWebsockets, leveraging Phoenix Channels and volatile Triggers.\nDue to the way volatile Triggers are implemented, it would be necessary to\nintroduce a large overhead to guarantee that all the data is correctly\ndelivered to Astarte Channels.\n\n## Decision\n","Since we already have a reliable way to receive the events of a given entity\n(persistent Triggers), we will not guarantee that Astarte Channels are a\nreliable source of events.\n","**Decision:** Do not guarantee the delivery of events to Astarte Channels.\n\n**Rationale:**\n\n* Guaranteeing delivery would require a significant overhead that would be detrimental to performance.\n* The loss of events is not critical for the functionality of the system, as events are not essential for the data integrity or consistency.\n* Users can easily redetermine the state of the system by querying the Astarte database.\n\n**Consequences:**\n\n* Events may occasionally be lost, but this will not affect the system's functionality or data integrity.\n* Users may need to periodically query the database to ensure that they have the latest information."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Establish an Architectural Decision Record (ADR) process to document and track architectural decisions made throughout the project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContext here...\n\n## Decision\n,Decision here...\n,The provided context does not include a Decision section. Please provide the complete Architectural Decision Record so that I can extract the Decision.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nhttps://github.com/microsoft/TypeScript/wiki/Using-the-Compiler-API\nhttps://github.com/dsherret/ts-morph#readme\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\n\n## Decision\n,Decision here...\n,We should use ts-morph to analyze and manipulate TypeScript code.\n\n**Benefits:**\n\n- Makes it easier to navigate and manipulate TypeScript and JavaScript code.\n- Provides a higher level of abstraction over the TypeScript compiler API.\n- Reduces the boilerplate code required for common tasks.\n\n**Drawbacks:**\n\n- Can add an additional layer of complexity to the codebase.\n- May not be compatible with all versions of TypeScript.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[WikibaseRepo](https://gerrit.wikimedia.org/r/plugins/gitiles/mediawiki/extensions/Wikibase/+/62ae43e/repo/includes/WikibaseRepo.php) and [WikibaseClient](https://gerrit.wikimedia.org/r/plugins/gitiles/mediawiki/extensions/Wikibase/+/62ae43e/client/includes/WikibaseClient.php) classes act as top level factories for services required inside of the respective extension.\nThe methods to create/retrieve them do not follow a standardized interface, and as many of them are required to build other services, the classes also hold a substantial amount of code to keep and pass references to the created instances once created. This code is low in conceptual value but highly repetitive and a burden on developers.\nServices which are required in WikibaseRepo and WikibaseClient alike, e.g. [FormatterCacheFactory](https://gerrit.wikimedia.org/r/plugins/gitiles/mediawiki/extensions/Wikibase/+/62ae43e/client/includes/WikibaseClient.php#1270), may also exhibit redundant implementations of their instantiator function as there is no ability to reuse them.\nMediaWiki supports a concept called [ServiceWiring](https://www.mediawiki.org/w/index.php?title=Dependency_Injection&oldid=3977354#Quick_Start) which allows for the registration of services, is an important building block of recent MediaWiki and implements the well-known [PSR 11 container interface](https://www.php-fig.org/psr/psr-11/).\n\n## Decision\n",We will use MediaWiki ServiceWiring to describe and connect the services on which the wikibase repo and client extensions rely.\n,The Wikibase extensions will migrate to using MediaWiki's ServiceWiring as a standard dependency injection framework to retrieve services. The aim of this decision is to simplify the code by moving the service factory code into separate classes and to enable the reuse of service factory code for services required in both WikibaseRepo and WikibaseClient.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions/Wikibase/repo/Wikibase.php:1020](https://github.com/wikimedia/mediawiki-extensions-Wikibase/blob/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057/repo/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\nway to to inject a mock of `WikibaseRepo` without dependency injection.\nA [RFC for enabling dependency injection](https://phabricator.wikimedia.org/T240307) in hooks is currently under way.\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\nand other places in the codebase.\nWhile reviewing this issue, two initial solutions were considered:\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\ninstead of using the real deal.\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\nthe real store by creating an `overrideStore` method. See in following:\n- [`client/tests/phpunit/includes/MockClientStore.php`](https://github.com/wikimedia/mediawiki-extensions-Wikibase/blob/master/client/tests/phpunit/includes/MockClientStore.php)\n- [`client/tests/phpunit/includes/DataAccess/ParserFunctions/PropertyParserFunctionIntegrationTest.php:42`](https://github.com/wikimedia/mediawiki-extensions-Wikibase/blob/master/client/tests/phpunit/includes/DataAccess/ParserFunctions/PropertyParserFunctionIntegrationTest.php#L42)\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\nhooks, rather than a one time big change.\n\n## Decision\n","It was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\nprovides at least four methods:\n- A constructor to make dependency injection easier.\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\n`newFromGlobalSettings`.\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\nThis is useful for hooks handlers which are called several times.\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\nhard to test without complete integration or e2e tests).\n- A public method to perform the actual handler logic in a testable way.\nThis class should be placed under the `includes/Hooks` directory. An example of this pattern can be seen in:\nhttps://gerrit.wikimedia.org/r/c/mediawiki/extensions/Wikibase/+/574495\n","The decision is to adopt a pattern similar to the one used in `WikibaseClient`, which enables us to mock several parts of it (namely the store), and replace the real store by creating an `overrideStore` method. This will allow us to gradually refactor hooks, rather than making a one time big change."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe [PropertyInfoStore] interfaces with the [wb_property_info] DB table holding information about wikibase properties.\nThe [CacheAwarePropertyInfoStore] stores the whole of the [wb_property_info]  table in a single cache key.\nThe Wikibase wiring is setup to provide this CacheAware store using the default cache (memcached for WMF).\nThe [CacheAwarePropertyInfoStore] has a high number of reads, and the method of storing the whole table in a single key\nresults in lots of traffic to a single memcached instance as described in [T97368].\nThe amount of traffic for the memcached key has steadily grown as the number of properties in the store have grown.\nThis traffic also moves between memcached servers after each WMF deploy as the cache key changes.\n\n## Decision\n",A layer of APC caching (per server) is added on top of the shared memcached caching.\nThis is done in the service wiring by wrapping our [CacheAwarePropertyInfoStore] in another [CacheAwarePropertyInfoStore].\nThis on APC cache has a short TTL to avoid the need to actively think about purging.\nAdding this extra layer of caching was chosen rather than anything more drastic as it is a trivial code change vs re-working how the [CacheAwarePropertyInfoStore] works.\n,The decision is to **Split the cached table into a number of smaller chunks**.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https://gerrit.wikimedia.org/r/#/c/mediawiki/extensions/Wikibase/+/176650/). As discussed there in the code review, and also in https://phabricator.wikimedia.org/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https://phabricator.wikimedia.org/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their ""short form"" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\n\n## Decision\n","Wikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https://phabricator.wikimedia.org/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their ""short form"" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\n",Deprecate wb_terms completely.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Wikibase CI which runs on WMF Jenkins is currently ""augmented"" by running php unit tests in a variety of additional configurations (e.g. non-English wiki, repo/client-only environment, etc) on legacy Travis CI infrastructure, via the GitHub mirror of our Gerrit code repository, see [https://travis-ci.org/github/wikimedia/Wikibase](https://travis-ci.org/github/wikimedia/Wikibase).\nThe Travis CI features we currently use include:\n* PHP installed (multiple versions),\n* MySQL running,\n* notifications via\n* email,\n* IRC,\n* composer cache.\nDue to a change in their business model, the travis-ci.org service is being phased out and replaced by (paid) travis-ci.com. Since we have no intention of dropping the extended CI testing, there are several options how to handle the situation:\n* migrate additional CI for Wikibase to some other CI infrastructure, e.g. _GitHub Actions_,\n* negotiate with the WMF changing the Travis CI plan to a paid one which would have unlimited/less limited resources available,\n* migrate additional CI for Wikibase to additional jobs on WMF Jenkins CI.\n\n## Decision\n","We will migrate the additional CI for Wikibase to the GitHub CI infrastructure, using _GitHub Actions_.\nReasons: The GitHub mirror of the Wikibase repository exists already and all of the Travis CI features we have been using so far, are available on _GitHub Actions_:\n* [setup-php](https://github.com/shivammathur/setup-php),\n* MySQL pre-installed on the runner or use [MySQL service container](https://firefart.at/post/using-mysql-service-with-github-actions/) (to be investigated),\n* notifications via\n* [email](https://docs.github.com/en/free-pro-team@latest/github/managing-subscriptions-and-notifications-on-github/configuring-notifications) (built-in),\n* [notify-irc](https://github.com/rectalogic/notify-irc) + [failure() condition](https://docs.github.com/en/free-pro-team@latest/actions/reference/context-and-expression-syntax-for-github-actions#failure),\n* [cache](https://github.com/actions/cache).\n",Migrate additional CI for Wikibase to additional jobs on WMF Jenkins CI.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUntil recently, frontend dependencies in Wikibase were managed through globally registered [ResourceLoader] modules with\nno distinction between internal dependencies and modules that needed to be publicly available. This resulted in more\nthan 250 [ResourceLoader] modules with a very complex dependency relationship. The names of all modules are loaded with\nevery page view, affecting page load time and network overhead. See [T228513]\nWe considered two options to reduce the number of globally registered ResourceLoader modules:\nwebpack and [ResourceLoader] [Package Modules].\nSolving the problem with webpack would have involved the introduction of a build step to create bundles for each of the\ntop-level (=entry point) modules. Advantages of this approach are superior minification at build time - better than the\nResourceLoader minification, and all the other benefits of introducing a build step such as transpilation from more modern ES versions,\nand easier reuse of 3rd party npm libraries. The risks we identified for this approach include;\n- Having to generate and check in the bundled js files in addition to their source. That would lead to most patches having a merge conflict.\n- Having to rewrite tests for internal files that can no longer run within the QUnit ResourceLoader test suite.\n- Webpack is not yet widely adopted within the MediaWiki ecosystem.\n- Putting in too much work into (legacy) parts of the code that are not going to change any time soon.\nFor [Package Modules], we agreed that we would be missing out many of the positive side effects of introducing webpack and\nthe build step. On the other hand, this solution would require fewer code changes, and would be the recommended MediaWiki way to solve the problem at hand.\n\n## Decision\n","We decided to go with [Package Modules] instead of webpack and reduced the number of modules that Wikibase registers from 260 to 85.\nThe reason we went with Package Modules was:\n- It's easier to migrate such a huge codebase to [Package Modules].\n- The tests are easier to adapt given the current paradigm of running QUnit tests in RL and browser context.\n- If we change our mind later and switch to webpack, it would be still doable and easier than now given that both understand require.\n- The rebase hell that would come with checking the dist files in VCS.\nWe still went with webpack in the submodules that work independently of Wikibase (like WikibaseSerializationJavaScript and\nWikibaseDataModelJavaScript) to run the tests in the context of karma + webpack + node but at the same time ResourceLoader understands\nthose notations when the code is being pulled as a RL module in the Wikibase extension (as a submodule).\n",Use [ResourceLoader] [Package Modules] to solve the problem of reducing the number of globally registered ResourceLoader modules.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWikibase is currently only tested to be compatible with MediaWiki core and other MediaWiki extensions for:\n- sets of commits that are the HEAD of the master branch at one point in time\n- cut releases: either for the alpha, weekly train releases (i.e. releases delivered on WMF wikis), or the twice-yearly(ish) official MediaWiki releases\nWe want to give 3rd party Wikibase users regular 6 monthly releases.\nMediaWiki release are currently not that regular; in part due to delays caused by the Covid-19 pandemic.\nWe intend to produce new Wikibase features on a more regular basis than 6 monthly and we want these to be available to users\nas soon as possible. Both for users to benefit from new features but also to shorten our development feedback cycles.\nTo be able to release Wikibase at any point in time such that it remains possible to apply security patches to MediaWiki core and to\nany extensions that are not maintained by WMDE these releases need to be compatible with a ""cut release"" of MediaWiki.\nTo ensure this compatibility there are two options:\n- Backport Wikibase features, and any dependent patches to the last stable release of MediaWiki before releasing.\nThis was performed for the upcoming wmde1 release of Wikibase where patches made against master (i.e. 1.36 alpha releases) were backported to the Wikibase 1.35 branch\n- Keep Wikibase `master` compatible with the last stable release of MediaWiki\nDoing the backporting results in leaving a possibly unknown amount of work for whoever is to make the release and adds lots of uncertainty.\nThe additional backporting effort will need to be repeated for all upcoming Wikibase releases.\nThis approach also means maintaining two different (even if only in the sense of git history) versions of the same functionality.\nKeeping Wikibase compatible with last stable MediaWiki adds to every developer's workload.\nIt may result in having to delay using new features in MediaWiki or having to write a compatibility layer in order to use them.\nIt may result in developers inadvertently using a new feature and then discovering they need to either\nwrite some backwards compatibility layer for it or put that new feature behind a flag.\nDoing so may result in being more decoupled from MediaWiki in the long run.\nThe overhead of the additional development effort will possibly be increasingly lower\nthe more code has been written that way, and the more Wikibase releases have been published following this approach.\nKeeping Wikibase compatible with last stable MediaWiki would likely mean avoiding the ""double work"" effort\nof developing the feature against the master branch, and ""backporting"" it to be compatible with the last stable version of MediaWiki\nneeded for the Wikibase release for non-WMF users.\n\n## Decision\n",Ensure that Wikibase `master` maintains compatibility with the last stable release of MediaWiki core.\n,Keep Wikibase `master` compatible with the last stable release of MediaWiki.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Wikidata Bridge app uses VueJs version 2 and Vuex version 3 for state management.\nBoth within the store and in vue components the calls to `dispatch()` and `commit()` are not type safe.\nThat means that they accept any arguments and TypeScript will still compile without error.\nSince `dispatch()` and `commit()` are typical seams that are usually mocked during unit testing,\nit is up to integration, end-to-end and browser tests to detect these errors.\nThis is particularly unfortunate as the store is one of the central locations where business logic happens.\nWe considered two options:\n1. writing our own set of wrappers for `dispatch()` and `commit()` to get type safety\n1. using vuex-smart-modules\nThe advantages of doing it ourselves included us not having another dependency.\nThe risks include that this would be yet another homebrew layer of abstraction.\nThe advantages of using vuex-smart-modules include:\n- it gives us proper native type safety both in the store and in components\n- we can call actions/getters/mutations with [method style access](https://github.com/ktsn/vuex-smart-module#method-style-access-for-actions-and-mutations)\n- we can get rid of all the `BRIDGE_SET_TARGET_VALUE` constants without having to fall back to string literals\n- we can actually use the IDE's `go to method definition` functionality\n- we can rely on the action's return type instead of dispatch's `Promise<any>` being used everywhere\n- we can drop the `vuex-class` dependency as we can use vuex-smart-modules for all store access in components\n- it is developed by a VueJs core contributor\nThe risks include:\n- it is still a very new project with a 0.x.y version number\n- it is another layer on top of vuex, which means the documentation may not be as good as it could be\n- we are the first big project to use it\n- mocking of dependencies and nested modules in testing still seems to be not handled as diligently as one would wish\n\n## Decision\n",We decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4\n,Use vuex-smart-modules for type safety in the Vuex store and component access.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (""property"") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\n\n## Decision\n",Make all relevant services source and type dispatching.\n,Dispatch services in Wikibase should be adjusted to dispatch based on both entity type and entity source.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nData Bridge is a browser-side application that allows users to edit Wikidata from Wikibase Client wikis.\nThis involves showing users references for the statements they are editing, in a readable format.\nThe guiding example for this format is the following reference from the [South Pole Telescope] article:\n> [""South Pole Telescope eyes birth of first massive galaxies""](https://antarcticsun.usap.gov/science/contenthandler.cfm?id=2737). United States Antarctic Program. 14 September 2012. Retrieved 11 February 2017.\nWhich is generated by Lua code from the following Wikidata reference:\n> - reference URL: https://antarcticsun.usap.gov/science/contenthandler.cfm?id=2737 (`url`)\n> - retrieved: 11 February 2017 (`time`)\n> - title: South Pole Telescope eyes birth of first massive galaxies (English) (`monolingualtext`)\n> - publication date: 14 September 2012 (`time`)\n> - publisher: [United States Antarctic Program][Q955267] (`wikibase-item`)\nThis reference already involves the formatting of four different datatypes, none of which are otherwise supported by Data Bridge in its initial iteration.\nAdditionally, other extensions can register additional datatypes (examples include WikibaseLexeme, Math, and Score), which may appear in references as well.\nThis makes formatting references browser-side unattractive:\nwe would have to implement formatting a great number of datatypes, and provide a mechanism for other extensions to hook formatters for their own datatypes into Data Bridge.\nBy contrast, the server-side code already knows how to format every datatype for presentation,\nboth for repository and client functionality.\nWikibase has an API for formatting individual values (`wbformatvalue`),\nbut this is limited to a single value at a time,\nwhich means formatting several rich references may require a lot of API requests.\nAdditionally, some values should be combined in the reference:\nmost importantly, the reference URL and title should be combined into a link with that target URL and text.\nIn the future, Data Bridge will also support editing of references ([T245640]),\nso there is also a need to support displaying references that have not yet been saved, and are not present on the target item.\n\n## Decision\n","To format these references, we will add an action API module to Wikibase Client.\nThis module will accept a reference JSON blob and return HTML.\nKnowledge such as combining certain values, or the order of properties, is encoded into this module:\nData Bridge should be able to use its output unmodified.\nWe envision that this functionality will also be useful to others:\nboth as an API module, for other API users,\nand as a Lua function, for wiki content\n(especially on wikis that don’t have their own sophisticated reference formatting modules yet).\nHowever, the initial version will be marked internal, and only serve the needs of Data Bridge.\nThe action API is chosen because Wikibase does not yet use any other style of API,\nso it is the most straightforward option.\nWikibase Client (rather than Wikibase Repository) is chosen\nbecause the desired snak formatting aligns more closely with that on the client.\n","Data Bridge will use an HTML template for reference formatting, stored on the client. \nThis template may be hard-coded or a specially designed datarow. \nThe template will be filled with placeholder values, which are replaced by server-side calls to `wbformatvalue`.\n\nReferences that have not yet been saved, and are not present on the target item when displaying the item, will also be rendered in this way."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen changes to the markup used on entity pages are made we often end up with outdated content in the ParserCache.\nThis can result in user facing errors from either the backend code attempting to perform processing on ParserCache output or frontend Javascript attempting to access parts of the DOM that have changed.\nExamples of this are:\n* [T228978] - Backend attempting to fill placeholders that it doesn't expect to be there.\n* [T205330] - Frontend looking for a newly introduced data-attribute that isn't present in historic parser cache entries.\nIn the not distant past we have solved these problems by introducing a custom [RejectParserCacheValue Hook] in operations/mediawiki-config e.g. (https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/463221) to reject cache entries made before deployment time.\nWe've also done it in stages leaving some invalid entries in the cache to avoid increased load from marking all entries as invalid at once.\nWhile this has worked for us in the past we have typically applied it after discovering a problem. Even then the way we gradually reject historic invalid cached results in user facing errors for some time.\n\n## Decision\n,"We should record things that impact the ParserOutput content (e.g. the version of the code used to generate the ParserOutput) in the parser options. These parser options should then be marked as used in generating the ParserCache key. The RejectParserCacheValue can be used if the two versions of the ParserOutput content cannot coexist and backwards compatibility is not possible.\nIn this way we will split the ParserCache into old and new versions. An example of this was trialled when introducing new mobile Termbox. See: https://gerrit.wikimedia.org/r/c/mediawiki/extensions/Wikibase/+/529055.\nIn the event that we are tracking a new part of the code that doesn't currently have a corresponding option then it may be necessary to ensure it is introduced. The ParserOptions used to calculate the ParserCache key are determined from existing cache entries. Thus on introducing a new key (but not a new key value) a custom RejectParserCacheValue Hook may still be required. See: https://gerrit.wikimedia.org/r/c/mediawiki/extensions/Wikibase/+/529059 for an example.\nAn option already exists (but hasn't been used in the last 4 years) on [EntityContent] ([EntityHandler::PARSER_VERSION]) which may be useful for some changes.\nHowever, currently this applies to all entities; in general we usually only make changes to one type of entity at a time so more options will be needed.\n",We will use an opt-in hook in Squid to invalidate the ParserCache whenever our config changes. This will avoid future cache invalidation issues when changes are made to the markup used on entity pages.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWikibase uses cache in different parts of PHP code base. Wikibase itself does not provide any abstraction for cache implementation. Instead MediaWiki-specific classes, such as abstract `BagOStuff`, or `WANObjectCache`, are used in code.\nThe [PSR-16] standard defines an abstract Simple Cache interface for use in PHP code.\nMore formal and verbose Caching interface has been defined as [PSR-6].\nWe find it overly verbose compared to simpler [PSR-16], which we favour here.\nPossible use of [PSR-16] in MediaWiki is out of scope of this decision.\nIt might be a possible next step.\nIn this regard it is possibly worth mentioning that using [PSR-6] interface in MediaWiki has been proposed in 2016, but it has been declined (see: [T130528]).\nIt should also be noted that there is already a PSR-6 adapter for the `BagOStuff`: https://packagist.org/packages/addshore/psr-6-mediawiki-bagostuff-adapter.\nThere are already adapters that can use a PSR-16 cache as a PSR-6 cache and vice versa: https://symfony.com/doc/current/components/cache/psr6_psr16_adapters.html.\nAlso, as `Psr\Cache\CacheItemPoolInterface` and `Psr\SimpleCache\CacheInterface` declare only one method of the same name, `clear()`, and as both declarations have the same signature, any cache system may implement both interfaces.\n\n## Decision\n","We will introduce PSR-16-compliant cache interface to Wikibase.\nNew PHP code using cache will use this abstraction, instead of binding directly to MediaWiki cache classes, or any other specific third-party implementations.\nWe will use [psr/simple-cache] library to add `CacheInterface` to Wikibase.\n","**Adopt PSR-16 as an abstraction for cache implementation in PHP**\n\n**Benefits:**\n\n* PSR-16 provides a simple and concise interface for cache implementation, which is easier to use and understand than PSR-6.\n* PSR-16 is supported by a variety of cache implementations, making it easy to choose and use the best one for a given application.\n* PSR-16 is compatible with PSR-6, allowing for easy migration to a more formal and verbose caching interface if desired.\n\n**Drawbacks:**\n\n* PSR-16 is less verbose than PSR-6, which may make it more difficult to understand for some users.\n* PSR-16 does not provide as much functionality as PSR-6, which may be a limitation for some applications.\n\n**Alternatives considered:**\n\n* PSR-6: A more formal and verbose caching interface that provides more functionality than PSR-16.\n* MediaWiki-specific classes (e.g., BagOStuff, WANObjectCache): These classes are difficult to use and understand, and they do not provide a consistent interface for cache implementation.\n\n**Decision rationale:**\n\nPSR-16 is a simple and concise interface that is easy to use and understand. It is supported by a variety of cache implementations and is compatible with PSR-6. These benefits make PSR-16 the best choice for an abstraction for cache implementation in PHP."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`wbeditentity` summary message for Wikibase Items and Properties provides\nvery little information on what have been changed on the entity, with the main\nmessage in the summary being ""Item Changed"".\nIn [T220696], we want to update those summary\nmessages generated for Wikibase Items and Properties when using `wbeditentity`\nto be more detailed and informative.\nWikibase Lexeme achieved generating detailed summary messages by:\n1. making its  implementations update the summary object passed to\nthe change op instance with granular information about the change.\n2. implementing [SummaryAggregator] class and using it internally in some\n[ChangeOp] implementations (some of those that are non-leaf nodes and\ncontain further change ops) in order to combine those summary messages\ngenerated by leaf nodes (the effective change ops in the tree).\n\n## Decision\n","We decided to go with the second option, the visitor pattern design.\nIn order to achieve that, we will make [ChangeOp::apply()] return\n[ChangeOpResult] that is defined by the following IDL:\n```php\n// encapsulates the result of applying a change op to an entity document\ninterface ChangeOpResult {\n// the id of the entity document that the change op was applied to\nEntityId getEntityId();\n// whether the entity document was actually changed in any way\n// as a result of applying the change op to it\nbool isEntityChanged();\n}\n```\n### Next steps\n1. Change [ChangeOp::apply()] to return [ChangeOpResult]\n2. Provide needed implementations of [ChangeOpResult] that can capture the result\nof applying the different [ChangeOp] implementations.\nExample:\n[ChangeOpLabel] will probably return something like [ChangeOpLabelResult] that\nis defined as:\n```php\ninterface ChangeOpLabelResult : ChangeOpResult {\nstring getOldLabel();\nstring getNewLabel();\nstring getLanguageCode();\n}\n```\n3. Update implementations of [ChangeOp] in Wikibase and [WikibaseLexeme] to conform\nwith the changes to the interface.\n","Implement the following two items for Wikibase Core:\n\n1. Like in the Lexemes case, update Wikibase Item and Property implementations that use `wbeditentity` to update the summary object passed to the change op instance with granular information about the change.\n2. Implement a `SummaryAggregator` class and use it internally in [ChangeOp] implementations (some of those that are non-leaf nodes and contain further change ops) in order to combine those summary messages generated by the leaf nodes (the effective change ops in the tree)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPHP’s [autoloading][] mechanism allows for a class or interface to be loaded from a file automatically when it is first needed,\nrather than having to `require_once` the correct file to ensure that it has been loaded.\nThe industry standard way to autoload classes is to follow the [PSR-4][] standard.\nIt took a while to make Wikibase fully follow this standard\n(more details may be found [on MediaWiki.org][Wikibase/History/Autoloading]),\nbut once the option was available via the `AutoloadNamespaces` in `extension.json`,\nthere was no question of whether we wanted to do this:\nit was just a matter of putting in the work.\nThe only real conceptual issue in migration were the maintenance scripts.\nMediaWiki maintenance scripts are traditionally stored in files beginning with lowercase letters –\nas in MediaWiki’s `dumpLinks.php`, so also in Wikibase’s `dumpJson.php`.\nHowever, the classes inside those files begin in uppercase, like other classes (`DumpLinks`, `DumpJson`).\nThis was not a problem back when we used other autoloading mechanisms,\nbut PSR-4 requires that the class and file name match exactly\n(aside from the `.php` file name extension),\nincluding in case.\nThis means that the maintenance script classes could not be autoloaded via PSR-4 in their current form.\nMost of the maintenance scripts did not *need* to be autoloaded at all.\nA few were referenced by class name in the `DatabaseSchemaUpdater` or in tests.\n\n## Decision\n","Use PSR-4 for all regular source code files.\nEvery file that resides under an `includes/` or `src/` directory must conform to PSR-4.\nFor the maintenance scripts,\nadd the two scripts that are needed for `DatabaseSchemaUpdater` to `AutoloadClasses`.\nScripts that are needed for unit tests are loaded via `require_once` directly in those unit tests.\n(This corresponds to option 4 above.)\nEncourage the use of PSR-4 compliant, initial upper case file names for any future new maintenance scripts.\n","Update test and schema-updater references to use namespaced class names, and keep the maintenance scripts with lowercase names."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome parts of Wikibase are needed in both WikibaseRepo and WikibaseClient extensions. They include:\n* PHP Code (Autoloaded)\n* MediaWiki Hooks\n* ResourceLoader Modules\n* i18n Messages\nCurrently, they are made available to Repo and Client by being stored in a separate, third, extension named WikibaseLib.\nThis extension has grown over time and become incoherent. When adding some code or functionality that can be shared between\nRepo and Client developers see this as the obvious place to put it.\nThis results in the continued growth and increased lack of structure.\nAlternative solutions were considered and rejected:\n* Registering shared features in Client and then making Repo depend on Client\n* Appears to go against the general logical flow of data\n* Gives the impression of tighter coupling\n* Keeping Lib as an extension and trying to keep the structure more defined\n* Still leaves a place future developers may be tempted to ""dump"" shared logic\n\n## Decision\n",We will stop registering WikibaseLib as a separate extension. Parts that are needed in either Wikibase Client or Repo will\nbe registered in both the Client and Repo `extension.json`. These parts encompass: Autoloaded PHP code; MediaWiki Hooks;\nResourceLoader Modules and i18n Messages.\n,Move some parts of WikibaseLib to separate extensions. Specifically:\n* Remove PHP code from WikibaseLib\n* Move autoloaded PHP code to extensions so that they are part of the main autoloader\n* Move MediaWiki hooks to extensions\n* Move ResourceLoader modules to extensions if they correspond to functionality that is in those extensions\n* Move i18n messages to MessageGroups (in extensions) as appropriate
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith Wikibase soon to have official releases there is a need to present the changes to current and potential users. The release strategy team has reviewed the release process of MediaWiki and decided to use this as a model for what to cover in the release notes of Wikibase.\nThe release notes of MediaWiki are kept within the source repository and worked on throughout the development process. Each release branch contain a document describing the changes that was added from the previous version.\nThese are the different topics covered in the MediaWiki release notes [template](https://gerrit.wikimedia.org/r/c/mediawiki/core/+/611247/3/RELEASE-NOTES-1.36):\n- **Configurations**: Configuration settings for system administrators with a link to the documentation of the setting that has changed.\n- **New configurations**: A list of new settings describing what setting has been introduced, and a short description on how to use it\n- **Changed configurations**: A list of settings that has changed behavior some way that needs announcement. This could be due to a change in the datatype, deprecation, changed default value etc.\n- **Removed Configurations**: List containing the settings that has now been removed and was announced deprecated in the previous version\n- **Features**\n- **New user-facing features**: List of changed users-facing features, changes to the user interface, changes to user accessible pages etc.\n- **New developer features**: Changes to the codebase that requires announcement, function signature changes, new hooks etc.\n- **External Library changes**: Changes to library dependencies, npm, composer etc.\n- **New external libraries**\n- **New development-only external libraries**\n- **Changed external libraries**\n- **Changed development-only external libraries**\n- **Removed external libraries**\n- **Action API changes**: Changes to the action api describing what endpoint has changed and what the new behavior is.\n- **Languages updated**: New languages added or other changes to the internationalization of the software.\n- **Breaking changes**: Changes to the software that was available in the previous version but now is no longer accessible due to changed visibility or being removed.\n- **Deprecations**: Parts of the codebase now marked as deprecated with a descriptive text suggesting what alternatives to use.\n- **Other changes**: Changes to this version that does not fit under any other topic but still needs announcement.\nThis template is a good start for what to include in the release notes, however as there are some fundamental differences between MediaWiki and Wikibase the release strategy team decided it's a good idea not to include some aspects of the template as they might not bring that much value to it's potential readers.\n\n## Decision\n","We will adopt the template but with the following changes.\n##### Include ""REST API changes""\nAs there is an ongoing effort to introduce support for the REST API we also think it's a good idea to include this topic already to announce any progress that is made on upcoming releases.\n##### Don't include ""New developer features"" or changes to ""development-only external libraries""\nOne main difference between MediaWiki and Wikibase is that Wikibase is an extension to MediaWiki and our external developer community is not as big. To reduce the burden on the team we think it's a good idea to not include these.\nThe final template will cover these topics.\n- **Configurations**\n- New configurations\n- Changed configurations\n- Removed Configurations\n- **Features**\n- New user-facing features\n- **External Library changes**\n- New external libraries\n- Changed external libraries\n- Removed external libraries\n- **Action API changes**\n- **REST API changes**\n- **Languages updated**\n- **Breaking changes**\n- **Deprecations**\n- **Other changes**\n","The release notes of Wikibase will be based on the template used in the release notes of MediaWiki, but with the following modifications:\n\n- The ""Configurations"" section will be omitted, as Wikibase does not have configurable settings for system administrators.\n- The ""New external libraries"" and ""New development-only external libraries"" sections will be omitted, as these are not relevant to Wikibase.\n- The ""Breaking changes"" section will be renamed to ""Backward-incompatible changes"" to better reflect the nature of the changes that will be documented in this section."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe [new Termbox (v2)](https://gerrit.wikimedia.org/g/wikibase/termbox) introduces an external ""SSR"" service to Wikibase that generates the server-side markup for the section showing labels, descriptions, and aliases of Item and Property pages. This section contains user-specific content such as their preferred languages (likely determined by the Babel extension), and whether the ""more languages"" section is expanded or collapsed on page load. User-specific configuration can result from a logged-in state or information persisted in an anonymous user's session/cookies.\nAt the time of writing the following parameters influence the result of the Termbox (v2) service and would have consequently to be taken into account when caching results:\n* entity id (✓ accounted for by `ParserCache`)\n* entity revision (✓ accounted for by `ParserCache`)\n* interface language (✓ accounted for by `ParserCache`)\n* user's preferred languages\n* toggle state of the ""in more languages"" section\nTypically, only non-user-specific markup of entity pages is added to MediaWiki's `ParserOutput` and cached within `ParserCache`.\nThe [PlaceholderEmittingEntityTermsView] PHP version of the Termbox (v1) works around the `ParserCache` by injecting markers in place of the user-specific markup before it's added to `ParserOutput` and replacing the markers with the user-specific markup before the page is rendered in `OutputPageBeforeHTMLHookHandler::doOutputPageBeforeHTML`. This option is not viable for the new Termbox (v2) as it would split the logic of injecting and replacing markers across two services and would require the new service to be able to produce results for individual components which contradicts the architectural reality that it itself decides what to display, not only how (cue: ""application vs. renderer"").\nThe `ParserCache` drastically reduces the time it takes to render entity pages, it should consequently be leveraged whenever possible.\nFor wikidata item and property pages it is hit 3000 - 15000 times per minute in total (measured in January 2019).\nOf these 16 - 66 times per minute are requests by logged in users (measured in January 2019), which influences the ""preferred languages"" dimension warranting a customized result. This number may be further reduced by only considering those who have their preferred languages set to something other than their preferred interface language.\nNumbers for users with ""in more languages"" toggled to non-default state are not available at the time of writing. It is assumed that only a small portion of users request a customized toggle state.\n\n## Decision\n","The non-user-specific version of the Termbox (v2) is cached as part of the `ParserOutput`.\nFor users with a user-specific configuration identical to the default values the cached result will be served.\nFor users with a user-specific configuration different from the default values, a request is performed against the Termbox (v2) service later in the request life cycle in `OutputPageBeforeHTMLHookHandler::doOutputPageBeforeHTML` and its result is served.\nIn case the SSR service does not produce a usable result (e.g. not reachable), [an empty element is served](https://gerrit.wikimedia.org/g/mediawiki/extensions/Wikibase/+/1a2a9397df3a6df8df5db346b7e8605c97ab9e2d/view/src/Termbox/TermboxView.php#22) that will be replaced by the client-side rendered version of the Termbox. More complex fallback scenarios are conceivable but not part of this ADR and up to product management to request depending on the value of focusing on this.\n","**Decision:** Cache termbox results for non-logged-in users.\n\n**Rationale:**\n\n* The Termbox service introduces user-specific content that affects the markup generated for entity pages.\n* The `ParserCache` should be leveraged to improve performance, but it cannot cache user-specific markup.\n* Logged-in users contribute a small portion of requests, so caching results for non-logged-in users will benefit a large majority of page views.\n* The toggle state of the ""in more languages"" section is expected to be customized by only a small portion of users, so caching results for the default state will also cover a significant percentage of requests."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Federated Properties MVP introduced a [federatedPropertiesEnabled] setting which enables a Wikibase instance (local wiki) to read all Properties from a remote Wikibase (source wiki). A wiki with this setting enabled can only use federated Properties and disallows the creation or use of any local Properties.\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\nTo achieve that we need to be able to differentiate between local and federated Properties.\n### Considered Actions\nWe considered two options:\n1. Build source information into the Property IDs via repository prefixes.\n2. Build source information into the Property IDs via Concept URIs.\n#### Prefixes\nThe administrator of the local Wikibase needs to configure a prefix for the source wiki, which will be used only for federated properties. Local properties will not have a prefix.\n**Pros**:\n- Easier to parse\n- Easier to type\n**Cons**:\n- Ambiguous without knowing the config\n#### Concept URIs\nConcept URIs can be set for any Wikibase and also have a default value. An example of referencing a federated property from Wikidata is `http://www.wikidata.org/entity/P31`.\nThe administrator of the local Wikibase needs to configure the concept URI for the source wiki in case it's not Wikidata.\nSince concept URIs guarantee to end in the entity's ID we can think of them as prefixes. In the above example `http://www.wikidata.org/entity/` is the prefix. This way we don't need to worry about parsing them to extract the entity ID.\n**Pros**:\n- Unambiguous\n- Standard practice in the linked data world\n**Cons**:\n- Typing urls in the browser when containing e.g. '&' is not nice. People might not remember to url encode.\nWe think having symbols like '&' in the URL in unlikely. Also, API Sandbox can be used to test the API requests and figure out the right format if needed.\n- Hard to change / Out of “our” control if they change\nIf we use the URI only as a prefix to the entity, and not as an actual URL that will be used as such, then this shouldn't be a problem. We acknowledge it's possible for people who use the API to treat it like an URL.\nWe feel responsible only if Wikidata's URI changes, as we anticipate shipping an example configuration for Wikidata which would then need updating. URIs configured for a source wiki different than Wikidata will have to be updated by the administrator of the local wiki in case they change.\n\n## Decision\n","Build source information into the Property IDs via Concept URIs, treating them like a prefix.\n","**Decision**: Use both Prefixes and Concept URIs.\n\n**Rationale**: Prefixes provide an easy way to differentiate between local and federated properties, while concept URIs provide a clear and unambiguous way to reference federated properties. By using both prefixes and concept URIs, we can:\n\n* - Provide an easy way to identify federated properties\n* - Ensure that federated properties can be unambiguously referenced\n* - Minimize the risk of confusion between local and federated properties\n* - Support a variety of use cases for federated properties"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWikibase-related code is found in various places and under different degrees of isolation.\nParts of the PHP code, especially early / fundamental parts like the data model and its serialization,\nare independent libraries in separate Git repositories, distributed via Composer.\nSome JavaScript code, such as the new Termbox,\nis also developed independently, and typically included as a Git submodule.\nOn the other hand, much of Wikibase is included directly in the Wikibase Git repository,\noften as part of the monolithic “Lib” component.\nPart of the goal of the Decoupling Hike (June to August 2020) has been to develop a strategy for refactoring this “Lib” component.\nWe found that the separate libraries have obvious benefits thanks to being stand-alone, separate components,\nbut also have downsides for development:\nit is inconvenient to have to develop the libraries in separate repository,\nand cumbersome to get the changes back into the main Wikibase Git repository.\nAt best, a submodule pointer needs to be updated;\nat worst, the library needs to publish a new release, which then needs to make its way into [mediawiki/vendor][].\nWe investigated a “monorepo” approach in [T254920][],\nand propose that it offers the best of both worlds.\nMonorepos are also used by others, including the Symfony PHP framework\n([blog post][Symfony blog post], [talk video][Symfony talk video]).\n\n## Decision\n","Wikibase.git will become a monorepo,\ncontaining not just the overall MediaWiki extension code\nbut also the code and history of libraries that can stand on their own.\nChanges to those libraries become immediately effective within Wikibase,\nbut are also published separately.\nWhere possible, sections of Wikibase Lib will be extracted into separate libraries.\nDependencies on MediaWiki are removed\n(or replaced with suitable MediaWiki libraries, e.g. [wikimedia/timestamp][] instead of `wfTimestamp()`).\nTheir source code is moved from subdirectories of `lib/includes/` and `lib/tests/phpunit/` into a subdirectory of `lib/packages/`\n(adjusting the paths in extension `AutoloadNamespaces` and elsewhere),\nnamed after the prospective Composer package name,\nand a `composer.json` file is added there.\nThe [git filter-repo][] tool can then be used to extract a subset of the Wikibase Git history with only the changes relevant to the new library;\nthis new read-only repository can be used as the VCS source of the Composer package,\nand is automatically updated through a GitHub action (see e.g. `.github/workflows/filterChanges.yml`).\nThe Decoupling Hike team demonstrated these steps for the [wikibase/changes][] library;\nsee [T256058][] and related tasks for details.\nFormerly stand-alone libraries will be merged into the Wikibase Git repository.\nTheir history will be preserved, and they will also be extracted into separate Git repositories again,\nusing the [git filter-repo][]-based process outlined above.\nWe expect that it will be possible to produce identical Git hashes,\nmaking this migration transparent to other users of the libraries’ Git repositories –\nthe repositories will simply no longer be the main source of truth.\nThe Decoupling Hike team has not done this for any existing library.\n",Separate all Wikibase-related code into independent Git repositories.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFederated Properties v2 aims to make it possible to use both federated and local Properties on a single Wikibase instance. We introduced a `FederatedPropertyId` class to refer to Federated Property IDs in the code. `FederatedPropertyId` objects are different from `PropertyId` objects in a few aspects since the former are intended to be used for data access of Properties via an API while the latter are used to get Properties directly from a database.\nThere are a few service interfaces such as `PropertyDataTypeLookup` which are used throughout Wikibase and need to work for Federated Properties and local Properties alike. To keep these type hints meaningful the two Property ID classes need to share a common parent type.\n\n## Decision\n,Create a Property ID interface.\n,Introduce an abstract base class `PropertyIdLike` as the parent type of both `PropertyId` and `FederatedPropertyId`.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWikibase is currently only tested to be compatible with MediaWiki core and other MediaWiki extensions for:\n- sets of commits that are the HEAD of the master branch at one point in time\n- cut releases: either for the alpha, weekly train releases (i.e. releases delivered on WMF wikis), or the twice-yearly(ish) official MediaWiki releases\nWe want to give 3rd party Wikibase users regular 6 monthly releases.\nMediaWiki release are currently not that regular.\nWe intend to produce new Wikibase features on a more regular basis than 6 monthly and we want these to be available to users\nas soon as possible. Both for users to benefit from new features but also to shorten our development feedback cycles.\nTo be able to release Wikibase at any point in time such that it remains possible to apply security patches to MediaWiki core and to\nany extensions that are not maintained by WMDE these releases need to be compatible with a ""cut release"" of MediaWiki.\nTo ensure this compatibility there are two options:\n- Backport Wikibase features, and any dependent patches to the last stable release of MediaWiki before releasing.\nThis was performed for the upcoming wmde1 release of Wikibase where patches made against master (i.e. 1.36 alpha releases) were backported to the Wikibase 1.35 branch\n- Keep Wikibase `master` compatible with the last stable release of MediaWiki\nDoing the backporting results in leaving a possibly unknown amount of work for whoever is to make the release and adds lots of uncertainty.\nThe additional backporting effort will need to be repeated for all upcoming Wikibase releases.\nThis approach also means maintaining two different (even if only in the sense of git history) versions of the same functionality.\nKeeping Wikibase compatible with last stable MediaWiki adds to every developer's workload.\nIt may result in having to delay using new features in MediaWiki or having to write a compatibility layer in order to use them.\nIt may result in developers inadvertently using a new feature and then discovering they need to either\nwrite some backwards compatibility layer for it or put that new feature behind a flag.\nDoing so may result in being more decoupled from MediaWiki in the long run.\nThe overhead of the additional development effort will possibly be increasingly lower\nthe more code has been written that way, and the more Wikibase releases have been published following this approach.\nKeeping Wikibase compatible with last stable MediaWiki would likely mean avoiding the ""double work"" effort\nof developing the feature against the master branch, and ""backporting"" it to be compatible with the last stable version of MediaWiki\nneeded for the Wikibase release for non-WMF users.\n\n## Decision\n",Only ensure Wikibase remains compatible with MediaWiki master. Do not enforce compatibility with last stable version of MediaWiki.\n,Keep Wikibase `master` compatible with the last stable release of MediaWiki.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Federated Properties MVP introduces a [federatedPropertiesEnabled] setting which enables a Wikibase instance (local wiki) to read all Properties from a remote Wikibase (source wiki). A wiki with this setting enabled can only use federated Properties and disallows the creation or use of any local Properties. Switching from Federated Properties mode back to using local Properties or vice versa is not supported.\nFederated Properties use API-based implementations of entity data access services. This ADR documents our decision regarding the dispatching mechanism for the respective services, i.e. how Wikibase determines whether to use a service for handling a local Property or a federated Property.\n\n## Decision\n","We decided that the second option outlined in the previous section is the better solution at this point in time, but we expect the decision to be revisited and likely superseded in the future. We are positive that most of the work we do now will be reusable if in the future we choose to go with the repository prefix dispatching approach. Any API-based service implementations can likely be reused without modification and only the surrounding wiring code will need adjustment. Choosing this simpler path leaves the decision of how to handle multiple Property sources with the responsible journey team.\n",Use a Service Dispatcher which encapsulates all of the different services and abstracts the dispatching mechanism.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe first patch introducing a usage of `declare( strict_types = 1 );` has [just been merged into Wikibase.git](https://gerrit.wikimedia.org/r/#/c/mediawiki/extensions/Wikibase/+/591971/).\ndeclare strict_types is also already used in some of our composer libraries.\n- Blog post explaining the feature: https://blog.programster.org/php-strict-types\n- Docs for strict types: https://www.php.net/manual/en/functions.arguments.php#functions.arguments.type-declaration.strict\nIn a nut shell, declaring strict types means that PHP will no longer automatically cast wrong scalar variables to the expected type, methods will require the correct type to be called.\n""Strict typing applies to function calls made from within the file with strict typing enabled, not to the functions declared within that file. If a file without strict typing enabled makes a call to a function that was defined in a file with strict typing, the caller's preference (weak typing) will be respected, and the value will be coerced.""\n**Why is strict typing beneficial**\n- avoid (subtle) bugs\n- make it easier to reason with code\n- [open up the possibility of (future) performance optimization at the language level](https://stackoverflow.com/questions/32940170/are-scalar-and-strict-types-in-php7-a-performance-enhancing-feature/32943651#32943651)\nIn order to proceed with use of strict_types we need to have an agreed upon strategy for rollout and maintenance.\nPHP Codesniffer already has a Generic.PHP.RequireStrictTypes sniff implemented that we can use to enforce a requirement of strict types declaration.\nCode for estimation of completion:\n```\nEST_DONE=$(grep --exclude-dir={vendor,node_modules,.idea,.phan} --include=\*.php -rnw . -e ""strict_types"" | wc -l) \\n&& EST_TODO=$(find ./ -type f -name ""*.php"" | wc -l) \\n&& EST_PRECENT=$(( $EST_DONE*100/EST_TODO )) \\n&& echo ${EST_DONE}/${EST_TODO} = ${EST_PRECENT}%\n```\n\n## Decision\n","We will progressively rollout and use strict_types across all PHP code as we touch the code in regular work.\nThis includes test classes and classes that may not immediately appear to benefit having strict types enabled (doesn't use scalar type hints).\nAny new code should always use strict_types and this should ideally be identified in code review.\nWe will continue to assess our advancement in rollout through time using the code snippet in the context section.\nThe [phabricator ticket](https://phabricator.wikimedia.org/T251382) should be updated so that progress is tracked.\nAt a point that we deem sensible we will enable the sniff for RequireStrictTypes, adding any missing files or directories to the exclusion list for the sniff.\nOnce strict_types is used everywhere this ADR will be deprecated.\n",**Decision:**\n\nIntroduce strict types incrementally to Wikibase.\n\n**Rationale:**\n\n* **Improved code quality:** Strict typing reduces the risk of subtle bugs and makes it easier to reason with code.\n* **Performance benefits:** Strict typing may open up the possibility for future performance optimizations.\n* **Phased approach:** Incremental introduction allows for gradual adjustment and minimizes disruption.\n\n**Implementation Plan:**\n\n1. Enable strict typing in new code and reintroduce it in code after refactoring.\n2. Use PHP Codesniffer to enforce strict typing requirements.\n3. Gradually increase the enforcement level in PHP Codesniffer until it becomes a hard requirement.\n4. Monitor code quality and performance metrics to assess the impact of strict typing.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRendering apps track pageviews and events to help us measure the performance\nof GOV.UK.\nStatic uses an [abstraction layer](https://github.com/alphagov/govuk_frontend_toolkit/blob/master/docs/analytics.md) in GOV.UK Frontend Toolkit, which reports the data to Google Analytics (but the underlying provider could be changed).\nFor analysing search performance, we want to be able to reliably track clickthrough rates for search result links.\n[Enhanced E-Commerce](https://developers.google.com/analytics/devguides/collection/analyticsjs/enhanced-ecommerce) is a plugin for google analytics that can track impressions as well as clicks. We model each search result page as a ""product list"", and each content item as a ""product"".\n\n## Decision\n","We added a plugin for E-Commerce to Static, so we could try using it\nto measure search performance.\nWe didn't add an abstraction layer to GOV.UK Frontend Toolkit, because we don't\nknow if there is a wider need for it, and we wanted to validate our use case with GOV.UK data. The code could be moved to the Frontend Toolkit at a later date if necessary.\nWe're sending the search query as a custom dimension. This is a workaround: without the custom dimension, Enhanced-Ecommerce\ncan infer it from the path, but only for impression data. We couldn't get it to set the query for the click data.\nFor each ""product"" we send the id, position, and list ('site search result'). To avoid unnecessary data transfer when users are interacting with the page, we use Query Time Imports to add\nmetadata to products later.\nWe use publishing platform `content_id` as the unique id where available, and use the path as a fallback.\n",**DECISION**\nEnable Google Analytics Enhanced E-Commerce for search result pages.\n\n**RATIONALE**\nUsing Enhanced E-Commerce will allow us to track impressions as well as clicks for search result links. This data will be valuable for analyzing search performance and improving the clickthrough rate for search results.\n\n**CONSEQUENCES**\n* The GOV.UK Frontend Toolkit abstraction layer will need to be updated to support Enhanced E-Commerce.\n* The Google Analytics configuration will need to be updated to enable Enhanced E-Commerce.\n* The data collected by Enhanced E-Commerce will be available in Google Analytics reports.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** Use an Architectural Decision Record (ADR) template to document and track architectural decisions.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture and communicate architectural decisions.\n* They facilitate collaboration, transparency, and accountability.\n* ADRs help align team members and stakeholders on architectural choices.\n* They serve as a valuable reference for future maintenance and evolution of the system.\n\n**Consequences:**\n\n* The project will have a centralized repository of architectural decisions.\n* Decisions will be made through a structured process and documented in a consistent format.\n* The team will have a clear understanding of the rationale behind architectural choices.\n* Future changes to the system can be informed by the decisions recorded in the ADRs.\n\n**Alternatives Considered:**\n\n* **Unstructured documentation:** Decisions could be documented in meeting minutes, emails, or spreadsheets. However, this approach lacks structure and can result in inconsistent and incomplete documentation.\n* **Custom ADR format:** The team could develop a custom ADR format. However, this requires additional effort and may introduce inconsistencies across projects.\n* **Proprietary ADR tool:** There are proprietary tools available for managing ADRs. However, these tools may be expensive and may not provide the desired level of flexibility or customization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[`ARM-software/CMSIS_5`](https://github.com/ARM-software/CMSIS_5) has been ported to Mbed OS. However, it is split across multiple directories and it is not immediately obvious that it comes from that repository when imported with the [importer script](../../tools/importer/importer.py ).\n\n## Decision\n","The current proposal is to create new directories within the existing `cmsis/` directory:\n* `CMSIS_5/`: mirrors closely the structure used by `ARM-software/CMSIS_5/` keeping only the directories and files needed by Mbed OS and renames some directories in order to work with Mbed OS build tools. See [importer configuration file](./../tools/importer/cmsis_importer.json).\n* `device`: includes Mbed OS specific files to configure RTX.\nThis will result in the removal of the `rtos/source/TARGET_CORTEX/` directory.\nAdditionally, `cmsis/TARGET_CORTEX_A/TOOLCHAIN_IAR/cmain.S` to `platform/source/TARGET_CORTEX_A/TOOLCHAIN_IAR/cmain.S` so it can be accessible when building with the bare metal profile. Note that we already have the equivalent file for TARGET_CORTEX_M at `platform/source/TARGET_CORTEX_M/TOOLCHAIN_IAR/cmain.S`.\nIt will provide the following advantages:\n* Better maintenance of the CMSIS component\n* Easy creation of an independent CMake target that can be built as a library that can be optionally be linked against by end users\n* Easy source navigation that mirrors closely the upstream CMSIS_5 repository\n",**Decision:** Consolidate the CMSIS_5 directory and its subdirectories\n\n**Rationale:** This would make it clearer that the code comes from CMSIS_5 and would make it easier to manage\n\n**Consequences:**\n\n* The directory structure of the Mbed OS repository would change\n* The importer script would need to be updated to reflect the new directory structure
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn many projects we have a need to display [Location based](https://en.wikipedia.org/wiki/Breadcrumb_navigation#Types) breadcrumbs.\nEach breadcrumb needs to be able to:\n- display its name\n- compose a redirect url\nImportant thing to note here is that there are two scenarios how a user can end up in a specific view:\n1. The user starts from home page and incrementally moves towards the target location.\nThis way it's easy to build the breadcrumbs as the link the user clicks is probably going\nto also be the name of the breadcrumb to add (provided that it's an incrementation and not\nmajor reroute)\n2. The user goes to a view directly from a bookmark/by refreshing. That way we need to ensure\nthat we know what data we want to query, query it and wait until the results are in.\nBig part of this was exploring how flexible this feature should be.\nAnd turns out that there are a lot of *it depends* factors.\n### Factor 1 - urls structure\nLet's assume we are building an ecommerce. That ecommerce has categories and items assigned to categories.\nA url to a specific item could look somewhat like this:\n```\n/#/shop/category/fruits-111/category/citrus-222/item/lemon-999\n```\nThis way we specify a full location path to an item. But if we assume that each category has exactly one parent\n(except the root categories)\nthen we are free to leave out only one category:\n```\n/#/shop/category/citrus-222/item/lemon-999\n```\nWe can squash it even more if we assume that each item has only one category (or we are able to reliably choose the *main one*):\n```\n/#/shop/item/lemon-999\n```\nSo the first scenario is ideal for knowing what info to query from the backend. We simply destructure the url\nfor the three ids and query the backend for their respective data.\nIn any other case however we'll need a recursion of sorts that will be sure to know the whole path. That could be\neither by the BE on its own (however that's not ideal as that way we couple FE navigational map with BE)\nor we let BE return relevant info about that node and we can let FE decided if that the end of the loop.\nBut even if we are fine with long urls they still have other drawbacks. Imagine this very likely scenario:\nIn the home page of the shop we have categories listing in a sidebar and *all products* listing in the main screen.\nThat way, in order to build a redirect url for `lemon-999` product, we still need to query for its tree ancestors.\n### Factor 2 - do node types repeat?\nIn the example above we saw that a shop item can be preceded by any number of categories. That means that when querying\nfor data we have to be prepared that the appdb will be populated with, probably, a collection of `category` maps.\nBut in other domains we might have a structure that is far simpler. Take a school registry as an example:\n`school-111/grade-222/student-1337`. We know that the structure is always `school->grade->strudent`. That way our appdb\ncould be populated with something as simple as\n```clojure\n{:student {.. ..} :grade {.. ..} :school {.. ..}}\n```\n\n## Decision\n","Because of all the complexity this feature involves, the template will only be given:\n- a namespace with a UI component for displaying breadcrumbs registered in the appdb\n- re-frame subscription for getting breadcrumbs from the appdb\n- re-frame event handler for populating appdb with breadcrumbs data.\n- The main container will include breadcrumbs/main component. The component itself will decide\nwhether or not the content should be displayed based on existence of breadcrumbs\ndata in appdb.\nThe missing piece - where to get the breadcrumbs data **from** - will have to be individually applied given the specific project domain.\nHowever I assume that using `re-frame-async-flow-fx` might be helpful:\n```clojure\n(defn- set-breadcrumbs-event-fx\n[{:keys [db]} [_ category-id id]]\n{:pre [(:category db)\n(:shop-item db)]}\n{:dispatch [::breadcrumbs/set\n[{:title ""Shop""\n:url ""/#/shop""}\n{:title (get-in db [:category :name])\n:url (str ""/#/shop/"" category-id)}\n{:title (get-in db [:shop-item :name])\n:url (str ""/#/shop/"" category-id ""/"" id)\n:disabled true}]]})\n(rf/reg-event-fx ::set-breadcrumbs set-breadcrumbs-event-fx)\n(rf/reg-event-fx\n::fetch-breadcrumbs\n(fn [_ [_ category-id id]]\n{:dispatch-n [[::hydrogen-demo.category/get category-id]]\n:async-flow {:rules [{:when :seen-all-of?\n:events [::got ::hydrogen-demo.category/got]\n:dispatch [::set-breadcrumbs category-id id]}]}}))\n(rf/reg-event-fx\n::go-to-shop-item\n(fn [{:keys [db]} [_ category-id id]]\n{:dispatch-n [[::view/set-active-view :shop-item]\n[::fetch-breadcrumbs category-id id]\n[::get id]]\n:db (dissoc db :shop-item)\n:redirect (str ""/#/shop/"" id)}))\n```\n",### Decision:\n\n\nImplement a hypothetical architecture where the breadcrumbs will be calculated separately from the locations (or in parallel to being calculated from the locations).\n\nSpecifically: On page load we will query for the full location data and present the breadcrumbs in a raw format (not very user friendly). Then we will kickstart a task (running either in the front or in the background) that will calculate the user friendly labels for each breadcrumb item. Once the task will finish calculating the labels we will update the breadcrumbs with the calculated data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn some of our projects we configure ragtime migrations by explicitly defining\neach individual migration like so:\n```edn\n{:duct.migrator/ragtime\n{:database   #ig/ref :duct.database/sql\n:logger     #ig/ref :duct/logger\n:strategy   :raise-error\n:migrations [#ig/ref :foo.migration/create-foo-table]}\n[:duct.migrator.ragtime/sql :foo.migration/create-foo-table]\n{:up   [#duct/resource ""foo/migrations/create-foo-table.up.sql""]\n:down [#duct/resource ""foo/migrations/create-foo-table.down.sql""]}}\n```\nIs other projects, however, we use the sugar to include all migrations found in\na path:\n```edn\n{:duct.migrator/ragtime\n{:database   #ig/ref :duct.database/sql\n:logger     #ig/ref :duct/logger\n:strategy   :raise-error\n:migrations [#ig/ref :foo.migrations/dev\n#ig/ref :foo.migrations/prod]}\n[:duct.migrator.ragtime/resources :foo.migrations/dev]\n{:path ""dev/migrations""}\n[:duct.migrator.ragtime/resources :foo.migrations/prod]\n{:path ""prod/migrations""}}\n```\nThis is useful, yes. But it has some serious downsides:\n1. It is harder to catch a conflict when 2 developers push new\nmigrations. Especially if the conflict is in the ordering of the migrations\nonly. While if we would be using explicit `:migrations []` listing then there\nwill be obvious code conflicts.\n2. There is a problem of partial changes being applied and the other few changes\nfailing when a SQL sentence fails within a single migration file. On the\nother hand, doing explicit listing treats the whole migration file like a\nsingle transaction. So either all the SQL sentences in the migration file are\napplied, or all are rolled back.\nThere is an orthogonal issue that is related to this. The development migrations\nalways go at the beginning of the migrations list (or at the end, depending on\nthe order you specify in the `:migrations` key). But once you have at least one\nproduction migration applied and you need to add a new for development migration\n(or vice-versa, depending on the order specified in `:migrations`), it fails. It\nexpects to add the new development migration after the last development one (and\nbefore any of the production ones) and it cannot. The reason why is that the\nfirst production migration is already at that position.\nThus we need to make sure development migrations and production migrations are\nnot intertwined. One way to achieve this is by using separate ragtime migration\ntables for each class of migrations. ragtime library let us specify the name of\nthe table where a given list of migrations will be recorded. By using different\ntable names for development and production we can keep the migrations separate\nand guarantee the order of application in all cases.\nIn this case, as we will have more than one `:duct.migrator/ragtime`\nconfiguration, we will need to use composite Integrant keys for the development\nand production configurations.\nWe need to change `:duct.migrator/ragtime` in `config.edn` to:\n```edn\n{[:duct.migrator/ragtime :foo/prod]\n{:database   #ig/ref :duct.database/sql\n:logger     #ig/ref :duct/logger\n:strategy   :raise-error\n:migrations-table ""ragtime_migrations""\n:migrations [#ig/ref :foo.migration/create-foo-table]}\n[:duct.migrator.ragtime/sql :foo.migration/create-foo-table]\n{:up   [#duct/resource ""foo/migrations/create-foo-table.up.sql""]\n:down [#duct/resource ""foo/migrations/create-foo-table.down.sql""]}}\n```\nand change `:duct.migrator/ragtime` in `dev.edn` to:\n```edn\n{[:duct.migrator/ragtime :foo/dev]\n{:database #ig/ref :duct.database/sql\n:logger #ig/ref :duct/logger\n:migrations-table ""ragtime_migrations_dev""\n:fake-dependency-to-force-initialization-order #ig/ref [:duct.migrator/ragtime :foo/prod]}\n:migrations [#ig/ref :foo.dev-migration/create-dev-table]}\n[:duct.migrator.ragtime/sql :foo.dev-migration/create-dev-table]\n{:up   [#duct/resource ""foo/dev_migrations/create-dev-table.up.sql""]\n:down [#duct/resource ""foo/dev_migrations/create-dev-table.down.sql""]}\n```\nWe also need to add a fake dependency in `[:duct.migrator/ragtime :foo/dev]`\n(that `:duct.migration/ragtime` library completely ignores) to force the order\nof application of the migrations. With the configuration shown above, production\nmigrations will always be applied before development ones.\n\n## Decision\n","We will change the template to configure ragtime to use explicit listings of\nmigrations. The template will also configure two `:duct.migrator/ragtime`\nIntegrant keys, one for development and one for production. And set the order of\napplication of the migrations to production before development.\n",Use separate Ragtime migration tables for development and production migrations to ensure the order of application and prevent conflicts. Implement this by using composite Integrant keys for the development and production configurations.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFrom [12factor.net](https://12factor.net/config):\n#### Store config in the environment\nAn app’s config is everything that is likely to vary between deploys (staging,\nproduction, developer environments, etc). This includes:\n* Resource handles to the database, Memcached, and other backing services\n* Credentials to external services such as Amazon S3 or Twitter\n* Per-deploy values such as the canonical hostname for the deploy\nApps sometimes store config as constants in the code. This is a violation of\ntwelve-factor, which requires strict separation of config from code. Config\nvaries substantially across deploys, code does not.\nA litmus test for whether an app has all config correctly factored out of the\ncode is whether the codebase could be made open source at any moment, without\ncompromising any credentials.\nThe twelve-factor app stores config in environment variables (often shortened\nto env vars or env). Env vars are easy to change between deploys without\nchanging any code; unlike config files, there is little chance of them being\nchecked into the code repo accidentally; and unlike custom config files, or\nother config mechanisms such as Java System Properties, they are a language-\nand OS-agnostic standard.\nAnother aspect of config management is grouping. Sometimes apps batch config\ninto named groups (often called “environments”) named after specific deploys,\nsuch as the development, test, and production environments in Rails. This\nmethod does not scale cleanly: as more deploys of the app are created, new\nenvironment names are necessary, such as staging or qa. As the project grows\nfurther, developers may add their own special environments like joes-staging,\nresulting in a combinatorial explosion of config which makes managing deploys\nof the app very brittle.\nIn a twelve-factor app, env vars are granular controls, each fully orthogonal\nto other env vars. They are never grouped together as “environments”, but\ninstead are independently managed for each deploy. This is a model that scales\nup smoothly as the app naturally expands into more deploys over its lifetime.\n\n## Decision\n","Follow the twelve-factor advice and use configuration from the environment.\nWe will centralize the interface for the environment into a single location so\nthat we can enforce strict requirements on vars. For example, the application\nshould fail fast if a requirement is not valid. A `PORT` environment variable\nshould be validated that it is positive number within the accepted port values.\n#### Strictness\nWe're currently using the npm package [`envalid`][1] to provide this\nfunctionality. There are several libraries that serve this purpose.\n[`envalid`][1] was specifically chosen because it doesn't have any\nfunctionality that would allow developers to break this decision.\nSome libraries allow merge of config files. Some libraries allow usage of\n`NODE_ENV`. These features allow (if not encourage) developers to do the wrong\nthing.\n","Store configuration in environment variables to easily change it between deploys, prevent accidental check-ins to the code repository, and ensure language and OS agnosticity."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNode Exporter needs to be installed on Verify's infrastructure so that machine metrics can be gathered.\nVerify runs Ubuntu Trusty which does not have an existing node exporter package.\nVerify has an existing workflow for packaging binaries which can be leveraged to package node exporter.\n\n## Decision\n,Node exporter will be packaged as a deb using FPM following Verify's exiting packaging workflow.\n,**Build a Node Exporter binary using Verify's existing workflow and install it on Verify's infrastructure.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are looking to offer prometheus to non-PaaS teams.  The\ninfrastructure to be monitored will be run by another team (called\n""client team"" in this document), but we will provide one or more\nprometheus servers which will will be responsible for gathering\nmetrics from the underlying infrastructure.\nLonger term, we are aiming to provide prometheus as a service to\nmultiple environments across multiple programmes.\n### Non-suitability of existing infrastructure\nOur existing prometheus infrastructure (for PaaS teams) works by using\nour [service broker][] to generate file_sd_configs which prometheus\nthen uses to scrape PaaS apps over the public internet.\nThis approach won't work for non-PaaS teams, because it's based on an\nassumption – that every app is directly routable from the public\ninternet – that doesn't hold in non-PaaS environments.  Instead, apps\nlive on private networks and public access is controlled by firewalls\nand load balancers.\n[service broker]: https://github.com/alphagov/cf_app_discovery\n### Main problem to be solved: scraping apps on private networks\nAs the previous section explained, our main problem is that we want a\nprometheus (provided by us) to be able to scrape apps and other\nendpoints (owned by the client team, and living on a private network).\nWe want to do this in a way which doesn't require clients to\nunnecessarily expose metrics endpoints to the public internet.\nSome other things we would like to be able to do are:\n- maintain prometheus at a single common version, by upgrading\nprometheus across our whole estate\n- update prometheus configuration without having to restart\nprometheus\n- allow client teams to provide configuration (for example, for alert\nrules)\n- perform [ec2 service discovery][] by querying the EC2 API (for\nwhich we need permissions to read client account EC2 resources)\nThere are several ways we might provide a prometheus pattern that\nallows us to scrape private endpoints:\n- provide an artefact to be deployed by the client team\n- client team provides IAM access to us and we deploy prometheus\nourselves, within their VPC\n- we build in our own infrastructure and use VPC peering to access\nclient team's private networks\n- we build in our own infrastructure and use VPC Endpoint Services as\na way to get prometheus to access client team's private networks\n[ec2 service discovery]: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cec2_sd_config%3E\n### Provide an artefact to be deployed by the client team\nThe artefact we provide could take several forms:\n- an AMI (amazon machine image) which the client team deploys as an\nEC2 instance\n- a terraform module which the client team includes in their own\nterraform project\nThis model has the downside that it doesn't allow us to maintain\nprometheus at a single common version, because we are at the mercy of\nclient teams' deploy cadences to ensure things get upgraded.\n### Client team provides IAM access so that we can deploy prometheus ourselves\nIn this model, the client team would create an IAM Role and allow us\nto assume it, so that we can build our own prometheus infrastructure\nwithin their VPC.\nThis would mean that the client team needs to do some work to provide\nus with the correct IAM policies so that we can do what we need to do,\nwithout giving us more capability than they feel comfortable with.\nThe client team would have visibility over what we had built, and\nwould be able to see it in their own AWS console.  However, they would\nlikely not have ssh access to the instance itself.\nOne possible issue with this model is that we're beholden on the\nclient to provide us with a CIDR range to deploy onto, and depending\non their existing estate, private IPs may be in short supply.\nYou're also dependent on their networking arrangements.  You will need\nto ask questions like:\n- how can you get in and out of the network?\n- do you need to download packages to install?  how will that work?\n- do you need to download security updates?  how will that work?\nThe answers to these questions may be different for different client\nteams.  This means that our prometheus pattern needs to be flexible\nenough to cope with these differences, which will take extra\nengineering effort.\nWe would need to work out what the integration point between our teams\nwould be.  This could be:\n- terraform outputs that appear in a remote state file\n- or stereotyped resource names/tags which can be matched using\ndata sources\nWhether or not we go with this option for deploying prometheus, if we\nwant to do ec2 service discovery (described above), prometheus will\nneed some sort of IAM access into the client team account anyway.\n### Use VPC Peering to provide access for prometheus to scrape target infrastructure\n[VPC Peering][] is a feature which allows you to establish a\nnetworking connection between two VPCs.  In particular, this is a\npeering arrangement which means that the two networks on either side\nof the VPC Peering arrangement cannot share the same IP address\nranges.\nCrucially for us, the VPCs can be in different AWS accounts.\nThis means that we could build Prometheus in an account we own, and it\ncould access client team infrastructure over the peering connection.\nRunning our own infrastructure in our own account without being\ndependent on client teams providing anything to us would make for\nsmoother operations and deployments for us.\nThis has a drawback for the client in that it adds extra points of\ningress and egress for traffic in the client networks.  This increases\nthe attack surface of the network which makes doing a security audit\nharder and makes it harder to have confidence in the security of the\nsystem.\nThere's also a drawback in terms of the combination of connections: as\nRE builds more services that might be provided to client teams, and as\nwe extend these services to more client teams, we end up with N*M VPC\npeering connections to maintain.\nAs RE (and techops more broadly) provides more services, client teams\nend up having to consider more VPC Peering connections in their\nsecurity analyses, and this doesn't feel like a particularly scalable\nway for us to offer services to client teams.\nFinally, we believe that VPC Peering is something that has to be\naccepted manually on the receiving account console (at least when\npeering across AWS accounts), which compounds the scaling problem.\nThere are two sub cases worth exploring here:\n#### A single prometheus VPC peers with multiple client VPCs\nIn this model, we would build a single prometheus service in a VPC\nowned by us, and it would have VPC peering arrangements with multiple\nclient team VPCs in order to scrape metrics from each of them.\nThis has the benefit that we can run fewer instances to scrape metrics\nfrom the same number of environments.\nThis has some drawbacks:\n- the single VPC becomes more privileged, because it has access to\nmore environments.  This means that a compromise of the prometheus\nVPC could lead to a compromise of more clients' VPCs.\n#### A single prometheus VPC peers with only a single client VPC\nIn this scenario, we would build a single prometheus in its own VPC\nfor each client team VPC we offer the service to.\nThis avoids some of the drawbacks of the previous case, in that the\nprometheus doesn't have privileges to access multiple separate VPCs.\n[VPC Peering]: https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/Welcome.html\n### Use VPC Endpoint Services to access scrape targets\nThis is a similar idea to VPC Peering.  [VPC Endpoint Services][] (aka\nAWS PrivateLink) provides a way to provide services to a VPC, again\npotentially in another account.\nThis allows you to make a single Network Load Balancer (NLB) appear\ndirectly available (ie without going through a NAT Gateway) in another\nVPC.\nIn the case of Prometheus, because it has a pull model, it seems\nlikely that the only way we could make this work would be by having\nthe client team provide the endpoint service and prometheus consume\nit.  This would mean the client team would need to add a routing layer\n(possibly path- or vhost-based routing, possibly using an ALB) to\ndistribute scrape requests to individual scrape targets.\nThis has the following advantages:\n- the IP address spaces in the prometheus VPC and the client team VPC\nare completely independent\nHowever it has some drawbacks:\n- it feels like we're not using Endpoint Services in a designed use\ncase. in other words, it feels like a bit of a hack.\n- Prometheus is designed to be close to its targets, so that there\nare fewer things to go wrong and prevent scraping.  The more layers\nof routing between prometheus and its scrape targets, the more\nchance we'll lose metrics during an outage, exactly when we need\nthem.\n[VPC Endpoint Services]: https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/endpoint-service.html\n\n## Decision\n",- Deploy Prometheus into Verify Performance Environment to test concept\n- Deploy an instance of Prometheus into the client teams VPC\n- Use Ubuntu 18.04 as the distribution for the instance [ADR-7](0007-use-ubuntu-18-04.md)\n- Use Cloud Init to configure Prometheus [ADR-9](0009-use-cloud-init-to-build-prometheus-server.md)\n- Use Verify infrastructure initially for the PoC [ADR-8](0008-use-of-egress-proxies.md)[ADR-10](0010-packaging-node-exporter-as-deb-for-verify.md)\n,"We will build a single prometheus service in a VPC owned by us, and it would have VPC peering arrangements with multiple client team VPCs in order to scrape metrics from each of them."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nhttps://trello.com/c/56qyWJ60/675-show-an-sli-how-reliably-do-we-deliver-pages\nWe wish to have a service level indicator for how reliably do we deliver pages. We believe the way to measure this is to calculate for a given time period:\n`the number of incidents created in pagerduty / the number of incidents that we expect to have been created in pagerduty`\n### Calculating the number of incidents created in pagerduty\nWe think we can work out how many incidents there have been within a provided timeframe using the pagerduty API. We have done this for our pagerduty account successfully using a few lines of Ruby code. We would need to have access to every other teams account in order to know about all incidents and not just ones for our team. We did not spend time trying to actually do this. We would also need to run a exporter to export this information from pagerduty so Prometheus could scrape it.\n### Calculating the number of incidents that we expect to have been created in pagerduty\nThe main source of information is the `ALERTS` metric in Prometheus but there are a few problems with this.\n#### Problem 1\nAt the moment we don’t have a way of identifying which alerts are tickets and which are pages. Some metrics include this information using labels but not all do. We could solve this if needed to by adding severity labels to all alerts and adding documentation so our users would also do this.\n#### Problem 2\nPrometheus doesn’t provide a metric for how many incidents we believe should have been created. Prometheus instead has metrics which measure if alerts are currently firing. We would need to reliabily turn the `ALERTS` metrics into a single metric for how many incidents we believe should have been created.\nWe came up with:\n```count(ALERTS{alertstate=""firing"", severity=""page""}) or vector(0)```\nWe think that from here we can use the `increase` function to tell us how many times alerts have begun firing. To use this we think we would need to use recording rules as per https://www.robustperception.io/composing-range-vector-functions-in-promql to turn our query into a single range vector.\nAt that point we should have a number for how many alerts have begun firing in a given time period. However we are not confident that this number is equal to the number of pagerduty incidents we expect to be created. The reason for this is because Alertmanager [groups firing alerts](https://prometheus.io/docs/alerting/alertmanager/#grouping), meaning multiple firing alerts may only result in one notification and therefore one incident. A potential way around this would be to try and edit the grouping behaviour of Alertmanager using it's config but it [doesn't look it's possible to turn it off completely](https://groups.google.com/forum/#!topic/prometheus-users/35znfrwu_z8). There could also be issues if an alert fires, then resolves itself, and then fires immediately after only triggering an single incident.\n\n## Decision\n",We have decided not to try and implement this SLI at the moment as we are not confident that we can accurately calculate the number of incidents we expect in Pagerduty for a given time period using metrics from Prometheus. It might be possible but would require a few days to investigate if so and would likely end up with a somewhat complex system to measure this SLI. We could change this decision if we become more confident.\n,"**Decision:**\n\nUse the following formula to calculate the service level indicator (SLI) for page delivery reliability:\n\n```\nSLI = (Number of incidents created in PagerDuty) / (Number of incidents expected to be created in PagerDuty)\n```\n\n**Rationale:**\n\nThis formula measures the percentage of expected incidents that were actually created in PagerDuty, providing a reliable indication of how well the system is delivering pages.\n\n**Implementation Details:**\n\n* Calculate the number of incidents created in PagerDuty using the PagerDuty API.\n* Calculate the number of incidents expected to be created in PagerDuty by converting the `ALERTS` metric in Prometheus into a single metric using the following query:\n\n```\ncount(ALERTS{alertstate=""firing"", severity=""page""}) or vector(0)\n```\n\n* Use the `increase` function and recording rules to convert the query into a range vector, which represents the number of alerts that have begun firing in a given time period.\n* Adjust the Alertmanager configuration to disable grouping if possible to ensure that each firing alert corresponds to a single PagerDuty incident.\n* Export the SLI metric from PagerDuty to Prometheus so it can be scraped and monitored."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVerify employ egress proxies to control access to external resources.\nThese are a security measure to help prevent data from being exfiltrated from within Verify.\nThe Prometheus server will need access to external resources, notibly an Ubuntu APT mirror during the bootstrap process.\nThe Prometheus server should not setup it's own routes to bypass the egress proxy i.e. use a NAT gateway or Elastic IP, as this will potentially open up a route for data exfiltration.\n\n## Decision\n",The Prometheus server should use Verify's egress proxies and choice of APT mirror.\n,The Prometheus server will connect to external resources through the egress proxy.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wanted to gather metrics from our own infrastructure so that we can drive alerts when things go wrong. Amazon exposes platform-level metrics via CloudWatch.\nPrometheus provides a [cloudwatch_exporter](https://github.com/prometheus/cloudwatch_exporter) which makes these metrics available to prometheus.\nWe had a [spike](https://github.com/alphagov/prometheus-aws-configuration-beta/tree/cloudwatch) to see if we could use the Cloudwatch exporter to get Cloudwatch metrics and trigger alerts.\nAfter getting it to work with a number of metrics we were able to start to estimate costs. From these estimates we had concerns regarding the [high costs](https://aws.amazon.com/cloudwatch/pricing/)\nin running it due to the number of metrics requested using the Cloudwatch API.\nEach time the `/metrics` endpoint is hit triggers the exporter to retrieve\nmetrics from the Cloudwatch API. Every Cloudwatch metric that you request costs\nmoney (regardless of if you ask for 100 metrics using 1 metric per api call or\n100 metrics using 100 metrics per api call).\ncost = `number of metrics on /metrics page` x `number of scrapes an hour` x `number of hours in a year` x `price of requesting a metric using the API` x `number of prometheis running across all our environments`.\nBased on a simple assumption of 100 metrics requested, being scraped once every 10 minutes (6 per hour), 3 Prometheis in production, 3 Prometheis in staging and 4 in dev accounts, it would work out:\n`100 x 6 x 24 x 365 x $0.00001 x 10 = $525.6 per year`\nHowever if we wish to scrape at the same rate we would a normal target, e.g. 30\nseconds that would become roughly $10,500 per year.\n100 metrics also appears to be unlikely. Based on asking for just these ALB and EBS\nmetrics:\n```\nApplicationELB/RequestCount\nApplicationELB/TargetResponseTime\nApplicationELB/HTTPCode_ELB_5XX_Count\nEBS/VolumeWriteBytes\nEBS/VolumeReadBytes\nEBS/VolumeReadOps\nEBS/VolumeWriteOps\n```\nwe appear to be requesting roughly 4000 metrics per scrape. If we scraped our current config at a period of once every 10 minutes we would end up roughly $21,000 a year. If we scraped it every 30 seconds it would be about $420,000.\nBy requesting only ALB metrics in the dev accounts, we still produce about 160 API requests according to the `cloudwatch_requests_total` counter for each scrape. Somewhat strangely, this only returns about 30 timeseries so we are not sure if our config is incorrect and if the number of API calls could be reduced to closer match the number of timeseries.\nNote, as dev accounts have lots of resources e.g. volumes, there may be fewer\nmetrics requested for staging and prod as unlike the dev account we wouldn't be\nexporting metrics for other stacks.\nIt takes a long time to get a response from the /metrics endpoint as it needs to make many API calls. This causes slow response times for which our prometheus scrape config needs to allow for using the `scrape_timeout` setting.\nWe found the Cloudwatch exporter app to be very CPU intensive, we had to double our standard instance size to a `m4.2xlarge` to get it to work. We were also concerned about having such a CPU intensive task running in the same instance as Prometheus.\nBecause we fetch config from S3 using a sidecar container, there is a race condition between fetching config and starting cloudwatch_exporet.  We found that this condition was encountered every time, meaning either a 2nd restart of the task was needed or we would need to add a delay to the exporter starting. We did not try and solve this.\nThe exporter task is slow to start up, the health check needs longer than usual to pass health checks.\nWe spotted that several targets we are scraping, such as prometheus and alertmanager, are set at very short scrape intervals (every 5s), this seems excessive and we can likely change down to every 30secs regardless of this story.\nThere is roughly a 15 minute delay in Cloudwatch metrics.  The [Prometheus CloudWatch exporter README](https://github.com/prometheus/cloudwatch_exporter/blob/master/README.md) explains it:\n> CloudWatch has been observed to sometimes take minutes for reported values to converge. The default delay_seconds will result in data that is at least 10 minutes old being requested to mitigate this.\n\n## Decision\n",We will not use the cloudwatch_exporter to gather Cloudwatch metrics into prometheus.\n,We will not implement the CloudWatch exporter. The costs associated with using this exporter are too high for our organization.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have the requirement of adding some resources to the base cloud instances. We currently do\nthis via the [cloud.conf](https://github.com/alphagov/prometheus-aws-configuration/blob/375f34600e373aa0e4c66fcae032ceee361d8c21/terraform/modules/prometheus/cloud.conf) system. This presents us with some limitations, such as configuration\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\n\n## Decision\n",We have decided to move away from cloud.conf as much as possible and instead use it to instantiate\na masterless puppet agent which will manage the resources.\n,Use the [cloudformation](https://github.com/alphagov/terraform-aws-cloudformation-stack) module to define the metadata for each instance type.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to have separate environments for running our software at different stages of release.\nThis will be used to provide a location where changes can be tested without impacting\nproduction work and our users.\n\n## Decision\n,"We have decided to have N+2 separate environments: development,\nstaging and production. In development, we can create as many separate\nstacks as we want. The staging environment will be where the tools\nteam will test their changes. The production environment will run all\nof our users monitoring and metrics and poll each of their\nenvironments.\nAny code can be deployed to development environments.  Only code on\nthe `master` branch can be deployed to staging and production.\n","Implement separate environments for each stage of the software release process, including development, testing, staging, and production."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPrometheus needs to run on a Linux distribution. The choice of distribution impacts the methods of packaging, deploying and configuring Prometheus. It is important to consider what is currently used across GDS as prior experience of distribution is helpful when supporting the service. Ubuntu is currently in use in Verify and GOV.uk (and possibly others) which provides a pool operators with the experience to be able to support the service.\nAlternatives considered were Amazon Linux, the advantage of this distribution is that it is optimised to the Amazon cloud, however this advantage did not overcome the advantage of familiarity across GDS Reliability Engineering\n\n## Decision\n",Use Ubuntu 18.04 LTS\n,Use Ubuntu Linux distribution for running Prometheus.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Observe team is a part of the new platform team, which is building out kubernetes capability in GDS.\nThere is a long-term goal that teams in GDS should avoid running bespoke infrastructure, specific to that team, so that any infrastructure we run is run in a common way and supportible by many people.\nWe also have a desire to migrate off ECS.  ECS is painful for running alertmanager because:\n- ECS doesn't support dropping configuration files in place\n- ECS doesn't support exposing multiple ports via load balancer for service discovery\nKubernetes does not have either of these limitations.\nCurrently, we have a plan to migrate everything to EC2, in order to get away from ECS.  We have quite a bit of outstanding pain from the old way of doing things:\n- we have two different deploy processes; one using the Makefile and one using the deploy_enclave.sh\n- we have two different code styles, related to the above\n- we have two different types of infrastructure\nWe haven't fully planned out how we would migrate alertmanager to EC2, but we suspect it would involve at least the following tasks:\n- create a way of provisioning an EC2 instance with alertmanager installed (probably a stock ubuntu AMI with cloud.conf to install software)\n- create a way of deploying that instance with configuration added (probably a terraform module similar to what we have for prometheus)\n- actually deploy some alertmanagers to EC2 in parallel with ECS\n- migrate prometheus to start using both EC2 and ECS alertmanagers in parallel\n- once we're confident, switch off the ECS alertmanagers\n- tidy up the old ECS alertmanager code\nThis feels like a lot of work, especially if our longer-term goal is that we shouldn't run bespoke infrastructure and should instead run in some common way such as the new platform.\nNevertheless, we could leave alertmanager in ECS but still ease some of the pain by refactoring the terraform code to be the new module-style instead of the old project-and-Makefile style, even if we leave alertmanager itself in ECS.\n(Prometheus is different: we want to run prometheus the same way that non-PaaS teams such as Verify or Pay run it, so that we can offer guidance to them. The principle is the same: we want to run things the same way other GDS teams run them.)\n\n## Decision\n","1. We will pause any work migrating alertmanager to EC2\n2. We will run an alertmanager in the new platform, leaving the remaining alertmanagers in ECS\n3. We will try to migrate as much of nginx out of ECS as possible; in particular, we want paas-proxy to move to the same network (possibly same EC2 instance) as prometheus.\n4. We will refactor our terraform for ECS to be module-based rather than the old project-and-Makefile style, so that we reduce the different types of code and deployment style.\n5. We will keep prometheus running in EC2 and not migrate it to the new platform (although new platform environments will each have a prometheus available to them)\n","Do not migrate alertmanager to EC2; instead, migrate it to Kubernetes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nExisting self-hosted infrastructure at GDS has been managed in code\nusing tools like puppet, but in a somewhat ad hoc way with each team\ndoing things differently, little sharing of code, and much reinvention\nof wheels.  We would like to learn about other ways of deploying\ninfrastructure which encourage consistency: in terms of code\nartifacts, configuration methods, and such like.\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\nas a standard for packaging software and managing configuration.\n\n## Decision\n","We will build our initial prometheus beta in Amazon ECS, and assess\nhow effective it is.  We will review this decision once we have learnt\nmore about both prometheus and ECS.\n","We will standardize on Kubernetes for orchestrating containers, and Docker for packaging software and managing configuration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Prometheus Server needs to be built in a reproducible way within AWS.\nReproducible in this context means that the server can be built, destroyed and rebuilt.\nThe rebuilt server will be identical to the original server and the is no external intervention required (i.e. logging into the server to make changes to configuration)\n\n## Decision\n",Cloud init will be used to build a reproducible server.\n,Use Packer to create an AMI from the built server image and use that AMI to create new instances of the server as needed.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur users need an easier way to write alerts.  They currently have no\neasy way to test queries before writing their alerts.\nIn principle, they could use Prometheus's expression browser for this,\nbut our Prometheus service requires basic auth, which locks our users\nout of it.\n\n## Decision\n",We will give our users access to Prometheus so they can use its\nexpression browser to test queries when writing alerts. We will\ndo this by using IP whitelisting instead of basic auth and\nonly allowing our office IP addresses.\nWe identified a number of different routes that we could have taken to\nallow our users to access Prometheus.  One possible route that we\nconsidered was using Oauth2 authentication. This would enable users to\nauthenticate themselves to the platform with their Google account.\nWe did not choose to go with this option this time for expediency.\nThe idea behind this was to try to deliver the fastest value as\npossible to the user. This method enables us to learn more about the user's\nusage pattern. We do intend to add authentication but this will be done at\na later date.\n,"**Decision:** Implement a simple query tester inside our monitoring UI.  \n\n**Consequences:** This will give our users an easier way to write alerts, and will reduce the number of support requests we receive about writing alerts.  It will also be useful for debugging, as it will allow our users to test their queries before writing them."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* We have a Clojure application\n* We do not know the security requirements for hiding the application\n* We would rather not pay for hosting\n\n## Decision\n,* We host on Heroku. A free PAAS provider.\n,Use Cloudflare Pages
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* Our application works by indexing an XML file given by WikiMedia\n* WikiMedia manually create a new XML file twice a month on average\n* We have not been given an indication on how fresh the data our service provides needs to be\n* We want the software to be deployable without relying on a shared file system\n\n## Decision\n,* We store the XML dump in this git repo and update it when we need to\n* The file will be accessed over http to allow us to move to a different storage solution in the future\n,Deploy a web service that fetches the XML file from WikiMedia's publicly visible HTTP endpoint.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* We need to index and search a 32Mb XML file.\n* The search is across multiple fields.\n* Only one search word is used.\n* We currently don't have any external platform tools like Mongo, ElasticSearch, an RDMS\n* We do not know how often the XML file will be updated.\n\n## Decision\n","* We will parse the file on startup, store it as a clojure collection that is available throughout the system.\n* We will do this before the web server http port is available.\n* Searches will be performed against this in memory collection.\n* The app would need to be restarted to parse a new XML file, unless an additional endpoint is added.\n","**Use XQilla or Saxon, both of which are open-source XQuery processors.**\n\n**Benefits:**\n\n* XQuery is a language specifically designed for querying XML documents.\n* XQilla and Saxon are both mature and well-supported processors.\n* They can be used to index and search XML documents efficiently.\n* They can be used to search across multiple fields.\n* They can be used to search on a single search word.\n* They do not require any external platform tools.\n\n**Drawbacks:**\n\n* XQilla and Saxon are not as scalable as some other options, such as Mongo or ElasticSearch.\n* They may not be suitable for very large XML files or for applications that require high throughput."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* As per ADR-001\n* In memory search is unacceptably slow\n\n## Decision\n,* We will use Clucy which is a Clojure library on top of Lucene\n,Implement a read-through caching layer to improve search performance.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* As per ADR-007\n* Clucy takes over an hour to process the index\n\n## Decision\n,* We will use ElasticSearch as a frontend to Lucene.\n,"Clucy should be split into smaller indices, each with its own thread."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* We are building a web app that has one official url that uses query params\n* The url is /search?q=search-term\n* We believe that future endpoints will be added, or more complicated search requests\n\n## Decision\n",* We will use Bidi to parse the routes.\n,Implement RESTful API for all endpoints. This will provide a more scalable and flexible solution for future growth and complexity.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* Running a service is easier if you know what it is doing\n\n## Decision\n,* We will follow the logging principles of the 12 factor app: http://12factor.net/logs\n* We will only ever send output to console. Never write logs to file.\n* For example in production we could redirect the app output using: ```lein run | logger``` then we can see the output by using  ```tail -n 0 -f /var/log/system.log```\n* Semantic logging should be used where appropriate\n,Use a dedicated monitoring service such as Cloud Monitoring or New Relic.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* We need to set the port and other config values for our application\n* We do not know the deployment environment yet\n\n## Decision\n,"* We will use environment variables to configure the app, rather than a file\n",We will use environment variables to set the port and other config values for our application. This will allow us to easily change these values based on the deployment environment.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* We want to write programmer tests to support a TDD workflow.\n* We want to be able to mock out functions.\n\n## Decision\n,"* We will use Midje to test our code.\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\n",**Use a mocking library.**\n\n**Benefits:**\n\n* Allows for easy mocking of functions.\n* Supports a TDD workflow.\n* Can be used to test different scenarios and edge cases.\n\n**Alternatives:**\n\n* **Manual mocking:** This involves creating fake objects that implement the interface of the object being mocked. This can be time-consuming and error-prone.\n* **Dependency injection:** This involves passing in dependencies to the object being tested. This can be difficult to set up and can make the tests more complex.\n\n**Additional Considerations:**\n\n* Choose a mocking library that is compatible with the programming language being used.\n* Consider using a mocking library that supports dependency injection.\n* Make sure to write tests that are clear and concise.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAsyncApis are not semantically the same that other kinds of apis, like Web Apis. Some adopters need to know if they are looking specifically an async api, regarding the structure is the same or no.\nAlso, we need to prepare our model to support different kinds of Apis.\n\n## Decision\n",Create a hierarchy based on Apis. A Document unit will encodes an Api. This Api is abstract. There will be different types of Apis depending on the specifications.\n,"Create a new entity type that specifies AsyncApis, and modify the existing API entity type to include a reference to the new AsyncAPI entity type."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently the async validation pipeline is missing many core resolution stages present in the default and editing pipeline, such as JsonMergePatch and AsyncExamplesPropagation. This is causes missing validations, as well as exceptions in resolution. In many cases fixes are applied to the default and editing pipelines but the validation pipeline is left behind.\n\n## Decision\n",The async editing pipeline will be used as the validation pipeline.\n,**Decision:** Implement all core resolution stages in the async validation pipeline that are present in the default and editing pipelines.\n\n**Rationale:**\n\n* Ensures consistency and completeness in validation across all pipelines.\n* Prevents missing validations and exceptions in resolution.\n* Allows for fixes and improvements to be applied consistently across pipelines.\n\n**Implementation:**\n\n* Identify the missing core resolution stages in the async validation pipeline.\n* Develop and implement these stages in the async validation pipeline.\n* Conduct thorough testing to ensure functionality and stability.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nJson Schema Draft 2019-09 indicates that the $ref entry can now have other keywords alongside it. This is a departure from previous drafts as they didn't allow it.\nThe validation result of $ref must be AND'ed to the validation of the other facets, similar to what an allOf, oneOf and others do.\n\n## Decision\n","Avoid creating a new field in the model for references as this would be confusing for clients as they would have to take into account that field on very particular ocasions.\nInstead, we decided that if there is a $ref with keywords beside it, it should be parsed into an allOf in the same shape.\n### Cases\n#### Standalone ref\nIf a json map has a $ref entry in it and it is a single entry map, then that $ref will be parsed as a link and returned as is.\n```json\n{\n""type"": ""object"",\n""properties"": {\n""name"": {\n""$ref"": ""#/somewhere""\n}\n}\n}\n```\n#### Ref with adjacent keywords\nOriginal\n```json\n{\n""type"": ""object"",\n""$ref"": ""#/somewhere"",\n""properties"": {\n""name"": {\n""type"": ""string""\n}\n}\n}\n```\nTransformed\n```json\n{\n""type"": ""object"",\n""allOf"": [\n{""$ref"": ""#/somewhere""}\n],\n""properties"": {\n""name"": {\n""type"": ""string""\n}\n}\n}\n```\n#### Ref with adjacent keywords and allOf\nOriginal\n```json\n{\n""type"": ""object"",\n""$ref"": ""#/somewhere"",\n""properties"": {\n""name"": {\n""type"": ""string""\n}\n},\n""allOf"": [\n{""$ref"": ""#/somewhereElse""}\n]\n}\n```\nTransformed\n```json\n{\n""type"": ""object"",\n""allOf"": [\n{""$ref"": ""#/somewhere""},\n{""$ref"": ""#/somewhereElse""}\n],\n""properties"": {\n""name"": {\n""type"": ""string""\n}\n}\n}\n```\n","Allow the $ref entry to have other keywords alongside it.\n\n## Rationale\nThis change is in line with the latest draft of the JSON Schema specification and will allow for more flexibility in defining schemas. By allowing other keywords alongside the $ref entry, we can define schemas that are more specific and tailored to our needs.\n\n## Consequences\nThis change may have the following consequences:\n\n* Increased complexity of schemas: Allowing other keywords alongside the $ref entry may make schemas more complex and difficult to understand.\n* Potential for conflicts: If multiple keywords are used alongside the $ref entry, there is the potential for conflicts between the keywords.\n* Reduced portability: Schemas that use the $ref entry with other keywords may not be as portable as schemas that do not use the $ref entry with other keywords."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter a clarification in the async specification, the examples facet of a message object contains a list of pairs of examples,\nwhere one is associated to the payload while the other to the header. Due to time restricitions, this change has to be introduced without a breaking change in the model.\n\n## Decision\n",Within the message model a new field has been added to store header examples. The payload examples are being stored in a separate field.\n,"**Decision:**\n\nIntroduce a new field to the Message model called `example_headers` to accommodate the additional examples associated with headers, while keeping the existing `example_payloads` field for examples associated with payloads.\n\n**Rationale:**\n\n* **Avoids breaking change:** The introduction of a new field does not affect the existing message format, ensuring backward compatibility.\n* **Maintains data integrity:** The new `example_headers` field explicitly separates examples related to headers from those related to payloads, preventing confusion and ensuring data integrity.\n* **Flexibility:** This approach provides flexibility for future changes to example handling, allowing for additional fields or changes to the format without disrupting the existing model."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAPIMF-2622 reports a StackOverflowError in Domain Element Merging when merging the same included recursive JSON Schema\nin a trait/resource type and an operation/endpoint. See `amf-cli/shared/src/test/resources/resolution/merge-recursive-json-schemas-raml10`\nas an example.\nWhy does this happen? Two reasons:\n* JSON Schemas are inlined. This means parsing the same included JSON-Schema will produce each time different\nin-memory Shape objects in every location it is included, rather than links to a single object placed in an\nExternalFragment (this is why RAML types work). When merging we skip merging the same in-memory object with itself\nbut since here objects are different, they get merged.\n* Resolution step ordering:\n1. Reference Resolution   -> Plain links get resolved\n2. Extends Resolution     -> Domain Element Merging (our case)\n3. Shape Normalization    -> Recursion detection\nExtends resolution does not check recursions, this is done in the next step called ""Shape Normalization"". However,\nbefore resolving extends we resolve links and this makes it susceptible to recursions. The only safe guard mechanism\nagainst recursions is skipping merging the same in-memory objects, which doesn't work here because of the\naforementioned cause.\n\n## Decision\n","Apart from skipping equal in-memory objects now we skip objects that: have the `SchemaIsJsonSchema` annotation and have\nthe same `ExternalSourceElementModel.ReferenceId` field.\n### Implementation\nThe included JSON Schema in the operation/endpoint has all the above mentioned information so there is no problem.\nThis is not the case for the operation/endpoint derived from the trait/resource type to be applied. The process deriving\nan operation/endpoint from a trait/resource type looks as follows (summarized):\n1. Parse trait/resource type as a Data Node\n2. Emit the Data Node as a YNode\n3. Parse the emitted YNode as an operation/endpoint\n**The problem**\nData Nodes do not have a `ReferenceId` field, only Shapes do.\n**The solution**\n_Step 1_:\nTo keep that information we first have to create a new annotation `ReferenceId` to hold it and that can be stored in the\nData Node.\n_Step 2_:\nWhen emitting the Data Node to a YNode we lose that annotation. The original YNode was a MutRef (will all the necessary\ninformation), but the emitted YNode is a plain YScalar of type string (without the information). Emitting the original\nMutRef breaks other cases, so the only available option was to collect the newly emitted YNode in a\n`Map[YNode, ReferenceId]` that relates it to the original `ReferenceId`.\n_Step 3:_\nNow that he have a `Map[YNode, ReferenceId]` relating each YNode to its original reference we need to delegate this\ninformation to the JSON Schema parser so that it can effectively create the `ExternalSourceElementModel.ReferenceId`\nfield in the parsed Shape. We do this by means of the `WebApiContext` that takes that Map as an implicit argument.\nNow the JSON Schema parser has access to this Map via the WebApiContext and simply checks if the YNode was previously\nrelated to some `RefenreceId`, creates the corresponding field and the `SchemaIsJsonSchema` annotation making them\navailable for Domain Element Merging checks.\n",Implement a check to prevent merging the same in-memory Shape object.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\n\n## Decision\n","In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\n",AMF should emit the external references in these cases since the referenced external elements are not present in the base unit.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen AMF builds a model from a content, it must justify from where each node and property came from, so the users can track and know for sure the origin of the values. This source map information is fundamental for editing features.\nIn a parsed API, the elements declared in the source document have their corresponding annotations (think of `LexicalInformation` or `SourceNode`) in the final model, but some elements in the AMF model are not explicitly declared. Moreover, some of this not-declared elements are fields we can easily infer from the source document (like the name of an operation) while others have no real counterpart in the source document.\n\n## Decision\n","For these reasons, the decision has been made in favor of adding 3 annotation types to make every object or field traceable to where it came from, whether it was from the source document or it was generated by AMF:\n- `Annotations.inferred()`\n- A field is **inferred** when the entry has been inferred but the value is real (is present in the source document)\n- Example: The name of an operation, which has no entry `name` but we can infer it from its declaration in the document\n- `Annotations.virtual()`\n- An object is **virtual** when it's generated based on one or more elements in the document, but doesn't have a specific declaration\n- Does not stop model traversal because its children nodes may have sourcemaps\n- Many objects in the AMF Model are virtual containers of information present in the document\n- Examples:\n- The `Request` object in the AMF model is virtual, because you can't assign a sourcemap to it, but the elements under it (like parameters) do have a specific definition and location in the source document\n- `Annotations.synthesized()`\n- A field is **synthesized** when the entry as well as the value are generated, it's not in the document\n- This field and its value have nothing that matches it on the AST, and the same is true for its children\n- Stops model traversal (won't look into value or its children)\n- You can't have a virtual value in a synthesized field, because that means that the value is not entirely made up\n- Examples:\n- `PropertyShape.MinCount` when it's set to 1 based on a `required: true`\n- Creating a default value when there is no value declared in the document\n","In the case of the not-declared elements that we can easily infer from the source document, we should create a `SourceNode` whose `source` and `range` properties are generated from the corresponding element in the parsed API if it exists and empty otherwise, and the `selector` property is null (or undefined when using TypeScript) as this is a non-parsed element. This will cover most of the cases, but it won’t be enough in those elements that are not-declared and cannot be inferred from the parsed API (like the name of the content header class). In these cases, we should create a `SourceNode` with `source` and `range` properties as empty strings and `selector` property equal to the string name of the element."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`IriTemplateMapping` is a node that stores mappings for OAS 3.0 discriminator values. It has two fields defined as follows:\n* `TemplateVariable`: Variable defined inside an URL template\n* `LinkExpression`: OAS 3 link expression\nTake the following OAS 3.0 discriminator example:\n```yaml\ndiscriminator:\npropertyName: petType\nmapping:\ndog: '#/components/schemas/Dog'\ncat: '#/components/schemas/Cat'\n```\nHere `TemplateVariable` wil be set to `dog` and `cat`, while `LinkExpression` will be set to `#/components/schemas/Dog` and `#/components/schemas/Cat`.\nThe naming of this model and fields do not reflect at all their functionality\n\n## Decision\n",Implement a new model called `DiscriminatorValueMapping` to hold this relation. It has two fields defined as follows:\n* `DiscriminatorValue`: Value given to a discriminator that identifies a target Shape\n* `DiscriminatorValueTarget`: Target shape for a certain discriminator value\n`DiscrimiatorValueTarget` changed from a raw reference to a link to an actual shape.\nThe old `IriTemplateMapping` model is to be deprecated when we enable field deprecation in the AMF model.\n,Rename `IriTemplateMapping` to `DiscriminatorMapping` and rename its fields as follows:\n- `TemplateVariable` to `DiscriminatorValue`\n- `LinkExpression` to `TargetSchemaLink`
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAsync Api 2.0 spec details that message headers property must be a Schema Object in which his properties will be the headers for that message.\nAMF currently is parsing that object as the schema of a parameter object. This is confusing and semantically incorrect, because that property represents a set of parameters, not just one\n\n## Decision\n","Create a new field to the Message model, similar to the query string property of Raml 1.0 operations, that will contains the Schema Object. Any user looking at the model, will be able to read that schema as a list of headers based on the field in which is contained:\n####ApiContract:HeaderSchema\nThe header example property of the message will be validated against that schema.\n","Update the AMF parser to correctly interpret the message headers property in the AsyncAPI 2.0 spec as a schema object representing a set of parameters, rather than as the schema of a single parameter object."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\nSome other useful links:\n[https://github.blog/2020-08-13-why-write-adrs/](https://github.blog/2020-08-13-why-write-adrs/)\n[https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/](https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/)\n",**Decision:** To implement an Architectural Decision Record (ADR) system to document and track architectural decisions made throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhenever a domain related events happens (checkin, checkout, project update etc.), a number of\ninterested components are to be notified. Those components might show warnings, update the UI etc.\nUnfortunately, not all components are being executed in the main process all the time, so Otto\nevents will not reach them. There needs to be a mechanism that reaches background services.\n\n## Decision\n","Jede an einem Domain Event interessierte Komponente stellt einen BroadcastReceiver bereit, der\nmittels Intents informiert wird. Alle DomainEvents werden per Broadcast verteilt.\nAlle Events sollen haben die Intent-Kategorie `com.tastybug.timetracker.LIFECYCLE_EVENT` sowie eine\nAction, die das Domainobjekt beschreibt: `com.tastybug.timetracker.PROJECT_CHANGE` oder\n`com.tastybug.timetracker.TRACKING_RECORD_CHANGE`.\nIn den Extras des Intents sind die UUIDs der betroffenen Entities abgelegt.\nEvery component interested in Domain Events provides a BroadcastReceiver that will be notified via\nIntents. Every Domain Event will be propagated using Broadcasts.\nAll events have category `com.tastybug.timetracker.LIFECYCLE_EVENT` an an action that describes the\naffected object type. Currently this is `com.tastybug.timetracker.PROJECT_CHANGE` and\n`com.tastybug.timetracker.TRACKING_RECORD_CHANGE`. UUIDs of affected objects can be taken from\nintent extras.\n","**Decision:** Implement a message broker that handles and distributes events to all relevant components, whether they are running in the main process or in the background."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\nWe all could see a working GUI yet, but now it's time to analyze the algorithm correctness.\nIt is a plotting/statistics library to pick.\nDecision\n--------\nUse `XChart`, a lightweight library with limited statistics capabilities, for plotting.\nRationalization\n---------------\nThe first choice `krangl` and `kravis` libraries turned out uncompatible with Windows for the variety of reasons.\nThis has been reported as an issue to the community of these Kotlin libraries, but meanwhile the progress of the project can't be blocked with that anymore.\nSo it's decided to pick XChart visualization abilities for now, which of course has less abilities than advanced `ggplot` and will require additional effort to bring, say, linear model predictions to the plot.\n\n## Decision\n","--------\nUse `XChart`, a lightweight library with limited statistics capabilities, for plotting.\nRationalization\n---------------\nThe first choice `krangl` and `kravis` libraries turned out uncompatible with Windows for the variety of reasons.\nThis has been reported as an issue to the community of these Kotlin libraries, but meanwhile the progress of the project can't be blocked with that anymore.\nSo it's decided to pick XChart visualization abilities for now, which of course has less abilities than advanced `ggplot` and will require additional effort to bring, say, linear model predictions to the plot.\n","Use `XChart`, a lightweight library with limited statistics capabilities, for plotting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\nDecision\n--------\nLeave with JavaFX/TornadoFX as constant rendering/animation framework.\nRationalization\n---------------\nAlthough there are probably better frameworks/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\nJavaFX seems to be a nice opportunity with relatively little to learn.\n\n## Decision\n","--------\nLeave with JavaFX/TornadoFX as constant rendering/animation framework.\nRationalization\n---------------\nAlthough there are probably better frameworks/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\nJavaFX seems to be a nice opportunity with relatively little to learn.\n",Keep using JavaFX/TornadoFX as the constant rendering/animation framework.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\nIn our approach, brains = weighted neural networks *with different amount of nodes* should be developed.\nDecision\n--------\nUse adjacency matrixes in the first step and grammar rules yielded from the first step in the second step for networks representation.\nRationalization\n---------------\nTo be able to develop brains with many nodes scalably and unevenly, the grammar approach to building the graphs was picked, as the best known one.\nHowever, for this approach there are some grammar patterns to exist beforehand, which we then take from the ""first step""\nof algorithm, running GA with adjacency matrices to figure out the most finesse brain patterns.\n\n## Decision\n","--------\nUse adjacency matrixes in the first step and grammar rules yielded from the first step in the second step for networks representation.\nRationalization\n---------------\nTo be able to develop brains with many nodes scalably and unevenly, the grammar approach to building the graphs was picked, as the best known one.\nHowever, for this approach there are some grammar patterns to exist beforehand, which we then take from the ""first step""\nof algorithm, running GA with adjacency matrices to figure out the most finesse brain patterns.\n",Use adjacency matrixes in the first step and grammar rules yielded from the first step in the second step for networks representation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\nBefore starting to commit in the main project, I was looking at various possibilities of writing a brain/cognitive simulation with GUI.\nDecision\n--------\nPick TornadoFX and Kotlin as main simulation framework.\nRationalization\n---------------\nWith lots of possibilities around, the 2nd place winning one being Simbrain,\nTornadoFX and Kotlin seemed to be the most modern and, hence, robust and extendable option.\nFurthermore, reduction of the amount and increase of the readability of code, which the framework provides, are especially important for an open-source project.\n\n## Decision\n","--------\nPick TornadoFX and Kotlin as main simulation framework.\nRationalization\n---------------\nWith lots of possibilities around, the 2nd place winning one being Simbrain,\nTornadoFX and Kotlin seemed to be the most modern and, hence, robust and extendable option.\nFurthermore, reduction of the amount and increase of the readability of code, which the framework provides, are especially important for an open-source project.\n",Pick TornadoFX and Kotlin as the main simulation framework.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\napps.\n\n## Decision\n",We will use Jest as our testing framework.\n,We will use Jest as the test framework for our React and TypeScript application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n\n## Decision\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,**Decision:** Use ESLint with TypeScript plugins for code linting.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNot everything that has a style has a component in\n[`lbh-frontend`](https://github.com/LBHackney-IT/lbh-frontend). We build the\n`lbh-frontend` dependency into our distributables, making it hard for importers\nto gain access to the stylesheets without duplicating some styles (see ADR 11).\nReact components insulates importers of this library from the internal\nimplementation details of how those components work. We want to continue to do\nthat, and not require users to understand the styling hierarchy and class names.\n\n## Decision\n","When components don't exist in `lbh-frontend` and it doesn't make sense to add\nthem there, we will create new components in this library.\n","Create a new scoped package that consumes `lbh-frontend` and exposes the stylesheets as-is, named as per their CSS source files."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are tightly coupled to `lbh-frontend` and `govuk-frontend`. We want to ensure\nusers having different versions doesn't cause problems.\nIt's also likely that users of this library will have no direct need of either\n`lbh-frontend` or `govuk-frontend` as neither supports React. They also both use\nSass for their stylesheets, and we don't want to force users to do the tooling\nto also support Sass themselves.\n\n## Decision\n",We will bundle `lbh-frontend` and `govuk-frontend` into our distributables.\n,"**Decision:**\n\nUse **peer dependencies** to declare the required versions of `lbh-frontend` and `govuk-frontend`. This ensures that users of the library have compatible versions of the dependencies installed, without forcing them to use Sass or other specific tooling."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n\n## Decision\n,We will use Dependabot to monitor dependency updates.\n,Use Dependabot to automatically update dependencies.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, and has become a standard.\n\n## Decision\n",We will enforce that everything supported by Prettier has its style enforced by\nit.\nWe will set up Git hooks to automatically run the formatter before committing.\nWe will set continuous integration up to reject commits that are not correctly\nformatted.\n,Use Prettier for code formatting across all JavaScript projects.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[govuk-frontend](https://github.com/alphagov/govuk-frontend) doesn't do a good\njob of detecting if it's in a browser environment or not, and therefore, whether\n`document` and friends are available or not. It also performs a number of\npolyfills that rely on the presence of those browser globals. This means that it\nthrows exceptions when attempting to import some JavaScript modules from that\nlibrary.\nThe way `govuk-frontend` JavaScript is generally written is under the assumption\nthat you'll want, and be able, to pass the raw DOM nodes to the module\nconstructors. React doesn't work that way, and we could dig into the prototypes\nof the objects created by those constructors, but given that these methods are\nnot intended to be used directly, it would be compliant with\n[Semantic Versioning](https://semver.org/spec/v2.0.0.html) to completely change\ntheir implementation without releasing a new major version. This makes relying\non their behaviour being fixed unreliable.\n\n## Decision\n","We will re-implement JavaScript features from `govuk-frontend` in more idiomatic\nReact in this library.\nWe will link to the version and the source of the original implementation\nalongside our implementation for reference, linking to the code in the specific\ncommit we were referring to, not `master`.\n",We will create a simple polyfill that can be require()'d before any other modules and will set `document` to an empty object. This will allow us to import `govuk-frontend` JavaScript modules without those exceptions being thrown.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to have API documentation for the code we publish. Rather than writing\nseparate API docs that quickly get out of date, we would like to be able to\ngenerate it from our code and comments in our code that live next to the thing\nthey refer to. For JavaScript, the standard is [JSDoc](https://jsdoc.app/), but\nwith TypeScript, we're already defining the types in our code, and duplicating\nthat in the associated comments is repeated effort and requires manual action to\nkeep up-to-date.\n[TypeDoc](https://typedoc.org/) is a documentation generator based on JSDoc. It\nuses a combination of comments and TypeScripts own types to generate API\ndocumentation automatically.\n\n## Decision\n",We will use TypeDoc to generate documentation.\nWe will document all exported code for the benefit of end users.\nWe will commit the documentation we generate to the repository alongside changes\nto behaviour.\n,Use TypeDoc for API documentation generation.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n[Rollup](https://rollupjs.org/guide/en/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n\n## Decision\n,We will build distributables using Rollup.js.\n,"Use Rollup to build and distribute the TypeScript library in CommonJS, ES modules, and UMD formats."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be confident about the code we write, and for it to be\nself-documenting as much as possible.\n[TypeScript](https://www.typescriptlang.org/) is a compiled language with\noptional typing. It's a superset of JavaScript, so is familiar to developers who\nknow JavaScript. It has wide editor support.\nAs of writing, TypeScript is used by over\n[1.4 million repositories](https://github.com/microsoft/TypeScript/network/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\non GitHub.\n\n## Decision\n",We will use TypeScript.\n,"**Decision:** Use TypeScript for code development.\n\n**Rationale:** TypeScript provides optional typing, which increases confidence in the code and makes it more self-documenting. It is a superset of JavaScript, making it familiar to developers. TypeScript has wide editor support and is used by a large number of GitHub repositories, indicating its popularity and acceptance in the industry."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to be consistent with the Hackney styles within lbh-frontend. We could\nderive styles from [lbh-frontend](https://github.com/LBHackney-IT/LBH-frontend),\nwhile reimplementating the HTML and JS. We could also use\n[govuk-react](https://github.com/govuk-react/govuk-react), a repository\npre-built that derives from\n[govuk-frontend](https://github.com/alphagov/govuk-frontend), and then apply the\nmodifications from lbh-frontend to it. The repository for govuk-react has a\nlarge amount of issues raised against it, which may also be issues we would run\ninto if we worked off the repository. It is also built using\n[styled-components](https://www.styled-components.com/) for styling, using a\nreflection of a version of govuk-frontend, meaning updates to govuk-frontend\nwill require the respective CSS to be updated. Given that lbh-frontend also\nutilizes SASS, we would run into the same problem again.\n\n## Decision\n","We will create a React component library that derives the styles from\nlbh-frontend, and will reimplement the HTML and JS so that it works correctly in\na React environment.\n",Use GovUK React and override styles as necessary.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere will be times when the user has external components that they wish to\nconnect to components within the component library, so they can use the core\nfunctionality of those external components, while also using the Hackney styles.\nAn example of this is routing, where they may have custom anchor components\nneeded to interface with that their routing library. In order to do this, we\nwill need to create a way for the user to tell `lbh-frontend-react` to use\nspecific components in certain roles.\n\n## Decision\n","We will build a component register, where the user can register custom\ncomponents to be used by the components in this library.\n",We should create an `overrideComponents` prop that can be passed to `<LbhLayout>` which will accept a new JSON object of components that should be used in place of the default components. This will allow the user to override any of the default components with their own custom components.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","Establish an Architectural Decision Record (ADR) process to document and maintain architectural decisions throughout the project lifecycle. ADRs will provide a shared understanding of the why, what, and how of architectural decisions, ensuring alignment and reducing risks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs it's hard to understand code, it is crucial that anybody can easily\nunderstand the code you're working on. This applies to all levels of code, not\nonly the code itself, but groups of code, complete applications and groups of\napplications.\n\n## Decision\n","I will always focus on simplicity, readaility and the ease of understanding\nany code or structure.\n","**Use a domain-driven design approach.**\n\nDomain-driven design is a software development approach that focuses on creating software that is closely aligned with the business domain. This means that the software is designed to reflect the real-world concepts and processes that the business uses.\n\nBy using a domain-driven design approach, you can create software that is:\n\n* **Easier to understand:** The software is designed to reflect the real-world concepts and processes that the business uses, so it is easier for people to understand how it works.\n* **More maintainable:** The software is designed to be modular and loosely coupled, so it is easier to maintain and update.\n* **More extensible:** The software is designed to be extensible, so it can be easily adapted to new requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\n\n## Decision\n","Integrations should expose all available data to the backend if that data is fetched in a single API request.\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\n```\nLayer model:\nFetch            Manage           View\n-------------      ---------      ------------\n| Interface | ---> | Core  | ---> | Frontend |\n-------------      ---------      ------------\n```\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https://developers.home-assistant.io/docs/entity_registry_disabled_by/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\n","The decision is to move to a opt-in model for the inclusion of sensors, and to introduce a new setting to allow the user to view unselected data points.\n\n**Benefits**\n- This would simplify the configuration process for users, as they would only need to select the data points that they want to expose.\n- It would also make it easier for users to discover new data points, as they would be able to see all of the available data points, regardless of whether or not they are currently selected.\n- It will reduce the redundancy of having to keep track of selection twice\n\n**Drawbacks**\n- This would require some changes to the Lovelace UI, in order to accommodate the new setting.\n- It could also lead to users being overwhelmed by the number of available data points.\n\nOverall, the benefits of moving to an opt-in model for the inclusion of sensors outweigh the drawbacks. This change would simplify the configuration process for users, make it easier for them to discover new data points, and reduce redundancy."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nYou never think of everything. Sticking to standards is a very good\nthing to prevent you from doing things that can go bad. Those also\nhelps making the code be more readable and structured.\n\n## Decision\n,Use Static Code Analysis to find violations of standards.\n,Use well-established standards for all aspects of the project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDefine a supported installation method as per [ADR-0012](https://github.com/home-assistant/architecture/blob/master/adr/0012-define-supported-installation-method.md).\n\n## Decision\n,"This installation method provides the full Home Assistant experience on a regular operating system. This means, all components from the Home Assistant method are used, except for the Home Assistant Operating System. This system will run the Home Assistant Supervisor. The Supervisor is not just an application, it is a full appliance that manages the whole system. It will clean up, repair or reset settings to default if they no longer match expected values.\nBy not using the Home Assistant Operating System, the user is responsible for making sure that all required components are installed and maintained. Required components and their versions will change over time. Home Assistant Supervised is provided as-is as a foundation for community supported do-it-yourself solutions. We only accept bug reports for issues that have been reproduced on a freshly installed, fully updated Debian with no additional packages.\nThis method is considered advanced and should only be used if one is an expert in managing a Linux operating system, Docker and networking.\n### Supported Operating System, System dependencies and versions\nDocker CE (Community Edition) is the only supported containerization method for Home Assistant Supervised. We only support FHS 3.0 on the host file system.\n- Docker CE >= 20.10.17\n- Systemd >= 239\n- NetworkManager >= 1.14.6\n- udisks2 >= 2.8\n- AppArmor == 2.13.x (built into the kernel)\n- Debian Linux Debian 12 aka Bookworm (no derivatives)\n- [Home Assistant OS-Agent](https://github.com/home-assistant/os-agent) (Only the [latest release](https://github.com/home-assistant/os-agent/releases/latest) is supported)\nOnly the above-listed version of Debian Linux is supported for running this installation method. When a new major version of Debian is released, the previous major version is dropped, with a deprecation time of 4 months. An exception to this rule occurs if the new version does not meet the requirements of the Supervisor.\n### Additional supported conditions\nThis installation method can easily be broken if one manages the operating system incorrectly. Therefore, the following additional conditions apply:\n- The operating system is dedicated to running Home Assistant Supervised.\n- All system dependencies are installed according to the manual.\n- No additional software, outside of the Home Assistant ecosystem, is installed.\n- Docker needs to be correctly configured to use overlayfs2 storage, journald as the logging driver with Container name as Tag, and cgroup v1.\n- NetworkManager is installed and enabled in systemd.\n- Systemd journal gateway is enabled and mapped into supervisor as `/run/systemd-journal-gatewayd.sock`\nIn case any abnormality is detected that prevents Home Assistant from functioning, the Home Assistant Supervisor will report this to the user and block updates to prevent installations from breaking.\n### Required Expertise\n- **Installation**\nThe user first needs to install Debian and make sure all the required components are installed and are the correct version. They then need to run the installer script.\n- **Start when the system is started:** done by the installer\n- **Run with full network access:** done by the installer\n- **Access USB devices:** done by the installer\n- **Maintaining the Home Assistant installation**\nHome Assistant can be maintained from the Supervisor. This includes a rollback when the update fails.\n- **Python upgrades:** Included in Home Assistant updates\n- **Installing Python dependencies:** Included in Home Assistant updates\n- **Updating Home Assistant:** Via the UI\n- **Maintaining the Operating System**\nThe user is responsible for maintaining the operating system. Since it is a supervised installation, the user is also responsible for updating the components that are required by the Supervisor. The user is also responsible for not installing or changing anything on their system that will interfere with the Supervised installation. Examples are software that will update Docker containers managed by the Supervisor.\n- **Security updates for OS:** Responsibility of the user.\n- **Maintaining the components required for the Supervisor:** Responsibility of the user. Over time as Supervisor requirements change, you might have to upgrade your OS to be able to use the required version.\n**Conclusion:** Expert. Maintaining a Debian installation to a very specific set of requirements is hard.\n",**Decision:**\n\nSupport a pre-built Docker image as an installation method for Home Assistant.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently have no restrictions on supported databases for the recorder, and the documentation states:\n> Home Assistant uses SQLAlchemy, which is an Object Relational Mapper (ORM). This means that you can use any\n> SQL backend for the recorder that is supported by SQLAlchemy, like MySQL, MariaDB, PostgreSQL, or MS SQL Server.\nThis claim is not correct, SQLAlchemy will behave differently on different databases, and features relied\non by the recorder may work differently, or not at all, in different databases.\nAs a matter of fact, we have a lot of special handling for the different databases out there, which makes\nmaintenance and implementing changes really hard. For contributors, it is impossible to test changes against\nall database services SQLAlchemy could work with. This often causes bugs being reported for the less common\ndatabases after a new release.\nTL;DR: We advertise a lot of different databases being supported, without version limitation mentioned. This\ncauses the end-user to think their database and version of choice will work with Home Assistant. Unfortunately,\nthat is not always the case; The user experience for those using a less popular database or an outdated version\nis very poor, with frequent issues preventing the recorder from working after an upgrade of Home Assistant.\n\n## Decision\n","Support a limited set of databases, and also limit the supported versions for the supported databases\nin a similar way as we have limited installation types for Home Assistant. This greatly reduces the\nmaintenance effort, making it easier to implement and test new features and giving the end-user a better\nexperience with reliable upgrades.\nLimit DB support to the following:\n- MariaDB ≥ 10.3 (2017)\n- MySQL ≥ 8.0 (2018)\n- PostgreSQL ≥ 12 (2019)\n- SQLite ≥ 3.31.0 (2020)\nStart to emit warnings about unsupported databases in Home Assistant 2021.11, the recorder will fail\nto start with Home Assistant 2022.2.\nThe above listed minimal versions may change over time. In case a minimal version bump is required, this will\nbe announced as a breaking change, with a depreciation period of 2 release cycles (2 months).\n",**Decision:** Restrict supported databases for the recorder to those that are actively tested and maintained.\n\n**Reasoning:**\n\n* Limiting supported databases reduces maintenance effort and simplifies database handling.\n* Active testing and maintenance ensure reliable operation and timely bug fixes.\n* Clear documentation will guide users towards compatible databases and versions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHome Assistant has been relying on YAML for its configuration format for a long\ntime. However, in certain cases, this caused issues or did not work at all.\nThese cases are best explained by listing the different categories of\nintegrations that we have in Home Assistant:\n- Integrations that integrate devices. Examples include Hue, TP-Link.\n- Integrations that integrate services. Examples include AdGuard, Snapcast.\n- Integrations that integrate transports. These integrations allow users to\ndefine their own protocol. Examples include MQTT, serial, GPIO.\n- Integrations that process Home Assistant data and make this available to\nother integrations. Examples are template, stats, derivative, utility meter.\n- Integrations that provide automations. Examples include automation,\ndevice_sun_light_trigger, alert.\n- Integrations that help controlling devices and services.\nExamples include script, scene.\n- Integrations that expose Home Assistant data to other services.\nExamples include Google Assistant, HomeKit.\nIn all but the first two cases, YAML does just fine. The configuration is static,\nis not discovered and relies on the user setting it up. These cases have been\nsolved by providing a hybrid approach. We offer YAML with a reload service and\nwe offer Storage Collections, which allows the user to create/manage these\nintegrations via the UI.\nHowever, in the first two cases, it doesn’t work. Integrations are discovered.\nIntegrations require users logging in on the vendor’s website and authorize\nlinking (OAuth2) or users are required to press buttons on the hub to authorize\nlinking (i.e. Hue).\nIn the cases that people can authorize an integration by just putting their\nusername and password in the YAML file, they don’t want to, because it prevents\nthem from sharing their configuration. This is solved currently by using YAML\nsecrets that are substituted during load. This results in one file that provides\nthe structure of your configuration and one file that provides the values.\nSee below for an anonymized example as can be found on GitHub:\n```yaml\ncamera:\nplatform: onvif\nname: bedroom\nhost: !secret camera_onvif_bedroom_host\nport: !secret camera_onvif_bedroom_port\nusername: !secret camera_onvif_bedroom_username\npassword: !secret camera_onvif_bedroom_password\n```\nSo to solve these first two cases, we’ve introduced config entries (a\ncentralized config object) and config flows. Config flows handle creating config\nentries with data from different sources. It can handle a new entry created via\nthe user interface, automatic discovery, but also is able to handle importing\nconfiguration from YAML. Config entries allow for migrations during upgrades,\nlimiting the breaking changes we have to induce on our users.\nConfig flows empower users of all knowledge levels, to use and enjoy\nHome Assistant. Since the introduction of config flows we’ve kept it open to\ncontributors of individual integrations to decide if they want to implement\nYAML and/or a user-facing config flow.\nSome contributors have decided to drop the YAML import to reduce their\nmaintenance and support burden. A burden that they volunteer to do in their\nspare time. This has sadly resulted in a few pretty de-motivating comments,\ntowards the contributors and the project in general. These comments often\nviolate the code of conduct we have in place to protect the Home Assistant\ncommunity.\nThis induces the risk of losing contributors and maintainers, halts our project\ngoals and slows down innovation. As an open source project, maintaining our\ncontributors is our highest priority as they are the creators of the project\nin the first place. They should be highly admired and valued for their\ncontributions.\nFrom a project perspective, we have not provided the necessary guidelines on\nthis matter for our contributors to work with and have therefore not managed\nthe expectations of our users to a full extent.\n\n## Decision\n","To protect project goals and to provide clarity to our users and contributors,\nwe’re introducing the following rules on how integrations need to be configured:\n- Integrations that communicate with devices and/or services are only configured via\nthe UI. In rare cases, we can make an exception.\n- All other integrations are configured via YAML or via the UI.\nThese rules apply to all new integrations. Existing integrations that should\nnot have a YAML configuration, are allowed and encouraged to implement a\nconfiguration flow and remove YAML support. Changes to existing YAML\nconfiguration for these same existing integrations, will no longer be accepted.\n",**Decision:** Use config entries and config flows to handle configuration for integrations that integrate devices or services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen creating tests, it helps to have good bean objects. But when you\nneed a bigger structure of some objects, it's a big thing to create them\non demand. It would be nice to have a builder pattern implemented for\nmost objects. This is of course depending on the language and the tools\nused.\n\n## Decision\n","For `java`, create a `TestBuilder` for each bean class used in tests.\nThe `TestBuilder` creates default values for the objects but can have\nthem easily be overwritten.\nHere's an example:\n``` java\npublic class CustomerTestBuilder {\nprivate String companyName;\nprivate Address address;\npublic CustomerTestBuilder() {\nthis.companyName = ""ACME Inc."";\nthis.address = new AddressTestBuilder().build();\n}\npublic CustomerTestBuilder withCompanyName(String companyName) {\nthis.companyName = companyName;\nreturn this;\n}\npublic CustomerTestBuilder withoutCompanyName() {\nthis.companyName = null;\nreturn this;\n}\npublic CustomerTestBuilder withAddress(Address address) {\nthis.address = address;\nreturn this;\n}\npublic CustomerTestBuilder withoutAddress() {\nthis.address = null;\nreturn this;\n}\npublic Customer build() {\nCustomer result = new Customer();\nresult.setCompanyName(this.companyName);\nresult.setAddress(this.address);\nreturn result;\n}\n}\n```\n","**Implement a Builder Pattern for Bean Objects**\n\n* **Rationale:** Creating complex bean objects can be tedious and time-consuming. A builder pattern allows for the creation of these objects in a modular and extensible way, making it easier to maintain and test the codebase.\n\n* **Details:**\n    * Define an interface or abstract class that represents the bean object to be created.\n    * Create a separate Builder class for each bean object that implements the interface or extends the abstract class.\n    * The Builder class provides methods to set individual properties of the bean object.\n    * Once all the necessary properties have been set, the Builder class can be used to construct the bean object."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have now `homeassistant/home-assistant` as an amd64 image, based on Debian Linux, with a size of 761 (compressed).\nOn the other side, we have `homeassistant/...-homeassistant` with support for 5 different architectures based on Alpine Linux with a size of 376 (compressed). Hass.io uses these images.\nThe Hass.io images follow (most) best practices, and the Dockerfiles are checked with hadolint.\nThe Hass.io images support more integrations out of the box. Also, we pre-build all the Python Wheels for Alpine in the background. Additionally, support for pre-build Alpine packages is planned for the future.\nEven though, the Hass.io images support more integrations, some things are not supported on Alpine. For example, TensorFlow and Chrome (the last one is less of an issue now web scraping is banned in [ADR-0004](0004-webscraping.md)).\nIn the current situation, we basically release and maintain two types of Docker images. The current Debian based one and the ones based on Alpine Linux. This requires us to solve issues twice or, in case of issues related to a specific Linux version, becomes harder to resolve to find out the culprit.\n\n## Decision\n","Our Docker team and resources are limited and we should focus all our resource to one docker image.\nOur regular Docker installation can be improved by tagging along with the Hass.io images, which features a more modular approach, follow better practices, are well maintained, supports five architectures, and have smaller images. Users will benefit from the pre-builds we provide and deliver, which results in a faster set up and installation of integrations.\nWe are going to transform our current `homeassistant/home-assistant` image into a Docker manifest image that provides multiple architectures. Each of the supported architectures in that manifest will point to one of the Alpine based Dockerfiles; we currently use for Hass.io: `homeassistant/...-homeassistant`.\nResulting in a single Docker base to maintain, while providing lots of benefits.\nTimeline:\n- **0.96**: Start with new Tagging system\n- **0.97**: Announce the end of Debian based homeassistant/home-assistant image with Release Blog Post\n- **0.98**: Start with manifest tagging\n","**Decision:**\n\nMigrate the AMD64 Home Assistant image to Alpine Linux, merge the Hass.IO images into the main Home Assistant image, and adopt the Hass.IO approach for building and maintaining all Docker images.\n\n**Rationale:**\n\n* **Reduced maintenance overhead:** Maintaining separate DEB and Alpine images is resource-intensive and can lead to duplicate efforts in resolving issues.\n* **Improved consistency:** Merging the images ensures that all Home Assistant instances run on a consistent Linux base, reducing potential compatibility problems.\n* **Enhanced security and best practices:** Hass.IO images follow industry best practices and are subject to rigorous testing, ensuring improved security and reliability.\n* **Broader support for integrations:** The Hass.IO images provide support for a wider range of integrations, giving users more flexibility in configuring their smart homes.\n* **Centralized build and deployment process:** Adopting the Hass.IO approach allows for a centralized build and deployment process, reducing the risk of inconsistencies and improving efficiency.\n\n**Consequences:**\n\n* **Potential compatibility issues:** Migrating to Alpine Linux may introduce compatibility issues with some existing integrations that rely on Debian-specific functionality.\n* **Increased image size:** The Alpine-based image may be larger than the current Debian image due to the inclusion of additional packages and dependencies.\n* **Effort required for migration:** Transitioning all Docker images to Alpine Linux will require significant development and testing efforts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen joining a project everybody needs some point where he can start stepping\ninto the system, building it, running it, understanding it. The main entrypoint\nis always the README.\n\n## Decision\n",I will consider the README as the most valuable place of documentation. The\nREADME of any project shall give a new team-member everything he needs to\njoin the team.\n,Declare the README file as the official entrypoint documentation for new project members.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a mechanism to ignore discovered config flows using unique ids\nattached to config entries but not all integrations may set an unique id\ndue to lack of truly unique identifiers or just not having been updated for\nconcerns of backward compatibility.\nThis creates a UX issue that has started to become more pronounced with more config flows\nbeing added every release (which is awesome!). The real issue is that not all discovery processes\nare created equal and that becomes an issue for consistency.\n\n## Decision\n,Integrations that are discoverable must provide a unique id (via `async_set_unique_id`)\nto allow the user to ignore the discovered config entry.\nThese rules apply to all new integrations. Existing integrations should be reviewed\nfor possible ways to improve identification during discovery.\n,**Set a time constraint for ignoring flow discovered entries.**\n\nThe decision is to set a time constraint for ignoring flow discovered entries. This means that any entries discovered by a flow that are not marked with a unique id will be automatically ignored after a certain period of time. This will help to ensure that the UX issue is resolved and that all discovery processes are treated equally. The length of the time constraint will need to be determined based on the needs of the system.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently have a lot of integrations that use or are based on GPIO (including SPI and I2C busses).\n- All of these integrations have a low usage count (source Home Assistant Analytics),\nbut add the same amount of maintenance and review time to the core.\n- Most of these integrations are unmaintained.\n- The use of GPIO via containers (including our Home Assistant OS, Container\n& Supervised installation methods) is often unstable or complicated.\n- Support for GPIO on specific platforms/SBCs: mileage varies (e.g., Raspberry Pi on 64 bits platforms).\n- The use of these sensors, in general, is better supported and done via dedicated projects like ESPHome.\n\n## Decision\n","- We no longer accept integrations that integrate with devices over GPIO.\n- We identify, deprecate and remove integrations that use GPIO.\n- It will still be possible to have custom integrations that use GPIO.\nGPIO, in this case, means interfacing directly with the individual GPIO pins\n(or I/O lines) on the board running Home Assistant. This includes bus\ncommunications like I2C and SPI.\nDevices connected on GPIO that are exposed on the system as regular serial\ndevices are exempted from this ADR.\n","**Decision:** Deprecate the support for GPIO in Home Assistant.\n\n**Rationale:**\n\n* **Low usage count:** The integrations that rely on GPIO have a low usage count, indicating that they are not widely used.\n* **Maintenance burden:** The integrations add a significant maintenance burden to the core, even though they are not heavily used.\n* **Lack of maintenance:** Most of the GPIO-based integrations are unmaintained, which increases the risk of security vulnerabilities and other issues.\n* **Instability and complexity:** The use of GPIO in containers can be unstable or complicated, leading to unreliable integrations.\n* **Platform-specific support:** GPIO support varies across different platforms and SBCs, making it difficult to ensure consistent functionality.\n* **Alternative solutions:** Dedicated projects like ESPHome provide better support for sensors and devices that typically use GPIO.\n\n**Consequences:**\n\n* **Removal of GPIO-based integrations:** The deprecation of GPIO support will result in the removal of all integrations that rely on it.\n* **Loss of functionality for some users:** Users who rely on GPIO-based integrations will need to find alternative solutions or switch to other platforms.\n* **Reduced maintenance burden:** Deprecating GPIO support will significantly reduce the maintenance burden on the Home Assistant core.\n* **Improved stability and security:** By removing GPIO support, Home Assistant will become more stable and secure.\n* **Focus on alternative solutions:** The deprecation of GPIO support will encourage users to adopt alternative solutions that are better supported and more reliable."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHome Assistant currently sets the minimum Python requirement to the penultimate minor Python version released upstream.\n\n## Decision\n,"Home Assistant will try to support the latest two released minor upstream Python versions. For example, if the current latest version is 3.11, we will support Python 3.10 and would like to support Python 3.11; however, Python 3.9 is no longer supported at this point.\nOnce a new minor Python version is released, the to be dropped minor version will be deprecated for a period of 2 months, after which it will be removed. While deprecated, Home Assistant will print a warning message to inform the user.\nSupported micro versions are decided on minor version basis, and codified in the version check code in Home Assistant proper.\n","**Decision:** Bump minimum Python requirement to Python 3.10.\n\n**Rationale:**\n\n* Home Assistant is a complex application that relies on many Python libraries and tools.\n* Python 3.10 offers significant performance improvements and new features that can benefit Home Assistant.\n* By bumping the minimum Python requirement, Home Assistant can take advantage of these benefits and improve the user experience.\n* The vast majority of Home Assistant users are already running Python 3.10 or later, so this change will not impact a significant number of users.\n\n**Consequences:**\n\n* Users running Python 3.9 or earlier will need to upgrade their Python installation to continue using Home Assistant.\n* This may require users to update their operating system or Python distribution.\n* Some third-party integrations may not be compatible with Python 3.10 and will need to be updated or replaced."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDefine a supported installation method as per [ADR-0012](https://github.com/home-assistant/architecture/blob/master/adr/0012-define-supported-installation-method.md).\n\n## Decision\n,"This is for running just the Home Assistant Core application directly on Python. It does not provide the full Supervisor experience and thus does not provide the Supervisor panel and add-ons.\n### Supported Operating Systems and versions\n- All major Linux distributions, latest stable and major versions.\n- Windows; only using WSL.\n- macOS; Python via Homebrew.\n### Supported Python versions\nRunning Home Assistant Core is only supported when running the application using the official Python virtual environment. Running Home Assistant Core without a virtual environment, system/globally installed Python packages, is not supported.\nDetails on the supported Python versions are defined in [ADR-0020](./0020-minimum-supported-python-version.md).\n### Documentation\nSome operating systems will require extra libraries or packages to be installed prior to installing the Python requirements. In this case our documentation shall link to the installation instructions of the Python requirement that requires them.\nIn case that is not available or possible, we will name the libraries or packages that need to be installed. We do not aim not include installation instructions for every OS.\n### Required Expertise\n- **Installation**\nRequires installing Python 3 with venv support (the default except on Debian based systems). Then create a virtual environment and install Home Assistant Core via pip.\nFor packages that require compilation, the user will need to install compilers and other development packages. If those development packages are not the same as provided by the operating system, you can break your system.\n* **Start when the system is started:** This is the responsibility of the user. It is based on their operating system.\n* **Run with full network access:** Works, is the only option.\n* **Access USB devices:** This works out of the box.\n* **Maintaining the Home Assistant installation**\nMaintenance requires more time, effort, skills, and experience than the other methods.\n- **Python upgrades:** Home Assistant upgrades Python every year. It can happen that your current operating system doesn’t support the new minimum required version out of the box. In that case, you need to find unofficial Python packages for your system or compile Python from source.\n- **Installing Python dependencies:** Some Python packages need compilation. Users are responsible for having the right compilers and development packages installed.\n- **Updating Home Assistant:** Updating happens via the pip command-line tool.\n- **Maintaining the Operating System**\nHome Assistant Core runs in a Python virtual environment. Anything outside of that is the responsibility of the user.\n* **Security updates for OS:** Responsibility of the user.\n* **Maintaining the components required for the Supervisor:** No supervisor, so N/A\n**Conclusion:**\nThis is an expert installation method. Based on the integrations that you’re running, you will need a lot of extra packages installed.\n","**Decision:** Support installation via Flatpak.\n\n**Rationale:**\n\n* Flatpak is a sandboxed package manager that provides a consistent and isolated environment for applications.\n* It simplifies the installation and update process for Home Assistant users, as it removes the need for manual dependency management.\n* Flatpak is supported on a wide range of Linux distributions and operating systems.\n* It aligns with the goal of [improving the user experience](https://github.com/home-assistant/architecture/blob/master/adr/0011-define-user-experience-guidelines.md) for Home Assistant installations.\n\n**Consequences:**\n\n* Additional development effort will be required to maintain the Flatpak package.\n* Users may encounter issues with Flatpak-specific configurations or dependencies.\n* Flatpak installations may have limitations or performance differences compared to other installation methods."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need take a decision for the minimal requirements of a IoT-Board to become a Home Assistant OS image.\nAlso how we deal with old or replaced generation/boards.\n\n## Decision\n,"Follow are the requirements of a Board:\n- The board needs to be available for private customers in most major economic regions\n- The manufacturer produces and sells the boards for at least 3 years\n- The manufacturer is know for his continuity\n- The board needs to be supported by the upstream Linux kernel\n- The board needs to be supported by the upstream U-Boot or Barebox boot loader\nIt's allowed to use some patches as long they only minimal touch the Kernel and marked for upstream.\nWe support the board as long as possible with updates. We might remove support if we see major downsides or we don't have hardware\nfor the sporadic tests.  When a next generation is available, we provide updates for the old but don't recommend it anymore.\nIf a board is no longer supported, no updates for the operating system or Home Assistant Core will be provided according ADR0015.\nIt's recommended to update the hardware to next generation or a new board.\n","**Decision:**\n\n### Minimal Requirements for IoT Boards to Become Home Assistant OS Images\n\n- **Hardware:**\n    - Single-board computer with ARM or x86 architecture\n    - At least 512MB of RAM\n    - At least 4GB of storage\n    - Built-in Ethernet port\n    - Optional: Built-in Wi-Fi and Bluetooth connectivity\n- **Software:**\n    - Support for the Home Assistant Operating System bootloader\n    - Ability to run a headless operating system\n    - Support for updating the operating system and packages over the network\n\n### Handling of Old or Replaced Generations/Boards\n\n- **Supported boards:**\n    - Maintain a list of supported IoT boards that meet the minimal requirements.\n    - Provide clear documentation on which boards are currently supported.\n- **Legacy boards:**\n    - Designate boards that no longer meet the minimal requirements as ""legacy"".\n    - Continue to support legacy boards for a limited time (e.g., one major Home Assistant OS release cycle).\n    - Announce deprecation of legacy boards in advance to allow users to upgrade.\n    - Provide instructions and resources for migrating from legacy boards to supported boards.\n- **Replaced boards:**\n    - Remove replaced boards from the supported list.\n    - Provide guidance to users on how to dispose of replaced boards responsibly."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDefine a supported installation method as per [ADR-0012](https://github.com/home-assistant/architecture/blob/master/adr/0012-define-supported-installation-method.md).\n\n## Decision\n,"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\nThe only supported way to run the container is on the host network as root with full privileges.\n### Supported Containerization system and version\n- Any Open Container Initiative (OCI) compatible containerization system.\n### Supported boards/hardware/machines\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\n### Supported Operating Systems and versions\n- Running Home Assistant Container is only supported on Linux.\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\n### Additional notes\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\n### Required Expertise\n- **Installation**\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\n* **Maintaining the Home Assistant installation**\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update – automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\n- **Python upgrades:** Included in the Home Assistant container\n- **Installing Python dependencies:** Included in the Home Assistant container\n- **Updating Home Assistant:** Included in the Home Assistant container\n- **Maintaining the Operating System**\nSince this is just the core container, all OS responsibilities are with the user.\n- **Security updates for OS:** Responsibility of the user.\n- **Maintaining the components required for the Supervisor:** No supervisor, so N/A\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\n","**Support Home Assistant using the official supervised installation method**\n\nRationale:\n\n* This method is the most reliable and secure way to install and run Home Assistant.\n* It provides features such as automatic updates, backups, and remote access.\n* It is easy to use and manage, even for beginners.\n* It is compatible with a wide range of hardware platforms."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\nNot adding oneself as a code owner has drawbacks for the project:\n- The contributor doesn't ""own"" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\nAs a result of this:\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\n- Integrations are more prone to break in the future.\n- Integration with a single code owner:\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\nDuring the design discussion of this ADR, it also became clear, that the term ""code owner"" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as ""taking responsibility"".\n\n## Decision\n","Code ownership for an integration defined:\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\n### Rules\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\nFor the following cases, adding oneself as a code owner is required:\n- When contributing a new integration.\n- When contributing a new platform to an integration.\n- When contributing a new feature to an integration.\n- When contributing a significant refactor or rewrite of an integration.\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\n### Exceptions\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\nIn the following cases, code ownership may be omitted:\n- Contributions that solely provides a bug fix(es).\n- Contributions that only provide additional unit test(s).\n- Contributions to integrations marked as ""internal"". These integrations are code owned by the Home Assistant core team.\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\n### Withdrawing as a code owner\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\n","**Decision:** All new integrations, and all major changes to existing integrations, must include at least two code owners.\n\n**Reasoning:** This decision will help to ensure that all integrations are properly maintained and that there is always someone who is available to address issues. This will improve the quality of the project and make it more sustainable in the long term.\n\n**Additional Considerations:**\n\n* The code owners should be people who are familiar with the integration and who are willing to take responsibility for it.\n* The code owners should be added to the integration's CONTRIBUTORS.md file.\n* The GitHub bot should be configured to notify the code owners when an issue for the integration is reported."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIf you look at our documentation, it’s all over the place. Install it in Docker, in a VM, on a NAS or on one of the many Linux distributions.\nThe reason we have this many guides is that since the start of the Home Assistant website, we have always gladly accepted every contribution to get Home Assistant running on any platform. The more the merrier!\nHowever, in software, nothing ever stays the same. All software gets updates to fix bugs, fix security vulnerabilities, improve performance or to add new features. As a software application you need to grow along or else you get stuck with an insecure system.\nSo as Home Assistant grows and evolves, some of these installation guides became outdated and stopped working. We wouldn’t even know it was broken until a user raised an issue. But when they do, we wouldn’t know how to fix it unless we could get a hold of the original contributor.\nThis can be frustrating. Any guide on our official website should lead to a working system. A system that not only works today, but also tomorrow.\n\n## Decision\n","A supported installation method in the Home Assistant context means:\n> A way of installing and running Home Assistant in a way that is supported by the Home Assistant developers. Supported means the installation method is tested and documented in the official documentation. Running Home Assistant using such a supported method, leads to the optimal user experience.\nThe Home Assistant team will not prevent you from running Home Assistant using an unofficial method. However, we cannot help with issues that you encounter. We are open for contributions that improve compatibility with a community-supported method as long as they do not impact officially supported methods, add a significant amount of code exceptions or future maintenance burden on the Home Assistant development team.\n","**Decision:** To ensure the accuracy and reliability of our documentation, we will centralize the management and maintenance of installation guides for Home Assistant.\n\n**Justification:**\n\n* Centralization allows for better control and coordination, ensuring that all installation guides are up-to-date and accurate.\n* By relying on a smaller group of maintainers, we can reduce the risk of errors and inconsistencies in the documentation.\n* This approach will enable us to respond more efficiently to updates and changes in Home Assistant, ensuring that users always have access to reliable installation instructions.\n* Furthermore, it will simplify the process of troubleshooting and resolving issues related to installation guides, as the maintainers will have a comprehensive understanding of the documentation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe backend has only one API endpoint to fetch translations: `frontend/get_translations`. It takes no parameters. This needs to load all relevant translations. Because of config flows, relevant translations are all translations of all our currently loaded integrations PLUS any integration that has a config flow.\nWith so many integrations with config flows, this API call is getting huge. On current dev, the response is 142KB!\nOur main translation files (`strings.json`) is keyed by ""area"" where they are used. There is:\n- `config` for config flow\n- `options` for option flow\n- `device_automation` for device automations\n- `state` for translations of states under your domain (example is entity `zwave.bla` with state `alive`). This is not used by any `strings.json`.\nIntegrations can also have platform specific translation files (`strings.sensor.json`). These files are merged into the integration that they are offering a platform to. So the values of `strings.sensor.json` will be merged into `sensor/strings.json`. This is only used to provide `state`. This is only useful for sensors with text values and is only used by [moon](https://github.com/home-assistant/core/blob/dev/homeassistant/components/moon/strings.sensor.json) and [season](https://github.com/home-assistant/core/blob/dev/homeassistant/components/season/strings.sensor.json).\nThe title of integrations sometimes requires being translated too (mainly internal integrations) and so is part of our translation file. However we made the mistake of putting the `title` under `config`. So even if we would open up our API to allow fetching just one area, we would still need to fetch all config flow info of all integrations in case we need to render the name of a single domain 🤷‍♂.\nWe should drop the `title` from `strings.json` if the integration is for a product who'se name does not require translation and so it should remain equal to the title in `manifest.json`. Examples are `zwave` or `hue`.\n\n## Decision\n","#### Backend\n- Move integration title out of `config` key so we can load it independently ([current PR](https://github.com/home-assistant/core/pull/33850))\n- Drop all titles from `strings.json` that are the same as `manifest.json` and should not be translated.\n- Remove support for integrations providing platform translations\n- When fetching titles for integrations, use the manifest name if title is not provided in translation file.\n- Change `frontend/get_translations` to allow specifying area that you are interested in\n- We need to start using references and write specific translations down once. Right now each integration defines `already_configured` etc. We use this syntax in the frontend: `[%key:state::binary_sensor::battery::off%]`.\n- Update scaffold scripts to use references.\n- Start using `state` in `strings.json`. It is going to be a dictionary that has a `default` entry and one entry per device class. Just like we have now in the frontend.\n```json\n{\n""state"": {\n""binary_sensor"": {\n""default"": {\n""off"": ""[%key:state::default::off%]"",\n""on"": ""[%key:state::default::on%]""\n},\n""battery"": {\n""off"": ""Normal"",\n""on"": ""Low""\n},\n}\n}\n}\n```\n- Import all existing state keys + translations from frontend\n- Add a new `state_attributes` to `strings.json` to allow offering translations for state attributes.\n```json\n{\n""state_attributes"": {\n""climate"": {\n""hvac_action"": {\n""off"": ""Off"",\n""heating"": ""Heating"",\n""cooling"": ""Cooling"",\n""drying"": ""Drying"",\n""idle"": ""Idle"",\n""fan"": ""Fan""\n}\n}\n}\n}\n```\n- Import all existing state attribute keys + translations from frontend\n- Replace as many existing translations as possible with references\n- Convert moon and season sensor to have their own entities and use `strings.json` for translation\n#### Frontend\nLoad part of backend translations when necessary.\n- On page load, load `state` only.\n- Fetch integration titles when opening config or dev tools panels\n- Fetch config flow translations when config flow started (not handler picked)\n- Fetch options flow translations when option flow started\n- Fetch device automation strings when opening script/automation editor\n- Remove built-in translations of domains (`domain.*`) and use backend provided strings instead.\n- Remove built-in translations for states (`state`) and use backend provided strings instead.\n- Remove built-in translations for state attributes (`state_attributes`) and use backend provided strings instead.\n- Adopt cleaning keys script from backend ([see this PR](https://github.com/home-assistant/core/pull/33802))\n- Remove all state and state attributes keys from translations once imported into backend\n","**Architectural Decision:**\n\nSplit the `frontend/get_translations` API endpoint into multiple endpoints, each responsible for fetching a specific area of translations.\n\n**Rationale:**\n\n* Reduces the size and complexity of the `frontend/get_translations` response by only loading the necessary translations.\n* Improves performance by reducing the amount of data transferred over the network.\n* Allows for more granular control over which translations are loaded.\n\n**Details:**\n\n* Create separate endpoints for each area of translations, such as:\n    * `frontend/get_config_translations`\n    * `frontend/get_options_translations`\n    * `frontend/get_device_automation_translations`\n* The `frontend/get_translations` endpoint will remain as a convenience endpoint that fetches all translations, but it will make internal calls to the individual endpoints to retrieve the data.\n* Drop the `title` field from `strings.json` for integrations that do not require title translation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\n\n## Decision\n",Any processes and workflows need to be very fast and fully automated.\n,**Decision:** Use asynchronous operations to avoid blocking developers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUsing TODO comments in the code are a good way to notify yourself on open topics when dealing with larger parts to deal with. Most IDEs have support for those comments as well so that it's easy to know what still has to be done.\nUnfortunately it's not easy to distinguish between issues that really should be done and others that are an advice.\nThere are multiple ways to deal with this. You can use further tags like `FIXME` or `XXX` to distinguish between those. You could also decide not to keep any open `TODO` issues in your code and (if valid) move them to your issues.\n\n## Decision\n,I favor not keeping any open issues in the code.\n,"Use a separate tool to manage TODOs, such as a task management system or a code review tool."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To implement an Architectural Decision Record (ADR) process to document and track architectural decisions made throughout the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are currently many different ways of structuring an integration and its config in Home Assistant. We allow config YAML, either under the integration key or under platform keys with the integration name. We allow a config flow with configuration entered via the GUI, stored in config entries. We allow importing config YAML to a config entry via config flow.\nThis ADR focuses on the configuration YAML structure and its use in integrations.\nThe many options for configuration impact how the integration is structured and what Home Assistant backend APIs are used.\n1. Integrations that only have a single platform, place the configuration under a platform key and use `async_setup_platform` to set up the platform.\n2. Integrations that have multiple platforms, sometimes centralize the configuration to the integration domain name key and load platforms via `discovery.async_load_platform`, which in turn calls `async_setup_platform`.\n3. If the integration has multiple platforms but doesn't have a centralized config, the user needs to add configuration under the integration domain to set up the integration and each platform key to set up each platform. This loads each platform via `async_setup_platform`.\nAll backend APIs in point 1-3 above also have sync versions, to further increase the options.\nThis multitude of options for config and integration structure makes it:\nA. Harder for users to know how to configure the integrations.\nB. Harder for contributors to know what way to implement and support.\nC. Harder for Home Assistant to support confused users and contributors and harder to support and extend a disparate code base.\n---\n\n## Decision\n","We limit the configuration YAML structure to one way for new integrations. We require all YAML configuration, if present, for an integration to be located under the integration domain key in configuration YAML, for all new integrations (point 2 above).\nFor existing integrations we don't allow changes to YAML configuration in platform sections until the integration has been refactored and the configuration moved under the integration domain key.\n```yaml\n# Allowed\nawesome_integration:\nusername: user\n# Not allowed\nsensor:\n- platform: awesome_integration\nusername: user\n```\n### Exceptions\n- For integrations that do not fall under [ADR-0010](https://github.com/home-assistant/architecture/blob/master/adr/0010-integration-configuration.md), implementing an optional `unique_id` configuration key to set the unique ID of the entity created by the platform is allowed.\n","Home Assistant will deprecate usage of platform keys for integration configuration.\n\nIntegrations that support multiple platforms will use the integration domain name key to store their configuration. YAML configuration will be moved out of platform keys and into the integration domain name key.\n\nIntegrations that only have a single platform may continue to use platform keys for configuration, but this will be deprecated in the future.\n\n**Rationale:**\n\nThis decision will make it easier for users to configure integrations, as they will no longer need to know which configuration key to use for each platform. It will also make it easier for contributors to implement and support integrations, as they will no longer need to support multiple configuration methods. Finally, it will make it easier for Home Assistant to support confused users and contributors, and to support and extend a more consistent code base."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDefine a supported installation method as per [ADR-0012](https://github.com/home-assistant/architecture/blob/master/adr/0012-define-supported-installation-method.md).\n\n## Decision\n,"Home Assistant OS is the full installation of our all-inclusive home automation system. Best in class home automation is complemented with a UI for configuring your system, making backups and safe updates with automatic rollback.\nThis is the generally recommended installation method and the one that our website and documentation should be focused on.\n### Supported boards/hardware/machines\n- Home Assistant Yellow\n- Home Assistant Green\n- Raspberry Pi 3 Model B and B+ 32-bit\n- Raspberry Pi 3 Model B and B+ 64-bit\n- Raspberry Pi 4 Model B 32-bit\n- Raspberry Pi 4 Model B 64-bit\n- Tinkerboard\n- ODROID-C2\n- ODROID-C4\n- ODROID-N2\n- ODROID-M1\n- ODROID-XU4\n- Bare-metal on x86-64 PCs (e.g. Intel NUC, via `generic-x86-64` image)\n- Virtual Machine (x86-64/AMD64 based by `ova` image)\n### Supported Operating Systems and versions\nThis installation method is only available and supported using the Home Assistant Operating System. Only the latest major version is supported.\nWhen a new major version is released, the previous major version will be dropped with a deprecation period of 2 months. The last 3 minor releases are supported.\n### Supported Hypervisors\nThe Home Assistant Operating System can be run on a Hypervisor and thus be run as a virtual machine. The following Hypervisors are supported:\n- KVM/QEMU\n- VirtualBox\n- VMWare\n- Xen\nWe will provide documentation for the following systems build on top of these technologies:\n- Proxmox (KVM/QEMU based)\n- Unraid (KVM/QEMU based)\n- VirtualBox\n- VMWare\n### Required Expertise\n- **Installation**\nInstallation requires the user to install the disk image for their device, install an image flasher (Etcher) and flash the image to the disk. Then insert image and boot their device.\n* **Start when the system is started:** Managed by installer\n* **Run with full network access:** Managed by installer\n* **Access USB devices:** Managed by supervisor\n* **Maintaining the Home Assistant installation**\nOngoing maintenance can be done via the user interface. User will be notified if an update is available for the operating system or for Home Assistant. Updates will be automatically rolled back if they fail.\n- **Python upgrades:** Managed via HA updates\n- **Installing Python dependencies:** Managed via HA updates\n- **Updating Home Assistant:** Via a UI\n- **Maintaining the Operating System**\nOngoing maintenance can be done via the user interface. User will be notified if an update is available for the operating system. Updates will be automatically rolled back if they fail.\n* **Security updates for OS:** Managed via HA OS updates\n* **Maintaining the components required for the Supervisor:** Managed via HA OS updates\n**Conclusion:** low expertise required. Besides the installation methods, everything is done via the UI.\n",Integrate Home Assistant to [Supervised Addons](https://github.com/hassio-addons/supervised-add-ons).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current ""naive"" IBC Relayer strategy currently establishes a single predetermined IBC channel atop a single connection between two clients (each potentially of a different chain).  This strategy then detects packets to be relayed by watching for `send_packet` and `recv_packet` events matching that channel, and sends the necessary transactions to relay those packets.\nWe wish to expand this ""naive"" strategy to a ""passive"" one which detects and relays both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.\nIn order to accomplish this, we propose adding more comprehensive events to expose channel metadata for each transaction sent from the `x/ibc/core/04-channel/keeper/handshake.go` and `x/ibc/core/04-channel/keeper/packet.go` modules.\nHere is an example of what would be in `ChanOpenInit`:\n```go\nconst (\nEventTypeChannelMeta = ""channel_meta""\nAttributeKeyAction = ""action""\nAttributeKeyHops = ""hops""\nAttributeKeyOrder = ""order""\nAttributeKeySrcPort = ""src_port""\nAttributeKeySrcChannel = ""src_channel""\nAttributeKeySrcVersion = ""src_version""\nAttributeKeyDstPort = ""dst_port""\nAttributeKeyDstChannel = ""dst_channel""\nAttributeKeyDstVersion = ""dst_version""\n)\n// ...\n// Emit Event with Channel metadata for the relayer to pick up and\n// relay to the other chain\n// This appears immediately before the successful return statement.\nctx.EventManager().EmitEvents(sdk.Events{\nsdk.NewEvent(\ntypes.EventTypeChannelMeta,\nsdk.NewAttribute(types.AttributeKeyAction, ""open_init""),\nsdk.NewAttribute(types.AttributeKeySrcConnection, connectionHops[0]),\nsdk.NewAttribute(types.AttributeKeyHops, strings.Join(connectionHops, "","")),\nsdk.NewAttribute(types.AttributeKeyOrder, order.String()),\nsdk.NewAttribute(types.AttributeKeySrcPort, portID),\nsdk.NewAttribute(types.AttributeKeySrcChannel, channelID),\nsdk.NewAttribute(types.AttributeKeySrcVersion, version),\nsdk.NewAttribute(types.AttributeKeyDstPort, counterparty.GetPortID()),\nsdk.NewAttribute(types.AttributeKeyDstChannel, counterparty.GetChannelID()),\n// The destination version is not yet known, but a value is necessary to pad\n// the event attribute offsets\nsdk.NewAttribute(types.AttributeKeyDstVersion, """"),\n),\n})\n```\nThese metadata events capture all the ""header"" information needed to route IBC channel handshake transactions without requiring the client to query any data except that of the connection ID that it is willing to relay.  It is intended that `channel_meta.src_connection` is the only event key that needs to be indexed for a passive relayer to function.\n### Handling Channel Open Attempts\nIn the case of the passive relayer, when one chain sends a `ChanOpenInit`, the relayer should inform the other chain of this open attempt and allow that chain to decide how (and if) it continues the handshake.  Once both chains have actively approved the channel opening, then the rest of the handshake can happen as it does with the current ""naive"" relayer.\nTo implement this behavior, we propose replacing the `cbs.OnChanOpenTry` callback with a new `cbs.OnAttemptChanOpenTry` callback which explicitly handles the `MsgChannelOpenTry`, usually by resulting in a call to `keeper.ChanOpenTry`.  The typical implementation, in `x/ibc-transfer/module.go` would be compatible with the current ""naive"" relayer, as follows:\n```go\nfunc (am AppModule) OnAttemptChanOpenTry(\nctx sdk.Context,\nchanKeeper channel.Keeper,\nportCap *capability.Capability,\nmsg channel.MsgChannelOpenTry,\n) (*sdk.Result, error) {\n// Require portID is the portID transfer module is bound to\nboundPort := am.keeper.GetPort(ctx)\nif boundPort != msg.PortID {\nreturn nil, sdkerrors.Wrapf(porttypes.ErrInvalidPort, ""invalid port: %s, expected %s"", msg.PortID, boundPort)\n}\n// BEGIN NEW CODE\n// Assert our protocol version, overriding the relayer's suggestion.\nmsg.Version = types.Version\n// Continue the ChanOpenTry.\nres, chanCap, err := channel.HandleMsgChannelOpenTry(ctx, chanKeeper, portCap, msg)\nif err != nil {\nreturn nil, err\n}\n// END OF NEW CODE\n// ... the rest of the callback is similar to the existing OnChanOpenTry\n// but uses msg.* directly.\n```\nHere is how this callback would be used, in the implementation of `x/ibc/handler.go`:\n```go\n// ...\ncase channel.MsgChannelOpenTry:\n// Lookup module by port capability\nmodule, portCap, err := k.PortKeeper.LookupModuleByPort(ctx, msg.PortID)\nif err != nil {\nreturn nil, sdkerrors.Wrap(err, ""could not retrieve module from port-id"")\n}\n// Retrieve callbacks from router\ncbs, ok := k.Router.GetRoute(module)\nif !ok {\nreturn nil, sdkerrors.Wrapf(port.ErrInvalidRoute, ""route not found to module: %s"", module)\n}\n// Delegate to the module's OnAttemptChanOpenTry.\nreturn cbs.OnAttemptChanOpenTry(ctx, k.ChannelKeeper, portCap, msg)\n```\nThe reason we do not have a more structured interaction between `x/ibc/handler.go` and the port's module (to explicitly negotiate versions, etc) is that we do not wish to constrain the app module to have to finish handling the `MsgChannelOpenTry` during this transaction or even this block.\n\n## Decision\n","- Expose events to allow ""passive"" connection relayers.\n- Enable application-initiated channels via such passive relayers.\n- Allow port modules to control how to handle open-try messages.\n","**Decision:** Implement a passive IBC Relayer strategy that detects and relays both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.\n\n**Rationale:**\n\nThis decision addresses the need for a more comprehensive IBC Relayer strategy that can handle a wider range of scenarios, including the detection and relaying of channels that are not known in advance. The passive relaying approach will provide greater flexibility and reliability for IBC communication.\n\n**Consequences:**\n\n* **Improved flexibility and reliability:** The passive relayer will be able to handle a wider range of IBC scenarios, including those with unknown channels.\n* **Reduced complexity:** The relayer will no longer need to maintain a list of known channels, which will simplify the codebase and reduce the potential for errors.\n* **Increased performance:** The passive relayer will be able to detect and relay channels more efficiently, which will improve the overall performance of IBC communication."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Summary\nAt launch, IBC will be a novel protocol, without an experienced user-base. At the protocol layer, it is not possible to distinguish between client expiry or misbehaviour due to genuine faults (Byzantine behavior) and client expiry or misbehaviour due to user mistakes (failing to update a client, or accidentally double-signing). In the base IBC protocol and ICS 20 fungible token transfer implementation, if a client can no longer be updated, funds in that channel will be permanently locked and can no longer be transferred. To the degree that it is safe to do so, it would be preferable to provide users with a recovery mechanism which can be utilised in these exceptional cases.\n### Exceptional cases\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\n1. The chain which the client is following has halted and is no longer producing blocks/headers, so no updates can be made to the client\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\n### Security model\nTwo-thirds of the validator set (the quorum for governance, module participation) can already sign arbitrary data, so allowing governance to manually force-update a client with a new header after a delay period does not substantially alter the security model.\n\n## Decision\n","We elect not to deal with chains which have actually halted, which is necessarily Byzantine behaviour and in which case token recovery is not likely possible anyways (in-flight packets cannot be timed-out, but the relative impact of that is minor).\n1. Require Tendermint light clients (ICS 07) to be created with the following additional flags\n1. `allow_update_after_expiry` (boolean, default true). Note that this flag has been deprecated, it remains to signal intent but checks against this value will not be enforced.\n1. Require Tendermint light clients (ICS 07) to expose the following additional internal query functions\n1. `Expired() boolean`, which returns whether or not the client has passed the trusting period since the last update (in which case no headers can be validated)\n1. Require Tendermint light clients (ICS 07) & solo machine clients (ICS 06) to be created with the following additional flags\n1. `allow_update_after_misbehaviour` (boolean, default true). Note that this flag has been deprecated, it remains to signal intent but checks against this value will not be enforced.\n1. Require Tendermint light clients (ICS 07) to expose the following additional state mutation functions\n1. `Unfreeze()`, which unfreezes a light client after misbehaviour and clears any frozen height previously set\n1. Add a new governance proposal with `MsgRecoverClient`.\n1. Create a new Msg with two client identifiers (`string`) and a signer.\n1. The first client identifier is the proposed client to be updated. This client must be either frozen or expired.\n1. The second client is a substitute client. It carries all the state for the client which may be updated. It must have identical client and chain parameters to the client which may be updated (except for latest height, frozen height, and chain-id). It should be continually updated during the voting period.\n1. If this governance proposal passes, the client on trial will be updated to the latest state of the substitute.\n1. The signer must be the authority set for the ibc module.\nPreviously, `AllowUpdateAfterExpiry` and `AllowUpdateAfterMisbehaviour` were used to signal the recovery options for an expired or frozen client, and governance proposals were not allowed to overwrite the client if these parameters were set to false. However, this has now been deprecated because a code migration can overwrite the client and consensus states regardless of the value of these parameters. If governance would vote to overwrite a client or consensus state, it is likely that governance would also be willing to perform a code migration to do the same.\nIn addition, `TrustingPeriod` was initially not allowed to be updated by a client upgrade proposal. However, due to the number of situations experienced in production where the `TrustingPeriod` of a client should be allowed to be updated because of ie: initial misconfiguration for a canonical channel, governance should be allowed to update this client parameter.\nIn versions older than ibc-go v8, `MsgRecoverClient` was a governance proposal type `ClientUpdateProposal`. It has been removed and replaced by `MsgRecoverClient` in the migration from governance v1beta1 to governance v1.\nNote that this should NOT be lightly updated, as there may be a gap in time between when misbehaviour has occurred and when the evidence of misbehaviour is submitted. For example, if the `UnbondingPeriod` is 2 weeks and the `TrustingPeriod` has also been set to two weeks, a validator could wait until right before `UnbondingPeriod` finishes, submit false information, then unbond and exit without being slashed for misbehaviour. Therefore, we recommend that the trusting period for the 07-tendermint client be set to 2/3 of the `UnbondingPeriod`.\nNote that clients frozen due to misbehaviour must wait for the evidence to expire to avoid becoming refrozen.\nThis ADR does not address planned upgrades, which are handled separately as per the [specification](https://github.com/cosmos/ibc/tree/master/spec/client/ics-007-tendermint-client#upgrades).\n",**Decision:**\n\nImplement a governance process to manually force-update expired/misbehaving clients after a delay period.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\n`modules/light-clients`. Adding support for new light clients or updating an existing light client in the event\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\nmessages to the appropriate handler function, inside the Wasm VM for execution.\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\n\n## Decision\n","We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\n```go\nparams := clientKeeper.GetParams(ctx)\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\nclientKeeper.SetParams(ctx, params)\n```\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\na [governance v1 proposal](https://docs.cosmos.network/main/modules/gov#proposals) that contains the `sdk.Msg` for storing\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\n```proto\n// MsgStoreCode defines the request type for the StoreCode rpc.\nmessage MsgStoreCode {\n// signer address\nstring signer = 1;\n// wasm byte code of light client contract. It can be raw or gzip compressed\nbytes wasm_byte_code = 2;\n}\n```\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\nsubmit this message (which is normally the address of the governance module).\n```go\n// StoreCode defines a rpc handler method for MsgStoreCode\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\nif k.GetAuthority() != msg.Signer {\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, ""expected %s, got %s"", k.GetAuthority(), msg.Signer)\n}\nctx := sdk.UnwrapSDKContext(goCtx)\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\nif err != nil {\nreturn nil, errorsmod.Wrap(err, ""failed to store wasm bytecode"")\n}\nemitStoreWasmCodeEvent(ctx, checksum)\nreturn &types.MsgStoreCodeResponse{\nChecksum: checksum,\n}, nil\n}\n```\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\n### How light client proxy works?\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\nreturned to the caller.\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\n```go\ntype QueryMsg struct {\nStatus               *StatusMsg               `json:""status,omitempty""`\nExportMetadata       *ExportMetadataMsg       `json:""export_metadata,omitempty""`\nTimestampAtHeight    *TimestampAtHeightMsg    `json:""timestamp_at_height,omitempty""`\nVerifyClientMessage  *VerifyClientMessageMsg  `json:""verify_client_message,omitempty""`\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:""check_for_misbehaviour,omitempty""`\n}\ntype verifyClientMessageMsg struct {\nClientMessage *ClientMessage `json:""client_message""`\n}\n// VerifyClientMessage must verify a ClientMessage.\n// A ClientMessage could be a Header, Misbehaviour, or batch update.\n// It must handle each type of ClientMessage appropriately.\n// Calls to CheckForMisbehaviour, UpdateStaåte, and UpdateStateOnMisbehaviour\n// will assume that the content of the ClientMessage has been verified\n// and can be trusted. An error should be returned\n// if the ClientMessage fails to verify.\nfunc (cs ClientState) VerifyClientMessage(\nctx sdk.Context,\n_ codec.BinaryCodec,\nclientStore storetypes.KVStore,\nclientMsg exported.ClientMessage\n) error {\nclientMessage, ok := clientMsg.(*ClientMessage)\nif !ok {\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, ""expected type: %T, got: %T"", &ClientMessage{}, clientMsg)\n}\npayload := QueryMsg{\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\n}\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\nreturn err\n}\n```\n### Global Wasm VM variable\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\nthe `ClientState` functions.\n","Implement an interchain account unit tests using the localnet testing framework.\n\nThe framework provides the functionality to set up a local Cosmos SDK network with multiple nodes, enabling testing of interchain account interactions in a realistic environment. This will allow the thorough verification of the interchain account and ensure its behavior aligns with the intended design.\n\nThis step is crucial because it provides a comprehensive and efficient way to test the interchain accounts in a controlled environment before deployment to a live network, minimizing potential risks and ensuring reliability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[ICS 26 - Routing Module](https://github.com/cosmos/ibc/tree/master/spec/core/ics-026-routing-module) defines a function [`handlePacketRecv`](https://github.com/cosmos/ibc/tree/master/spec/core/ics-026-routing-module#packet-relay).\nIn ICS 26, the routing module is defined as a layer above each application module\nwhich verifies and routes messages to the destination modules. It is possible to\nimplement it as a separate module, however, we already have functionality to route\nmessages upon the destination identifiers in the baseapp. This ADR suggests\nto utilize existing `baseapp.router` to route packets to application modules.\nGenerally, routing module callbacks have two separate steps in them,\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\nin order to increase developer ergonomics by reducing boilerplate\nverification code.\nFor atomic multi-message transaction, we want to keep the IBC related\nstate modification to be preserved even the application side state change\nreverts. One of the example might be IBC token sending message following with\nstake delegation which uses the tokens received by the previous packet message.\nIf the token receiving fails for any reason, we might not want to keep\nexecuting the transaction, but we also don't want to abort the transaction\nor the sequence and commitment will be reverted and the channel will be stuck.\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\n\n## Decision\n","`PortKeeper` will have the capability key that is able to access only the\nchannels bound to the port. Entities that hold a `PortKeeper` will be\nable to call the methods on it which are corresponding with the methods with\nthe same names on the `ChannelKeeper`, but only with the\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\neasily construct a capability-safe `PortKeeper`. This will be addressed in\nanother ADR and we will use insecure `ChannelKeeper` for now.\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\n`Result.Code == CodeTxBreak`.\n```go\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\nmsgs := tx.GetMsgs()\n// AnteHandler\nif app.anteHandler != nil {\nanteCtx, msCache := app.cacheTxContext(ctx)\nnewCtx, err := app.anteHandler(anteCtx, tx)\nif !newCtx.IsZero() {\nctx = newCtx.WithMultiStore(ms)\n}\nif err != nil {\n// error handling logic\nreturn res\n}\nmsCache.Write()\n}\n// Main Handler\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\nresult = app.runMsgs(runMsgCtx, msgs)\n// BEGIN modification made in this ADR\nif result.IsOK() || result.IsBreak() {\n// END\nmsCache.Write()\n}\nreturn result\n}\n```\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\n`AnteDecorator` will iterate over the messages included in the transaction, type\n`switch` to check whether the message contains an incoming IBC packet, and if so\nverify the Merkle proof.\n```go\ntype ProofVerificationDecorator struct {\nclientKeeper ClientKeeper\nchannelKeeper ChannelKeeper\n}\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\nfor _, msg := range tx.GetMsgs() {\nvar err error\nswitch msg := msg.(type) {\ncase client.MsgUpdateClient:\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\ncase channel.MsgPacket:\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\ncase channel.MsgAcknowledgement:\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\ncase channel.MsgTimeoutPacket:\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\ncase channel.MsgChannelOpenInit;\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\ndefault:\ncontinue\n}\nif err != nil {\nreturn ctx, err\n}\n}\nreturn next(ctx, tx, simulate)\n}\n```\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\nrespectively.\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\n`VerifyTimeout` will be extracted out into separated functions,\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\nwhich will be called by the application handlers after the execution.\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\nverified by the counter-party chain and increments the sequence to prevent\ndouble execution. `DeleteCommitment` will delete the commitment stored,\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\nof ordered channel.\n```go\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\n}\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\n}\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\nif channel.Ordering == types.ORDERED [\nchannel.State = types.CLOSED\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\n}\n}\n```\nEach application handler should call respective finalization methods on the `PortKeeper`\nin order to increase sequence (in case of packet) or remove the commitment\n(in case of acknowledgement and timeout).\nCalling those functions implies that the application logic has successfully executed.\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\nwhich will persist the state changes that has been already done but prevent any further\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\nincreased in the previous IBC packets(thus preventing double execution) without\nproceeding to the following messages.\nIn any case the application modules should never return state reverting result,\nwhich will make the channel unable to proceed.\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\nunder the routing module specification. Instead of define each channel handshake callback\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\n`CheckOpen` will find the correct `ChennelChecker` using the\n`PortID` and call it, which will return an error if it is unacceptable by the application.\nThe `ProofVerificationDecorator` will be inserted to the top level application.\nIt is not safe to make each module responsible to call proof verification\nlogic, whereas application can misbehave(in terms of IBC protocol) by\nmistake.\nThe `ProofVerificationDecorator` should come right after the default sybil attack\nresistant layer from the current `auth.NewAnteHandler`:\n```go\n// add IBC ProofVerificationDecorator to the Chain of\nfunc NewAnteHandler(\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\nreturn sdk.ChainAnteDecorators(\nNewSetUpContextDecorator(), // outermost AnteDecorator. SetUpContext must be called first\n...\nNewIncrementSequenceDecorator(ak),\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), // innermost AnteDecorator\n)\n}\n```\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\nExample application-side usage:\n```go\ntype AppModule struct {}\n// CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\nif channel.Ordering != UNORDERED {\nreturn ErrUncompatibleOrdering()\n}\nif channel.CounterpartyPort != ""bank"" {\nreturn ErrUncompatiblePort()\n}\nif channel.Version != """" {\nreturn ErrUncompatibleVersion()\n}\nreturn nil\n}\nfunc NewHandler(k Keeper) Handler {\nreturn func(ctx Context, msg Msg) Result {\nswitch msg := msg.(type) {\ncase MsgTransfer:\nreturn handleMsgTransfer(ctx, k, msg)\ncase ibc.MsgPacket:\nvar data PacketDataTransfer\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\nreturn err\n}\nreturn handlePacketDataTransfer(ctx, k, msg, data)\ncase ibc.MsgTimeoutPacket:\nvar data PacketDataTransfer\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\nreturn err\n}\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\n// interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\n// MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\ncase ibc.MsgChannelOpen:\nreturn handleMsgChannelOpen(ctx, k, msg)\n}\n}\n}\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\nif err != nil {\nreturn sdk.ResultFromError(err)\n}\nreturn sdk.Result{}\n}\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\nif err != nil {\n// TODO: Source chain sent invalid packet, shutdown channel\n}\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) // WriteAcknowledgement increases the sequence, preventing double spending\nreturn sdk.Result{}\n}\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\nif err != nil {\n// This chain sent invalid packet or cannot recover the funds\npanic(err)\n}\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\n// packet timeout should not fail\nreturn sdk.Result{}\n}\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\nreturn sdk.Result{}\n}\n```\n","**Decision:** Utilize existing `baseapp.router` to route packets to application modules and introduce a new `CodeType`, `CodeTxBreak`, to handle atomic multi-message transactions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe specification for IBC cross-chain fungible token transfers\n([ICS20](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer)), needs to\nbe aware of the origin of any token denomination in order to relay a `Packet` which contains the sender\nand recipient addresses in the\n[`FungibleTokenPacketData`](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer#data-structures).\nThe Packet relay sending works based in 2 cases (per\n[specification](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer#packet-relay) and [Colin Axnér](https://github.com/colin-axner)'s description):\n1. Sender chain is acting as the source zone. The coins are transferred\nto an escrow address (i.e locked) on the sender chain and then transferred\nto the receiving chain through IBC TAO logic. It is expected that the\nreceiving chain will mint vouchers to the receiving address.\n2. Sender chain is acting as the sink zone. The coins (vouchers) are burned\non the sender chain and then transferred to the receiving chain through IBC\nTAO logic. It is expected that the receiving chain, which had previously\nsent the original denomination, will unescrow the fungible token and send\nit to the receiving address.\nAnother way of thinking of source and sink zones is through the token's\ntimeline. Each send to any chain other than the one it was previously\nreceived from is a movement forwards in the token's timeline. This causes\ntrace to be added to the token's history and the destination port and\ndestination channel to be prefixed to the denomination. In these instances\nthe sender chain is acting as the source zone. When the token is sent back\nto the chain it previously received from, the prefix is removed. This is\na backwards movement in the token's timeline and the sender chain\nis acting as the sink zone.\n### Example\nAssume the following channel connections exist and that all channels use the port ID `transfer`:\n- chain `A` has channels with chain `B` and chain `C` with the IDs `channelToB` and `channelToC`, respectively\n- chain `B` has channels with chain `A` and chain `C` with the IDs `channelToA` and `channelToC`, respectively\n- chain `C` has channels with chain `A` and chain `B` with the IDs `channelToA` and `channelToB`, respectively\nThese steps of transfer between chains occur in the following order: `A -> B -> C -> A -> C`. In particular:\n1. `A -> B`: sender chain is source zone. `A` sends packet with `denom` (escrowed on `A`), `B` receives `denom` and mints and sends voucher `transfer/channelToA/denom` to recipient.\n2. `B -> C`: sender chain is source zone. `B` sends packet with `transfer/channelToA/denom` (escrowed on `B`), `C` receives `transfer/channelToA/denom` and mints and sends voucher `transfer/channelToB/transfer/channelToA/denom` to recipient.\n3. `C -> A`: sender chain is source zone. `C` sends packet with `transfer/channelToB/transfer/channelToA/denom` (escrowed on `C`), `A` receives `transfer/channelToB/transfer/channelToA/denom` and mints and sends voucher `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` to recipient.\n4. `A -> C`: sender chain is sink zone. `A` sends packet with `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` (burned on `A`), `C` receives `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom`, and unescrows and sends `transfer/channelToB/transfer/channelToA/denom` to recipient.\nThe token has a final denomination on chain `C` of `transfer/channelToB/transfer/channelToA/denom`, where `transfer/channelToB/transfer/channelToA` is the trace information.\nIn this context, upon a receive of a cross-chain fungible token transfer, if the sender chain is the source of the token, the protocol prefixes the denomination with the port and channel identifiers in the following format:\n```typescript\nprefix + denom = {destPortN}/{destChannelN}/.../{destPort0}/{destChannel0}/denom\n```\nExample: transferring `100 uatom` from port `HubPort` and channel `HubChannel` on the Hub to\nEthermint's port `EthermintPort` and channel `EthermintChannel` results in `100\nEthermintPort/EthermintChannel/uatom`, where `EthermintPort/EthermintChannel/uatom` is the new\ndenomination on the receiving chain.\nIn the case those tokens are transferred back to the Hub (i.e the **source** chain), the prefix is\ntrimmed and the token denomination updated to the original one.\n### Problem\nThe problem of adding additional information to the coin denomination is twofold:\n1. The ever increasing length if tokens are transferred to zones other than the source:\nIf a token is transferred `n` times via IBC to a sink chain, the token denom will contain `n` pairs\nof prefixes, as shown on the format example above. This poses a problem because, while port and\nchannel identifiers have a maximum length of 64 each, the SDK `Coin` type only accepts denoms up to\n64 characters. Thus, a single cross-chain token, which again, is composed by the port and channels\nidentifiers plus the base denomination, can exceed the length validation for the SDK `Coins`.\nThis can result in undesired behaviours such as tokens not being able to be transferred to multiple\nsink chains if the denomination exceeds the length or unexpected `panics` due to denomination\nvalidation failing on the receiving chain.\n2. The existence of special characters and uppercase letters on the denomination:\nIn the SDK every time a `Coin` is initialized through the constructor function `NewCoin`, a validation\nof a coin's denom is performed according to a\n[Regex](https://github.com/cosmos/cosmos-sdk/blob/a940214a4923a3bf9a9161cd14bd3072299cd0c9/types/coin.go#L583),\nwhere only lowercase alphanumeric characters are accepted. While this is desirable for native denominations\nto keep a clean UX, it presents a challenge for IBC as ports and channels might be randomly\ngenerated with special and uppercase characters as per the [ICS 024 - Host\nRequirements](https://github.com/cosmos/ibc/tree/master/spec/core/ics-024-host-requirements#paths-identifiers-separators)\nspecification.\n\n## Decision\n","The issues outlined above, are applicable only to SDK-based chains, and thus the proposed solution\nare do not require specification changes that would result in modification to other implementations\nof the ICS20 spec.\nInstead of adding the identifiers on the coin denomination directly, the proposed solution hashes\nthe denomination prefix in order to get a consistent length for all the cross-chain fungible tokens.\nThis will be used for internal storage only, and when transferred via IBC to a different chain, the\ndenomination specified on the packed data will be the full prefix path of the identifiers needed to\ntrace the token back to the originating chain, as specified on ICS20.\nThe new proposed format will be the following:\n```go\nibcDenom = ""ibc/"" + hash(trace path + ""/"" + base denom)\n```\nThe hash function will be a SHA256 hash of the fields of the `DenomTrace`:\n```protobuf\n// DenomTrace contains the base denomination for ICS20 fungible tokens and the source tracing\n// information\nmessage DenomTrace {\n// chain of port/channel identifiers used for tracing the source of the fungible token\nstring path = 1;\n// base denomination of the relayed fungible token\nstring base_denom = 2;\n}\n```\nThe `IBCDenom` function constructs the `Coin` denomination used when creating the ICS20 fungible token packet data:\n```go\n// Hash returns the hex bytes of the SHA256 hash of the DenomTrace fields using the following formula:\n//\n// hash = sha256(tracePath + ""/"" + baseDenom)\nfunc (dt DenomTrace) Hash() tmbytes.HexBytes {\nreturn tmhash.Sum(dt.Path + ""/"" + dt.BaseDenom)\n}\n// IBCDenom a coin denomination for an ICS20 fungible token in the format 'ibc/{hash(tracePath + baseDenom)}'.\n// If the trace is empty, it will return the base denomination.\nfunc (dt DenomTrace) IBCDenom() string {\nif dt.Path != """" {\nreturn fmt.Sprintf(""ibc/%s"", dt.Hash())\n}\nreturn dt.BaseDenom\n}\n```\n### `x/ibc-transfer` Changes\nIn order to retrieve the trace information from an IBC denomination, a lookup table needs to be\nadded to the `ibc-transfer` module. These values need to also be persisted between upgrades, meaning\nthat a new `[]DenomTrace` `GenesisState` field state needs to be added to the module:\n```go\n// GetDenomTrace retrieves the full identifiers trace and base denomination from the store.\nfunc (k Keeper) GetDenomTrace(ctx Context, denomTraceHash []byte) (DenomTrace, bool) {\nstore := ctx.KVStore(k.storeKey)\nbz := store.Get(types.KeyDenomTrace(traceHash))\nif bz == nil {\nreturn &DenomTrace, false\n}\nvar denomTrace DenomTrace\nk.cdc.MustUnmarshalBinaryBare(bz, &denomTrace)\nreturn denomTrace, true\n}\n// HasDenomTrace checks if a the key with the given trace hash exists on the store.\nfunc (k Keeper) HasDenomTrace(ctx Context, denomTraceHash []byte)  bool {\nstore := ctx.KVStore(k.storeKey)\nreturn store.Has(types.KeyTrace(denomTraceHash))\n}\n// SetDenomTrace sets a new {trace hash -> trace} pair to the store.\nfunc (k Keeper) SetDenomTrace(ctx Context, denomTrace DenomTrace) {\nstore := ctx.KVStore(k.storeKey)\nbz := k.cdc.MustMarshalBinaryBare(&denomTrace)\nstore.Set(types.KeyTrace(denomTrace.Hash()), bz)\n}\n```\nThe `MsgTransfer` will validate that the `Coin` denomination from the `Token` field contains a valid\nhash, if the trace info is provided, or that the base denominations matches:\n```go\nfunc (msg MsgTransfer) ValidateBasic() error {\n// ...\nreturn ValidateIBCDenom(msg.Token.Denom)\n}\n```\n```go\n// ValidateIBCDenom validates that the given denomination is either:\n//\n//  - A valid base denomination (eg: 'uatom')\n//  - A valid fungible token representation (i.e 'ibc/{hash}') per ADR 001 https://github.com/cosmos/ibc-go/blob/main/docs/architecture/adr-001-coin-source-tracing.md\nfunc ValidateIBCDenom(denom string) error {\ndenomSplit := strings.SplitN(denom, ""/"", 2)\nswitch {\ncase strings.TrimSpace(denom) == """",\nlen(denomSplit) == 1 && denomSplit[0] == ""ibc"",\nlen(denomSplit) == 2 && (denomSplit[0] != ""ibc"" || strings.TrimSpace(denomSplit[1]) == """"):\nreturn sdkerrors.Wrapf(ErrInvalidDenomForTransfer, ""denomination should be prefixed with the format 'ibc/{hash(trace + \""/\"" + %s)}'"", denom)\ncase denomSplit[0] == denom && strings.TrimSpace(denom) != """":\nreturn sdk.ValidateDenom(denom)\n}\nif _, err := ParseHexHash(denomSplit[1]); err != nil {\nreturn Wrapf(err, ""invalid denom trace hash %s"", denomSplit[1])\n}\nreturn nil\n}\n```\nThe denomination trace info only needs to be updated when token is received:\n- Receiver is **source** chain: The receiver created the token and must have the trace lookup already stored (if necessary *ie* native token case wouldn't need a lookup).\n- Receiver is **not source** chain: Store the received info. For example, during step 1, when chain `B` receives `transfer/channelToA/denom`.\n```go\n// SendTransfer\n// ...\nfullDenomPath := token.Denom\n// deconstruct the token denomination into the denomination trace info\n// to determine if the sender is the source chain\nif strings.HasPrefix(token.Denom, ""ibc/"") {\nfullDenomPath, err = k.DenomPathFromHash(ctx, token.Denom)\nif err != nil {\nreturn err\n}\n}\nif types.SenderChainIsSource(sourcePort, sourceChannel, fullDenomPath) {\n//...\n```\n```go\n// DenomPathFromHash returns the full denomination path prefix from an ibc denom with a hash\n// component.\nfunc (k Keeper) DenomPathFromHash(ctx sdk.Context, denom string) (string, error) {\nhexHash := denom[4:]\nhash, err := ParseHexHash(hexHash)\nif err != nil {\nreturn """", Wrap(ErrInvalidDenomForTransfer, err.Error())\n}\ndenomTrace, found := k.GetDenomTrace(ctx, hash)\nif !found {\nreturn """", Wrap(ErrTraceNotFound, hexHash)\n}\nfullDenomPath := denomTrace.GetFullDenomPath()\nreturn fullDenomPath, nil\n}\n```\n```go\n// OnRecvPacket\n// ...\n// This is the prefix that would have been prefixed to the denomination\n// on sender chain IF and only if the token originally came from the\n// receiving chain.\n//\n// NOTE: We use SourcePort and SourceChannel here, because the counterparty\n// chain would have prefixed with DestPort and DestChannel when originally\n// receiving this coin as seen in the ""sender chain is the source"" condition.\nif ReceiverChainIsSource(packet.GetSourcePort(), packet.GetSourceChannel(), data.Denom) {\n// sender chain is not the source, unescrow tokens\n// remove prefix added by sender chain\nvoucherPrefix := types.GetDenomPrefix(packet.GetSourcePort(), packet.GetSourceChannel())\nunprefixedDenom := data.Denom[len(voucherPrefix):]\ntoken := sdk.NewCoin(unprefixedDenom, sdk.NewIntFromUint64(data.Amount))\n// unescrow tokens\nescrowAddress := types.GetEscrowAddress(packet.GetDestPort(), packet.GetDestChannel())\nreturn k.bankKeeper.SendCoins(ctx, escrowAddress, receiver, sdk.NewCoins(token))\n}\n// sender chain is the source, mint vouchers\n// since SendPacket did not prefix the denomination, we must prefix denomination here\nsourcePrefix := types.GetDenomPrefix(packet.GetDestPort(), packet.GetDestChannel())\n// NOTE: sourcePrefix contains the trailing ""/""\nprefixedDenom := sourcePrefix + data.Denom\n// construct the denomination trace from the full raw denomination\ndenomTrace := types.ParseDenomTrace(prefixedDenom)\n// set the value to the lookup table if not stored already\ntraceHash := denomTrace.Hash()\nif !k.HasDenomTrace(ctx, traceHash) {\nk.SetDenomTrace(ctx, traceHash, denomTrace)\n}\nvoucherDenom := denomTrace.IBCDenom()\nvoucher := sdk.NewCoin(voucherDenom, sdk.NewIntFromUint64(data.Amount))\n// mint new tokens if the source of the transfer is the same chain\nif err := k.bankKeeper.MintCoins(\nctx, types.ModuleName, sdk.NewCoins(voucher),\n); err != nil {\nreturn err\n}\n// send to receiver\nreturn k.bankKeeper.SendCoinsFromModuleToAccount(\nctx, types.ModuleName, receiver, sdk.NewCoins(voucher),\n)\n```\n```go\nfunc NewDenomTraceFromRawDenom(denom string) DenomTrace{\ndenomSplit := strings.Split(denom, ""/"")\ntrace := """"\nif len(denomSplit) > 1 {\ntrace = strings.Join(denomSplit[:len(denomSplit)-1], ""/"")\n}\nreturn DenomTrace{\nBaseDenom: denomSplit[len(denomSplit)-1],\nTrace:     trace,\n}\n}\n```\nOne final remark is that the `FungibleTokenPacketData` will remain the same, i.e with the prefixed full denomination, since the receiving chain may not be an SDK-based chain.\n### Coin Changes\nThe coin denomination validation will need to be updated to reflect these changes. In particular, the denomination validation\nfunction will now:\n- Accept slash separators (`""/""`) and uppercase characters (due to the `HexBytes` format)\n- Bump the maximum character length to 128, as the hex representation used by Tendermint's\n`HexBytes` type contains 64 characters.\nAdditional validation logic, such as verifying the length of the hash, the  may be added to the bank module in the future if the [custom base denomination validation](https://github.com/cosmos/cosmos-sdk/pull/6755) is integrated into the SDK.\n### Positive\n- Clearer separation of the source tracing behaviour of the token (transfer prefix) from the original\n`Coin` denomination\n- Consistent validation of `Coin` fields (i.e no special characters, fixed max length)\n- Cleaner `Coin` and standard denominations for IBC\n- No additional fields to SDK `Coin`\n### Negative\n- Store each set of tracing denomination identifiers on the `ibc-transfer` module store\n- Clients will have to fetch the base denomination every time they receive a new relayed fungible token over IBC. This can be mitigated using a map/cache for already seen hashes on the client side. Other forms of mitigation, would be opening a websocket connection subscribe to incoming events.\n### Neutral\n- Slight difference with the ICS20 spec\n- Additional validation logic for IBC coins on the `ibc-transfer` module\n- Additional genesis fields\n- Slightly increases the gas usage on cross-chain transfers due to access to the store. This should\nbe inter-block cached if transfers are frequent.\n","The protocol prefixes the denomination with the port and channel identifiers in the following format:\n```typescript\nprefix + denom = {destPortN}/{destChannelN}/.../{destPort0}/{destChannel0}/denom\n```\nIn the case those tokens are transferred back to the source chain, the prefix is trimmed and the token denomination updated to the original one.\n\nIf a token is transferred n times via IBC to a sink chain, the token denom will contain n pairs of prefixes, as shown on the format example above. This poses a problem because, while port and channel identifiers have a maximum length of 64 each, the SDK Coin type only accepts denoms up to 64 characters. Thus, a single cross-chain token, which again, is composed by the port and channels identifiers plus the base denomination, can exceed the length validation for the SDK Coins.\n\nIn the SDK every time a Coin is initialized through the constructor function NewCoin, a validation of a coin's denom is performed according to a Regex, where only lowercase alphanumeric characters are accepted. While this is desirable for native denominations to keep a clean UX, it presents a challenge for IBC as ports and channels might be randomly generated with special and uppercase characters as per the ICS 024 - Host Requirements specification.\n\nTo address these problems, the protocol will use the following encoding for cross-chain token denominations:\n```typescript\nprefix + denom = {destPortN}/{destChannelN}/.../{destPort0}/{destChannel0}/{denomHash}\n```\nwhere the denomHash is the SHA256 hash of the original denomination. This encoding has the following advantages:\n- The length of the denomination is fixed regardless of the number of times the token is transferred.\n- The denomination is guaranteed to be lowercase and alphanumeric."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe IBC module was originally developed in the Cosmos SDK and released during the Stargate release series (v0.42).\nIt was subsequently migrated to its own repository, ibc-go.\nThe first official release on ibc-go was v1.0.0.\nv1.0.0 was decided to be used instead of v0.1.0 primarily for the following reasons:\n- Maintaining compatibility with the IBC specification v1 requires stronger support/guarantees.\n- Using the major, minor, and patch numbers allows for easier communication of what breaking changes are included in a release.\n- The IBC module is being used by numerous high value projects which require stability.\n### Problems\n#### Go module version must be incremented\nWhen a Go module is released under v1.0.0, all following releases must follow Go semantic versioning.\nThus when the go API is broken, the Go module major version **must** be incremented.\nFor example, changing the go package version from `v2` to `v3` bumps the import from `github.com/cosmos/ibc-go/v2` to `github.com/cosmos/ibc-go/v3`.\nIf the Go module version is not incremented then attempting to go get a module @v3.0.0 without the suffix results in:\n`invalid version: module contains a go.mod file, so major version must be compatible: should be v0 or v1, not v3`\nVersion validation was added in Go 1.13. This means that in order to release a v3.0.0 git tag without a /v3 suffix on the module definition, the tag must explicitly **not** contain a go.mod file.\nNot including a go.mod in our release is not a viable option.\n#### Attempting to import multiple go module versions for ibc-go\nAttempting to import two versions of ibc-go, such as `github.com/cosmos/ibc-go/v2` and `github.com/cosmos/ibc-go/v3`, will result in multiple issues.\nThe Cosmos SDK does global registration of error and governance proposal types.\nThe errors and proposals used in ibc-go would need to now register their naming based on the go module version.\nThe more concerning problem is that protobuf definitions will also reach a namespace collision.\nibc-go and the Cosmos SDK in general rely heavily on using extended functions for go structs generated from protobuf definitions.\nThis requires the go structs to be defined in the same package as the extended functions.\nThus, bumping the import versioning causes the protobuf definitions to be generated in two places (in v2 and v3).\nWhen registering these types at compile time, the go compiler will panic.\nThe generated types need to be registered against the proto codec, but there exist two definitions for the same name.\nThe protobuf conflict policy can be overridden via the environment variable `GOLANG_PROTOBUF_REGISTRATION_CONFLICT`, but it is possible this could lead to various runtime errors or unexpected behaviour (see [here](https://github.com/protocolbuffers/protobuf-go/blob/master/reflect/protoregistry/registry.go#L46)).\nMore information [here](https://developers.google.com/protocol-buffers/docs/reference/go/faq#namespace-conflict) on namespace conflicts for protobuf versioning.\n### Potential solutions\n#### Changing the protobuf definition version\nThe protobuf definitions all have a type URL containing the protobuf version for this type.\nChanging the protobuf version would solve the namespace collision which arise from importing multiple versions of ibc-go, but it leads to new issues.\nIn the Cosmos SDK, `Any`s are unpacked and decoded using the type URL.\nChanging the type URL thus is creating a distinctly different type.\nThe same registration on the proto codec cannot be used to unpack the new type.\nFor example:\nAll Cosmos SDK messages are packed into `Any`s. If we incremented the protobuf version for our IBC messages, clients which submitted the v1 of our Cosmos SDK messages would now be rejected since the old type is not registered on the codec.\nThe clients must know to submit the v2 of these messages. This pushes the burden of versioning onto relayers and wallets.\nA more serious problem is that the `ClientState` and `ConsensusState` are packed as `Any`s. Changing the protobuf versioning of these types would break compatibility with IBC specification v1.\n#### Moving protobuf definitions to their own go module\nThe protobuf definitions could be moved to their own go module which uses 0.x versioning and will never go to 1.0.\nThis prevents the Go module version from being incremented with breaking changes.\nIt also requires all extended functions to live in the same Go module, disrupting the existing code structure.\nThe version that implements this change will still be incompatible with previous versions, but future versions could be imported together without namespace collisions.\nFor example, let's say this solution is implemented in v3. Then\n`github.com/cosmos/ibc-go/v2` cannot be imported with any other ibc-go version\n`github.com/cosmos/ibc-go/v3` cannot be imported with any previous ibc-go versions\n`github.com/cosmos/ibc-go/v4` may be imported with ibc-go versions v3+\n`github.com/cosmos/ibc-go/v5` may be imported with ibc-go versions v3+\n\n## Decision\n","Supporting importing multiple versions of ibc-go requires a non-trivial amount of complexity.\nIt is unclear when a user of the ibc-go code would need multiple versions of ibc-go.\nUntil there is an overwhelming reason to support importing multiple versions of ibc-go:\n**Major releases cannot be imported simultaneously**.\nReleases should focus on keeping backwards compatibility for go code clients, within reason.\nOld functionality should be marked as deprecated and there should exist upgrade paths between major versions.\nDeprecated functionality may be removed when no clients rely on that functionality.\nHow this is determined is to be decided.\n**Error and proposal type registration will not be changed between go module version increments**.\nThis explicitly stops external clients from trying to import two major versions (potentially risking a bug due to the instability of proto name collisions override).\n",Move the protobuf definitions to their own go module which uses 0.x versioning and will never go to 1.0.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUsers must authenticate before using the Publish Data app. An authenticated user can belong to many organisations, of which one of these is their primary organisation e.g. for creating datasets.\nWhen an unauthenticated user goes to any page of the app, they are redirected to the home page, which asks them to 'Sign in' or 'Create an account'. Once a user is logged in, they will be redirected to their requested page, or the tasks page.\nBefore this work started, we were planning to roll our own internal user management system, for which some initial work was completed. From this initial work we concluded that making our own custom solution would involve a lot of effort.\nGOV.UK publishing apps use a single signon service called [GOV.UK Signon](https://github.com/alphagov/signon), which provides a standardised mechanism for user authentication and permission management, including a [rails adapter](https://github.com/alphagov/gds-sso). We think using GOV.UK Signon will require less implementation effort, as well as reduce support issues by re-using an established part of the GOV.UK platform.\n\n## Decision\n",We will replace our custom user management system with GOV.UK Signon and re-address permission and organisation assignment in later work. Authentication for each environment will be controlled by the instance of GOV.UK Signon running in the corresponding environment.\n,Use GOV.UK Signon for user authentication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLicences stored in the legacy system are messy, allowing selection from a predefined list, entry of a custom licence, or a combination of both. This leads to complexity in determining what should be shown, and how to handle the presentation of large custom licences.\nCurrently this is handled by two fields in 'Publish'.  The licence field holds the legacy database's `license_id` field. In cases where the `license_id` field has a value of `__other__` then the `license_id` is stored in the `licence_other` field of our Dataset model. The legacy system's custom licence field is ignored entirely.\nPublishing users expect that any custom licence information they provide will be stored, and shown in the user interface of [Find data](https://github.com/alphagov/datagovuk_find).\n\n## Decision\n","We will simplify the storing of licence information to:\n* Create new fields `licence_code`, `licence_title`, `licence_url`, and `licence_custom`\n* Publish will continue (temporarily) fill the old fields so we can migrate the frontend\n* We should remove the `licence` and `licence_other` fields once both publish and find apps are using the new fields.\n","Use the legacy database `licence_id` field to store the licence, and add an optional free text `custom_licence` field for users to input any custom licence text they wish. Only show licence text in the [Find data](https://github.com/alphagov/datagovuk_find) user interface when it has been provided in the `custom_licence` field."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to aid the performance team in selecting which views and/or functionalities lead to an increase in user completion rate for verification with IDPs we decided to implement AB testing.\nEach AB test is a result of user testing lab feedback and ideas. This is not to replace lab user tests.\nTests will run  until the number of users in both groups reach significance. After tha all code should be torn down.\n\n## Decision\n,"Users involved in AB testing will be put in groups according to the alternative name in the AB test cookie. These are configured in YAML files in Verify Federation Config with the experiment name, alternative names and percentage split.\nUsers in each group will have a different journey depending on the alternative in that AB test. Alternative A is always the default current journey.\nOne would not usually have more than two alternatives. If more than one AB test is running concurrently the performance team must ensure the tests to not compound each other.\nWe looked into the gem [splitrb](https://github.com/splitrb/split) but that required a Reddis database to be install on all environments. It was decided this was too large an infrastructure effort to make it worthwhile using the gem.\nThe AB test cookie with the alternative name is dropped in the start page but reported to analytics of the page that displays the difference.\n","**Decision:** Implement AB testing to determine the impact of specific views and functionalities on user completion rate for verification with Identity Providers (IDPs).\n\n**Justification:**\n\n* AB testing provides a data-driven approach to evaluate the effectiveness of different design elements.\n* It allows the performance team to isolate the variables that contribute to user completion rate, leading to informed decisions about view optimization.\n* Regular user testing lab feedback and ideas provide valuable insights for designing testable hypotheses.\n* Setting a significance threshold for user groups ensures that results are statistically meaningful.\n* Tearing down test code after reaching significance ensures that the codebase remains clean and bloat-free."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPrior AB testing involved changing existing model/view/controller files using if/else blocks to distinguish between A or\nB test behaviour. This also involved changing the various tests to accomodate the new B route. This approach made it\ndifficult for a future team to identify and remove the B code. This approach was developed in response to a need to\nspeed up the process of setting up and tearing AB tests in anticipation that a lot of them would be run.\n\n## Decision\n,"Rather than trying to modify files to accommodate B changes, this approach applies changes to cloned copies of files\nthat relate to the B behaviour.  A and B routes are defined through a new piece of code in routes.rb, which takes\nadvantage of Rail's constraints.  Cloning any JavaScript files is helped by code(ab_test_selector.js) used in\napp/javascripts/application.js.erb\nSimilarly, test files are also cloned and specific B tests are added to the copy.\n","To create a configurable, isolated AB testing framework within the company."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\nwith a parameter `journey_hint=submission_confirmation` the `/confirm-your-identity` page was rendered with the\nIDP from the cookie. The cookie was a simple JSON object:\n```\n{\nentity_id: ""https://idp-entity-id.com""\n}\n```\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\nThe expiry date was also extended to 18 months. For example:\n```\n{\nentity_id: ""https://idp-entity-id.com"",\nATTEMPT: ""https://idp-entity-id.com"",\nSUCCESS: ""https://idp-entity-id.com"",\nFAILED: ""https://idp-entity-id-1.com"",\n...\n}\n```\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\n\n## Decision\n","While implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\n```\n{\nATTEMPT: ""https://idp-entity-id.com"",\nSUCCESS: ""https://idp-entity-id.com"",\nSTATE:  {\nIDP: ""https://idp-entity-id.com"",\nRP: ""https://rp-entity-id.com"",\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\n}\n}\n```\n","**Decision:** Deprecate the `verify-front-journey-hint` cookie and migrate its functionality to a new mechanism.\n\n**Rationale:**\n\n* The `verify-front-journey-hint` cookie has become overloaded with multiple purposes, making it difficult to manage and maintain.\n* The extended expiry date of 18 months introduces unnecessary security risks, as it may store sensitive information for a prolonged period.\n* A new mechanism can provide more granular control over the storage and retrieval of journey hints, allowing for improved security and flexibility.\n\n**Consequences:**\n\n* The existing functionality of the `verify-front-journey-hint` cookie will need to be migrated to a new mechanism.\n* User experience may be affected if the sign-in hint feature is unavailable during the transition.\n* Additional resources will be required to implement and maintain the new mechanism."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","Record all architectural decisions in an Architectural Decision Record (ADR). ADRs should include the following information:\n\n* **Title:** A brief, descriptive title for the decision.\n* **Date:** The date the decision was made.\n* **Authors:** The people who made the decision.\n* **Status:** The current status of the decision (e.g., proposed, approved, implemented).\n* **Context:** A description of the problem or opportunity that led to the decision.\n* **Decision:** The decision that was made.\n* **Rationale:** The reasons for making the decision.\n* **Consequences:** The expected consequences of the decision.\n* **Alternatives:** The alternative decisions that were considered and rejected."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nAlthough we endeavour to make API changes backwards compatible, and such keep all clients on the same (shared) version of the API contract, there will be instances where this is not possible and a breaking change is required. Examples include: deprecating a route; correcting a typo; changing a data-type; re-organising the response schema to allow for metadata alongside the data requested.\nWhen a breaking change is required we need a way to apply the change without breaking existing clients.\n# Options\n- Support multiple versions simultaneously from the same codebase, utilizing some data in the request to determine which version should process the request. Any piece of request data could be used, but we should use a common method for support:\n- Specify a header in the request (e.g. `Accept: application/vnd.example.v2+json`)\n- Include a version number in the URL (e.g. `/v2/`)\n- Support multiple versions simultaneously by running multiple versions of the code in production. Typically this approach would involve an additional component (an API Gateway) to route traffic to the correct back-end service.\nThere are further considerations depending on the option chosen.\n# Decision\n- To support a maximum of two simultaneous major versions from within the same codebase, utilizing the version number contained within the request URL.\nWe will proactively look to move clients to the most recent version and to make the previous version obsolete in order to simplify the codebase.\n# Rationale\nGiven we have a small number of consumers at the moment, with the codebases within our control and within SDD, we can opt to keep only one previous major version at maximum.\nThere is not much between the URL vs. header versioning other than:\n- it is more transparent to the developer what is happening when we version using the URL\n- some integrations do not support setting custom headers, so we would be potentially limiting the future ability for others to use our API\nThe option to version using multiple simultaneous versions was not chosen as it is more complex in most areas, with the exception of the codebase within a branch:\n- added infrastructural complexity and cost - additional components (API Gateway and previous version instance of API) add cost and complexity.\n- added branching strategy complexity - generally the ability to apply a 'hotfix' to old versions is desired, meaning a more complex branching strategy is required. Hotfixes would have to be applied to branches for each supported version.\n- a hidden constraint exists - the need for database compatibility between the two versions, which is difficult to check for.\n\n## Decision\n","- To support a maximum of two simultaneous major versions from within the same codebase, utilizing the version number contained within the request URL.\nWe will proactively look to move clients to the most recent version and to make the previous version obsolete in order to simplify the codebase.\n# Rationale\nGiven we have a small number of consumers at the moment, with the codebases within our control and within SDD, we can opt to keep only one previous major version at maximum.\nThere is not much between the URL vs. header versioning other than:\n- it is more transparent to the developer what is happening when we version using the URL\n- some integrations do not support setting custom headers, so we would be potentially limiting the future ability for others to use our API\nThe option to version using multiple simultaneous versions was not chosen as it is more complex in most areas, with the exception of the codebase within a branch:\n- added infrastructural complexity and cost - additional components (API Gateway and previous version instance of API) add cost and complexity.\n- added branching strategy complexity - generally the ability to apply a 'hotfix' to old versions is desired, meaning a more complex branching strategy is required. Hotfixes would have to be applied to branches for each supported version.\n- a hidden constraint exists - the need for database compatibility between the two versions, which is difficult to check for.\n",Implement API versioning using the version number contained within the request URL. Support a maximum of two simultaneous major versions from within the same codebase. Proactively move clients to the most recent version and make the previous version obsolete in order to simplify the codebase.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nThe Trust, Record and Management System (TRAMS) is a core system that manages the data and business logic that is central to managing the Academies and Free Schools category of services for the Department for Education.\nAt present the access to the TRAMS central data store is controlled via a set of Dynamics hosted OData generated endpoints used to enable integration of external services or applications with schemas managed by Dynamics 365 (see [Current contextual architecture view PNG](diagrams/adr_a001_as-is_contextual_trams.png). Dynamics 365 is a proprietary licensed system from Microsoft that is modular in design (see [supporting info](#supporting-info))\nOData as a standard is optimised for focusing on business logic when building RESTful APIs over focusing on adhering to REST. There is room within OData to deviate from REST principles if there is a compelling business reason to do so. This deviation from REST is prevalent within our current OData API and its tight coupling to Dynamics requires business knowledge for external consumers to obtain data from TRAMS.\nThe endpoints are tied closely to the underlying Dynamics data schema. This schema is optimised for the use of Dynamics over optimised for external consumers. For example its data model is non-relational and requires knowledge of Dynamics in order to traverse.\nWe need a solution that supports our objectives for creating a sustainable development capability within SDD, aligns to our desired technical strategy where we would like to decouple from Dynamics where possible by building loosely coupled and extensible services within TRAMS that are easier to develop against.\n# Options\n1. Do nothing and developers continue to use OData endpoints generated by Dynamics.\n2. Allow developers to integrate directly with the relational data store.\n3. Develop a bespoke interface for developers to integrate with as an abstraction from the relational data store.\n4. Utilise a new Dynamics integration option such as a Custom Service (see [supporting info](#supporting-info)) as part of Dynamics in order to replace OData interface.\n# Decision\nOption 3\n# Rationale\n- Allows us to utilise commonly known open standards when designing our API that is not coupled tightly to business logic within Dynamics.\n- Provides us with greater flexibility to implement the access controls we need for external consumers such as rate limiting.\n- Allows us to provide access to data in a standardised format and schema that is abstracted from the Dynamics schemas (see [to-be contextual architecture view PNG](diagrams/adr_a001_to-be_contextual_trams.png))\n- We leverage the O in SOLID by being open to extension, closed for modification supporting the technical strategy. Currently data is open for modification and is not governed appropriately within Dynamics which is then exposed to consumers directly via the OData endpoints.\n- Allows us to ensure technology selection is maintainable and supportable by the department's digital teams and targeted capability areas. We are able to leverage our existing developer team skills to the maximum.\n- Avoids vendor lock-in by not adding or increasing the dependency on Dynamics 365.\n- Long term departmental goal is to gain the capability to run a full live-service support team - this decision enables this goal to be achieved by a shorter distance. By us deciding to build it we will have inherent low level knowledge of how to run it and support it without being reliant on additional support contracts.\n- Provides greater architectural options in the future by applying a strangler and appropriately abstracted interface pattern giving us greater flexibility to evolve this offering.\n- This decision has the lowest impact on all other existing and planned development and business initiatives within the SDD TRAMS domain.\n- Adding a bespoke interface that interacts directly with the central data store, rather than via Dynamics, provides greater control on how data flows and propagates to all other data stores such as Dynamics or KIM. This gives us greater control over how we manage/update our data within TRAMS.\n- Continuing to use the OData interface is considered to be inefficient (XML) requiring multiple calls for data traversal and retrieval. We will not be able to implement access controls for external consumers such as rate limiting.\n- Makes it easy for developers to reuse their developed integration patterns against our bespoke interface as we can establish a standard pattern for all APIs in any other areas of SDD.\n- It is not considered to be best practice to provide external developers direct access to our data store for performance, governance and security reasons to name a few.\n# Dependencies\n- High investment decision requiring time, skills and personnel to achieve this decision.\n- Incremental migration needs to occur for teams that are already utilising the underlying data that they access via the current OData interface.\n- Introduces a new dependency on non TRAMS SDD development teams who have committed to using the underlying data but need to develop a new integration with this to-be developed interface.\n- Wide range of department, team and user needs on the data which requires prioritisation on API development approach. This requires a series of prioritised vertical slices of data interface use cases to drive the development approach.\n# Impact\n- We will need to agree the appropriate security approach for securing access to this new interface.\n- Data in transit will need to be secured for external consumption.\n- To mitigate we will develop the API contract ahead of implementation to allow other teams to develop to the contract in parallel.\n- Implement controls to maintain data consistency in the source data store as well as any downstream stores that need updating (such as KIM).\n- An additional ADR is required in order to decide on what technology (inc. languages and frameworks) to develop a bespoke TRAMS data interface.\n- An additional ADR is required in order to decide what platform or infrastructure will be used to host and manage the TRAMS data interface.\n# Contributors\n- Steven Leighton\n- Dan Burnley\n- Rob Marshall\n- Miranda Hawkes\n- Nathan Ouriach\n- Andrew Emley\n- Chris Oram\n# Supporting Info\nOData standard - https://www.odata.org/documentation/ \\nREST overview -- https://en.wikipedia.org/wiki/Representational_state_transfer \\nDynamics implementation of OData Protocol - https://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/data-entities/odata \\nDynamics 365 overview - https://dynamics.microsoft.com/en-gb/what-is-dynamics365/ \\nDynamics integration options overview - https://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/data-entities/integration-overview \\n[As-is contextual architecture view](diagrams/adr_a001_as-is_contextual_trams.png) \\n[To-be contextual architecture view](diagrams/adr_a001_to-be_contextual_trams.png)\n\n## Decision\n","Option 3\n# Rationale\n- Allows us to utilise commonly known open standards when designing our API that is not coupled tightly to business logic within Dynamics.\n- Provides us with greater flexibility to implement the access controls we need for external consumers such as rate limiting.\n- Allows us to provide access to data in a standardised format and schema that is abstracted from the Dynamics schemas (see [to-be contextual architecture view PNG](diagrams/adr_a001_to-be_contextual_trams.png))\n- We leverage the O in SOLID by being open to extension, closed for modification supporting the technical strategy. Currently data is open for modification and is not governed appropriately within Dynamics which is then exposed to consumers directly via the OData endpoints.\n- Allows us to ensure technology selection is maintainable and supportable by the department's digital teams and targeted capability areas. We are able to leverage our existing developer team skills to the maximum.\n- Avoids vendor lock-in by not adding or increasing the dependency on Dynamics 365.\n- Long term departmental goal is to gain the capability to run a full live-service support team - this decision enables this goal to be achieved by a shorter distance. By us deciding to build it we will have inherent low level knowledge of how to run it and support it without being reliant on additional support contracts.\n- Provides greater architectural options in the future by applying a strangler and appropriately abstracted interface pattern giving us greater flexibility to evolve this offering.\n- This decision has the lowest impact on all other existing and planned development and business initiatives within the SDD TRAMS domain.\n- Adding a bespoke interface that interacts directly with the central data store, rather than via Dynamics, provides greater control on how data flows and propagates to all other data stores such as Dynamics or KIM. This gives us greater control over how we manage/update our data within TRAMS.\n- Continuing to use the OData interface is considered to be inefficient (XML) requiring multiple calls for data traversal and retrieval. We will not be able to implement access controls for external consumers such as rate limiting.\n- Makes it easy for developers to reuse their developed integration patterns against our bespoke interface as we can establish a standard pattern for all APIs in any other areas of SDD.\n- It is not considered to be best practice to provide external developers direct access to our data store for performance, governance and security reasons to name a few.\n# Dependencies\n- High investment decision requiring time, skills and personnel to achieve this decision.\n- Incremental migration needs to occur for teams that are already utilising the underlying data that they access via the current OData interface.\n- Introduces a new dependency on non TRAMS SDD development teams who have committed to using the underlying data but need to develop a new integration with this to-be developed interface.\n- Wide range of department, team and user needs on the data which requires prioritisation on API development approach. This requires a series of prioritised vertical slices of data interface use cases to drive the development approach.\n# Impact\n- We will need to agree the appropriate security approach for securing access to this new interface.\n- Data in transit will need to be secured for external consumption.\n- To mitigate we will develop the API contract ahead of implementation to allow other teams to develop to the contract in parallel.\n- Implement controls to maintain data consistency in the source data store as well as any downstream stores that need updating (such as KIM).\n- An additional ADR is required in order to decide on what technology (inc. languages and frameworks) to develop a bespoke TRAMS data interface.\n- An additional ADR is required in order to decide what platform or infrastructure will be used to host and manage the TRAMS data interface.\n# Contributors\n- Steven Leighton\n- Dan Burnley\n- Rob Marshall\n- Miranda Hawkes\n- Nathan Ouriach\n- Andrew Emley\n- Chris Oram\n# Supporting Info\nOData standard - https://www.odata.org/documentation/ \\nREST overview -- https://en.wikipedia.org/wiki/Representational_state_transfer \\nDynamics implementation of OData Protocol - https://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/data-entities/odata \\nDynamics 365 overview - https://dynamics.microsoft.com/en-gb/what-is-dynamics365/ \\nDynamics integration options overview - https://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/data-entities/integration-overview \\n[As-is contextual architecture view](diagrams/adr_a001_as-is_contextual_trams.png) \\n[To-be contextual architecture view](diagrams/adr_a001_to-be_contextual_trams.png)\n",Option 3: Develop a bespoke interface for developers to integrate with as an abstraction from the relational data store.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nWe're creating a replacement for the Dynamics 365 interface to our data, which does not include sensitive data but does include personally identifiable information.\nThe Dynamics API requires authentication with a set of credentials configured within the relevant Dynamics environment.\nThe API _must_ not be open to public use (must not provide data without authentication)\nThe API is hosted on our Azure infrastructure, but is assumed to be publicly discoverable\n# Options\n- Use a service provide a means to authenticate the request such as Azure API Management & Active Directory\n- Manually provision and check user identities\n- Block all traffic to the API and use an IP whitelist\nThere are further considerations depending on the option chosen.\n# Decision\n- Manually provision a way for users to authenticate requests using the simplest means possible that provides necessary security.\n# Rationale\nWe've chosen to create a simple API key system where each request to the API is required to provide a valid API key or the request is rejected.\nWe will manually provision and check these keys\nWe will load our provisioned keys from config on start up as a lookup - [User, key]\nEach request will be checked for the presence of of a header key/value pair with the key `X-API-Key`, and the provided value must be a valid configured key or the request will be rejected with the FORBIDDEN status code.\nWe believe using an API gateway or having consumers authenticate via Azure Active Directory/API Management and present a token are valid options but require more development investment, where a simple set of provisioned keys can be secure, performant and _importantly_ controlled by the team that owns the API.\nBecause we have designed our API key check to check against an in-memory lookup, the following apply:\n- Key checks can be made performantly for every request\n- We can provision or delete keys in seconds, via configuration\n- Changes in configuration (at this time) require redeployment to take effect, but we can offer seamless deployment\n- Configuration values are _never_ committed to source code, so we can continue to work in the open\n- Testing (UI and integration) is easily done in an automated way using test configurations, and we can provision unique keys for test environments\nThis approach was also implemented very quickly and before any other API functionality was implemented. Its low resistance as an approach allowed us to deploy working test environments rapidly with confidence that _no_ deployment would ever expose data unsecured.\nIn future, we will consider migrating to a provided service or integrate with the existing Active Directory system in order to allow more fine-grain control of permissions, and to allow support of larger numbers of unique consumers. This approach will work for our expected scale, but could become difficult to maintain at larger than expected numbers. To do so now, compared to the very low overhead of implementing the actual header key check would be a significant investment for a small gain.\n# Dependencies\nConfirmation this does not affect the authority to operate.\nTeams need to know how to make configuration changes to provision or delete keys.\nUsers are required to request access to the API environments and await key provisioning, keys need to be generated, shared and configured in a secure way.\n# Impact\nUsers will have a lead up time to integration and cannot self-service generate keys, conversely key/account provisioning is not reliant on Active Directory Administration and the lead up time is expected to be less than one day.\nThis approach is as secure as the keys are treated - we will need to establish an appropriate process and communication channel (via Live Service Support) to report accidental compromise of keys. Keys can be revoked and replaced in a matter of minutes.\n# Contributors\n- Christopher Gunn\n- Steve Leighton\n- Lewis Dale\n- Daniel Burnley\n- Vivian Roberts\n# Supporting Info (optional)\nN/A\n\n## Decision\n","- Manually provision a way for users to authenticate requests using the simplest means possible that provides necessary security.\n# Rationale\nWe've chosen to create a simple API key system where each request to the API is required to provide a valid API key or the request is rejected.\nWe will manually provision and check these keys\nWe will load our provisioned keys from config on start up as a lookup - [User, key]\nEach request will be checked for the presence of of a header key/value pair with the key `X-API-Key`, and the provided value must be a valid configured key or the request will be rejected with the FORBIDDEN status code.\nWe believe using an API gateway or having consumers authenticate via Azure Active Directory/API Management and present a token are valid options but require more development investment, where a simple set of provisioned keys can be secure, performant and _importantly_ controlled by the team that owns the API.\nBecause we have designed our API key check to check against an in-memory lookup, the following apply:\n- Key checks can be made performantly for every request\n- We can provision or delete keys in seconds, via configuration\n- Changes in configuration (at this time) require redeployment to take effect, but we can offer seamless deployment\n- Configuration values are _never_ committed to source code, so we can continue to work in the open\n- Testing (UI and integration) is easily done in an automated way using test configurations, and we can provision unique keys for test environments\nThis approach was also implemented very quickly and before any other API functionality was implemented. Its low resistance as an approach allowed us to deploy working test environments rapidly with confidence that _no_ deployment would ever expose data unsecured.\nIn future, we will consider migrating to a provided service or integrate with the existing Active Directory system in order to allow more fine-grain control of permissions, and to allow support of larger numbers of unique consumers. This approach will work for our expected scale, but could become difficult to maintain at larger than expected numbers. To do so now, compared to the very low overhead of implementing the actual header key check would be a significant investment for a small gain.\n# Dependencies\nConfirmation this does not affect the authority to operate.\nTeams need to know how to make configuration changes to provision or delete keys.\nUsers are required to request access to the API environments and await key provisioning, keys need to be generated, shared and configured in a secure way.\n# Impact\nUsers will have a lead up time to integration and cannot self-service generate keys, conversely key/account provisioning is not reliant on Active Directory Administration and the lead up time is expected to be less than one day.\nThis approach is as secure as the keys are treated - we will need to establish an appropriate process and communication channel (via Live Service Support) to report accidental compromise of keys. Keys can be revoked and replaced in a matter of minutes.\n# Contributors\n- Christopher Gunn\n- Steve Leighton\n- Lewis Dale\n- Daniel Burnley\n- Vivian Roberts\n# Supporting Info (optional)\nN/A\n",Manually provision a way for users to authenticate requests using the simplest means possible that provides necessary security.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe plan around UI was always to create a separate NPM library that developers could pull in to implement and build out components. The intended result of this was to create a solution that made it easy to implement and maintain the desired design. Having a separate NPM library that developers can pull in means that we have greater control over library versioning and a single source of truth.\nWhile the first set of components were implemented via Material UI React, Martin and Soheima consulted each other and decided to go for React Semantic UI, with tailwind.css to fill in the gaps. This was due to the fact that Martin had a good experience with Semantic in the past, and because its documentation and visual examples are easy to follow. Overall, Martin has said that he prefers a larger community with a more robust system out of the box for its easy and fat deployment.\nHowever, after attempting to work with Semantic for some time, Soheima found that it uses a standard palette with extremely limited customization - only two other colors for primary/secondary are allowed. We are therefore revisiting this question of UI libraries to see if we can identify a better alternative.\n\n## Decision\n","1. Branding Compliance: does the library allow for us to apply our branding guidelines to the product (colors, font, etc.)?\n2. Fit for Use Case: does it have components that support the functionality we need? Does it work across both mobile and desktop?\n3. Developer Experience: How easy is it for front-end developers to render the UI as provided in the mockups in a clean and consistent way?\n4. Maintainability: As the team grows and changes, will it be easy to maintain and evolve the UI?\n5. Speed of Development: How fast is it to get the design we need up and running?\nRebassJS. Although in an ideal world, we would have liked to start with a libaray that has many pre-built components out of the box, in reality, libraries that had this sort of set up were either too restrictive what their components could do (Semantic - weak theme customizations; Material, Ant Design - UI components do not fit and cannot be modified to behave the way we want them to behave) or too incomplete in their pre-built components (Grommet), meaning that a separate library would have to be brought in anyway to create custom components for Boxtribute. Since there did not seem to be an easy or elegant way to integrate new custom components with an existing library, we decided to go directly with the library that was created with custom UI building in mind.\n",The team should explore alternative UI libraries that offer greater customization options while maintaining the benefits of a centralized library approach.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp to now, database changes have been managed through [Phinx](https://phinx.org/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python / React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\n\n## Decision\n","1. Learning Curve\n2. Community Support\n3. Power and Reliability considering it will be used in a production environment\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the ""easier"" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\n","**Decision:** Move to a Python-based ORM to manage database interactions.\n\n**Rationale:**\n\n* **Simplified codebase:** Eliminating strings for SQL queries reduces code complexity and improves maintainability.\n* **Improved type safety:** ORMs enforce data types, reducing errors and improving code reliability.\n* **Consistency with Python ecosystem:** Python-based ORMs align with the current codebase, making integration and debugging easier.\n* **Reduced database workload:** ORMs can optimize database queries, reducing load and improving performance.\n* **Collaboration improvements:** Using an ORM common to Python developers fosters collaboration and knowledge sharing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context or Problem Statement\nOur handling of user data is far from bullet-proof. We have stumbled upon cases where SQL injection is possible and the current password encryption is only md5. Additionally, we are building a new mobile app in React and Flask and need to implement a way to handle authentication there.\n## Decision Drivers\n1. Security / Risk for us\n2. Ease of Use\n3. Cost\n\n## Decision\n","1. Security / Risk for us\n2. Ease of Use\n3. Cost\nWe are going for Auth0 since\n- we have prior experience with Auth0 in the team,\n- a first test in an afternoon coding session were satisfying and\n- Auth0 offers a free plan for Open-source projects.\nWe are not building our own authentication solution to reduce the security risks coming with handling g passwords.\n","Implement Single Sign-On (SSO) with a trusted third-party provider such as Google, Azure AD, or Okta.\n\n**Justification**\n\nThis decision addresses all three decision drivers:\n\n* **Security / Risk:** SSO provides a secure way to manage user authentication and authorization, reducing the risk of SQL injection and weak password encryption.\n* **Ease of Use:** SSO provides a seamless user experience by allowing users to sign in with their existing accounts from trusted third-party providers.\n* **Cost:** SSO can be implemented at a reasonable cost, especially compared to developing and maintaining a custom authentication system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBoxwise is in the middle of a planned migration from the old PHP-based Dropapp, to a new app based on a new stack (Python/React). In the old app, the backend and the data model were closely intertwined, with SQL strings written directly within PHP. This made it challenging to evolve the data model for the app, which in turn imposed many product functionality constraints and slowed development time - especially since the Dropapp data model was a prototype rather than designed for scalability and product-market fit. As the team migrates to the new app and explores the possibility of entering new markets, it is time for us to reexamine if and how the team might benefit from a separation of concerns on the data layer.\n\n## Decision\n","1. **Scalability:** how well will it support expected future changes such as DB restructuring, database migrations, etc.? What timescale is the technology expected to be defunct on?\n2. **Developer experience:** given the rotating environment of loosely affiliated developers of different backgrounds, what will support the rapid onboarding of developers with our data structure? Once onboarded, is the chosen technology pleasant to use? Is it useful from a career progression standpoint?\n3. **Maintainability:** we expect to have rapid changes to the DB structure as we expand functionality. How easy is our solution to maintain and evolve? What about documentation?\n4. **Support and production-readiness:** Is the library mature enough to use in a production environment? Is there an active community or support channel should we run into problems?\nGraphQL with single endpoint for everything was selected, paired with Ariadne server-side and Apollo client side.\nReasoning: while GraphQL may have a steeper learning curve for professional developers who are not familiar with the standard, in the long run this should be more scalable for iterations and easier to maintain than  multiple REST endpoints. In the future, should we end up ingesting external data APIs such as the UNHCR data, it will be easier to pull that all from the GraphQL endpoint as well. This should also be more favorable from a developer experience standpoint for both onboarding and maintaining the codebase, due to GraphQL's introspective capabilities, human-readable JSON query structure, and degree of client-side specificity in requesting field-level data.\nApollo was selected on the client-side due to its maturity as a product, robust features including sophisticated caching, excellent documentation, and huge community. Ariadne was selected over Graphene on the server side due to it being designed to deliberately intended to mimic Apollo Server. With Ariadne being under active development by Mirumee software, and its excellent documentation that developers can cross-reference with Apollo server documentation, I believe this outweighs any cons that come from it being a less mature library than Graphene.\nFinally, I believe any performance concerns that could result from queries being abstracted from SQL into resolvers will be compensated for by less load on the network due to overfetching, as long as no N+1 queries are created.\n","**Decision:** Introduce a declarative data modeling tool to separate concerns on the data layer.\n\n**Rationale:**\n\n* Decoupling the data model from the backend will provide greater flexibility and agility in evolving the data structure as needed.\n* It will remove the constraints imposed by the prototype data model and allow for scalability and product-market fit.\n* Using a declarative data modeling tool will streamline the process of creating and modifying data models, reducing development time and complexity."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe main programming language for the present repository is Python. The Python ecosystem features a rich set of tools and libraries that facilitate developing an industry-standard codebase. Development includes implementation of production and test code in an environment built on linting and formatting tools.\n\n## Decision\n,"1. Test-driven development\n1. Code maintainability\n1. Code scalability\n1. Code format consistency\n1. Testing: `pytest`. Compared to unittest, test code is more concise yet readable, test fixture setup is straightforward, and test assertion output is very clear and helpful for debugging.\n1. Code formatting: `black`. Developed by the Python Software Foundation themselves. Uncompromising, fast, deterministic.\n1. Linting: `flake8`. Detection of common code smells. `pylint` tends to be pickier and might hinder rapid initial development.\n1. Integration with `git`: `pre-commit`. Automatic style checks prior to committing. Orchestration of style check tools.\n1. Isolated Python development environment: `venv`. Isolation of system and project Python packages in so called 'virtual environments'.\n",Use Python as the main programming language for this repository.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are free to pick any JVM language with static typing.\nDevelopers are not expected to have any background with jvm languages except java.\nKotlin looks like a ""better Java"" option with painless on-boarding.\nScala is a radical move towards functional paradigm.\n\n## Decision\n",We will adopt Kotlin as a project language.\n,"If developer onboarding is a priority, use Kotlin. If functional programming is desired, use Scala."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDevelopers need to be able to run application on their local machines in isolated environment.\nDockerized DB provides required service without the need to setup\nfull-blown DB server on each local machine.\n\n## Decision\n,We will provide docker compose setup for Postgre SQL server.\n,Use Docker to containerize the database service.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need some framework with fast startup time and low memory footprint.\nSpark looks like a good fit due to project's maturity and good reception by the industry.\n\n## Decision\n,We will build app temple based on Java Spark framework\n,Use Spark as the framework for the project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDevelopers need some tool for DB versioning and safe migrations.\n\n## Decision\n,We will introduce dockerized flyway setup for local development.\nMigration scripts will also be provided.\n,"**Decision:** Use Liquibase\n\n**Rationale:**\n\n* Liquibase is an open-source database versioning tool that enables developers to track and manage database schema changes.\n* It supports multiple database types, including MySQL, PostgreSQL, Oracle, and SQL Server.\n* Liquibase uses a DSL to describe database changes, which allows for automated testing and deployment.\n* It provides robust rollback functionality, ensuring data integrity in the event of errors.\n* Liquibase has a large community of users, extensive documentation, and a proven track record in production environments.\n\n**Consequences:**\n\n* Developers will have a reliable tool for managing database schema changes.\n* Automated testing and deployment will reduce manual errors and improve the reliability of database migrations.\n* The risk of data loss or corruption during migration will be minimized.\n* The team will benefit from a widely adopted tool with a proven track record and community support."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\nIntellij IDEA has great support for Kotlin based builds.\n\n## Decision\n,We will adopt Kotlin as a Gradle build's definition language.\n,Use Kotlin DSL for Gradle builds.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\n\n## Decision\n",Use Terraform\n,Use AWS CloudFormation to create and manage AWS resources.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a fixed amount of budget for this project, since this is supposed to be a micro scale project\nand the costs of the resources used can go up pretty quickly.\n\n## Decision\n","Create an alarm for billing, if the predicted billing for the month is more than $5.\n","**Decision:**\n\nTo minimize resource costs, utilize cost-effective resource solutions such as lighter-weight instances, spot instances, and open-source software wherever possible."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nChoosing the default region to start the resources\n\n## Decision\n,Using eu-west-1 Ireland as default\n,"**Decision: **\n\nThe default region for all new resources created will be `us-central1`.\n\n**Rationale: **\n\n* `us-central1` is a widely used region that offers high availability and low latency for most applications.\n* It is a popular choice for deploying applications that need to be close to users in North America.\n* It has a wide range of available services, including Compute Engine, App Engine, Cloud SQL, and Cloud Storage.\n\n**Consequences:**\n\n* All new resources created will be located in `us-central1` unless explicitly specified otherwise.\n* This may impact performance and availability for applications that require data or resources to be located in a specific region.\n* It may also impact costs, as pricing for services can vary by region."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCreate an alarm for budgets for free\n\n## Decision\n,Use AWS budgets\n,"**Decision:** Add the ability to create an alarm for budgets for free.\n\n**Rationale:**\n\n* **Alignment with user needs:** Alarms are a critical tool for budget management, allowing users to monitor their spending and be notified when they approach or exceed their limits.\n* **Competitive advantage:** Offering this feature for free would differentiate our service from competitors and provide a significant value proposition for users.\n* **Technical feasibility:** The necessary infrastructure and monitoring tools are already in place, making it relatively straightforward to implement this feature.\n* **Cost-effectiveness:** Adding this feature would require minimal additional investment, making it a cost-effective way to enhance user experience.\n\n**Consequences:**\n\n* **Positive:**\n    * Improved user experience and satisfaction.\n    * Increased adoption and retention of our platform.\n    * Potential new revenue streams from premium features that build on top of alarms.\n* **Negative:**\n    * Minor additional development and maintenance costs."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Use an Architectural Decision Record (ADR) template to capture and document architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo use terraform one needs to setup s3 bucket and dynamo lock for multi user access which creates the chicken and egg problem, where the bucket and dynamodb resources have to be present first to start managing Infrastructure with terraform.\n\n## Decision\n",The requried resources would be bootstrapped using a cloudformation stack and this requires a initial admin user which will be deleted right after the creation\n,"**Decision:**\n\nCreate a separate Terraform module that is responsible for setting up the necessary infrastructure (S3 bucket and DynamoDB lock) for multi-user access. This module can be used as a prerequisite for any Terraform modules that require those resources.\n\n**Rationale:**\n\nThis approach solves the chicken-and-egg problem by providing a modular and reusable solution for setting up the required infrastructure. The separate module can be deployed independently, ensuring that the necessary resources are in place before any other Terraform modules are used. This allows for easier multi-user access management and provides a more consistent and scalable solution."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n",The change that we're proposing or have agreed to implement.\n,The decision that was made is missing from the context. Please provide the full Architectural Decision Record to get the answer.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUsing once shell file makes for one very big complicated file\nUsing smaller self contained files means new developers can more easily make useful changes and adding new files has less risk\n\n## Decision\n,Each file should do one thing and do it well\n,Use smaller self-contained shell files.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA language should be universal, simple and easily testable\nOptions\n* Shell\n* Go\n* Java\n* JavaScript\nThere should be very few dependencies\n\n## Decision\n",Shell\n* No dependencies\n* Installed pretty much everywhere developers are\n,Go
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement an Architectural Decision Record (ADR) process to document and track architectural decisions throughout the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith the goal of producing services (e.g., REST APIs) to support the UniProt website, a programming\nlanguage should be chosen so that the requirements can be implemented. The choice will\nleverage the skills of the team of developers working on this project.\n\n## Decision\n","This particular UniProt REST API project will use Java, and currently version 8.\nEven though Java 11 is already available, many companies still have not made the leap from 8 to 9\ndue to the possible amount of work required. In future, we do seek to upgrade to Java 8+.\n","**Decision:** Use Python\n\n**Rationale:**\n\n* Python has a strong ecosystem of libraries and frameworks for web development, including Django and Flask.\n* Python is a high-level language, making it easy to learn and use, which suits the team's skills.\n* Python has excellent documentation and community support, ensuring that developers can easily find help and resources.\n* Python is widely adopted by other web services, facilitating integration if needed."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nJava projects often contain a large amount of boilerplate code, e.g., defining data/value classes, builders, etc. All\nsuch code follows a certain pattern and needs testing -- and writing both of these can be error prone. A library\nthat enables cutting down boilerplate code, and which generates tested code would be beneficial to the project.\n\n## Decision\n",We will use the [Lombok](https://projectlombok.org/) library to reduce the amount of boilerplate code we need to write.\n,"**Decision:**  *Adopt the Lombok library for code generation and testing.*\n\n**Rationale:** Lombok is a widely used and mature library that addresses the problem of boilerplate code in Java projects. It offers features for automatic generation of getters, setters, constructors, and other common code patterns. Additionally, Lombok provides support for testing, including the generation of test cases and assertions. By adopting Lombok, the project can benefit from reduced boilerplate code, improved code quality, and increased productivity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a search engine to which we will send user queries, and from which we will receive their results.\nMoreover, we need an engine that can scale with our data and be resilient to faults (network, filesystem, etc.).\n\n## Decision\n","The current UniProt website uses lucene as the search engine. This is very fast. However, the drawback is that it does not easily scale.\nThis can be provided by Solrcloud. We have used this for the Proteins API with success, and therefore have 4 years of experience with it.\n","**Use a distributed, fault-tolerant search engine such as Apache Solr or ElasticSearch.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\nadditional features (than one would otherwise write themselves), etc.\n\n## Decision\n","We have used the [Spring framework](https://spring.io/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\n","**Decision:** Use a programming framework.\n\n**Rationale:**\n\n* Programming frameworks can improve productivity by reducing boilerplate code and providing pre-built features.\n* They can enhance reliability by enforcing best practices and reducing the risk of errors.\n* Frameworks can offer a range of additional features that would be time-consuming to implement from scratch.\n* They can facilitate code maintainability and extensibility by providing a consistent structure and organization.\n* Frameworks can support collaboration and code sharing within teams, as they provide a common set of tools and conventions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA build tool is necessary for various tasks such as compile, test, packaging and deploying our code.\n\n## Decision\n",We will use [Maven 3.2.5+](https://maven.apache.org) to build the back-end UniProt REST API components.\n,Use Maven as the primary build tool for the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a store from which entry data will be retrieved. Moreover, this store should scale as our data grows,\nand should also be resilient to issues it might face (e.g., network, filesystem, etc.).\n\n## Decision\n","We will use Project Voldemort, as we have 4+ years of experience with this already with the Proteins API and UniProt's Java API.\n","**Decision:** Use a distributed, multi-master database such as Apache Cassandra.\n\n**Rationale:**\n\n* Cassandra is a highly scalable, distributed database that can handle large datasets.\n* It is fault-tolerant and can continue to operate even if one or more nodes fail.\n* It is designed for high availability and low latency, making it suitable for applications that require fast access to data.\n* It is a popular choice for storing time-series data, which is a common use case for entry data."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\nThe basic functionalities known to be required are:\n- secure storage and management of documents provided by residents and other third parties\n- tracking of rights and retention over those documents by the council\n- requests and approval of evidence from third parties\n- management of evidence requests by officers\nInfluencing factors:\n- HackIT's [API Playbook](https://github.com/LBHackney-IT/API-Playbook-v2-beta)\n- [Clean Architecture](https://github.com/madetech/clean-architecture) principles\n\n## Decision\n",We decided to create two Platform APIs (as defined in the API Playbook):\n- A [Documents API](https://github.com/LBHackney-IT/documents-api) for the storage and management of documents and claims over them\n- An [Evidence API](https://github.com/LBHackney-IT/evidence-api) for the management of evidence requests and reviews\n,"**Decision:** Adopt a **microservice architecture** with a **Domain Driven Design** approach.\n\n**Rationale:**\n\n* **Microservice architecture** aligns with HackIT's API Playbook, promoting modularity and scalability. It enables each functionality (e.g., document storage, rights tracking) to be developed and deployed independently, fostering flexibility and agility.\n* **Domain Driven Design** ensures that the architecture reflects the core concepts and terminologies of the business domain, resulting in a clear and maintainable system.\n\n**Implications:**\n\n* The application will consist of multiple interconnected microservices, each responsible for a specific functionality.\n* Each microservice will have its own data storage and communication mechanisms.\n* The domain model will be central to the architecture, defining the core business concepts and their relationships.\n* The application will adhere to the principles of loose coupling, high cohesion, and single responsibility, enhancing modularity and maintainability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur service allows members of the public to upload files which are viewed, and potentially downloaded, by council officers. There is a possibility that the files could contain malware or other malicious code, so we should scan all uploaded files before making them available to officers\nWe also evaluated other approaches including Trend Micro Cloud One, and building a bespoke solution with ClamAV\n\n## Decision\n",Leverage HackIT infrastructure which uses Palo Altos to inspect all network traffic.\nA diagram can be found on [Miro](https://miro.com/app/board/o9J_kgEWTBE=/) but at a high level we:\n1. Attempt to download the file uploaded by the resident. This will be routed via the Palo Altos network for malware scanning\n2. Trigger processing based on the download status\n- If the download is not terminated then the file is clean and can be viewable by officers\n- If the download is terminated then the file contains malware\n,"We decided to use VirusTotal for file scanning because it provides a comprehensive and up-to-date database of malware signatures, and it is easy to integrate with our existing systems."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA critical part of building these services is being able to track who evidence belongs to, and therefore we must be able to identify residents in a platform-wide manner.\nAt the time of this decision, the HackIT team is in the process of building the Platform APIs necessary to provide access to this sort of core data, but they are not available yet.\n\n## Decision\n","To build a resident data source in this application, but make the extra effort to abstract it appropriately so that when an external resident data source and accompanying API is available.\n","Use the vendor's identifier for now, switch to platform-wide identity when available."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPrevious APIs at Hackney have been written in a variety of languages and frameworks, including Ruby on Rails, C# and ASP.NET, and Node/Typescript.\n\n## Decision\n","Use C#, ASP.NET and Hackney's Clean Architecture framework, as specified in the [API Playbook](https://github.com/LBHackney-IT/API-Playbook-v2-beta). These applications are intended to be a robust part of HackIT's platform and iterated upon and the HackIT team has mostly C# skillset.\n","Standardise on a single programming language and framework for future API development, to improve code quality, maintainability, and developer productivity."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\nThis tool was previously used by the HackIT team (for example in the [React component library](https://github.com/LBHackney-IT/lbh-frontend-react#architecture-decision-records)).\n","**Document Architectural Decisions**\n\nEstablish and maintain a system for documenting architectural decisions. This system should clearly capture the decision, its rationale, and any relevant supporting information. It should also provide traceability between decisions and the artifacts they impact."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are certain pieces of configuration which do not change often but need to be easy to change for colleagues who are not developers. Some examples include:\n- the document types that the API recognises and their metadata (like ther ID, title and description)\n- the council services which the API recognises and their metadata (like their name and google group ID).\nThe choice came down to either storing these configurations in the database, or storing them in text files.\n\n## Decision\n",Use text files for this configuration.\n,Store the configurations in a database.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* We have several different roles for using a Holochain app (through a *container*):\n* local, QML based UI components\n* as a special case of the above: administration UI\n* externally located (living in a browser) web UIs connected through some form of IPC\n* *bridged* apps, i.e. Holochain app as the user of another Holochain app\n* Services hosting multiple Holochain apps and exposing their zome functions to UIs selectively\n* With HoloSqape and the Holochain-nodejs, we already have two container implementations\nA Holochain app cannot be used directly. Holochain is built as a library that exposes an interface for\ncreating Holochain instances and calling their zome functions (see [container_api](/container_api/src/lib.rs)).\nThis *container_api* requires its client to provide a context, which holds the representation of the agent\n(name and keys), a logger and persistence and will also include a networking proxy in the future.\nWhile it is possible to use this library directly in a monolithic app where the UI is tightly\ncoupled to the app and everything linked to the Holochain library into the same executable, we regard this\nas a special case, since this does not allow for app composability (which we think is a crucial\nconcept for the Holochain app ecosystem).\nInstead, in the context of the Rust iteration of Holochain, we have been following the notion of\nproviding a *Container* as a relevant concept (and implemention) for the deployment of Holochain apps:\nwe provide a Container (i.e. [HoloSqape](https://github.com/holochain/holosqape)) for each supported platform (Linux, MacOS, Windows, iOS, Android)\nthat gets installed on a host machine. Holochain apps get installed into the container.\nThe Container:\n* manages installed hApps (Holochain Apps),\n* manages agent identities (keys) ,\n* should also enable hApps to call functions of other hApps - what we call *bridging*,\n* has to implement access permissions to installed hApps.\nSo far, the interface our Container implementation provides was growing organically\nin [container.h](https://github.com/holochain/holosqape/blob/master/bindings/container.h).\nWith upcoming alternative container implementations (based on [Holochain-nodejs](https://github.com/holochain/holochain-nodejs)\nor a custom one for HoloPorts) we should drive the process of building this Container API\nconsciously and with more coherence and visibility amongst our separate dev teams.\nWe need a protocol for communication with a Holochain container and a specification of what upcoming\ncontainers have to implement, so that apps and UIs can be build against a standardised interface.\n\n## Decision\n","We establish the **Container as a fundamental module of the Holochain architecture/stack**\nby specifying its **API**, that can be assumed by UI components, Holochain apps (i.e. zome\nfunctions in the case of bridging)\nand remote processes alike to be implemented by the context a Holochain app is executed in.\nFundamental to this API is **user/client roles and permissions**.\nClients will be able to use different subsets of the Container's API depending on their specific permissions.\nSo an implicit aspect of this API is that every potential API call happens in the context of a known\nclient identified through a client ID that the API manages and returns to the admin client as handles.\nWe will specify this API in a separate *specification document*.\nThe following subsection provide examples for how this *could* look like:\n### Example API\nEvery client (of the Container API, i.e. QML root widgets, admin UI in the case of HoloSqape,\nand network services built on top of the Container)\nwill have these functions available, though functions can return with a permission denied\nerror in case an app or capability was given that the current client is not allowed to use.\n* `call(app_hash, function_name, parameters) -> {results}`\n* `connect(app_hash, capability_name, signal_name, callback_function)`\n* `installed_apps() -> [hash]`\n* `capabilities_of(app_hash) -> [String]`\n* `functions_of(app_hash, capability_name) -> [String]`\n* `request_access(app_hash, capability)`\n* `can_access?(app_hash, capability) -> bool`\n### Admin\nOnly the admin user can manage apps and permissions:\n* `install_app(Dna) -> hash`\n* `uninstall_app(hash)`\n* `start_app(hash)`\n* `stop_app(hash)`\n* `promote_to_admin(client_id)`\n* `retract_admin(client_id)`\n* `commit_capability_grant(client_id, app_hash, capability)`\n* `deny_capability(client_id, app_hash, capability)`\n### Extensible\nMore API capabilities might be added in the future.\n","We define a standard API that containers must implement to enable cross-platform app deployment. This API will include methods for managing installed hApps, managing agent identities, and enabling hApps to call functions of other hApps."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a WASM intepretor in Rust.\nThere seems to be a lot of momentum behind WASM:\n* all big browser support WASM since of Feb 2018\n* recent blockchain projects jump on WASM\nWe will get more languages for Holochain for free just by those communities building WASM support\nJS, C++ and Rust already with stable support.\nBy writing one WASM Ribosome we support more languages than sticking to a JS and Zygo Ribosome each.\n\n## Decision\n",Implement only one WASM ribosome for holochain and have it working for 2 different toolchains/languages (Javascript and Rust or C++ - to be decided)\n,Implement a WASM Ribosome to support more languages in Holochain.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the alpha version we had *hcd* that runs a single Holochain app/agent as a background demon.\nGoing forward we need a more sophisticated conductor of several Holochain apps for several reasons:\n* UX - One end-user friendly tool with GUI to administer all installed and running Holochain apps\n* coalescing of the network layer / having one network manager, to handle ports etc.\n* Mid-term roadmap entails having a generalized Holochain UI / browser that would act as an app conductor as well\nSince with ADR 8 we want to go mobile first, we need to have an easy way to run Holochain apps on mobile phones.\nQt sports QML for rapid UI development and compiles the same code natively to Windows/Linux/MacOS/Android/iOS/Blackberry.\nFor system and network capabilities, C++ would be at our disposal.\n\n## Decision\n","Use Qt framework for building a cross platform Holochain app conductor as a replacement for *hcadmin* for Holochain app management & deployment, and possiblly integrating *hcd* into it if we can manage the Rust integration.\n",We will develop a standalone cross-platform **app conductor** that can be used as a command line interface (CLI) and graphical user interface (GUI) on mobile and desktop platforms. The app conductor will use Qt and C++ for cross-platform compatibility and performance.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to have holochain run in the browser (for Holo).\nRust can compile to WASM. Rust is language for experts. Rust is trending.\nThere is a WASM interpreter in Rust (WASMI).\nOnly problem might be problems integration with Promether / linking against C++:\n* https://doc.rust-lang.org/nomicon/ffi.html\n* https://users.rust-lang.org/t/linking-with-custom-c-library/637\n\n## Decision\n,Rewrite Holochain in the Rust programming language\n,"**Decision:** Use WASMI to run Holochain in the browser.\n\n**Rationale:**\n\n* Rust can compile to WASM, which is necessary for running Holochain in the browser.\n* Rust is a trending language with a strong community.\n* WASMI is a WASM interpreter in Rust that can be used to run Holochain.\n* The only potential issue is integrating with Prometheus and linking against C++, but this can be addressed by using the techniques outlined in the provided links."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDistributed applications, like centralized ones, often need a source of randomness.  Having confidence and verifiability of that randomness poses particular challenges in the the distributed context. Specifically, we need a source of randomness with some of the following properties:\n1. It is outside their control or influence so it can be used for a dice roll, card deck shuffle, or something where they may have an interest in skewing the results.\n2. It is predictably reproducable, so that other nodes, whether players in a game, or validating nodes later, can reproduce the SAME random seed to reach the same random output for purposes of validation.\n3. It is generalizable. Ideally, every application won't have to build their own approach to randomness, but can take advantage of a useful underlying convention. There are pitfalls we want to help them avoid in a distributed, eventually consistent system.\n4. It doesn't require specific parties to be online to access the randomness data, so that later validators can confirm it even if parties with private entries are not online or no longer part of the network.\nIn the case of multiple parties wanting to generate randomness together, [cointoss](https://github.com/holochain/cointoss) provides an example of sharing of the hash of a secret which when later revealed can be combined to create a random number seed.  This method can be generalized into storing a bunch of private secrets, and publishing the hashes of those secrets, and then later revealing the secret to be combined with another party doing the same.  In cointoss this revelation happens via node-to-node communication, but in the general case it doesn't have to work that way.\nOur application environment includes interactions (gossip and validation)  the combination of which are highly unpredicable (they include things like network latency, and timestamps) but verifiable after the fact. So for example, using the ""first"" four validation signatures on your opponent's last move as a random seed, could be one approach.\n\n## Decision\n","We will:\n1. Implement mixins to provide randomness generation for some usecases using the cointoss combined secret method (both n2n for single events, and dht pushed for multiple events)\n2. Provide app level access to unpredictable gossip/validation events for explicit use as seeds for random number generators.\n","To generate predictably reproducible random numbers in a distributed application, we will use a combination of the following techniques:\n\n* **CoinToss**: A protocol for securely sharing a random seed among multiple parties.\n* **Hashing**: A function that takes an arbitrary input and produces a fixed-size output.\n\n**Implementation**:\n\n1. Each party generates a random seed Si and hashes it, producing Hi.\n2. The hashes {H1, H2, ..., Hn} are published to a shared ledger.\n3. To generate a random number, the parties reveal their seeds {S1, S2, ..., Sn} and combine them using a deterministic function, such as a hash of their concatenation.\n\n**Benefits**:\n\n* **Verifiable randomness**: The random seed is generated collectively and verifiably by multiple parties.\n* **Predictable reproducibility**: Other parties can reproduce the same random number by using the published hashes and revealed seeds.\n* **Generalizable**: This approach can be used by any application requiring random numbers.\n* **No need for specific parties to be online**: The randomness can be generated and validated even if some parties are offline or have left the network."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA complete Rust rewrite is planned (see ADR #0002)\nGo code has code debt -> substantial effort to refactor\nGo implementation would not be reusable in Holo front-end (whereas portions of rust compilable into WASM could be)\nGo code has dependencies that make it hard to compile for mobile\n\n## Decision\n,"- Alpha1 go release is last major go release of holochain because energy will be focused on the new Rust version.  One team, one code base for now, may revisit this later\n- We will call the rust release Alpha2, and will have at least the functionality of Alpha 1 plus World-model & new network-transport\n","**Decision:** Rewrite the service in Rust.\n\n**Rationale:**\n\n* A complete Rust rewrite will address the code debt and dependencies in the Go implementation, making it easier to maintain and portable to multiple platforms.\n* Rewriting in Rust will enable the reuse of code in the Holo front-end, leveraging the Rust compiler's ability to target WebAssembly (WASM).\n* The Rust rewrite will improve the overall performance and stability of the service, reducing the risk of unexpected failures.\n* The Go implementation would require significant refactoring to address its issues, while a Rust rewrite would allow for a more efficient and extensible design from scratch."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the go code we used polling from the UI to discover local state changes, be they in the source-chain or in the DHT.  We want to change this to an event based model where UI (or other clients, like bridged apps) can become listeners to signals.  Additionally there are system level signals (like warrants) as well as other app level definable signals that we want consumers of exposed holochain app APIs to be able to access on a push (i.e. listening) basis.\nNote that this is only about signals that can be listened to by conductor context, i.e. by the client of the core_api, NOT by the app nodes themselves.\n\n## Decision\n","We will extend the API in a way that's roughly equivalent to the [Signal-slot pattern](https://en.wikipedia.org/wiki/Signals_and_slots), i.e. in the following ways:\n1. In the DNA you can declare observable signals anywhere you can declare functions.  You can think of this almost identically to declaring a function except that it ""goes the other way,"" i.e. a function def exposes an entry point where an signal def exposes an exit point.  It would look like this:\n``` javascript\n""signals"": [\n{\n""name"": ""Post"",\n""description"": ""signal emmited when a post is committed"",\n""config-params"": null,\n""arguments"": {\n""name"": ""hash"",\n""type"": ""hash""\n},\n// ...\n],\n```\nThe above declaration defines what arguments the signal will send to the listeners and additionally a config-param object to be passed in on the listen request which may be useful for qualifying some aspect of what to listen for.\n2. App developers can emit signals from their code via a new `emit()` function to be added to the api, e.g. like this:\n``` javascript\npostHash = commit(""Post"",{content:""foo""})\nemit(""Post"",postHash)\n```\n3. Finally, just as you can call any function using the `core_api::call()`, you can register a listener with `core_api::listen()` and you and unregister a listener with `core_api::unlisten()`\n","**Decision:** Implement an eventing system for Holochain that allows UI and other clients to listen to signals from the source chain, DHT, and other app-level definable signals.\n\n**Rationale:**\n\n* The current polling model is inefficient and can lead to performance issues, especially for large or complex Holochain apps.\n* An event-based model would allow UI and other clients to receive updates in a more timely and efficient manner, improving the user experience and reducing resource consumption.\n* By allowing app developers to define custom signals, the eventing system would provide flexibility and extensibility for Holochain apps."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* We have an implementation of a Holochain conductor that can load QML based UIs: https://github.com/holochain/holosqape\n* We also want to support HTML/JS/CSS based UIs\n* We need to enable pure Web clients to connect to a HoloPort for the Holo use-case\n* With [ADR 13](https://github.com/holochain/holochain-rust/blob/develop/doc/architecture/decisions/0013-signals-listeners-model-and-api.md)\nwe defined *app signals*, i.e. events that a Holochain app can push - we need to make sure UIs can receive these as\npushed notifications from the Holochain node\n\n## Decision\n","Start with: Option 2.\nThe existing Holochain conductor project HoloSqape is to be extended to provide a **WebSocket interface**\n(next to the QML based GUI plug-in system) that enables many different kinds of external\nuser interfaces to connect to the local Holochain node, authenticate, issue zome calls and receive zome signals,\ndepending on permissions administered by the local user of the node through an admin UI or default settings.\nA **JavaScript based client library** is to be implemented that makes it easy to use this interface from the context\nof a web browser, hiding the complexity and wire-protocol of the websocket interface itself, only offering high-level\nfunctionality such as:\n* connecting to a local or remote node\n* authentication (public-key? JWT tokens?)\n* quering installed/running apps, the session has access to\n* issuing zome function calls and receiving results\n* registering callbacks for zome signals\n* lifecycle management of the websocket connection\n![](../WebSocket-interface-HoloSqape.png)\nFollowup with: Option 1 and/or 4\nProviding some kind of RESTful interface to Holochain is needed if we are\nserious about reaching the widest development audience across the most client\nlanguages/environments possible.\nThe good news is that there are many existing examples of realtime providers and\nlibraries/frameworks offering both WebSockets and REST APIs (e.g. Twillio, Ably,\nFeathers, Slack, etc.).\nLong term: Holo server/clients and native electron apps\nWoo!\n","We will use holochain-conductor for managing platform and device specific bindings and for handling messages from mobile and/or native apps, and we will use electron-based approach for web UIs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\nSince we are switching over to Rust, we can't easily continue using that code base.\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\n\n## Decision\n",We will build our own DHT implementation in Rust from scratch.\n,"We will create our own DHT implementation from scratch in Rust. As we do not have time to implement our own network layer, we will use libp2p for that purpose, which provides many of the features we need for an entry resilient network."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are doing a rewrite.\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\n\n## Decision\n","The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\nWe will apply nested state objects which represent a state tree, with sub states for each module.\nWe use reference counting smart pointers for the sub\nstates such that it is possible for each module's\nreducer to decide if the sub state is to be mutated or reused.\n",Adopt an actor model with a single global state as the central authority of the agent.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGoing forward with the rust implementation, we recognize that some 32bit architectures exist that people may want to run Holochain on.  Supporting 32bit architectures may have particular consequences in the realm of cryptography.  We have limited resources.\n\n## Decision\n",For now we will assume availability 64bit CPUs and not use our resources testing against 32bit targets.\n,"**Decision:** We will focus on supporting 64-bit architectures for the time being.  However, we will keep an eye on the demand for 32-bit support and may reconsider this decision in the future if necessary.\n\n**Rationale:**\n\n* 64-bit architectures are more common and provide better performance.\n* Supporting 32-bit architectures would require additional development effort and testing.\n* We have limited resources and need to focus on the most critical tasks.\n* Rust 1.54 onwards removed support for 32bit ARM targets, so there is a smaller community to provide support.\n* The cryptography libraries we use will need to have 32bit builds available for us to support 32bit architectures. This means we'll need work from the community upstream to get these builds provided.\n\n**Consequences:**\n\n* Holochain will not be able to run on 32-bit architectures.\n* Users who want to run Holochain on 32-bit architectures will need to use a 64-bit operating system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA unified security model for Holochain applications:\n* Each zome must be able to represent and enforce its own security modeling because that is the appropriate place to do so. (Push the intelligence to the edges.)\n* Developers must be able to build in granualar specificity and revokability of access to functions and entries.\n* We must be able to distinguish between application security model, and architectural and code security model.  I.e. what is the security model application developers build into their apps, and the security model of Conductor/Core, etc.  E.g. we have to ensure that zome calls aren't subject to replay attacks in general, and also allow zome developers to declare and create security policies in specific.\n\n## Decision\n","Holochain will use a variant of the [capabilities](https://en.wikipedia.org/wiki/Capability-based_security) security model. Holochain DNA instances will grant revokable, cryptographic capability tokens which are shared as access credentials. Appropriate access credentials must be used to access to functions and private data.\nThis enables us to use a single security pattern for:\n- connecting end-user UIs,\n- calls across zomes within a DNA,\n- bridging calls between different DNAs,\n- and providing selective users of a DNA the ability to query private entries on the local chain via send/receive.\nEach capability grant gets recorded as a private entry on the grantor's chain, and are validated against for every zome function call.\n","**Decision:**\nEach zome will define its own security model, using a standard DSL. This will allow developers to build in granularity specificity and revokability for access to to functions and entries. Additionally, we will introduce a new security DSL for zomes.\n\n**Reasoning:**\nThis allows applications to define their own security model, which gives them the flexibility to enforce their own specific security requirements. The standard DSL will ensure that all zomes are using the same approach for specifying their security models, which will make it easier to manage and maintain. The new security DSL will provide a more powerful way for zomes to define their security models, which will allow them to implement more complex and specific security requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn switching to Rust, we can no longer use libp2p directly, and we want our own Kademlia implementation anyways (see ADR: 0010)\nWhat we want from a network library:\n* Public Key cryptography for end-to-end encryption.\n* Hash of public key as network address which wraps the transport layer for use by DHT.\n* Possibility of multiple transport layers.\n* Built in support for NAT/firewall traversal for case of IP transport.\nUpdated 7/20 context:\n* Conversations with the IPFS folks about libp2p as well as its wider adoption by hyperledger and parity/polkadot as a P2P transport layer, along with the progress of libp2p-rust, and thinking about strategic alignment with other efforts and stewarding of our dev resources make it a more interesting candidate to use as a transport layer to handle the above requirements.\n* ZeroMQ looks very interesting.\n* neonphog has written lib3 as a prototyping transport.\n* Promether questions.\n\n## Decision\n","1. Write our own abstraction layer for the networking library with an API that allows us to build against our needs, and thus makes it easier to choose/switch a networking stack.\n2. Assume that this allows connection to a directly compiled-in p2p layer or connection via thin layer to a p2p transport system service (see ADR #14)\n3. Assume transport layer handles all multiplexing across different physical transports as well as topological problems (NAT relay etc)\n",Use libp2p as the transport layer and develop a custom Kademlia implementation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMobile is the main platform today. We want to reach as many users as possible.\nMobile use has two big constraints:  battery life (and consequent sleeping of apps), and bandwidth because of costs\nGo platform development ignored mobile from the start and we found out late about the problems of compiling to mobile.\n\n## Decision\n","Target a mobile build from the start.\nDo not initially worry about battery/bandwidth constraints, assuming that ADR 0006 will handle solve this issue in the medium term, and that advance in technology will handle it in the long-term.\n","**Decision:** Use Flutter for mobile development.\n\n**Reasons:**\n\n* Flutter is a cross-platform development framework that compiles to native code, providing high performance and a native user experience.\n* Flutter is specifically optimized for mobile development, addressing the challenges of battery life and bandwidth.\n* Flutter has a strong community and support ecosystem, with a wide range of libraries and tools available.\n* Flutter is a relatively new technology, but it has quickly gained popularity and adoption in the mobile development community."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\n* the agent's keys are by definition part of the front-house\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network/DHT while the user is offline and keys not accessible**\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\n\n## Decision\n","We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\n","**Front-house and back-house split with separate keys**\n\nThe Holochain agent will be split into two parts, called front-house and back-house. The front-house will be responsible for source chain and private key management, as well as providing interfaces for the UI and in the non-Holo case for bridging. The back-house will be responsible for the DHT shard, routing table, metrics, etc.\n\nEach part will have its own set of keys. This will allow the back-house to communicate with the network without needing to request signatures from the front-house every time."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRust as a language, in my opinion, does not lend itself to rapid prototyping. Writing efficient Rust code, while absolutely possible, is a problem domain unto itself, and takes time. E.g. Is it worth returning borrowed references in this case to save memory? Am I able to craft the lifetimes appropriately given all the other api usages of my struct? What about synchronization? These questions are implementation details, not architectural. If we are still working out the details with the underlying architecture, we don't want to be spending brainpower on * how * we accomplish our experiments.\nIn this prototyping / proof-of-concept phase, I don't want to take the time to write efficient rust code, but I don't want to leave us with a bunch of unmaintainable inefficient spaghetti if we *do* decide to go forward with the prototype.\n\n## Decision\n","Abstract the p2p library at the process level.\n- The P2P process will host a [ZeroMQ](http://zeromq.org/) ROUTER socket. This process can be in any language that supports zmq, and using any of the transports it supports. (Likely start with unix domain sockets for their high throughput).\n- The holochain rust code will connect to the P2P process with a ROUTER socket using the [zmq](https://crates.io/crates/zmq) crate.\n- The holochain rust code will access this ipc abstraction through the [network-abstraction](0007-abstraction-for-network-layer.md) framework allowing the option to implement, for example, the rust version of libp2p both internally, and as a separate process.\n","Use a higher level language for prototyping, such as Python or Node.js. This will allow you to experiment with the underlying architecture more quickly and easily, without having to worry about the low-level details of Rust. Once you have a better understanding of the requirements and constraints of the system, you can then decide whether or not to re-implement it in Rust."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To implement an Architectural Decision Record (ADR) process to document and track the architectural decisions made on the project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTraceability of a Docker image to a source code repository is valuable to debug any problem that comes up during testing and deployment.\nDependency pinning on project images pinning a particular version of the base image they are using is also valuable for build reproducibility.\n\n## Decision\n,"We will tag every new, tested version of an image using the commit SHA value that produced it.\n",Use a Container Registry to host our Docker images and automatically tag them with the version of the source code from which they were built. This provides traceability and reproducibility without the need for manual intervention.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDummy services such as api-dummy are stateless, do not require many components such as databases and caches, and do not produce logs.\nThe ease with of pushing, pulling and running a single image inside larger orchestration is its selling point.\nReal projects have dependencies that should be centralized to specialized containers such as `nginx` or `postgres`. These dependencies are not maintained in-house. These dependencies also have to be bootstrapped in a certain order.\nReal projects also have various volumes that need to be mounted for them to function, such as configuration files.\n`docker-compose` has some support for IDEs, and is portable between laptops and headless environments such as `ci` and `prod`.\n\n## Decision\n","By default, use a single docker image to ship dummy projects. Provide additional options for setup if needed to satisfy other requirements.\nUse one or more `docker-compose.yml` files to build and deploy real projects.\n","Use `docker-compose` for the following reasons:\n\n- It's easy to push, pull, and run a single image inside larger orchestration.\n- It centralizes dependencies to specialized containers such as `nginx` or `postgres`, which are not maintained in-house.\n- It allows for bootstrapping dependencies in a certain order.\n- It provides support for mounting volumes, which is necessary for real projects to function.\n- It has some support for IDEs and is portable between laptops and headless environments such as `ci` and `prod`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nApplications often need to drop in binaries or scripts, either downloaded or picked up from other images such as `proofreader-php`.\nThe binaries needed do not need `root` permissions or system-wide installation.\nThe binaries may need to modify the application files.\nA container image may be an alien environment, as it makes it difficult to find out which of the files were provided by the Dockerfile build process.\n\n## Decision\n","Every image should provide a standard `/srv/bin` folder, on the PATH, containing utilities owned by `elife`.\n",Use a volume mount to make the binaries available in the container. The volume can be set up with the needed permissions (e.g. 755) and mounted into the container under the right paths.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nProcesses running the application should not be able to modify files in general, as only particular subfolders should be writable, but not the code itself.\nIt should not be necessary to use `root` privileges to build an application once a folder with the right permissions is available.\nMany tools (rightly) also have built-in limitations against running as `root`, or provide flashy red warnings when run as `root`.\nUbuntu Linux distributions provide a standard user `www-data` with UID `33`.\n\n## Decision\n",Every image should provide a `elife` and a `www-data` user:\n- `elife` should be the owner of all the application files.\n- `www-data` should be used to run all application containers and any shell executed inside them.\n,Configure the application to run as `www-data` user with a unique UID and GID of `33`.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAn image is a powerful mechanism for deployment, as it is portable across environments.\nSome images need to specify the version of their dependencies (such as other images) that should be run as sidecars.\n\n## Decision\n",Use image labels of the form `org.elifesciences.dependencies.api-dummy` to store the version of a dependency.\n,Use an image tag to specify the version of the sidecar images.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDockerfiles make no provision for extracting duplication of steps unless they are rendered from a template. Similar `RUN` steps can multiply across different files.\n`RUN` steps also allow no logic or encapsulation, and promote long chains of commands due to the necessity of producing a single layer.\n`RUN` steps are not testable in isolation or re-runnable inside an image for debugging.\n\n## Decision\n","Extract long `RUN` steps (or sequences of steps) into a bash script. If the script is only to be used by `root`, place it in `/root/scripts`.\n","Use multi-stage builds to reduce image size and improve security, and to enable testing of individual steps."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTest suites and other tools may produce output files while running inside a container.\nDue to environmental differences, container users and groups often do not match the host's users and groups, blocking one side from deleting the files of the other; often with corner cases such as files being deleted but not subdirectories.\nAutomated infrastructure usually runs as the `elife` or `jenkins` user, not as `root`. The docker daemon runs as root and is capable of bridging differences.\n\n## Decision\n","Use `docker cp` to exchange files between containers, especially from inside a container to the outside.\n","Mount host paths into containers, setting `fsgid` and `fsuid` to specify the UID and GID of the remote location, making the contents of the host mount owned by the UID and GID of the host directory so that the host can subsequently remove any files or subdirectories created by the container"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\n## Decision Drivers <!-- optional -->\n* High availability/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\n\n## Decision\n","* High availability/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of ""maintenance only"" since June 2013.\n### Positive Consequences\n* PostgreSQL can be clustered, making it highly available\n* PostgreSQL is broadly available as a managed service\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\n### Negative Consequences\n* We have to design our own schema and code for interacting with the PostgreSQL database\n",Use a graph database for storing and retrieving SKOS taxonomies.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n*See [frontend architecture sessions](https://docs.google.com/document/d/1VW3ah5VAvAz2KnqNZlNmVqCzFhBMlIcjPPUlsHMFRIY/)\nfor background information on micro-frontend/SPA architecture.*\n\n## Decision\n,We are going to merge the following apps together in a *@molgenis-ui/core* app:\n* navigator\n* data explorer 2\n* data-row-edit\n,"**Decision:** Implement a micro-frontend architecture using server-side rendering (SSR)\n\n**Rationale:**\n\n* **Improved performance:** SSR addresses the initial load performance concerns of single-page applications (SPAs) by pre-rendering pages on the server, resulting in faster page loads.\n* **Search engine optimization (SEO):** SSR is more SEO-friendly than client-side rendering because search engine crawlers can access the pre-rendered content. This is crucial for improving visibility and ranking in search results.\n* **Flexibility and extensibility:** Micro-frontends allow for independent development and deployment of different pieces of functionality, making it easier to scale and update the overall application.\n* **Compatibility with existing systems:** SSR can be implemented in a way that integrates smoothly with existing server-based systems, leveraging existing infrastructure and data sources.\n\n**Consequences:**\n\n* **Increased server load:** SSR requires server-side rendering, which can increase the load on the server, especially during high traffic periods.\n* **Complexity:** Implementing SSR can be more complex than client-side rendering due to the need for coordination between the server and client.\n* **Potential security risks:** SSR involves sending rendered HTML from the server to the client, which can introduce security risks if not implemented properly."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n*See [frontend architecture sessions](https://docs.google.com/document/d/1VW3ah5VAvAz2KnqNZlNmVqCzFhBMlIcjPPUlsHMFRIY/)\nfor background information on micro-frontend/SPA architecture.*\nWe need a consistent method to serve the index file for our frontend application.\n\n## Decision\n,We are going to use a clientside index file for the *@molgenis-ui/core* app.\n,Use a backend API to serve the index file.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n","**Decision:** To use an Architectural Decision Record (ADR) template to document the architectural decisions made on this project.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture architectural decisions, making them easier to track, understand, and communicate.\n* ADRs help ensure that decisions are made in a deliberate and considered manner, based on a clear understanding of the context and implications.\n* ADRs serve as a valuable knowledge repository for future reference, allowing architects, developers, and other stakeholders to quickly grasp the rationale behind design choices.\n\n**Consequences:**\n\n* Time and effort will be required to create and maintain ADRs.\n* ADRs should be reviewed and updated regularly to reflect evolving project requirements and decisions.\n* ADRs should be accessible to all relevant stakeholders to ensure transparency and alignment."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\n\n## Decision\n","We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\nRejected alternatives:\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\n",**Decision:** Use a Molgenis AppKey to link the app to settings data. Use the Molgenis AppApiKey to control access to the settings.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to describe the structure of Javascript objects for readers of the code to be able to understand the structure.\nAnd we need a way to detect null references at build time Javascript.\nOur Javascript passes though a build step.\nOur Javascript is maintained by people who did not write the code.\n\n## Decision\n,We will use Typescript as a tool to describe object structure.\n,"**Decision:** Use flow type annotations.\n\n**Justification:**\n\nFlow type annotations allow us to describe the structure of JavaScript objects, which will help readers of the code to understand the structure. They also allow us to detect null references at build time, which will help us to catch errors early on. Flow type annotations are maintained by the team who writes the code, so they will be up to date with the latest changes to the codebase."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe old skin uses the Bootstrap 3, which uses the `float` and `clear` CSS properties for layout, instead of the more modern `flexbox`. Therefore, we looked at different frameworks which can help reduce our efforts in writing CSS classes and Vue components from scratch.\n### [Vuetify](https://vuetifyjs.com/en/)\nIt is a big collection of ready-made UI elements based on Google's Material Design. It looks like it could bloat our JavaScript and CSS and make the page look very generic. Also, binding to Material Design might be tricky due to overwriting lots of Material Design assumptions.\n### [Buefy](https://buefy.org/)\nA ""lightweight"" component library. Looks nice and does a lot of work for us. It is based on [Bulma CSS](https://bulma.io/) which seems to be established and is a good combination between usable and adaptable.\n### [Element.io](https://element.eleme.io/#/en-US)\nThe documentation does not look good enough. The Q&A page on their website is not in English.\n### [Tailwind CSS](https://tailwindcss.com/docs/what-is-tailwind/)\nIt is not a component library, but a collection of useful CSS classes for rapid prototyping components (by adding lots and lots of class names to them). When the prototyping phase is over, we need to combine the CSS classes into components. We are not sure we want to go into the direction of ""CSS Framework external to components"".\n\n## Decision\n","We will use Buefy, a UI components framework for Vue.js based on Bulma which is a free, open source CSS framework based on Flexbox.\n","**Decision:** Use Buefy as the UI component library.\n\n**Rationale:**\n\n* Buefy is a lightweight component library that can help reduce the effort in writing CSS classes and Vue components from scratch.\n* It is based on Bulma CSS, which is an established and widely used CSS framework that provides a good balance between usability and adaptability.\n* The documentation for Buefy is comprehensive and easy to follow, making it easy to learn and use."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n* How do we create instances of our classes in the application?\n* How to pass dependencies to the constructors?\n* How to ensure that some objects can be shared, while others are created every time they are needed?\n## Decision Drivers\n* We want the object creation to be type-safe to be able to use the code intelligence features of the IDE,\nthe strict type checking of PHP and the static analysis capabilities of PHPStan.\n* We want the object creation to support adhering to the SOLID principles:\n* The single responsibility principle will lead to lots of classes, depending on each other in a hierarchy.\nAlso, our instance creation solution should separate the concerns of instantiation from what the classes are doing.\n* The Open-Closed principle, when solved with interfaces, makes it necessary to create different instances for the\nsame interface and support switching out instances, e.g. for testing.\n* The Dont-Repeat-Yourself principle means we should have one place where classes are instantiated.\n* The Edge2Edge and integration tests need an environment that resembles the production environment as closely as\npossible, while still being able to switch out services to isolate from side effects (database contents, filesystem,\nrandomness, date, etc)\n* We want to keep the public API of our code (the methods exposed to the web and command line framework code) as small\nas possible. Ideally, the API exposes only the use cases.\n\n## Decision\n","* We want the object creation to be type-safe to be able to use the code intelligence features of the IDE,\nthe strict type checking of PHP and the static analysis capabilities of PHPStan.\n* We want the object creation to support adhering to the SOLID principles:\n* The single responsibility principle will lead to lots of classes, depending on each other in a hierarchy.\nAlso, our instance creation solution should separate the concerns of instantiation from what the classes are doing.\n* The Open-Closed principle, when solved with interfaces, makes it necessary to create different instances for the\nsame interface and support switching out instances, e.g. for testing.\n* The Dont-Repeat-Yourself principle means we should have one place where classes are instantiated.\n* The Edge2Edge and integration tests need an environment that resembles the production environment as closely as\npossible, while still being able to switch out services to isolate from side effects (database contents, filesystem,\nrandomness, date, etc)\n* We want to keep the public API of our code (the methods exposed to the web and command line framework code) as small\nas possible. Ideally, the API exposes only the use cases.\nWe chose the ""Central Factory"" (a class called `FunFunFactory`) because type-safety and IDE comfort (auto-completion)\ntrumped the concerns about boilerplate. We also saw not binding to a specific library or framework a plus.\nWe're still using the Pimple DIC internally to get shared objects. We wrapped the access to instances in Pimple with type-safe in getters.\nIn hindsight, this use of Pimple as a holder of shared objects turned out to be a bad\nidea because the creation functions (how we build objects) are too far away from the code that requests them and there\nis no easy way to jump from a getter to the creation function. When we realized that, we switched from Pimple to the\n`createSharedObject` function, but did not refactor all creation functions.\nWe use naming conventions to give the consumers a hint if an object is shared (a singleton) or newly created: methods\nstarting with `get` return shared objects, methods with `new` return ""fresh"" objects on every call.\nWe extracted the use cases into bounded contexts at a late stage of the project. We attempted to split the factory\nfor use cases into  ""ContextFactory"" classes, but that attempt was half-hearted and difficult to do, since some of the\nunderlying dependencies (db connection, entity manager) only make sense when an application uses a bounded context.\nThe `FunFunFactory` is has become long and disorganized. Unfortunately, because we did not pay attention to boundaries\nacross bounded context or abstraction layers. We did not apply the naming conventions consistently.\n`FunFunFactory` contains setters that make it possible to switch out implementations for testing. They are also used to set\nup environment-specific services like logging (See the classes in the `WMDE\Fundraising\Frontend\Factories\EnvironmentSetup`\nnamespace). From an architectural perspective this is bad, since it potentially allows controllers to switch out their\nservices, which shouldn't be allowed.\n","**Dependency Injection Container (DIC)**\n\n**Why:**\n\n* **Type Safety:** DIC ensures type-safe object creation, using dependency injection to provide explicit dependencies to classes.\n* **SOLID Principles:** DIC separates instantiation from class functionality, adheres to the single responsibility and open-closed principles, and promotes DRY by centralizing object creation.\n* **Testing:** DIC allows easy switching of dependencies for testing, isolating services and reducing side effects.\n* **API Minimization:** By centralizing object creation, DIC keeps the public API minimal and focused on use cases."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nCurrently, both the client-side code (JavaScript and SCSS) and the\nserver-side code are in the same repository. This leads to long build\ntimes and unneeccessary CI runs if either of the code parts change.\nAlso, having both ""parts"" in the same repository makes it harder to talk\nabout each part, because we used the terminology of the Fundraising\nDepartment, which called the Fundraising Application ""Frontend"" and the\nFundraising Operation Center ""Backend"". For us as developers, ""Frontend""\nand ""Backend"" have different meanings.\n## Decision Drivers\n* Improve separation of concerns\n* Allow parallel development of two distinct and independent domains\n* Have two separate CI steps for each part\n* Unblock the potential creation of a server-side JSON-based API\n* Reduce deployment time for the backend repository\n\n## Decision\n","* Improve separation of concerns\n* Allow parallel development of two distinct and independent domains\n* Have two separate CI steps for each part\n* Unblock the potential creation of a server-side JSON-based API\n* Reduce deployment time for the backend repository\nSince the tweaking of the build system would introduce addionional\ncomplexities in the build and CI system and would not solve the naming\nconfusion, we decided to split the repository in two, named\n`fundraising-app` for the server-side code and `fundraising-app-frontend`\nfor the client-side code. We'll also take the opportunity to move the code\nform GitHub to GitLab.\nWe track the renaming of the `fundraising-backend` repository to\n`fundraising-operation-center`\nin a [separate ticket](https://phabricator.wikimedia.org/T246796)\n","Move both code parts into separate repositories. The client-side repository should be called ""fundraising-frontend"" and the server-side one ""fundraising-backend""."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWith the Fundraising App split into two repositories (see [ADR\n017](./017_Split_Repositories.md)), we need to deploy two specific\nbranches of the repositories to test and production. We need a mechanism\nor process to define which two branches should be deployed together.\n## Decision Drivers\n* **Speed**: Currently, the deployment playbook builds the assets of the\nfrontend branch from scratch. If we could use pre-built assets, the deployment\nwould be faster.\n* **Developer experience**: The app won't work as expected or represent\nthe required test state if we deploy the wrong branches together. Our\nsolution should prevent such an ""out of sync"" scenario as good as\npossible while at the same time not requiring too many manual steps from\nthe developers.\n* **Traceability**: We should be able to check which branches were used\nfor a deployment.\n\n## Decision\n","* **Speed**: Currently, the deployment playbook builds the assets of the\nfrontend branch from scratch. If we could use pre-built assets, the deployment\nwould be faster.\n* **Developer experience**: The app won't work as expected or represent\nthe required test state if we deploy the wrong branches together. Our\nsolution should prevent such an ""out of sync"" scenario as good as\npossible while at the same time not requiring too many manual steps from\nthe developers.\n* **Traceability**: We should be able to check which branches were used\nfor a deployment.\nChosen option: ""Decide at deploy time"", because it's the least amount of\ninitial effort and the least amount of ongoing effort for each client-side change.\nWe accept the additional risks of making mistakes at deploy time. If we\nmake too many mistakes, we'll develop checks to mitigate them.\n","The Fundraising App should use a well-defined deployment branch strategy and enforce it with a Git pre-receive hook. Each environment where the app is deployed is associated with a specific pair of branches, one from each repository. A Git pre-receive hook enforces the deployment branch strategy. Every push to the Fundraising App repositories is first validated by the hook. If the push attempts to modify one of the deployment branches in a way that doesn't adhere to the strategy, the hook rejects the push."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe ""Fundraising Frontend Content"" repository is a git repository where the Fundraising Department can make edits to the translated messages and texts of the Fundraising Application. Those changes get deployed automatically, independently from the code deployments.\nThe [`wmde19` skin](008_Client_Side_Rewrite.md) uses client-side rendering and the [Vue i18n](https://kazupon.github.io/vue-i18n/) plugin for translating messages. There are several possibilities to get the translated strings into the client-side code:\n1. Importing it directly in JavaScript, with an `import` statement. This requires a continuous delivery pipeline that creates a new client-side code bundle on every content change.\n2. Loading it asynchronously when the client-side code loads. This has the benefit of working out of the box, but the drawback of an additional HTTP request.\n3. Reading the file on the server side and putting its contents in a HTML [data attribute](https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes) where the bootstrap code will read it an inject it into the i18n plugin.\n\n## Decision\n","Since we don't have the engineering resources to create a continuous delivery pipeline, only options 2 and 3 remain. We choose the data attribute method for performance reasons: We want one less HTTP request and the size of the messages is acceptable: At the time of writing this ADR, it's 30K uncompressed, 7K compressed.\nWe want to keep the message size down and implement the server-side code in a way that allows for splitting the messages into ""common"" and page-specific bundles.\n",Use option 3: read the file on the server side and put its contents in a HTML data attribute where the bootstrap code will read it and inject it into the i18n plugin.\n\nThis option has the following advantages:\n\n- No need for a continuous delivery pipeline.\n- No additional HTTP request.\n- Works out of the box.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe decision was made in [ADR 21](https://github.com/wmde/fundraising-application/blob/main/doc/adr/021_Single_or_Multi_Page_Application_Architecture.md) to move towards an API and Single-Page-Application. This document covers the decisions made around the design of the API.\n## Decision Drivers\n* Maintainability - The API should be easy to maintain.\n* Consistency - The API controllers should be consistent in how they handle requests and send responses.\n* Accessibility - The API should be well documented.\n* Separation of concerns - We want to separate our domain classes\n(entities, use cases) from the framework. Domain classes MUST NOT\nhave framework-specific annotations or attributes.\n\n## Decision\n","* Maintainability - The API should be easy to maintain.\n* Consistency - The API controllers should be consistent in how they handle requests and send responses.\n* Accessibility - The API should be well documented.\n* Separation of concerns - We want to separate our domain classes\n(entities, use cases) from the framework. Domain classes MUST NOT\nhave framework-specific annotations or attributes.\n* We will use Swagger for the API documentation. This will eventually be the source of truth.\n* We will eventually use the Swagger yaml files to generate automated tests.\n* The API URL schema should be REST-ish: We define resources like ""donation"", ""comment"", ""membership"", ""payment"", etc. and use HTTP verbs to retrieve or change the resources. Internally, we map each resource to a controller and each HTTP verb to an action in the controller. We call these ""resource controllers"".\n* Symphony has no resource controllers, instead we follow the [Laravel style guide](https://laravel.com/docs/8.x/controllers#actions-handled-by-resource-controller) for controller [method naming](#controller-actions).\n* Response Status' are HTTP codes.\n* Responses and Requests are JSON.\n* POST/PUT requests contain the data as a JSON object in the Request body.\n* Responses also return JSON in the body.\n* API response messages are verbose snake_case strings. This means they can be read by humans and machines.\n* UseCases should return data objects and not domain entities.\n* Entity create and save should return the updated entity in the response.\n* Keys in the responses are camelCase. This allows us to serialise our PHP data entities without having to transform the property names into snake_case.\n### Controller Actions\n| HTTP Method | URL | Controller Method |\n|:--|:--|:--|\n| GET | `/entity` | `index()` |\n| POST | `/entity` | `store()` |\n| GET | `/entity/{id}` | `show()` |\n| PUT | `/entity` | `update()` |\n| DELETE | `/entity/{id}` | `destroy()` |\n","**Decision:** Use a RESTful API with a consistent and well-documented controller design.\n\n**Rationale:**\n\n* RESTful APIs are widely adopted, easy to maintain, and provide a consistent interface for clients.\n* Consistent controller design ensures that requests are handled and responses sent in a predictable way, reducing cognitive load for developers.\n* Thorough documentation improves accessibility and reduces the learning curve for new users.\n* Separation of concerns allows us to maintain the integrity of our domain classes and prevent framework-specific code from polluting our domain logic."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\n* ""Slashy"", pseudo-[REST][1]-style URLs like `/donation/add`\n* ""action-sentences"" like `/apply-for-membership`\n* a combination of both like `/contact/get-in-touch`\nWe don't do search engine optimization (SEO) in the form of ""meaningful, localized and stable URLs"", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\n\n## Decision\n","We will use the ""action-sentence"" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\nOur reasoning behind the decision:\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\n* We can still have a dedicated REST API in the future, by using the `/api` route.\n* The sentence style fits better to our use case architecture, which also read more like sentences.\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional ""endpoint"" like `donation/update`, then we don't need a redirect.\nIf we need to add i18n information to the URL at some point, we will do it with a ""subdirectory prefix"", e.g. `/de/apply-for-membership`, `/en/apply-for-membership`. The cons listed at https://support.google.com/webmasters/answer/182192?hl=en do not outweigh the benefits.\n","Migrate all URLs to a consistent ""slashy"" style."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\nand was only noticed some days after when the Project Manager needed to\nexport the data. Upon investigation, it was discovered that:\n* Error logging was inactive on the application. (Now fixed)\n* We can’t debug using the responses our system returned to PayPal as\nwe don’t have access to the IPN log.\nThis led to a situation where we couldn't get the information required\nto debug the error. It was suggested we queue all incoming requests from\nPayPal on our own system for processing by our system.\n## Decision Drivers\n* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\n\n## Decision\n","* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\nSince this was the first occurrence of the problem, and the Fundraising\nApplication system is now running well again, we decided against introducing\nthe IPN queue feature.\n",**Implement a queue to store incoming IPNs from PayPal for debugging purposes.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe original Fundraising Application was a classic web-based Multi-Page-Application with some JavaScript for progressive enhancement and state stored on the Server. The 2016 rewrite introduced some state on the client. The new skin we introduced in 2019 renders the template variables in an HTML data attribute, but the client-side markup is rendered with Vue. It is a ""hybrid"" Multi-Page-Application, where the client-side code still depends on the server side variables, but the server-side ""templating"" is obsolete.\nThe current architecture has the following drawbacks:\n* Switching pages does a full page reload\n* Our naive client-side ""subpage"" mechanism breaks the browser history (see https://phabricator.wikimedia.org/T285046 )\n* Components shared across pages need to be compiled into the entry point\nfor each page.\n* We need ""Full Stack"" Developers who are familiar with backend\ntechnologies (PHP, Symfony, databases, PHP CI tools, Docker) and frontend technologies\n(TypeScript, Vue, SASS, bundler and CI tools).\n## Decision Drivers\n* User experience - fast page load times, browser history, immediate feedback of\nwhat's happening, keep focus on current task.\n* Developer experience (ease-of-use, fewest dependencies possible)\n\n## Decision\n","* User experience - fast page load times, browser history, immediate feedback of\nwhat's happening, keep focus on current task.\n* Developer experience (ease-of-use, fewest dependencies possible)\nA separation into an API and Single-Page-Application looks like the best\noption, regarding our decision drivers:\n* It's improving on an already good user experience.\n* It won't add complexity for developers beyond the status quo\n* It's ""open"" enough to improve our technology stack, we don't tie\nourselves to a specific library.\nA Server-Rendered Multi-Page-Application with progressive enhancement\nlooks like a promising architecture for the far future. We will keep\nobserving the available technologies.\n","The decision is to migrate the Fundraising Application to a Single-Page-Application.\n\n**Benefits of a Single-Page-Application:**\n\n* **Improved user experience:** Faster page load times, maintained browser history, immediate feedback, and focus retention.\n* **Enhanced developer experience:** Simplified codebase, reduced dependencies, and improved developer productivity.\n* **Reduced maintenance costs:** Elimination of server-side rendering and dependency management for multiple entry points.\n* **Increased flexibility:** Ability to easily introduce new features and handle dynamic content.\n* **Improved collaboration:** Clear separation of frontend and backend responsibilities, enabling better collaboration between teams.\n\n**Implementation Plan:**\n\n* **Step 1: Migrate shared components to a shared library**\n* **Step 2: Implement client-side routing**\n* **Step 3: Refactor pages to use components and client-side routing**\n* **Step 4: Remove server-side rendering**\n* **Step 5: Implement necessary backend changes for API-based communication**\n\n**Timeline:**\n\n* **Phase 1: Migration of shared components (Q1 2023)**\n* **Phase 2: Implementation of client-side routing (Q2 2023)**\n* **Phase 3: Refactoring of pages (Q3 2023)**\n* **Phase 4: Removal of server-side rendering (Q4 2023)**\n* **Phase 5: Backend API implementation (Q1 2024)**\n\n**Monitoring and Evaluation:**\n\n* Track key performance indicators such as page load times, browser history functionality, and user feedback.\n* Conduct regular code reviews to ensure maintainability and performance.\n* Seek feedback from stakeholders to assess the effectiveness of the new architecture."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are rewriting our client-side from scratch. That gives us the opportunity to restructure and rethink our css classes.\nWe discussed two CSS schemas `naming classes as components` vs `naming classes according to visual properties`.\nNaming classes as components - This will make it more clear where the css classes are used, but will duplicate what we already reflect in our markup. A variation of this would be to use the [Block-Element-Modifier (BEM)](http://getbem.com/introduction/) pattern.\nNaming classes as styling - This will make the CSS classes reusable and fits well with the naming schema of our CSS framework, [Bulma](https://bulma.io/). Examples of this schema are the [Atomic CSS](https://github.com/nemophrost/atomic-css) schema and [Tailwind CSS](https://tailwindcss.com/docs/what-is-tailwind/). There were several small concerns with this style:\n* The style makes our markup less performant: because each element has several classes, the resulting size will be bigger. Since the markup will be compressed anyway and the compression algorithm is efficient for reducing repeating patterns, this is not an issue.\n* We might end up with ""lying"" class names, e.g. `.is-red { color: blue}`. This will be mitigated by the style names having a slightly higher and more purpose-oriented schema, e.g. `.primary-color`. We strive to achieve a sweet spot between the extremes of BEM and Atomic CSS.\n* The added classes make the HTML source less readable. ""Readability"", especially when compared to BEM, is kind of subjective.\n\n## Decision\n","We will name CSS classes according to the styling they produce instead of the component they are used in. Adding component class names to Vue components feels like a violation of the [DRY principle](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nWe will share as much of those styling classes between components as possible, instead of each component defining their own styles.\n",We decided to name CSS classes according to visual properties.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDuring the modernization of the client-side code outlined in ""[ADR 006 - Using Vue.js](006_Vue.js.md)"" and the first prototype for that project - the Address Change Form - we also tried out using [TypeScript](https://www.typescriptlang.org/). After the project we evaluated if we want to continue to use TypeScript for client side code. We collected following arguments.\n### Learning Curve\nNobody of us has experience with TypeScript, there will be a learning curve and possible slowdowns while setting up features.\n### Migration Pain\nMigrating the existing JavaScript code to TypeScript looks daunting. However, we're already planning a rewrite of the the Frontend with Vue and Vuex and want to spare us the pain of another ""rewrite"" with Typescript. Instead, all code will be written in TypeScript.\n### How ""Future-Proof"" is TypeScript?\nWith the decline of CoffeeScript in our minds, we don't know if TypeScript is ""just a fad"" and we'll have to port the client-side code to another language in 3-5 years. We're optimistic that TypeScript will still be in active development in the Future, since it's both open source and has Microsoft as a corporate ""sponsor"" behind it.\n### How well-established is TypeScript?\nCompared to JavaScript there are less tutorials and reference articles. However, since TypeScript is ""JavaScript with types"", we can apply all the existing material. We are confident that the TypeScript-specific documentation will improve. We are hopeful that the `tslint` linter will get feature-parity with `eslint`.\n### Onboarding\nHaving to learn TypeScript might slow down the onboarding process of new developers.\n### ""Sunk Cost""\nWe now have working code that uses TypeScript. Removing it would incur additional effort. In terms of effort spent, we encountered biggest time sink already - the initial setup of the test and build environment. We can build on that from now on.\n### Maintainability and Readability\nTypescript will make our code more maintainable, because it helps us to avoid low-level type errors during code changes. The TypeScript interfaces document our intentions and our domain language. Our experiences with PHP, moving from code without type hints to code with type hints support that argument. This is also the main argument and driver of the final decision.\n### Tooling support\nTyped code helps IDEs to show errors while writing the code and allows for easy refactoring and code navigation.\n### Vue.js 3.0\nVue.js 3.0 is rewritten in TypeScript. We assume there will be benefits from using TypeScript as well and that their decision is based on criteria that we are not even aware of yet (see ""Learning Curve"").\n\n## Decision\n","* We will use TypeScript as our client-side language, because the benefit of maintainability is the most important argument for us.\n* We will document the personal ""best practices"" in our team, documenting the structure of our client side code and how we write TypeScript.\n","We decided to use TypeScript for all new client-side code. The main reason for this decision is the improved maintainability and readability of the code. TypeScript will help us to avoid low-level type errors during code changes, and the TypeScript interfaces will document our intentions and our domain language.\n\nWe are aware of the learning curve associated with TypeScript, but we believe that the benefits of using TypeScript outweigh the costs. We are also confident that the TypeScript community will continue to grow and that the tooling support for TypeScript will improve.\n\nWe will continue to monitor the development of TypeScript and Vue.js 3.0, and we will reconsider our decision if there are any significant changes in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTo prepare for the transition from the Silex to the Symfony framework we need to convert the anonymous route functions to controller classes. This document is about how to name those classes and their methods.\nSome anonymous route functions already had classes, called ""RouteHandlers"", with a single method called `handle`. Some controllers have adopted this scheme as their action method name, while others have more informative method names.\nThe state of the controller naming(October 2020): Most of the controllers have an `ActionSentenceController::action` schema with only one action, two controllers, `ApplyForMembershipController` and `ContactController` have a ``NounController::action`` schema.\nSome of the code in the controllers (e.g. validation) might become obsolete if we use more Symfony features, making the controllers shorter.\n## Decision Drivers\n* **Consistency** - We want to have some general rule of thumb to apply to all controller classes.\n* **Developer Experience**\n* We want to jump quickly to a controller class via the code search features of the IDE.\n* We want the controllers to be as short as possible, avoid scrolling.\n* We want to easily navigate to the right controller in the file system, keeping the number of files in a directory/namespace as small as possible.\n* **Following established standards** - We want to do what other frameworks are doing\n* **Gradual refactoring** - The naming changes must not force the transition form Silex to Symfony by choosing a pattern that's incompatible with Silex.\n\n## Decision\n","* **Consistency** - We want to have some general rule of thumb to apply to all controller classes.\n* **Developer Experience**\n* We want to jump quickly to a controller class via the code search features of the IDE.\n* We want the controllers to be as short as possible, avoid scrolling.\n* We want to easily navigate to the right controller in the file system, keeping the number of files in a directory/namespace as small as possible.\n* **Following established standards** - We want to do what other frameworks are doing\n* **Gradual refactoring** - The naming changes must not force the transition form Silex to Symfony by choosing a pattern that's incompatible with Silex.\nTBD: Chosen option: """", because (justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | … | comes out best (see below)).\n","**Decision:**\n\n**Controller Class Naming:**\n* Prefer `Controller` as the class suffix.\n* Use the PascalCase naming convention and prefix the class name with the controller's namespace.\n* If there are multiple actions in a controller, append the action name to the class name, separated by an underscore.\n* Use the same naming convention for RouteHandlers.\n\n**Controller Method Naming:**\n* For controllers with a single action, use `__invoke` method.\n* For controllers with multiple actions, use `action[ActionName]` method.\n\n**Examples:**\n\n**Controller Class:**\n\n* `App\Controller\UserController`\n* `App\Controller\UserController` (single action)\n* `App\Controller\UserController_Edit` (multiple actions)\n* `App\RouteHandler\UserRouteHandler`\n\n**Controller Method:**\n\n* `__invoke` (single action)\n* `actionEdit` (multiple actions)"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe fundraising department wants to address donors and members in emails\nand other communication with a personalized greeting, tailored to the\npreferred way people want to be adressed.\nOur external data warehousing provider only allows for a limited selection\nof three German genders: ""Herr"", ""Frau"" and ""Divers"".\nWe want to keep the business logic (bounded contexts) free from gender\nnorms and I18N concerns. We also want to keep the UI extensible (for\nfuture options in the selection field) and translatable.\n\n## Decision\n","We put salutation information into the content repository that contains\nthe following information for each of the three possible salutations:\n- The ""label"", used in the selection field in forms and for display on\nconfirmation pages.\n- The ""value"", used in the domain (will probably the same as label)\n- Translation keys for addressing in varying degrees of formality\nOur frontend code will use the label and value to construct selection fields.\nOur server code will use the value and the translation keys for creating\nemails and writing to the database. The export script will map the\ndifferent values from the database to the allowed 3 values for the data\nwarehousing provider.\n","Use a repository pattern to store the preferred salutations and map them to the three genders allowed by the external data warehouse provider. The repository should be responsible for converting the preferred salutation to the appropriate gender for the external data warehouse provider. This will allow the business logic to remain free from gender norms and I18N concerns, and will also keep the UI extensible and translatable."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn 2017, the fundraising frontend was redesigned, to be more mobile-friendly and more aesthetically pleasing. The internal name of that design was `cat17`. Unfortunately, it did not gather as many donations as the ""old"" design, called `10h16`. UX engineering, product management and stakeholders from the fundraising department decided against going ""back"" to the mobile-unfriendly and hard-to-maintain `10h16` design and instead improving the existing `cat17` design by applying UX best practices and making educated guesses at what made `10h16` so attractive to donors.\nThe engineering team, planning the modernization of the frontend code base as outlined in ""[ADR 006 - Using Vue.js](006_Vue.js.md) and ""[ADR 007 - Using Typescript](007_Typescript.md) now had to choose between an *evolution* of `cat17` and a *rewrite* of the client-side code. The benefits of one approach would be the drawbacks of the other, so the following sections will describe each approach through the lens of benefits.\n### Benefits of an evolution of `cat17`\nBy applying small, incremental changes to the existing design, we are following good agile practices - always delivering value, always being able to stop what we're doing and always having a running application.\nWe don't risk breaking something that already works (ui/ux wise). [A rewrite always carries the risk of throwing away past bugfixes, ""microfeatures"", lessons learnt and implicit organizational knowledge](https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/).\nWe can improve the skin ""step by step"" so to speak. First implement the Vue components, then get rid of Bootstrap 3 and replace it with Bulma possibly, then transform our CSS structure into a more meaningful one.\nThis approach doesn't stop us from developing new features if need be. If we have more than 1 or 2 skins, the cost of UI changes due to new features becomes prohibitive.\nWe have already invested effort in experiments (i.e. the BankData component in `cat17`) that show the evolution from the old code base to Vue components.\nWe are avoiding the temptation to test 3 different designs (`10h16`, `cat17` and `new`) against each other. From a UX perspective, testing 3 designs does not make sense, however experience shows that if the option is there, it might be done.\n### Benefits of a rewrite\nThe new design ditches some behaviors and styles of `cat17`. A rewrite avoids having to re-implement what we might later throw away.\nThe Javascript architecture of `cat17` makes it hard to find the right place for improvements. Rewriting with Vue.js and Typescript makes sure all developers have the same knowledge about the code base and can look up patterns and examples in the official documentation. See ""[ADR 006 - Using Vue.js](006_Vue.js.md) and ""[ADR 007 - Using Typescript](007_Typescript.md).\nThe CSS organization of `cat17` does not contain enough abstractions, the CSS is very tailored to each individual page. A rewrite allows us to discover the inherent components and their hierarchy, avoiding duplication and creating a consistent design language.\nA rewrite that does not have to integrate with existing client-side code can be done faster than transitioning to Vue.js through incremental refactorings. A transition would require additional compatibility layers in the architecture during the transition phase. Those layers would affect the performance and make the code base even harder to understand. When the transition is finished, the compatibility layers would have to be removed, making the effort spent on them ""wasted"". The compatibility layers would also affect the overall architecture: They could improve it by making everything decoupled and testable, on the other hand could make it ""non-standard"" and avoid the easy integration of data model and view thet VueX and Vue offer.\nThe build and dev system are easier to set up: We want to use [`vue-cli`](https://cli.vuejs.org/) which generates useful and usable webpack configurations for development and testing.\n### General risks\nRegardless of the approach (evolution or rewrite), it could happen that the new design shows no improvement in donations. The logical consequence would be to stay with 10h16 or to reimplement 10h16 with the new technology. This is the worst-case scenario and we can not fully mitigate the risk. The risk has two factors: The new technology negatively affecting the user experience and the user experience itself. Both factors have mitigation strategies.\n\n## Decision\n","Given the benefits of knowledge sharing and the assumptions about a greater velocity, we decide to do a rewrite of the client-side code.\nTo avoid the technology (especially client-side rendering) affecting the user experience we will measure the functionality and performance of the new design throughout the transition. The best performance metrics of both `10h16` and `cat17` are the baseline for the performance budget of the new design.\nWhen the design has feature-parity with `cat17` and has been successfully deployed to production, we will delete `cat17`.\nWhen the new design has proven successful (i.e. the donation rate is at least not worse than with `10h16`), we will remove `10h16`.\n","**Evolve the existing `cat17` design.**\n\nThis decision is based on the following considerations:\n\n* **Lower risk:** Evolving the existing design is less risky than a complete rewrite, as it avoids the risk of breaking something that already works.\n* **More flexibility:** An evolutionary approach allows for small, incremental changes to be made over time, which provides more flexibility and allows for feedback to be incorporated into the design process.\n* **Avoids duplication:** By evolving the existing design, the team can avoid duplicating work that has already been done in the `cat17` design.\n* **Takes advantage of existing knowledge:** The team already has experience with the `cat17` design, which will help them to evolve it effectively.\n* **Faster time to market:** An evolutionary approach is generally faster than a complete rewrite, which will allow the team to get the new design to market sooner."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMost of the JavaScript Code on the FundraisingFrontend already has the [[ https://facebook.github.io/flux/docs/in-depth-overview.html#content | Flux architecture ]] with its one-way data flow (using [[ https://redux.js.org/ | Redux ]]), but it does not use one of the popular reactive ""component"" frameworks like Vue or React. Instead, it uses self-written ""components"" (two-way binding with handling of DOM events) and ""view handlers"" (one-way binding DOM manipulators) that are tied to the markup of the page via jQuery, in an attempt to do progressive enhancement and decouple the markup from the functionality.\nThe current JavaScript code has several drawbacks:\n* **JavaScript resource size.** The components and stores are built as one big ""library"" file (called `wmde.js`, with a global object called `WMDE`), instead of having separate ""entry points"" for the different pages (donation, membership). The current ""entry points"", `donationForm.js` and `membershipForm`, add one more JavaScript resource that the browsers needs to download, adding HTTP overhead and latency.\n* **Hard to understand.** While the [[ https://github.com/wmde/FundraisingFrontend/blob/master/doc/HOWTO_Create_a_form.md | architecture is documented ]], the setup lacks the in-depth explanations, code snippets and tutorials that common frameworks come with. Also, the code itself does not use modern ECMAScript features (classes, spread operator, arrow functions). Instead it uses  custom, factory-function based ""classes"", making the code harder to understand.\n* **Hard to extend.** Where Vue and React have ecosystems attached to them, all code for the current JavaScript - asynchronous validation, connecting the store to the views, etc - is custom, adding to the accidental complexity and maintenance burden.\n* **Hard to reuse.** While the self-written components are quite flexible and ""pluggable"", the entry point scripts are very long and hard to understand, since there is no hierarchy of elements. Instead, they are a big factory that initializes all the classes. That initialization code is duplicated across scripts.\n\n## Decision\n","Going forward, we will use Vue to render the frontend. We chose it for the following reasons:\n* It's a mature, tested, well-documented widely used framework with an open source license and an active ecosystem\n* There is already some knowledge about Vue in the developer and UX teams\n* Vue is used in wikidata, making knowledge sharing easier.\nWe will write all new features in Vue and ECMAScript 2015 (and later), refactoring and cleaning up the existing code base. The redux ""reducers"" and ""state aggregators"" will move to Vuex modules, while the Twig templates, ""view handlers"" and ""components"" of the old code base become Vue components.\n","**Migrate to Vue or React**\n\nUse a popular reactive ""component"" framework like Vue or React to replace the current custom JavaScript components and view handlers. This will address the drawbacks of the current codebase, including:\n\n* **Reduced JavaScript resource size:** Separate entry points for different pages will eliminate the need for a large ""library"" file.\n* **Improved readability and maintainability:** Modern frameworks come with comprehensive documentation and developer toolchains that make code easier to understand and debug.\n* **Enhanced extensibility and reusability:** Frameworks like Vue and React have established ecosystems with libraries and plugins that can streamline development and promote code sharing.\n* **Improved separation of concerns:** Component-based frameworks enforce a clear separation between presentation logic (components) and data management (store)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe need a way to deploy specific versions of our bounded contexts, having\nreproducible releases of our software in testing and production .\n## Decision Drivers\n* We want to have frequent releases of the software, adding new features\netc.\n* We want to keep our dependencies up to date on a monthly basis, using\n`composer update`\n* We want to have reproducible versions of our software - `composer\ninstall` must install the same code on the CI machine, each\ndevelopers machine, the user acceptance environment and the production\nenvironment.\n* We want to have a trunk-based development process, where the current\nmaster of each repository is always working and we can deploy at any time.\n* We regularly update the `wmde/fundraising-frontend-content` dependency.\n\n## Decision\n","* We want to have frequent releases of the software, adding new features\netc.\n* We want to keep our dependencies up to date on a monthly basis, using\n`composer update`\n* We want to have reproducible versions of our software - `composer\ninstall` must install the same code on the CI machine, each\ndevelopers machine, the user acceptance environment and the production\nenvironment.\n* We want to have a trunk-based development process, where the current\nmaster of each repository is always working and we can deploy at any time.\n* We regularly update the `wmde/fundraising-frontend-content` dependency.\nWe will use semantic versioning for 3 months (until 2020-08-14). After\nthat period we'll evaluate the actual benefits and drawbacks of semantic\nversioning. If the drawbacks outweigh the benefits and we can't fix them\nthrough other means (automation, CI), we'll abandon semantic\nversioning and return to trunk-based dependencies.\n","**Decision:** Use Git tags to denote releases.\n\n**Rationale:**\n\n* Git tags are a lightweight and straightforward way to mark specific versions of code.\n* They are easy to create and manage, and they provide a clear and immutable way to identify specific releases.\n* By using Git tags, we can ensure that the code deployed to testing and production is the same code that was built and tested in the CI environment.\n* This will help us to achieve our goal of having reproducible versions of our software.\n* Additionally, using Git tags will not interfere with our trunk-based development process.\n\n**Consequences:**\n\n* We will need to update our deployment process to use Git tags.\n* We will need to train our team on how to use Git tags.\n* We may need to make some changes to our CI pipeline to support Git tags."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\n* Write our own validation logic.\nThe arguments in favor of writing our own logic were:\n* We don't want to bind our domain layer to a concrete validation library implementation.\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\n* We don't know the ""maintenance cycles"" of the library, either we need to constantly update or the library is not maintained properly.\n* Every developer would have to learn the API of the external library.\nAt the start of the project we did not know where we should put the validation logic:\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\n* At the use case layer, making validation part of the use case.\n\n## Decision\n","For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https://github.com/wmde/fundraising-frontend-content/blob/test/i18n/de_DE/messages/validations.json) in the [content repository](https://github.com/wmde/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\n",Use the Symphony Validation library for validation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want to automatically deploy the fundraising content repository\nwhenever there is a change (i.e. a Git commit on the `test` or `production`\nbranch) to the destination servers associated with the branch.\n## Decision Drivers\n* For security reasons, we want to host the system that does the\ndeployment on our own infrastructure.\n* The repository consists of several files. We want the deployment to be\n*atomic*, i.e our application either uses the new set of files or the\nold one, but never a mix.\n* We have three production servers that need to receive the new version\nsimultaneously (although not atomically).\n* We would like the system to notify us (via email) when a deployment\nfailed.  We would like to have a record (e.g. log file) of successful\ndeployments.  We have an Ansible playbook that does the atomic\ndeployment to the test and production servers.\n* We have a `Dockerfile` definition that packages Ansible, the deployment\nplaybooks and our server configuration into a standalone Docker image.\nThe deployment software can run this Docker image to do the deployment.\n* We prefer ""configuration as code"" over setting up workflows in a GUI.\n* The maintenance and onboarding effort for the system should be as low\nas possible.\n* (Optional) It would be nice if we had a ""button"" to trigger the\ndeployment\n* (Optional) It would be nice if feature branches of the application\nrepository could be associated with feature branches in the content\nrepository (instead of using `test`).\n\n## Decision\n","* For security reasons, we want to host the system that does the\ndeployment on our own infrastructure.\n* The repository consists of several files. We want the deployment to be\n*atomic*, i.e our application either uses the new set of files or the\nold one, but never a mix.\n* We have three production servers that need to receive the new version\nsimultaneously (although not atomically).\n* We would like the system to notify us (via email) when a deployment\nfailed.  We would like to have a record (e.g. log file) of successful\ndeployments.  We have an Ansible playbook that does the atomic\ndeployment to the test and production servers.\n* We have a `Dockerfile` definition that packages Ansible, the deployment\nplaybooks and our server configuration into a standalone Docker image.\nThe deployment software can run this Docker image to do the deployment.\n* We prefer ""configuration as code"" over setting up workflows in a GUI.\n* The maintenance and onboarding effort for the system should be as low\nas possible.\n* (Optional) It would be nice if we had a ""button"" to trigger the\ndeployment\n* (Optional) It would be nice if feature branches of the application\nrepository could be associated with feature branches in the content\nrepository (instead of using `test`).\nChosen option: ""Drone CI"", because the benefits outweigh the drawbacks (see below).\n",Use GitLab CI/CD with Docker as the deployment software.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nCreating a central factory has served us well, but has drawbacks, as\noutlined in [ADR 013](013_Main_Factory.md). This document is about\nexploring ways to mitigate those drawbacks, while keeping the benefits of\ntype-safety and an architecture based on the SOLID principles.\n## Decision Drivers\n* FunFunFactory is a long class with too many extension\npoints for tests and exposing too many instances.\n* Ideally, the class should not have any branching logic. Without\nbranching logic, the only test we need for this class is that it\nproduces the right instances. We do this with our edge-to-edge\n(integration) tests.\n* We will switch our web framework layer from the discontinued Silex\nframework to Symfony. Symfony comes with a powerful [Dependency\nInjection Container\n(DIC)](https://symfony.com/doc/current/components/dependency_injection.html)\nthat has the same purpose as our factory.\n\n## Decision\n","* FunFunFactory is a long class with too many extension\npoints for tests and exposing too many instances.\n* Ideally, the class should not have any branching logic. Without\nbranching logic, the only test we need for this class is that it\nproduces the right instances. We do this with our edge-to-edge\n(integration) tests.\n* We will switch our web framework layer from the discontinued Silex\nframework to Symfony. Symfony comes with a powerful [Dependency\nInjection Container\n(DIC)](https://symfony.com/doc/current/components/dependency_injection.html)\nthat has the same purpose as our factory.\nThe presented options are don't exclude each other, they are  the start\nand end state of a refactoring. In reality, we will start with injecting\nthe FunFunFactory (without refactoring it) into the controllers. The next\nstep is to swap the factory and inject the services instead, defining the\nFunFunFactory as the service creating factory in the service configuration\nfile. Over time, we will define more and more services in the service\nconfiguration, extracting them from FunFunFactory, until we can delete it.\n","**The decision is to use Symfony's dependency injection container (DIC) to replace the central factory.**\n\n**Benefits**\n\n* The DIC is a lightweight and extensible component that is well-suited for dependency injection.\n* It supports type-safe dependency injection, which helps to ensure that the correct dependencies are injected into each object.\n* It is also easy to test, making it possible to verify that the correct dependencies are being injected.\n\n**Drawbacks**\n\n* The DIC can be more complex to configure than a simple factory.\n* It can also introduce a performance overhead, especially if it is used to inject a large number of dependencies.\n\nOverall, the benefits of using the DIC outweigh the drawbacks. It is a more flexible and extensible solution than a simple factory, and it supports type-safe dependency injection."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe fundraising software is split into several Git repositories, hosted on GitHub: Applications (Fundraising Application and Fundraising Operation Center), Bounded Contexts (containing the business logic of the applications), I18N data.\n### Bounded contexts split into Git repositories\nDuring the rewrite of the Fundraising Application we introduced the concept of ""[Bounded Contexts](https://codeburst.io/ddd-strategic-patterns-how-to-define-bounded-contexts-2dc70927976e)"". To better enforce the architectural separation of the bounded contexts, we put them in separate GitHub repositories. For some time that independence was only surface-level deep because all the contexts depended on the same database ""abstraction"" library (FundraisingStore), but that is now fixed.\n#### Benefits of Bounded Contexts in separate Git repositories\n* Business logic and presentation/ingress layer (web, command line, etc.) are clearly separated.\n* It's nearly impossible to introduce dependencies between the bounded contexts, as that would cause circular dependency warnings.\n* When working on the bounded contexts, the CI runs faster, since the CI tests only the business logic of the bounded context and not the whole integrated application.\n* Features and changes can be worked on independently from the Application - as long as the Applications dependencies are not updated, the changes are not integrated.\n#### Drawbacks of Bounded Contexts in separate Git repositories\n* The code search feature of the IDE does not find classes from dependencies by default. This makes it harder to navigate the code.\n* To work on a feature we need to have several project windows open in the IDE.\n* We have to keep the dependencies (PHP version, CI tool versions, coding style conventions) of the bounded contexts in sync and maintain their CI pipelines.\n* Refactoring class names in bounded contexts leads to errors in the Fundraising Application and the Fundraising Operation Center, where we must make the changes manually.\n* It's too easy to accidentally integrate a change from a bounded context by updating the dependencies.\n### I18N data in a Git repository\nThe I18N data of the Fundraising Application is in a separate repository on GitHub. It has a separate, automated deploy cycle - a Jenkins instance immediately deploys changes in the I18N data to test and production, as opposed to code deploys which happen manually. We chose this deploy mode to enable the Fundraising department to change text in the Fundraising Application themselves, without developer intervention. This leads to problems where we develop a new feature and need to change the i18n data:\n* The necessary I18N changes for a feature might break the existing application.\n* The dev environment has the I18N repository as a dev dependency, leading to frequent but unnecessary package updates.\n### Separation of Fundraising Application and Fundraising Operation Center in different GitHub repositories\nHistorically, the Fundraising Application and Fundraising Operation Center were two different code bases of varying quality. There is duplication between the code bases, with refactoring efforts to move more and more business logic into the bounded contexts which both applications share. But even with shared bounded contexts, both applications have to be deployed in sync whenever the data model changes.\nThe code quality of the Fundraising Operation Center currently is not high enough to be in the same Git repository and on the same ""level"" (directory-wise) as the Fundraising Frontend, it would break too many CI checks. The long-term goal of the dev team is to move the Fundraising Operation Center to the same domain-driven clean architecture like the Fundraising Frontend.\n\n## Decision\n","* When the code quality of the Fundraising Operation Center has become acceptable, we will merge the Fundraising Application, the Fundraising Operation Center and the bounded contexts into one Git repository.\n* We will keep the benefits of the separated bounded contexts with the following measures:\n* Investigate & discuss if the different parts will be separated just with by directory structures and different test suites for PHPUnit or if we want still want [separate composer packages inside a monorepo](https://medium.com/sroze/managing-monolithic-repositories-with-composers-path-repository-c28af031746d).\n* We will make architecture checks part of our CI. We will evaluate the tools [dephpend](https://github.com/mihaeu/dephpend), [deptrac](https://github.com/sensiolabs-de/deptrac) and [phparch](https://github.com/j6s/phparch) to see which one(s) fit our requirements best.\n* New features will be developed more strictly with [Feature toggles](https://en.wikipedia.org/wiki/Feature_toggle) to keep the ability to deploy small changes and bug fixes at any point.\n* Improving the situation with the I18N data repository and deployment will be discussed in a separate ADR.\n","**Decision:** Consolidate the repositories for the Fundraising Application, Fundraising Operation Center, and Bounded Contexts into a single repository.\n\n**Rationale:**\n\n* **Improved developer productivity:** Having all code in a single repository eliminates the need to maintain multiple IDE windows and simplifies code navigation.\n* **Reduced accidental integrations:** Consolidating the repositories makes it more difficult to accidentally merge changes from bounded contexts into the applications.\n* **Simplified dependency management:** Maintaining a single repository simplifies dependency management and CI pipelines.\n\n**Mitigation of drawbacks:**\n\n* **Code search:** Configure the IDE to search all project files by default.\n* **Refactoring:** Implement automated renaming to avoid manual updates in the applications.\n* **I18N data:** Move I18N data to a dedicated branch in the consolidated repository and create a pull request merge process to manage changes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFrom time to time the Fundraising team wants to try out changes in the fundraising frontend to see if they improve the amount of donors, the donation sum, the membership application rate and so on.\nIn the past, the developers implemented those experiments in an ad-hoc fashion, with different places in the code implementing the branching, and with different places in the code and database to store the outcomes.\nFrom a developer perspective, the new implementation of bucket testing must not affect the code quality and spread throughout the code base. To achieve that, we should collect the relevant code in one agreed-upon central location. Also, there should be some mechanism in place to determine, if experiments are still ongoing. Ideally, our CI should help us to prune dead code related to bucket tests which have completed.\nWhen our system places a visitor in a bucket, it must store that assignment persistently in the database. When querying the database, we must be able to link the record to donations/memberships if needed. No need to build an analysis software yet: The FUN team will provide the results of each bucket testing campaign as raw/aggregated data obtained though querying the database.\n\n## Decision\n","* Create campaign configuration which describes the experiments in a human-readable format (YAML) with bucket names, start and end dates and contextual information.\n* For better performance, the configuration is validated in CI, at runtime it's assumed to be correct.\n* Create CLI-based campaign validator for logical and technical errors.\n* Create CLI-based validator which verifies that each campaign has the necessary code logic implemented and ensures that removed campaigns have had their code properly pruned to prevent dead code from accumulating\n* Add code that translates the configuration into feature toggles. Use [Doorkeeper](https://github.com/remotelyliving/doorkeeper) as a feature toggle library. We [evaluated different feature toggle libraries](https://gist.github.com/gbirke/ab53316c69341718a9dd5cb79ed32642) and chose Doorkeeper because\n* It has the most modern code (PHP 7), 100% test coverage and the code looks most SOLID of the considered options.\n* It already implements toggle conditions we need (check date range, check user bucket).\n* All branching based on feature toggles *must* take place at creation time, i.e. in the central factory that creates the use cases and their dependencies. We created the `ChoiceFactory` for that purpose.\nRemoving the old code that stores the result of old A/B tests (but was not used in the 2016 and 2017 campaigns and stores empty default values) is not as part of the acceptance criteria.\n","We will create a new component: Bucket Testing Service (BTS). The BTS will be responsible for:\n\n- Storing the bucket assignment for each visitor in the database\n- Providing an API to create, update, and delete bucket tests\n- Pruning dead code related to bucket tests which have completed\n\nThe BTS will be implemented as a microservice. It will be deployed to a separate server from the fundraising frontend. This will help to ensure that the BTS does not affect the performance or stability of the frontend.\n\nThe BTS will use a database to store the bucket assignment for each visitor. The database will also store the results of each bucket testing campaign. This will allow the FUN team to easily query the database to obtain the results of their campaigns.\n\nThe BTS will provide an API to create, update, and delete bucket tests. This API will be used by the fundraising frontend to manage their bucket tests.\n\nThe BTS will use CI to prune dead code related to bucket tests which have completed. This will help to keep the code base clean and organized."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to give users quick feedback if their form input is valid, without having to submit the form and waiting for the page reload.\n\n## Decision\n","We don't want to duplicate complex server-side business logic (e.g. minimum\nyearly membership fees dependent on the address type of the member) on the client,\nbecause that would mean duplicating work in different programming\nlanguages and having tools and processes to check that the validation\nworks the same both on the client and the server.\nWe *can't* perform the bank data validation on the client side, because\nit's a PHP extension written in C and comes with a large data file.\nWe want the feedback for the user to be as fast as possible. This means\nreducing the server-side communication to a minimum.\nOur client-side code follows the following pattern:\n1. Pre-validate on the client side for empty fields and invalid input\n(pattern matching, length checking), whenever the user changes a field.\nGive immediate feedback.\n2. When the user has filled in all fields on a page, send the fields to\nthe server for validation. The validation result from the server\noverrides the validation performed by the client.\n3. When the user submits the form, the code checks if the server has\nsuccessfully validated the fields. If the background validation is\nongoing, the code delays the submission of the fields until all\nvalidation requests have returned.\nThe messages we show to the user should come from the client-side code. In\nthe error case, the server JSON response contains a `messages` field (of\ntype object), with a mapping of field names (or rather ""source of error"")\nto error causes. The client-side code should care about the field names,\nand not display any error-case data from the server, but should use its\nown I18N library to determine the message content.\n","**Decision:** Implement real-time form validation using JavaScript.\n\n**Justification:**\n\n* **Real-time feedback:** JavaScript allows for immediate validation of user input, providing instant feedback on the validity of their entries.\n* **No page reloads:** Validating forms via JavaScript eliminates the need for page reloads, resulting in a seamless and responsive user experience.\n* **Client-side validation:** JavaScript performs validation on the client-side, reducing server load and improving performance.\n* **Ease of implementation:** JavaScript is a widely supported language that can be easily integrated into web forms.\n* **Extensibility:** JavaScript can be used to implement custom validation rules and integrate with other frontend frameworks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe started with waiting code spread throughout the different layers of the DockerComposition stack:\n* DockerComposition\n* Container\n* DockerPort\nThis makes our custom waits depend on specific functionality rather than general functionality that external devs can also use. Now we're making all `waitForService` calls go through just a couple of API calls with flexible Healthchecks this is going to bite us.\n\n## Decision\n,"Wait code (for the purposes of powering `waitForService`) does not go in the Composition stack. Instead, waits depend on external observations of this stack.\n","**Migrate waiting code to a central location in the API.**\n\nThe waiting code should be moved to a central location, such as the API layer. This will make it easier to maintain and test, and will reduce the risk of side effects from waiting code that is spread across multiple layers. The specific location of the waiting code will depend on the specific architecture of the DockerComposition stack."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Establish an Architectural Decision Record (ADR) process to document and track the architectural decisions made on this project.\n\n**Benefits:**\n\n* Improved transparency and visibility of architectural decisions\n* Facilitation of stakeholder alignment and understanding\n* Reduced risk of architectural defects and rework\n* Enhanced knowledge transfer and onboarding for new team members\n* Improved communication and collaboration between stakeholders\n\n**Considerations:**\n\n* **ADR Template:** Define a standardized template for ADRs to ensure consistency and completeness.\n* **Review and Approval Process:** Establish a process for reviewing and approving ADRs to ensure stakeholder alignment and avoid unilateral decision-making.\n* **Version Control:** Implement version control for ADRs to track changes and facilitate collaboration.\n* **Central Repository:** Create a central repository for storing and managing ADRs to ensure easy access and traceability.\n* **Enforcement:** Establish mechanisms to enforce the use of ADRs and ensure that all architectural decisions are properly documented.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\nmore lambda functions to maintain.\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\n* We can re-use most of the infra setup from the last project so we can get started fast\n* Easy to run and develop locally, just run like a normal Flask app\n* Easy to test as it's a normal Flask app\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\nanti-pattern?\n* Maintaining a single lambda function is much easier than maintaining many\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support/contradict this)\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\n* [Flask-Lambda](https://github.com/sivel/flask-lambda) is a small package with only 1 contributor, though it is a pretty\nsimple script that just maps some request variables depending on the source of the request\n\n## Decision\n",* Single lambda function containing small Flask app that provides all endpoints\n* Flask-Lambda to help easily switch between local dev and AWS\n,"**Decision:** Use Flask in a single lambda function for this project.\n\n**Reasoning:**\n\n* **Reduced maintenance burden:** Managing a single lambda function is easier than maintaining multiple functions.\n* **Reusability:** Infrastructure setup can be reused from previous projects, enabling rapid development.\n* **Ease of development and testing:** Flask is easily run and developed locally like a normal Flask app, and testing is straightforward.\n* **Established documentation and community support:** Flask is widely used and has a wealth of documentation and support.\n* **Alignment with AWS practices:** Chalice, an AWS library, supports a similar approach, indicating that it is not an anti-pattern.\n\n**Mitigating Factors:**\n\n* **Cold-start concerns:** Cold-start times may be longer due to the initialization of the entire Flask app. To mitigate this, consider using a keep-warm strategy.\n* **Potential anti-pattern:** Some may argue that packaging a whole app in a FaaS is an anti-pattern. However, the project's small size and well-defined boundaries make this a suitable candidate for experimentation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently use Terraform to manage infrastructure across OPG projects. This is great for AWS services but\nmakes local development of certain AWS services (eg Lambda functions, API Gateway) difficult.\nThere are a lot of frameworks around that claim to make developing & deploying serverless apps easier. Our real interest\nis making local dev simpler, as we already have a nice pipeline working with Terraform & CircleCI for the infrastructure.\nHere is a (not exhaustive) list of options:\n* The big one: [Serverless](https://www.serverless.com/)\n* Python specific: [Zappa](https://github.com/Miserlou/Zappa)\n* Made by AWS: [Chalice](https://chalice.readthedocs.io/en/latest/)\n* More AWS: [SAM](https://github.com/awslabs/serverless-application-model)\n* Not a framework: [LocalStack]()\n#### Serverless\nThe only instructions I can find for local dev are basically how to test a function by mocking out AWS things, there's\nnothing specific to Serverless. It is mainly deployment focussed, and all that is done through Cloud Formation and 'magic'.\nIt also appears that the Pro service is [not free](https://www.serverless.com/pricing/)\n[Serverless Local Development](https://www.serverless.com/blog/serverless-local-development/)\n[AWS HTTP APIs](https://www.serverless.com/aws-http-apis/)\n#### Zappa\nYou can start up a full local version of your stack using Zappa, but that requires you to set up all the deployment\nthrough the framework too, which we would rather not do.\n>Zappa has one thing I personally don't like. I want my cloud deployments to be managed through an orchestration service, as this gives you a single place from where all the resources that belong to a deployment originate. For AWS, you can use the native Cloudformation service for this. Unfortunately Zappa uses Cloudformation only for a portion of its deployment tasks, and for the rest, it invokes AWS APIs directly. This makes keeping track of resources associated with a Zappa deployment harder.\n-- Miguel Grinberg - [Serverless Deployments of Python APIs](https://blog.miguelgrinberg.com/post/serverless-deployments-of-python-apis#comments)\nIf Miguel ""Ultimate Master of Flask"" Grinberg doesn't like it, then we don't like it.\n#### Chalice\n>After a couple of hours, it’s pretty clear that Chalice is a clone of Zappa that adds some automatic IAM magic but offers a tiny subset of its functionality and requires significant code changes to my app.\n-- A Cloud Guru - [The fear and frustration of migrating a simple web app to serverless](https://read.acloud.guru/adventures-in-migrating-to-serverless-a63556b39042)\nIt is actually a python app framework similar to Flask or DJango (tho way smaller), so if we were intending to use this\nwe'd have to write our code using Chalice from the start (or refactor).\nI had a little play with this at the start of the Documents integration and had a lot of problems, though I can no longer\nremember the details, I do remember having a lot of difficulty triggering the lambda locally, I think you have to use it\nin combination with SAM CLI below.\n#### SAM CLI\nIf you don't use it for deployments this can pretty much be ignored as a 'framework'. It can be used to invoke lambdas\nlocally however, see [Invoking Functions Locally](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-using-invoke.html)\nbut it also cannot recreate API-Gateway functionality.\n#### LocalStack\nNot made a final decision on this, as it does a LOT more than just invoke a lambda function. The big downside here\nis that [it is not free](https://localstack.cloud/#pricing). Might be worth returning to when we get deeper into DynamoDB\nas it can mock out an entire AWS stack locally.\n\n## Decision\n","We will not use a serverless framework, we will use Flask and Flask-Lambda to run and test locally.\nThis will be reviewed again once we start properly working with DynamoDB\n","**Decision:** Use LocalStack for local development of serverless applications.\n\n**Rationale:**\n\nLocalStack provides a comprehensive local AWS environment that can mimic API Gateway and other AWS services, making it an ideal choice for local development of serverless applications. While it is not free, the benefits it offers in terms of simplifying local development outweigh the cost.\n\nOther options considered include Serverless, Zappa, Chalice, and SAM CLI:\n\n* **Serverless:** Focuses on deployment and lacks specific support for local development.\n* **Zappa:** Requires using its own deployment framework.\n* **Chalice:** Requires significant code changes to use.\n* **SAM CLI:** Can invoke lambdas locally but cannot recreate API Gateway functionality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOPG Metrics will live in the opg-management account, which is home to other products and services.\nWe want to keep OPG Metrics infrasture separate from those other products so that it can be more portable across accounts and avoids impacting the performance or configuration of those other products.\n\n## Decision\n",OPG Metrics will have it's own VPC.\n,All OPG Metrics infrastructure should be deployed in a new project in the opg-management account. The project should be named opg-metrics and should be used exclusively for OPG Metrics infrastructure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to visualise our time series data for users to view and analyse data.\nWe need the ability for authenticating users with levels of permissions for creation and viewing. A way to organise dashboards by project or area of interest should also be available from the solution to help navigate to information.\nIt is not to be used for debugging, we have other solutions that provide and handle this sort of functionality. This should be for analysing metrics across a spectrum of sources and be able to overlay key points on top of each other.\n\n## Decision\n","We have chosen Grafana for its ease of use, popularity, plugin support and Open Source status.\n","Use Grafana as our visualisation tool for time series data.\n\n**Rationale:**\n\n* Grafana is a popular open-source platform specifically designed for visualizing time series data.\n* It provides rich features for creating customizable dashboards, allowing users to organize and analyze data from various sources.\n* Grafana supports authentication and user permissions management, ensuring secure access to dashboards.\n* It integrates with multiple data sources, enabling the visualization of metrics from a range of systems.\n* Grafana includes tools for overlaying key points and events on graphs, facilitating deeper analysis.\n* Its intuitive user interface simplifies dashboard creation and sharing, making it accessible to users of all skill levels."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs our API Endpoint will be exposed to the world, we require it to have some sort of protection and authentication against attackers.\nWe will initially only be connecting internal facing services (AWS) and thrid party tooling, however in the future we would like to begin recording real user metrics (RUM).\nWe would need to have a way of protecting against attackers spamming our API.\n\n## Decision\n","We have chosen to use AWS Api Gateway API Keys for our authorisation and usage limits. This will require an AWS Signature being generated from your credentials and API Key.\nEach service or integration should have their own key.\nEach service should also set their own usage limits which are contextual to their service. For example, CircleCI integration could be monitored for the average use during a day and have this set, as well as having a secure way to request the keys when they are rotated.\nWe have also looked at [AWS WAFV2](https://docs.aws.amazon.com/waf/latest/APIReference/Welcome.html) and this is something we may look at in the future depending on our usage of the service.\n","We should use [Amazon Cognito](https://aws.amazon.com/cognito/). This will allow us to protect our API against attackers, authenticate users, and manage user access. Cognito can be used to protect both internal and external facing services. It can also be used to implement RUM."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\n\n## Decision\n",To run in a fully managed AWS cloud environment using Terraform to manage it.\n,"**Decision:** Utilize a serverless architecture.\n\n**Reasoning:**\n\n* **Lightweight:** Serverless architectures eliminate the need for provisioning and managing servers, reducing overhead and minimizing infrastructure costs.\n* **Fully managed:** Cloud providers handle all infrastructure management responsibilities, including scaling, patching, and security, freeing up development teams to focus on application functionality.\n* **Integrable:** Serverless functions can be easily integrated with other cloud services and third-party applications, enabling seamless data sharing and automation.\n* **Cost-effective:** Pay-as-you-go pricing models ensure that organizations only pay for the resources they consume, leading to significant cost savings.\n* **Scalable:** Serverless architectures automatically scale up or down based on demand, ensuring optimal performance and preventing resource wastage."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nData collected within the solution is stored in a Time Series format. This data can be stored in any type of database if needed, however we should use, if possible, a database that natively supports it. This will result in a more optimised system and therefore cheaper to run.\nThe amount of data that gets sent to the database will grow over time and so it will need to be scalable and managable over this time.\n\n## Decision\n","We will use Amazon Timeseries for our data store. It is a fast, scalable and serverless time series database that fits well with our existing use of AWS infrastructure.\n","Use a Time Series Database, such as InfluxDB or Prometheus."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","To record architectural decisions, we will use an Architectural Decision Record (ADR) template. The ADR template will include the following sections:\n\n* **Title:** A brief summary of the decision.\n* **Date:** The date the decision was made.\n* **Author:** The person or team who made the decision.\n* **Background:** The context of the decision, including the problem being solved and the options that were considered.\n* **Decision:** The decision that was made, including the rationale for the decision.\n* **Consequences:** The expected consequences of the decision, both positive and negative.\n* **Related Architectural Decisions:** Any other architectural decisions that are related to this decision.\n\nThe ADRs will be stored in a central repository and will be reviewed regularly by the architecture team."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis codebase should be easily approachable by all those contributing to it current and future, consistency of coding style is thus an important aspect.\n\n## Decision\n","For TypeScript based projects we will use eslint to enforce consistent coding styles and prettier to enforce consistent code formatting.\nFor C# based projects I have not made a decision yet, this is something that should be addressed in a future extension.\nAn editorconfig file will be available where needed so that editors can pick up common settings.\nThese rules should not be written in stone. In the future, it should be possible given consensus to add/remove or override certain rules.\nCommits introducing code that does not adhere to the above settings should fail the CI build.\n","**Decision:** Adopt a consistent coding style and enforce it using a linter.\n\n**Rationale:**\n\n* **Improved readability and maintainability:** A consistent coding style makes it easier for developers to read, understand, and modify code.\n* **Reduced errors:** A linter can automatically detect and enforce coding style guidelines, reducing the likelihood of errors and inconsistencies.\n* **Encourage collaboration:** By adopting a common coding style, it becomes easier for developers to collaborate on the codebase, regardless of their experience or background.\n* **Increased code quality:** A consistent coding style can help improve the overall quality of the codebase, making it more reliable and easier to maintain."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis repository could potentially contain multiple solutions that consist of multiple projects. For this, a build system is needed that is capable of creating multiple assemblies or executables. Some of those projects may require more advanced tooling because of the need to target multiple platforms or because of the requirement of a different dev stack (eg front end development). Thus it could be interesting to look at some build tools. Tools I considered using:\n- Makefile\n- Fake\n- Cake\n- Nuke\n- Npm\n- Yarn\n- Webpack\n\n## Decision\n","The idea is to keep things simple and easy to use by leveraging build tooling provided by dotnet. This means that `csproj` will be leveraged as much as possible. Any TypeScript related tools should be called via MSBuild targets. This makes sure that it's easy to get up and running via a CLI or an IDE.\nFor the front-end and TypeScript related projects, the decision is made to use `Yarn` to manage and install packages and `Webpack` to bundle any assets. Yarn installs and Webpack builds should be mainly called from MSBuild.\nA general Makefile will be provided for Unix environments to ease CLI based development. This way many dotnet CLI commands can be orchestrated together. The plan is to only provide this for Unix based environments no such efforts will be made to add a higher level orchestration for Windows.\nWhen extra commands are needed to bootstrap certain parts of the application those should then be thoroughly documented in the `README.md` file. We should try to avoid this as much as possible and when needed try to put measures in place to move away from this extra step.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:**\n\nRecord architectural decisions in an Architectural Decision Record (ADR) repository.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOrigin SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\n\n## Decision\n","Use OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\n",Migrate to [EIP-1967](https://eips.ethereum.org/EIPS/eip-1967) upgradable proxy pattern.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOrigin as an open-source platform should stay open for custom modules. This should not come with a cost of forking core projects but should be allowed from the topmost application layer.\nA good example is features like WebSockets, reporting services, or analytics. While this is not a core concern of the Origin domain, it is definitely important from the operations point of view. Additionally, given the number of specialized solutions, it is virtually impossible for Origin to provide solutions for them.\n\n## Decision\n",We decided to start the transition to event-based architecture for core components. Currently leveraging `@nestjs/cqrs` component of Nest.JS framework for Event emitting and handling parts.\nThis approach allows developers to create custom event handlers and provide them in the runner projects for e.g @energyweb/origin-backend-app.\n,**Decision:** Allow the extension of Origin functionality through custom modules at the topmost application layer.\n\n**Rationale:**\n\n* Maintaining Origin's open-source nature supports a vibrant ecosystem of custom modules.\n* Extensions at the application layer reduce the risk of forking core projects.\n* The wide range of specialized solutions in the market makes it impractical for Origin to provide exhaustive solutions.\n* Extensions enable customization and adaptability to specific operational needs without compromising the core functionality of Origin.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use the `utils-demo` package to deploy the whole Origin system. After receiving some feedback, we noticed that the name `utils-demo` didn't reflect the true intention of that package.\n\n## Decision\n",We've decided to rename the `utils-demo` package to `migrations` and publish it on NPM.\n,Rename the `utils-demo` package to `utils-deployment`.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHandling external device registries like for e.g I-REC registry become impossible due to current code and API structure.\n\n## Decision\n,"We took a decision to deliver separate API for Origin part and leave the external device registry part not opinionated and tailored to exact external registry requirements.\nOrigin Device Registry responsibilities:\n-   record external device registry id and smart meter id\n-   record platform enhancement content like: story, images and other information\nEach External Device Registry data will be held in a separate package for e.g `@energyweb/origin-device-registry-irec-local-api` and respectively `@energyweb/origin-device-registry-irec-local-api-client` for mock version of the I-REC registry.\nSince in most of the cases the External Registry is not consistent across the registries, each external registry api would require specific UI component implementation.\n","**Decision:** \n\nIntegrate a new API layer that exposes external registry endpoints.\n\n**Rationale:**\n\nThis decision allows for easy integration of new external registries without the need to modify existing code. It also provides a consistent API for interacting with external registries, making it easier to manage and maintain the system.\n\n**Consequences:**\n\n* Additional complexity in the system due to the new API layer.\n* Potential for performance issues if the API layer is not implemented efficiently."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere has been confusion on why we call generating and consuming devices Assets in Origin. We have noticed that most Origin users would refer to Assets as Devices and not Assets.\n\n## Decision\n,"We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\n",Rename Assets to Devices in the Origin platform to align with user expectations and reduce confusion.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur current certificate structure is based on ERC-721 non-fungible tokens. This presents an issue when a part of a certificate's volume has to be transferred to another owner.\nIn cases like these, we currently ""split"" the certificate into 2 smaller certificates, and then transfer one of the certificates to the new owner, and leave the original certificate to the original owner - deprecating the old certificate.\nThis approach is not ideal, so we started looking into better ways of changing owners for smaller parts of the certificates.\n\n## Decision\n",We decided to use the [ERC-1888](https://github.com/ethereum/EIPs/issues/1888) Certificate structure so that we can comply and work on standardizing Certificates.\n,"**Proposed Decision:**\n\nImplement a certificate splitting mechanism based on ERC-1155 semi-fungible tokens.\n\n**Rationale:**\n\n* ERC-1155 allows for the transfer of specific quantities of a token, addressing the issue of splitting certificate volume.\n* It provides a more efficient and scalable solution compared to the current approach of ""splitting"" ERC-721 tokens.\n* ERC-1155 is a well-established and widely adopted standard in the blockchain industry, ensuring interoperability with other platforms and applications.\n\n**Consequences:**\n\n* The current ERC-721 certificate structure will need to be migrated to ERC-1155.\n* The certificate splitting mechanism will require development and integration into the system.\n* The use of ERC-1155 may introduce additional gas fees and complexity compared to ERC-721."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrent on-chain approach to handle demands and certificates matching is not scalable enough, prone to front-running and hard to extend with new features.\n\n## Decision\n",The change provides a new way of trading/matching certificates using off-chain order book matching engine.\n,"Migrate to an off-chain matching process with a scalable and flexible architecture, implemented as a sidechain, to handle demands and certificates matching."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\n\n## Decision\n,"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\n","**Decision:** Migrate all data from off-chain storage to on-chain storage.\n\n**Benefits:**\n\n* Eliminates data inconsistency risks by storing all data on-chain.\n* Simplifies data management and eliminates the need for complex synchronization mechanisms.\n* Improves data security by ensuring that all data is protected by the blockchain's consensus mechanisms.\n\n**Drawbacks:**\n\n* Increased on-chain storage costs.\n* Potential performance impact due to the increased on-chain data volume.\n\n**Alternatives Considered:**\n\n* **Implement a more robust off-chain storage solution:** This would require significant investment in development and maintenance, and may still not fully address the data inconsistency risks.\n* **Limit data stored off-chain:** This would reduce the risk of data inconsistency, but may limit the functionality and scalability of the application.\n* **Use a hybrid approach:** This would involve storing critical data on-chain and less important data off-chain. While this could reduce costs and performance impact, it would still introduce some risk of data inconsistency."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOrigin project consist of multiple packages which are the part of Origin SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\n\n## Decision\n,Migrate code base to monorepo structure and use `lerna` for versioning management.\n,"**Decision:**\n\nImplement and use a dependency management tool such as Maven or Gradle to automate dependency management.\n\n**Rationale:**\n\nDependency management tools provide several benefits:\n\n* **Centralized management:** All dependencies are managed in a single place, making it easier to track and control.\n* **Automatic updates:** The tool can automatically download and update dependencies as needed, reducing manual effort and reducing the risk of errors.\n* **Conflict resolution:** Dependency management tools can detect and resolve dependency conflicts by choosing the most suitable version or rejecting conflicting versions.\n* **Dependency version locking:** The tool can enforce version locking, preventing accidental updates that could break the build or introduce unexpected changes.\n* **Support for multiple repositories:** Dependency management tools support multiple repositories, allowing teams to use both public and private repositories efficiently.\n\nBy implementing a dependency management tool, the team can streamline the dependency management process, reduce manual effort, improve dependency visibility, and enhance the reliability of the build process."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\n\n## Decision\n,Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\n,"**Decision:** Implement a library to abstract away direct REST calls to off-chain data.\n\n**Rationale:**\n\n* Abstracting away the direct REST calls will make unit testing easier by decoupling the system components from the REST API implementation.\n* The library can also provide additional functionality, such as caching, error handling, and rate limiting, which can improve the performance and reliability of the system.\n* A library would also make it easier to maintain the codebase by centralizing the logic for accessing off-chain data."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\n\n## Decision\n","A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\n","Migrate the backend storage to a managed database service like Amazon DynamoDB or Google Cloud Datastore. This will provide several benefits, including:\n\n* Reduced operational overhead: Managed database services handle the tasks of managing the database, such as provisioning, backups, and scaling, freeing up development teams to focus on building applications.\n* Increased scalability: Managed database services can automatically scale to handle increased traffic, ensuring that applications can handle sudden spikes in demand.\n* Improved performance: Managed database services often use advanced technologies and optimizations to provide high performance and low latency, which can improve the user experience of applications.\n* Enhanced security: Managed database services typically provide robust security features, such as encryption, access control, and auditing, to protect data from unauthorized access or theft."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","We will use the ADR (Architectural Decision Record) template to document architectural decisions made on this project. The ADR template includes the following sections:\n\n- **Title**: A short, descriptive title for the decision.\n\n- **Context**: A brief description of the problem or opportunity that led to the decision.\n\n- **Decision**: The decision that was made, including any relevant rationale.\n\n- **Consequences**: The potential consequences of the decision, both positive and negative.\n\n- **Trade-offs**: The trade-offs that were considered when making the decision.\n\n- **Related Decisions**: Any other architectural decisions that are related to this decision.\n\n- **Status**: The current status of the decision, e.g. proposed, accepted, rejected, implemented.\n\n- **History**: A record of any changes that have been made to the decision."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use architectural decision records (ADR) on this project.\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived ""journey"" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\n\n## Decision\n",We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\n,"RFCs for ADRs should be time-limited and open to feedback from the entire team to ensure the quality and validity of architectural decisions, while preventing long-running feedback loops that could hinder project progress."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBefore using ADR we decided that we would use vue. Vue needs a form of state management to work with. State in vue can be persisted on a vue component level as raw data objects but with growing scale and complexity of the application the need to share this state between components, without additional safety measures, can lead to code that is hard to understand and/or debug.\nThere is an officially recommended state management implementation for vue [[0]] - Vuex; which is community-supported, free, and follows the renowned Flux pattern [[1]].\nVuex offers using a ""single source of truth"" for the application [[2]] with added guards. Only mutation through events are possible. This helps to protect from the down-sides of scattered code holding reference to the same objects.\nVuex ships with an existing best practice for structure [[3]] and out-of-the-box integration with debug tools. The nature of event-based state mutation provides benefits like ease of debugging (""time-travel"") and promises maintainability in a growing application.\nWe already use Vuex within the WikibaseLexeme codebase. See: `resources/widgets/LexemeHeader.newLexemeHeaderStore.js`\nThere is also an existing approach within WikibaseLexeme to persist, pass and partially validate state using rich models (`wikibase.lexeme.datamodel.Lexeme` et al.). These were written to mimic the backend data model but did so incompletely. To accomodate the need for components in a (temporarily while editing) ""dirty"" state, another `wikibase.lexeme.LemmaList` kind of state was added pointing out the need for flexibility beyond the model's capability in this regard.\nLosing object-oriented accessors and existing ""validation"" rules, for an apparent anemic model [[4]], could feel cumbersome at first [[5]] and consequences of actions are not always immediately obvious. But Vuex’s event-based store system provides us with an extensible interface for us and 3rd parties that offers flexibility, robustness, existing & well-maintained documentation.\n\n## Decision\n",We will use Vuex as state management.\n,Use Vuex as the central state management for WikibaseLexeme.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Establish an Architectural Decision Record (ADR) process to document and track architectural decisions throughout the project lifecycle.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe make it easier to debug the code without deploying we need a way to run all the Lambdas locally.\n\n## Decision\n,As we are already using the [AWS Serverless Application Model (SAM)](https://github.com/awslabs/serverless-application-model) to define our serverless application we can use [AWS SAM CLI](https://github.com/awslabs/aws-sam-cli) to run things locally.\nBy wrapping up AWS SAM CLI in a Docker container we can also use (Docker Compose)[https://docs.docker.com/compose/] to spin up our stack locally.\nWith Docker Compose we can spin up DynamoDB locally and API gateway so that we can invoke the Lambdas as if they were deployed.\n,**Run Lambdas locally using SAM CLI**\n\n**Benefits:**\n\n* Convenient and efficient way to debug locally\n* Avoids the need for remote debugging or deploying to a test environment\n* Simplifies testing and troubleshooting code issues\n* Allows for faster development iterations and quicker identification of potential problems\n\n**Drawbacks:**\n\n* May not fully replicate the production environment\n* Requires installation and setup of the SAM CLI\n* Can consume significant local resources depending on the complexity of the Lambda functions
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo make the benchmark more meaningful we will want to make them to be more like real world Lambdas. A typical pattern we see with lambdas is fetching data from Amazon DynamoDB.\n\n## Decision\n,With the deployment of the Lambdas and API gateway we will also deploy a DynamoDB table.\n,"**Decision:** Introduce a DynamoDB table to support the benchmark.\n\n**Rationale:**\n\n* **Real-world relevance:** Incorporating a DynamoDB table will simulate a common real-world scenario where Lambdas interact with a database.\n* **Benchmark enhancement:** The addition of the DynamoDB table will introduce additional factors that influence the benchmark results, such as database latency and read/write operations.\n* **Data persistence:** The DynamoDB table can store data for subsequent benchmark runs, allowing for comparisons over time.\n* **Increased accuracy:** Using a DynamoDB table will provide a more accurate representation of the performance of Lambdas in a real-world setting.\n\n**Consequences:**\n\n* **Increased complexity:** Introducing a DynamoDB table will add complexity to the benchmark setup and maintenance.\n* **Additional dependencies:** The benchmark will become dependent on the availability and stability of the DynamoDB service.\n* **Potential performance overhead:** Accessing the DynamoDB table may introduce additional latency and resource consumption."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe project needs a way to be built and deployed. It needs to be quick and easy to use so has to be able to pick up changes to the git repo. It should be easy for anyone recreate the workflow used here and flexible enough to handle building multiple languages. At the end of successful build the artifact should get deployed.\n\n## Decision\n,As this is project is benchmarking AWS Lambdas it makes sense to use the services AWS has for building (AWS CodeBuild) and deploying (AWS CodePipeline) code.\n,"**Decision:** Implement a CI/CD pipeline using Jenkins.\n\n**Rationale:**\n\n* Jenkins is a well-known and widely-used CI/CD tool.\n* It is open-source and has a large community of users and contributors.\n* It supports a wide range of languages and technologies.\n* It is easy to set up and configure.\n* It can be integrated with a variety of other tools and services.\n\n**Details:**\n\nThe CI/CD pipeline will be implemented using the following steps:\n\n1. Create a Jenkinsfile that defines the build and deployment process.\n2. Configure Jenkins to run the Jenkinsfile on every git push.\n3. Set up a build agent that will run the build process.\n4. Configure Jenkins to deploy the artifact to a production environment after a successful build.\n\nThe CI/CD pipeline will be tested by running the following commands:\n\n```\ngit commit -m ""feat: add CI/CD pipeline""\ngit push\n```\n\nThe expected result is that the CI/CD pipeline will run successfully and deploy the artifact to the production environment."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use an Architectural Decision Record (ADR) template to document architectural decisions.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince the product is going to be based on Microservices architecture there is a need for good orchestrator to host its different parts.\n## Decision Drivers\n* Easy to use\n* Easy to learn\n* Possibility to host modules as Docker containers\n* Well integrated with Azure\n* Well integrated with Azure DevOps\n\n## Decision\n,"* Easy to use\n* Easy to learn\n* Possibility to host modules as Docker containers\n* Well integrated with Azure\n* Well integrated with Azure DevOps\nDecided to use AKS since\n* It is more powerfull than ASF\n* It is more suitable for Docker containers hosting\n* ASF have to be used when you plan to use its programming model (stateless services, stateful services, actors), but I didn't need that\n* Kubernetes seems more popular and evolved\n* Kubernetes has managed implementation for Azure - AKS\n### Positive Consequences\n* I will be able to use orchestrator to host app's modules.\n### Negative Consequences\n* n/a\n","**Decision:** Use Azure Kubernetes Service (AKS) as the orchestrator for the Microservices architecture.\n\n**Justification:**\n\nAKS meets all the decision drivers:\n\n* **Easy to use:** AKS is a fully managed Kubernetes service, making it easy to set up and manage Kubernetes clusters.\n* **Easy to learn:** AKS provides a user-friendly interface and documentation, making it easy for developers to get started.\n* **Possibility to host modules as Docker containers:** AKS supports running Docker containers, making it easy to deploy and manage Microservices.\n* **Well integrated with Azure:** AKS is a native Azure service, providing seamless integration with other Azure services such as Azure Storage, Azure Cosmos DB, and Azure DevOps.\n* **Well integrated with Azure DevOps:** AKS is tightly integrated with Azure DevOps, enabling continuous integration and continuous delivery (CI/CD) for Microservices."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nMap have to be added to the product so missiles can be shown there at real time.\n## Decision Drivers\n* Easy to use\n* Some knowledge already exists\n* Native for other product technologies\n\n## Decision\n,* Easy to use\n* Some knowledge already exists\n* Native for other product technologies\nDecided to use Azure Maps since\n* They are native to Azure and .NET which are used for the product\n* They are well integrated with Azure SignalR that is going to be used for real-time notifications\n* I got some experience with Azure Maps already\n### Positive Consequences\n* Implementation will be faster and easier in this particular case\n### Negative Consequences\n* I planned to get some Google Maps API knowledge as well\n,Use the Google Maps API.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThere is a need to send real time SignalR-based notifications about current missile position to the map.\n## Decision Drivers\n* Need easy way to post missile status updates\n* Need easy way to get those updates using SignalR on client side\n\n## Decision\n,"* Need easy way to post missile status updates\n* Need easy way to get those updates using SignalR on client side\nDecided to simply push SignalR messages directly from MGCC module.\n### Positive Consequences\n* Initially I planned to post current missile status to Message Bus, then process it to CosmosDB, then have Azure Function to send SignalR messages. But that seemed too complicated. With direct SignalR messages from MGCC module the implementation will be much easier and faster.\n### Negative Consequences\n* n/a\n",Implement a simple SignalR hub to send missile position updates.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor Microservices architecture it is a good idea to have API Gateway service to manage interservices communication and handle cross-cutting concerns (logging, security, caching, etc.).\n## Decision Drivers\n* Easy to use\n* Easy to learn\n* Can be hosted on Azure\n* Can be hosted in Docker container\n\n## Decision\n",* Easy to use\n* Easy to learn\n* Can be hosted on Azure\n* Can be hosted in Docker container\nNo outcome yet. Have to investigate both a bit and then will decide.\n### Positive Consequences\n* I will be able to manage communication between services more effectively and securely with API Gateway.\n### Negative Consequences\n* n/a\n,"Implement API Gateway service using **Azure API Management** service.\n\n**Rationale:**\n\n* Azure API Management meets all the decision drivers:\n    * **Easy to use:** Azure API Management provides a user-friendly interface and intuitive tools for managing APIs.\n    * **Easy to learn:** The service has extensive documentation and tutorials available, making it easy for developers to get started.\n    * **Can be hosted on Azure:** Azure API Management is a fully managed service that is hosted and managed by Microsoft.\n    * **Can be hosted in Docker container:** Azure API Management can also be deployed in a Docker container, providing flexibility and portability.\n* Azure API Management provides additional benefits such as:\n    * **Centralized API management:** It allows for centralized creation, management, and monitoring of all APIs, simplifying governance and control.\n    * **Cross-cutting concerns handling:** It can handle cross-cutting concerns such as authentication and authorization, caching, rate limiting, and monitoring, reducing the need for custom development.\n    * **Traffic management:** Azure API Management provides advanced traffic management capabilities, including load balancing, routing, and caching, ensuring high availability and performance."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nOperation API need some data storage to be able to list/commission/decommission/launch missiles.\n## Decision Drivers\n* Easy to use storage\n* Fast storage\n* Dynamic and/or schemaless storage\n\n## Decision\n,"* Easy to use storage\n* Fast storage\n* Dynamic and/or schemaless storage\nDecided to use MongoDB since\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\n* No need to use RDBMS just to store some random missiless on a stock\n* I wanted to get some experience in MongoDB\n### Positive Consequences\n* Implementation will be faster and easier since schema is going to be changed lot of times\n### Negative Consequences\n* n/a\n",Use MongoDB as data storage.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\n## Decision Drivers\n* Easy to use\n* Easy to learn\n* Possibility to run from Docker\n\n## Decision\n,* Easy to use\n* Easy to learn\n* Possibility to run from Docker\nDecided to use RabbitMQ since\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\n* It is easy to learn and use and I already had some knowledge of how to use it\n* It has official container on Docker Hub\n* It seems to be popular and trending these days\n### Positive Consequences\n* I will be able to setup EDA messaging in the product.\n### Negative Consequences\n* n/a\n,Use Apache Kafka as the messaging server.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\n\n## Decision\n",[Gulp](https://gulpjs.com/) will be used to bundle client-side code.\n,Use Gulp to bundle the JavaScript code.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis project was first implemented in plain JavaScript. Its only third-party dependency was on SoundCloud's JavaScript SDK.\nI had recently discovered frontend web frameworks and had been learning about AngularJS. I was interested in applying it to a larger project.\n\n## Decision\n,The application will be rewritten in AngularJS 1.x.\n,Migrate the project to AngularJS.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen this project was first starting out [Bower](https://bower.io/) was the tool of choice for managing frontend dependencies.\nBower has since been deprecated and its creators have [advised its users to migrate to other solutions](https://bower.io/blog/2017/how-to-migrate-away-from-bower/).\n\n## Decision\n,Dependencies will be managed using [Yarn](https://yarnpkg.com/).\n,Migrate away from Bower and replace it with a modern frontend dependency manager such as [Yarn](https://yarnpkg.com/) or [npm](https://www.npmjs.com/).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen this application was first built [AngularJS 1](https://angularjs.org/) was one of the more popular frameworks for building frontend applications.\nSince then [Angular 2](https://angular.io/) and beyond was released. Angular 2 is not backwards compatible and a non-trivial amount of effort was required to learn the new version and to migrate the application over.\n[React](https://reactjs.org/) had also gained more market share and had become the framework I had grown most familiar with.\n\n## Decision\n,The application will be rewritten in React.\n,Migrate the frontend of the application to React.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe CSS declarations for all components was declared in a single file. This has become difficult to manage as the relevant CSS is located separately from its component and there are a lot of declarations in one file.\n\n## Decision\n,We will enable [CSS Modules](https://github.com/css-modules/css-modules) and co-locate each component's CSS declarations with its code.\n,"Refactor the CSS architecture by splitting the CSS declarations into multiple files, one for each component. This will improve maintainability and readability by keeping the CSS declarations closely associated with their respective components."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are currently no tests in this application. This was partly because there were a lot of options to choose from out of all the testing frameworks or libraries available for JavaScript, and because it felt a bit excessive for a side project.\nSince then testing frameworks such as [Jest](https://jestjs.io/) and [Cypress](https://www.cypress.io) have surfaced. Both of these promise a more developer-friendly experience. I figured I should get more practice with these tools and decided it'd be best to try them on an existing project.\n\n## Decision\n",Jest and Cypress will be used to test this application.\n,Add Jest to the project to test the application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis project was first implemented in plain JavaScript. Its only third-party dependency was on SoundCloud's JavaScript SDK, which was included via a `script` tag.\nI had come across package managers while learning AngularJS 1.x. Bower was one of the popular tools of choice at the time.\n\n## Decision\n",Client-side dependencies will be managed using [Bower](https://bower.io/)\n,Use Bower as a package manager for frontend dependencies.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen this project was first starting out [Gulp](https://gulpjs.com/) was one of the tools of choice for running tasks such as bundling and minifying code.\nSince then task runners seem to have fallen out of popularity and bundlers such as Webpack and Parcel are the preferred approach.\n\n## Decision\n,Webpack will be used to bundle source code.\n,Migrate to using Webpack or Parcel for bundling and minifying code instead of Gulp.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs mentioned in [ADR # 8. Use CSS modules to create scoped CSS for components](0008-use-css-modules-to-create-scoped-css-for-components.md) this application has separate CSS files for each component's CSS. We now require a need to define commonly used variables such as colours, spacing and so on in a centralised location so that these values are not duplicated across the codebase.\nThere were two solutions considered:\n1. [Sass](https://sass-lang.com/)\n- Superset of CSS that compiles to CSS.\n- Supports variables, nesting, partials, operators, and so on.\n- Has two major implementations in JavaScript:\n- [`sass`](https://www.npmjs.com/package/sass) which is implemented in Dart.\n- [`node-sass`](https://www.npmjs.com/package/node-sass) which provides bindings to the C library LibSass.\n2. [PostCSS](https://postcss.org/)\n- Transforms CSS with JavaScript plugins.\n- Plugins that add support for Sass features such as variables, nesting, imports, and so on are available.\n- Allows for auto-vendor prefixing via [`autoprefixer`].\n\n## Decision\n",This system will use PostCSS to process CSS.\n,"Use SASS\n\n## Rationale\nSASS and PostCSS are both viable options for centralising CSS variables, but SASS is a better fit for this project for the following reasons:\n\n- SASS provides all the features needed to define and use variables, including nesting, partials, and operators.\n- SASS has a more mature ecosystem of plugins and integrations than PostCSS.\n- SASS is easier to learn and use than PostCSS.\n- SASS is more performant than PostCSS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis has been a long-running side project. I am interested in capturing what design decisions have been made, and why.\n\n## Decision\n","This repo will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** Document design decisions in a central repository.\n\n**Rationale:**\n\n* **Consistency:** Ensures that all design decisions are recorded in a single, accessible location.\n* **Transparency:** Facilitates collaboration and knowledge sharing by making design rationales available to all stakeholders.\n* **Learning:** Allows for future reference and retrospection of past design choices.\n* **Accountability:** Provides a record of decision-making for future audits or evaluations.\n* **Knowledge preservation:** Captures valuable design knowledge that would otherwise be lost when team members leave."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe stream encoder component defined in this project is designed to subscribe\nto the MQTT broker provided by SmartCitizen. This broker is used to support\nSmartCitizen's existing platform and will not be modified specifically for\nDECODE.\nEvents published via the broker look like this:\n```json\n{\n""data"": [\n{\n""recorded_at"": ""2018-11-01T13:59:06Z"",\n""sensors"": [\n{\n""id"": 10,\n""value"": 0\n},\n{\n""id"": 10,\n""value"": 0\n},\n{\n""id"": 14,\n""value"": 0\n},\n{\n""id"": 55,\n""value"": 29.1\n},\n{\n""id"": 56,\n""value"": 36.64\n},\n{\n""id"": 16,\n""value"": 41.4\n}\n]\n}\n]\n}\n```\nThat is we receive a timestamp for the event, and then an array of readings\ncomprising an `id` (which is a numeric id that relates to specific sensor type\nas described by SmartCitizen) and a numeric value.\nNot included in the received event are the following fields:\n* The location of the device\n* The exposure of the device (indoor, outdoor)\n* The name, description or units of received sensor measurements\nSensor id values are described by SmartCitizen via an API endpoint:\nhttps://api.smartcitizen.me/v0/sensors\nThis returns JSON that looks like this:\n```json\n[\n{\n""id"": 3,\n""uuid"": ""ac284ba3-e2fc-4795-b2b1-530b32a9b05b"",\n""parent_id"": null,\n""name"": ""DHT22"",\n""description"": ""A digital temperature and humidity sensor. It uses a capacitive humidity sensor and a thermistor to measure the surrounding air, and spits out a digital signal on the data pin (no analog input pins needed)"",\n""unit"": null,\n""created_at"": ""2015-02-02T18:14:15Z"",\n""updated_at"": ""2015-02-02T18:14:15Z"",\n""measurement"": null\n},\n{\n""id"": 20,\n""uuid"": ""4a2e9c80-748c-44a3-b400-8824f50d19cd"",\n""parent_id"": null,\n""name"": ""MiCS4514"",\n""description"": ""Gas Sensor"",\n""unit"": null,\n""created_at"": ""2015-02-02T18:31:50Z"",\n""updated_at"": ""2015-02-02T18:31:50Z"",\n""measurement"": null\n}\n]\n```\nSo it is this JSON that encodes all metadata about each sensor including names,\ndescriptions and units.\n### Constraints\n### Options\nWe identified the following options:\n* The encoder could encrypt the data exactly as received to the encrypted\ndatastore, meaning that when the receiving dashboard fetches and decrypts the\ndata it will have to make a request to the SmartCitizen API to convert the\nnumerical sensor IDs into something more meaningful to know how to represent\nthe data.\nThis fetching of data from the SmartCitizen API could happen every time the\ndashboard scrapes data from the encrypted datastore, or it could be updated\nto include a local copy of the sensor ID mappings.\n* The encoder could enrich the data, so that the data that is then written to\nthe datastore has locations, and sensor metadata added directly to the file\nmeaning that the receiver can immediately understand the data in order to\nrender it appropriately.\nNote that again here we could add this enriching metadata dynamically on\nevery request, by fetching from the SmartCitizen API, or by caching a local\ncopy of the sensor mappings such that this information is already known to\nthe encoder.\n\n## Decision\n","We will enrich the data within the encoder using a cached copy of\nSmartCitizen's sensor JSON.\nReasoning:\n* Removes any coupling between the final receipient of the data and\nSmartCitizen. This avoids consumers having to build knowledge of the\nSmartCitizen API into their systems, and would be more scalable if other data\nsources were ever added.\n* Sensor types included by SmartCitizen do not change frequently (last update\nwas 2016, so 2 years ago at time of writing).  Because of this low update\nfrequency there is likely to be little issue with having to update the local\ncached copy frequently.\nThe output JSON we will publish will look like this:\n```json\n{\n""longitude"": 2.234,\n""latitude"": 54.213,\n""exposure"": ""INDOOR"",\n""recordedAt"": ""2018-11-01T15:06:23Z"",\n""userUid"": ""abc-123-fbd"",\n""sensors"": [\n{\n""sensorId"": 29,\n""name"": ""MEMS Mic"",\n""description"": ""MEMS microphone with envelope follower sound pressure sensor (noise)."",\n""unit"": ""dBC"",\n""type"": ""SHARE"",\n""value"": 64.252\n},\n{\n""sensorId"": 14,\n""name"": ""BH1730FVC"",\n""description"": ""Digital Ambient Light Sensor"",\n""unit"": ""Lux"",\n""type"": ""BIN"",\n""bins"": [5,10,50,500],\n""values"": [0,1,0,0,0]\n},\n{\n""sensorId"": 4,\n""name"": ""HPP828E031"",\n""description"": ""Temperature"",\n""unit"": ""ºC"",\n""type"": ""MOVING_AVG"",\n""interval"": 900,\n""value"": 18.2\n}\n]\n}\n```\n","Given that the receiving dashboard will be unaware of the device location or exposure and will need to make a request to the SmartCitizen API to retrieve this information, either dynamically on every request or by caching a local copy of the sensor mappings, the preferred solution is to enrich the data in the encoder such that the data that is then written to the encrypted datastore has locations, and sensor metadata added directly to the file, meaning that the receiver can immediately understand the data in order to render it appropriately."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis project is a component within a project called DECODE, which is a\nEuropean Commission funded project which aims to explore and pilot new\ntechnologies that give people more control over how they store, manage and\nuse personal data.\nThe specific subtask this project belongs to is an Internet of Things (IoT)\npilot which aims to demonstrate an architecture by which individuals retain\nautonomy over data collected and published by IoT devices within their homes.\nThe agreed architecture for the pilot requires the implementation of three\nindependant components which together provide a mechanism for:\n1.  registering and claiming ownership of an IoT device\n2.  creating encrypted stream of data from the device\n3.  providing a mechanism for storing this encrypted data somewhere\n4.  provide a mechanism for allowing users with the correct decryption keys to\ndownload and decrypt saved data.\nThe components required to do this are as follows:\n1.  An encrypted datastore - must be able to persist encrypted data to robust\nstorage, and then allow that data to be retrieved by clients, who can then\ndecrypt and read the data provided they have the correct key.\n2.  A stream encoder - must be able to subscribe to raw events published by\nthe IoT device, encrypt that data using encryption keys supplied for the\ndevice, and then write these encrypted events to the datastore.\n3.  A device registration service - must be able to allow users to claim a\ndevice or devices, and then create encrypted streams for those devices.\nBecause DECODE is explicitly attempting to provide tools that can work in a\ndecentralised way, the decision has already been taken to implement the above\nfunctionality via the separate components described above, so this decision\nrecord is not about that; rather here we are just proposing a technology we\ncan use to expose and implement the API for the datastore, such that it is\neasily usable by other components within the system.\n### Constraints\n* Components must support running on distributed nodes, meaning all\ncommunication between components must happen over the network using standard\nprotocols.\n* Components are to be operated by SmartCitizen once running in ""production"".\n* Components are required to support a range of client languages, Go being used\ninternally to the Thingful team, then perhaps one or more of Ruby, Python or\nJavaScript for Eurecat or SmartCitizen (TBD).\n### Options\nWe identified the following options for the component:\n* RESTful JSON over HTTP - stateless, v.simple to understand conceptually, uses standard\nHTTP, easy to integrate with other services, easy to test and debug via curl\nfrom the command line.\n* JSONRPC (http://www.jsonrpc.org/) - lightweight remote procedure call\nprotocol, uses JSON (readable), transport agnostic, can be tested/debugged\nvia curl from the command line.\n* Twirp (https://twitchtv.github.io/twirp/docs/intro.html) - lightweight RPC\nframework developed by TwitchTV, uses protocol buffers\n(https://developers.google.com/protocol-buffers/) to define messages and\nservices, uses HTTP 1.1, natively exposes both protocol buffer and JSON\nendpoints, automatic generation of clients and server stubs, able to be\ntested from the command line via curl as all protocol buffer endpoints have\na matching JSON endpoint that we can simply `POST` requests to.\n* GRPC (https://grpc.io/) - modern RPC framework open sourced by Google, uses\nprotocol buffers to define messages and services, allows automatic generation\nof clients and server stubs, requires HTTP 2.\n* SOAP/XMLRPC - an XML based RPC framework, v.heavyweight. No one wants to use\nthis.\n\n## Decision\n","We will use Twirp to implement the datastore service's API.\nReasoning:\n* automatic client generation from a single protocol buffer definition is a\nreally nice feature.\n* uses HTTP 1.1 so easier to integrate with existing infrastructure than GRPC\n* natively exposes a JSON/HTTP endpoint meaning we have the same relatively\neasy CLI testing or debuggability as a standard REST API, but with the more\nefficient protocol buffer implementation that real clients should use.\nA single repository has been created that will hold all protocol buffer\nservice definitions for the services being developed for DECODE. The location\nof that repository at time of writing is:\nhttps://github.com/thingful/decode-protorepo\nThe language bindings for clients generated from this repo will be pushed\ninto their own repositories via an automated script. These language bindings\nwill then be packaged in a language appropriate way so meaning they are easy\nto use for any developers attempting to write code that interacts with the\nservice (i.e. Go will just use the repo, but for other languages the bindings\nwill be packaged into the expected format for the language and deployed to\nthe standard package index).\nGiven this project uses Go, the Go bindings for the datastore are available\nhere: https://github.com/thingful/twirp-datastore-go\n","**Decision**: Use Twirp\n\n**Rationale**: Twirp meets all of the constraints and is a good fit for the project's requirements. It is lightweight, uses standard protocols, and is easy to use. Additionally, Twirp provides automatic generation of clients and server stubs, which can save time and effort during development."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\n\n## Decision\n",Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\n,"**Decision:** Colocate test files next to source files.\n\n**Rationale:** Colocating test files next to source files significantly improves the developer experience during migration by reducing the distance and time required to navigate between the two. This allows developers to quickly find the corresponding test file for a given source file and vice versa, eliminating the need to switch between folders and reducing the cognitive load associated with locating the related file."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\n- [define databases in tests][defined-test-database]\n- [randomly create databases for each test][random-test-database]\n\n## Decision\n","In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\n",[Randomly create databases for each test][random-test-database]
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI tried updating the schema for `RawTopTracks` to include the `importedDate` and discovered that I had to update quite a few files:\n1. The Schema\n2. The validators in the pipeline package\n3. The tests for the validators and pipeline\n4. The type definitions\nIt would be more convenient if the schema definition could also be used to generate the types and validate the inputs so that it would be easier to make changes. [Typegoose][typegoose-docs] seems to be something to explore. My concern is that I don't understand Typescript reflection and so Typegoose becomes a type of magic that will be difficult to debug and modify.\nAn alternative is to create classes in Typescript that [wrap the Mongoose schema][mongoose-schema-class]. This seems to be a more approachable solution.\n\n## Decision\n,No decisions made.\n,"Generate types and validators from the Mongoose schema using [mongoose-schema-class][mongoose-schema-class]. This approach is more approachable and easier to debug and modify than using Typegoose, which relies on Typescript reflection."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCreating new and updating existing data transformations, whether from APIs or the database, is still a pain point. After many refactors I feel that the abstractions I am using are not helping. The goal for the data transformations were:\n- Take data from an API or database\n- Modify or enrich it\n- Save or display the results to the user\nTo achieve this, there are two abstractions that I rely on:\n- Hapi plugins: an interface defined by Hapi server that can add features to the server object.\n- Plugin helpers: ad hoc functions that I use to decompose plugin actions.\nThe problem with these are:\n- Mixing of concerns: data transformation and server processes need different abstractions because they should be able to change independently. As it is, I have coupled them together.\n- Broken interfaces: related to the first point, [Hapi plugins][hapi-plugins] have an interface that makes them useful for the server. Trying to use the same for creating a data pipeline is very ""hacky"" - open to frequent changes and bugs.\n- There is a better way: data transformations will benefit from a different abstraction solving the current problem and opening up new possibilities.\nTwo abstractions present themselves:\n- Transducers\n- Streams\nThey both create programming interfaces that allow chaining of operations on a data set: e.g. `A |> B |> C` where the input for the next step is the output of the previous.\nTransducers are an abstraction implemented by 3rd party libraries while streams are native to [NodeJs][nodejs-streams]. In practice, I would use a library for either approach to deal with [possible pain points][nodejs-streams-readable-streams].\n\n## Decision\n","In the context of lacking a well established method to build data pipelines in JavaScript, and facing the concern of fragile data transformations then use Streams. They can be used for large files since they have a low memory footprint and can have other uses e.g. request/response streams.\n","**Adopt streams as the abstraction for data transformations.**\n\nStreams provide a native and well-supported interface in Node.js for creating data pipelines. They allow for chaining of operations, which is essential for the data transformation use case. Additionally, streams can be used to handle both synchronous and asynchronous data sources, providing flexibility and ease of use."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\n```shell\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\n2019-08-09T07:25:06+01:00 at Module.require (internal/modules/cjs/loader.js:683:19) {\n2019-08-09T07:25:06+01:00 at Module.load (internal/modules/cjs/loader.js:643:32)\n```\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\n\n## Decision\n","In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\n",Use `npm` as the package manager and Docker for deployments.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nToday I was trying to configure eslint to allow me to add ""use strict"" to my files. This project uses CommonJS modules which are not strict by default. I have several `.eslintrc.json` files in package folders which [behave correctly][eslint-folders] but I'm struggling to override the airbnb config in `./packages/server` to allow strict mode. If I used Typescript I would not need ""use strict"" mode, because it would be added by the compiler. And I could easily compose my `tsconfig` and `tslint` setup in the monorepo. I have tried using `@hapi/eslint-config-hapi` eslint settings but I'm not comfortable with that coding style. Creating my own custom eslint settings is one alternative.\nThe root cause of this is I thought strict mode was not necessary because of my misunderstanding of [ES modules][es-modules]. `require` is CommonJS, `import ... from` is ES modules.\nUsing Typescript:\n### Pros\n- Type checking\n- Compile to target\n- Sane configuration using base config and overriding in packages\n- Compiler takes care of some linting\n- Works really well with VS code\n- Contemporary\n- I have some types as js docs\n- Works with [Cypress][cypress-ts]\n- [ts-eslint][ts-eslint] combines best of both worlds\n- Demonstrable use of Typescript\n- Better VS code intellisense\n### Cons\n- Yet another thing to maintain - versions, types, compatibilty\n- Not sure I need types everywhere\n- Cannot be gradually adopted - in reality mixing js/ts doesn't work out well\n- Code must be compiled or run with [`ts-node`][ts-node]\n- Seperate commands to copy files into build folder\n- Build times\n\n## Decision\n","In the context of setting up this repo in strict mode. And facing the concern of being contempory, consistent use of types and lint rules let's adopt Typescript. I accept that there will be an initial work of conversion and some type wrangling.\n","In order to improve code quality and ensure strict mode in all JavaScript files, TypeScript will be adopted in the project. TypeScript provides type checking, compilation to target, and a sane configuration system that makes it easy to override settings in specific packages. Additionally, TypeScript integrates well with VS Code, provides better intellisense, and is compatible with Cypress and ts-eslint. While maintaining TypeScript can add complexity and increase build times, the benefits of improved code quality, type checking, and enhanced developer experience outweigh the drawbacks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\n\n## Decision\n,Jest seems like the way to go:\n- Typescript support\n- Large community\n- Familiar\n,"Use Jest instead of Lab or Ava for testing. Jest is a popular testing framework for JavaScript and TypeScript with a large community and a wide range of plugins and integrations. It is known for its performance and reliability, and it has been used in many large-scale projects."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBy live application I mean that the APIs are required for the lifetime of the application and by static, the APIs are only required once - at build time or for storage.\nBuilding a data-driven application for a portfolio has the risk that the data will become stale or inaccessible - for example if the API keys expire, the API changes or the service is no longer available. This can break the consuming application. In maintaining the Korin app, I have experienced Last.fm and IBM Watson APIs breaking due to API changes or expired API keys.\n\n## Decision\n","Facing the concern that data-driven applications will break due to maintenance issues, I have decided to build them as static apps i.e. process or store the data once and present the results.\n","**Decision:** Store live data in a database and static data in files.\n\n**Rationale:**\n\n* **Live data:**\n    * Storing live data in a database ensures that it remains accessible and updated in real-time. \n    * By regularly refreshing the data using API calls, the application can mitigate the risk of stale data. \n* **Static data:** \n    * Storing static data in files is efficient and prevents unnecessary API calls. \n    * The data can be bundled with the application or stored in a centralized location, ensuring its availability even when the API is unavailable. \n\nThis approach balances the need for up-to-date live data with the efficiency and reliability of static data storage, minimizing the risk of data-related application failures and providing a stable and reliable data foundation for the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen HapiJS plugins are registered they expect an object like this:\n```JavaScript\n{\nname: 'plugin-name',\nversion: '1.0.0',\nregister: (server, options) => {}\n}\n```\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\n```JavaScript\nimport plugin from './plugin';\nexport default {\nplugin,\n}\n```\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\nSo let's use `module.exports` instead of `export default`.\n\n## Decision\n",Rejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\n,Use `module.exports` instead of `export default` for plugin registration in HapiJS.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[ExpressJS][1] is the default server for most NodeJS projects. However I have found that it does not include the features that I would prefer -\n- Robust and extensible architecture to decompose a project\n- Handle common use cases like file uploads\n- Nice error handling e.g. 404, route collisions\n- Easy to test server behaviour including middleware\n[HapiJS][2] has the above features and\n- All essential dependencies are maintained by the same team\n- Accessible and friendly community\n[Read more][3] about HapiJS\n\n## Decision\n","In the context of a fullstack NodeJS project and facing the concern of making better architecture decisions, I decided to use HapiJS to achieve better quality measured by test coverage and composable architecture. I accept that I will have to rely on documentation and source code more since HapiJS is not as well written about because of its lower popularity.\n",Use HapiJS as the server for NodeJS projects.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\n\n## Decision\n",Spotify is the only viable alternative.\n,Select Music Story API as it provides both artist images and lyrics.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\n### [11ty][4]\nPros\n- Easy setup\n- Predictable static file output\n- Relatively small codebase\n- Supporst different template engines\nCons\n- The frontmatter parser [didn't work as I expected][7]\n- Template errors were not reported and would only fail on build\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\n### [Gatsby][5]\nPros\n- Modern [JAMstack][8] project with React\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\n- Opportunity to use GraphQL\nCons\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\n- Developing static files based on API was difficult because of caching and unexpected error handling\n- Relatively complex project with separate build, develop steps, config, plugins etc\n### [Hugo][6]\nPros\n- Fast and mature static site generator\n- Small footprint\nCons\n- Written in Go - I prefer fullstack JavaScript for this site\n\n## Decision\n","In the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\n",**Decision:** Use 11ty as the static site generator for the website blog.\n\n**Rationale:**\n\n* Easy setup and predictable static file output\n* Supports different template engines\n* The cons of 11ty (frontmatter parser issues and template errors) are outweighed by the simplicity and ease of use.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI'm getting Typescript errors in the e2e tests. If I'm relying on Typescript for linting then keep errors clean is good hygiene. I'v tried ignoring the `e2e/` or `cyperss/` folders and that is not working. Why? Not sure.\n\n## Decision\n,"In the context of getting Typescript errors in my e2e tests and facing the concern to keep errors clean, then use Typescript for e2e tests. Especially since it seems to be [well documented][cypress-ts] with examples. Therefore the burden should be light.\n","Use `exclude` from `tsconfig.json`:\n\n```json\n{\n  ""compilerOptions"": {\n    // ...\n    ""exclude"": [""e2e/*""]\n  }\n}\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFacing the concern that by serving views from the backend API, I have less opportunity for reusing APIs, creating new apps and staying on trend, I propose to decouple the frontend and backend architecture of the site.\nThe opportunities that this creates would be to learn new technologies such as\n- GraphQL: new approach to APIs to solve the problem of creating different endpoints for different frontend requirements.\n- Server-side rendering: important for Search engine optimization and performance.\n\n## Decision\n",- Re-write the backend in Fastify for better Typescript support.\n- Create a server-side rendered frontend.\n,"**Decision:** Decouple the frontend and backend architecture of the site.\n\n**Rationale:**\n\n* **Increased flexibility for API reuse:** Separating the frontend and backend allows for greater flexibility in reusing APIs across different frontend applications. This eliminates the need to create separate endpoints for each frontend requirement, making it easier to maintain and update the APIs.\n* **Opportunity to explore new technologies:** Decoupling the architecture opens up the possibility to explore and implement cutting-edge technologies such as GraphQL and server-side rendering. GraphQL provides a more efficient approach to APIs, while server-side rendering enhances performance and improves Search Engine Optimization (SEO).\n* **Improved performance:** Server-side rendering improves page load times and provides better user experience by delivering fully rendered HTML to the browser, reducing the amount of work the browser needs to do on its own.\n* **Better separation of concerns:** Decoupling the frontend and backend ensures a clear separation of concerns, making it easier to maintain and update each component independently."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI'm trying to make changes to this monorepo but I keep encountering dependency problems. Ideally the root package.json contains the devDependencies allow the `packages/` folder's package.json to contain the dependecies. This works reasonably well with yarn, lerna or pnpm but using npm I see errors like:\n```shell\n> site@1.0.0 lint /home/iampeterbanjo/clever-cloud/iampeterbanjo.com\n> npx eslint ""**/*.{ts,js}"" --fix\nCannot read config file: /home/iampeterbanjo/clever-cloud/iampeterbanjo.com/node_modules/@typescript-eslint/eslint-plugin/dist/index.js\nError: Cannot find module 'typescript'\nReferenced from: /home/iampeterbanjo/clever-cloud/iampeterbanjo.com/packages/server/.eslintrc.json\n```\nPackages that could be shared:\n- Typescript\n- Eslint\n- Prettier\nIt is possible to share configurations from a base config with Typescript. e.g.\n```json\n{\n""extends"": ""../../tsconfig.base.json"",\n""compilerOptions"": {\n""outDir"": ""./build""\n},\n""include"": [""src"", ""__tests__""]\n}\n```\nFor prettier I can publish my [custom configuration][published-prettier-config] and reference it in `package.json` like:\n```json\n{\n""name"": ""my-cool-library"",\n""version"": ""9000.0.1"",\n""prettier"": ""@company/prettier-config""\n}\n```\n\n## Decision\n","- All packages should contain all their dependencies.\n- The root `package.json` contains only convenience scripts or base configs.\n- Drop eslint because it's value is not clear: prettier formats, Typescript checks - done.\n","**Establish a shared workspace for development dependencies.**\n\nThis workspace will house the shared development dependencies (e.g., TypeScript, ESLint, Prettier), allowing the packages within the monorepo to reference them without causing dependency conflicts.\n\n**Configure TypeScript with a base configuration.**\n\nCreate a `tsconfig.base.json` file in the root of the monorepo, containing common TypeScript configurations. Individual packages can then extend this base configuration and specify any package-specific overrides.\n\n**Publish a custom Prettier configuration.**\n\nPublish the custom Prettier configuration as a separate package and reference it in the `package.json` files of the packages that need it. This ensures that all packages use the same consistent Prettier settings.\n\n**Benefits:**\n\n* **Reduced dependency conflicts:** By sharing development dependencies in a dedicated workspace, dependency conflicts between packages are minimized.\n* **Consistent configuration:** Using a shared TypeScript base configuration and a published Prettier configuration ensures that all packages adhere to the same coding standards.\n* **Improved maintainability:** Changes to shared dependencies only need to be made in one location, making it easier to keep the monorepo up-to-date."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"In the context of making several architecture and design decisions for this site, I was concerned that I would not have a way to audit and learn from them. So I decided to use Architecture Decision Records, as described by Michael Nygard in [this article][1], to achieve better learning outcomes in the future. I accept it's more work to do so.\n","**Decision:** Implement an Architectural Decision Record (ADR) process.\n\n**Rationale:** \n- ADRs provide a structured and consistent way to document architectural decisions, making it easier to understand the rationale behind the design and to track changes over time. \n- They can help to prevent architectural decisions from being made without proper consideration and to ensure that decisions are aligned with the overall project goals. \n- ADRs can also serve as a valuable reference for future project participants or for auditing purposes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter implementing several real business domains we have found that it can be\ndifficult to use `AggregateCommandScope.Destroy()` effectively.\nIdeally, `Destroy()` could be called after recording any event that effectively\n""resets"" the aggregate's state, from the perspective of the business logic.\nIn practice, logic that results in a call to `Destroy()` may be followed by some\nconditional logic that records a new event. In existing implementations this\nresults in a panic from the engine, though as of ADR-16, which removed\n`Create()` this is no longer documented as required engine behavior.\n\n## Decision\n","Remove the requirement that any call to `Destroy()` be preceeded by a call to\n`RecordEvent()` within the same scope.\nCalling `RecordEvent()` *after* `Destroy()` event should ""negate"" the call to\n`Destroy()`, as though it were never called.\n",The decision is to modify the engine to allow `Destroy()` to be called multiple times during command scope.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen handling a message in an `AggregateMessageHandler` or\n`ProcessMessageHandler`, there is a practical need to check if the instance\nalready exists before performing an operation on the scope, such as producing a\nnew message or destroying/ending the instance.\nFor example, it may be necessary to ignore a command that has been sent to an\naggregate that has been destroyed. There is currently no idiomatic way to do\nthis, short of calling `Create()` only to immediately call `Destroy()` if\ncreation succeeded.\n\n## Decision\n","We have chosen to add an `Exists()` method to aggregate scopes, and an analogous\n`HasBegun()` method to process scopes.\n","Add a new `Exists()` method to `AggregateMessageHandler` and `ProcessMessageHandler` that returns `true` if the instance exists in the scope and `false` otherwise.\n\nThis method can be used to check if an instance exists before performing an operation on the scope, such as producing a new message or destroying/ending the instance.\n\nThe following code shows an example of how to use the `Exists()` method:\n\n```\nif (messageHandler.Exists())\n{\n    // The instance exists, so perform the operation.\n}\nelse\n{\n    // The instance does not exist, so do not perform the operation.\n}\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany projections produce data that is only required for a limited period of\ntime.\nAn application developer needs to consider how long projection data will be\nretained and ideally implement measures to remove any unnecessary data.\nOften such data can be removed when some future event occurs. However, in some\ncases there is no future event that appropriately indicates the data is no\nlonger required.\nWe have encountered a genuine example of this when implementing an SQL\nprojection that inserts a row whenever a user performs a specific action. The\ndata is queried in order to enforce a ""cool down"" that prevents the user from\nrepeating that action again within a specific time frame.\nThe insert is triggered by the occurrence of an event, but the data becomes\nunnecessary whenever the ""cool down"" time has elapsed.\nIn this particular use case the ""cool down"" was not part of the business logic,\nbut rather an API level restriction. Hence, processes/timeout messages were not\nthe appropriate solution.\n\n## Decision\n",We have decided to add a `Compact()` method to `ProjectionMessageHandler`.\nThe implementation of `Compact()` can modify the projection's data by whatever\nmeans is appropriate such that unnecessary data is removed but the projection\nstill serves its purpose.\n,"**Decision:** Implement a time-based expiration policy for the projection data.\n\n**Rationale:** Since there is no event that indicates when the data is no longer required, a time-based expiration policy is the most appropriate solution. This policy will automatically remove the data after a specified period of time, ensuring that it is not retained unnecessarily.\n\n**Consequences:**\n\n* **Pros:**\n    * Data is not retained longer than necessary, reducing storage costs and improving performance.\n    * Data is automatically removed, eliminating the need for manual intervention.\n* **Cons:**\n    * Data may be removed before it is actually needed, if the expiration period is set too short.\n    * It may be difficult to determine the appropriate expiration period for some types of data."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEngine implementations require a mechanism for associating ancillary data with\nDogma applications and handlers.\nFor example, such data might include application state in the form of aggregate\nand process roots, or historical events in an event sourcing system.\nCurrently, engine implementations rely on application and handler names as a key\nfor associated data. This is especially problematic for handlers as the name\ninitially chosen for a handler may become misleading over time as the handler's\nimplementation changes.\n\n## Decision\n","We've decided to add an additional identifier to applications and handlers\ncalled the ""key"".\nThe key's express purpose is for identifying associated data, and therefore has\nmore stringent requirements on its immutability than the name.\nWe further recommend the use of an RFC 4122 UUID as the format of all keys.\nUUIDs can be generated at the time the application or handler is first\nimplemented. Many IDEs support generation of UUIDs.\nApplications and handlers retain their names as a human-readable identifier.\n",**Use a user-defined key to associate ancillary data with Dogma applications and handlers.**\n\n**Consequences**:\n\n- Allows for more flexibility in associating data with applications and handlers.\n- Reduces the risk of data being lost or corrupted due to changes in application or handler names.\n- Improves the maintainability of engine implementations by making it easier to update the association of ancillary data with applications and handlers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether Dogma's testing features should reside in the `dogma`\nmodule itself, or a separate `dogmatest` module.\n### Benefits to separate module\n1. They can be versioned separately. A `dogma` release can be made without\nhaving to update the testing tools. This is a _pro_ for the releaser, but\npossibly a _con_ for the users.\n1. Users that opt not to test, or not to test using our tools do not need to\ndownload `dogmatest` at all. This is not a particularly strong argument.\n1. We can make BC breaking changes to `dogmatest`, without having to make\na new major release of `dogma`. We would need to document clearly which\nmajor `dogma` versions are supported by which major `dogmatest` versions.\n### Benefits to a single module\n1. The user does not need to determine which `dogmatest` version to use with\ntheir application; compatible tools are always available right from `dogma`. If\nwe want to encourage users to test their domain logic using these utilities;\nthey should be easy to access.\n1. As mentioned in [#16](github.com/dogmatiq/dogma), splitting the testing\ntools would mean that the example code would also need to be moved elsewhere.\nHowever, we have since already decided to move the example code to a separate\nmodule in [ADR-7](0007-location-of-examples.md).\n\n## Decision\n",For the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\nimplement testing tools in their own module.\n,"Dogma's testing features should reside in a separate `dogmatest` module.\n\nThis decision is made because the benefits of having a separate module outweigh the benefits of having a single module.\n\nThe benefits of having a separate module are:\n\n* They can be versioned separately. A `dogma` release can be made without having to update the testing tools. This is a _pro_ for the releaser, but possibly a _con_ for the users.\n* Users that opt not to test, or not to test using our tools do not need to download `dogmatest` at all. This is not a particularly strong argument.\n* We can make BC breaking changes to `dogmatest`, without having to make a new major release of `dogma`. We would need to document clearly which major `dogma` versions are supported by which major `dogmatest` versions.\n\nThe benefits of having a single module are:\n\n* The user does not need to determine which `dogmatest` version to use with their application; compatible tools are always available right from `dogma`. If we want to encourage users to test their domain logic using these utilities; they should be easy to access.\n* As mentioned in [#16](github.com/dogmatiq/dogma), splitting the testing tools would mean that the example code would also need to be moved elsewhere. However, we have since already decided to move the example code to a separate module in [ADR-7](0007-location-of-examples.md).\n\nThe benefits of having a separate module outweigh the benefits of having a single module because the ability to version the testing tools and the ability to make BC breaking changes to the testing tools without having to make a new major release of `dogma` are more important than the convenience of having the testing tools available in a single module."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter implementing several real engines and business domains we have found the\nrules governing how `AggregateCommandScope.Create()`, `Root()`, `RecordEvent()`\nand `Destroy()` can be called in relationship to one another tends to lead to\noverly cumbersome implementations.\n### Effect on handler implementations\nFrom the handler implementors perspective there are numerous subtle interactions\nbetween the aggregate instance's ""state of existence"" and the domain logic.\nPerhaps most egregiously is the requirement that an instance already exist\nbefore calling `Root()`. This means that the state of the aggregate root itself\ncan not be used to determine which events should be recorded without first\ncreating the instance. A less obvious problem is that when the domain logic is\nsuch that `RecordEvent()` is not called within the same scope as a call to\n`Create()` the engine must panic.\nThe net result of this is that the handler becomes equally concerned with the\nnotion of whether the instance exists or not than it is with the actual business\nlogic. The subtle interactions between the domain implementation (especially\nwhen located within methods of the aggregate root) and the handler itself are\ndifficult to glean by reading the code, and hard to reason about.\nIn summary, the semantics of `Create()` do not help the handler implementor to\nimplement their domain logic in a clear and concise way.\n### Effect on engine implementations\nFrom the engine implementors perspective it seems that a non-trivial amount of\nvalidation logic needs to be implemented within the scope to verify that the\napplication code is using the scope correctly, even though the engine does not\nreally benefit from these requirements.\nFor example, event sourced engine implementations tend to call\n`AggregateMessageHandler.New()` to construct a new instance before applying\nhistorical events via `ApplyEvent()`. This means that the root instance is\nconstructed in memory before `HandleCommand()` is even invoked. The requirement\nthat `Root()` panic if the instance does not exist does not save the engine from\nconstructing the in-memory root value.\n\n## Decision\n","- Remove `AggregateCommandScope.Create()`.\n- Remove `AggregateCommandScope.Exists()`.\n- Allow `AggregateCommandScope.Root()` to be called at any time.\n- Allow `AggregateCommandScope.Destroy()` to be called at any time.\n- Remove `StatelessAggregateRoot` and `StatelessAggregateBehavior`. With the\nnotion of ""existence"" being removed from the public API a stateless aggregate\nbecomes nonsensical.\n- Reinstate the hard requirement that the handlers MUST panic with\n`UnexpectedMessage` when asked to handle a message type that was not\nconfigured as being consumed by that handler. Removing the requirement to call\n`Create()` should simplify the dispatching logic sufficiently such that no\nextra logic is required to produce the panic.\nIn essence, the aggregate instance is ""automatically created"" the first time an\nevent is recorded.\n",**Enforce the following rules for aggregate command scopes:**\n\n1. **Create() must be the first operation called on a new scope.** This establishes the existence of the aggregate instance.\n2. **Root() must be called before any other operations.** This ensures that the aggregate instance is accessible.\n3. **RecordEvent() can be called multiple times within a scope.** This allows for the recording of multiple events within a single command.\n4. **Destroy() must be the last operation called on a scope.** This ensures that the aggregate instance is properly destroyed.\n\n**These rules simplify the implementation of aggregate handlers and engines by removing the need for complex validation logic. They also make it easier to reason about the state of the aggregate instance within the scope.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether message timing information should be exposed via the\nAPI. In this context ""timing information"" refers to important points in time\nthroughout the lifecycle of a message.\nThe initial rationale for *not* exposing these timestamps was that any business\nlogic that depends on time in some way should explicitly include any timing\ninformation within the message itself. We call such logic ""time-based"" and the\napproach of including explicit timing information ""modeling time"".\n\n## Decision\n","The sections below focus on each of the message roles, their respective\ntimestamps of interest, and the decisions made in each case.\n### Command Messages\nWe believe the existing requirement that the application ""model time"" is still\nappropriate for command messages. The time at which the command message is\ncreated or enqueued is irrelevant; any time information relevant to the domain\nlogic should be included in the message itself.\n**We have decided not to expose the command creation time.**\n### Event Messages\nThe time at which an event is recorded is a fundamental property of the event\nitself. Put another way, every event occurs at some time regardless of whether\nthe domain is time-based.\nFurthermore, the time at which the event occurs may be relevant to some\nancillary domain logic that is *triggered* by the event, even if the aggregate\nthat *produced* the event has no time-based logic.\nThe inclusion of the ""occurred time"" as a fundamental property of the event is\nsupported by [Implementing Domain Driven\nDesign](https://www.amazon.com/Implementing-Domain-Driven-Design-Vaughn-Vernon/dp/0321834577),\nChapter 8, in the ""Modeling Events"" section.\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\nwithout any supporting ADR, this method is to be renamed.\n### Timeout Messages\nThe time at which a timeout message is scheduled to be handled is a fundamental\nproperty of the timeout concept.\nBy definition, the use of a timeout message indicates that there is time-based\nlogic. It seems like an unnecessary imposition to require the application\ndeveloper to include the scheduled time in the message.\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\n",**Decision:** Expose message timing information via the API.\n\n**Justification:**\n\n* The need for message timing information is widespread throughout the system.\n* Modeling time can lead to fragmented and inconsistent handling of timing information within the application.\n* Exposing timing information via the API allows for a centralized and consistent approach to time handling.\n* The security risks of exposing timing information can be mitigated with proper authentication and authorization mechanisms.\n\n**Consequences:**\n\n* Increased complexity of the API\n* Potential security risks if timing information is not properly protected
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide how to represent aggregates and processes that do not have\nany state.\nThis may sound nonsensical at first, but there are legitimate implementations of\nboth that do not require any state, or perhaps more correctly the only state is\nwhether or not they exist at all.\nThis is perhaps more likely to occur in a CQRS/ES environment where the state\nassociated with an aggregate is a ""write-model"". In this case, the only state\nthat needs to be maintained is that which is required to make decisions about\nwhich events to produce.\n\n## Decision\n","We've opted to have Dogma provide empty ""root"" implementations out of the box,\nfor both aggregates and processes. The implementations should be unexported\nstructs made available by exposed global variables in the `dogma` package.\nHandler implementations can return these values from their `New()` methods to\nindicate that they do not keep state.\nAs these values are valid implementations of the `AggregateRoot` / `ProcessRoot`\ninterfaces, engine implementations need not handle these impementations specially,\nthough they can opt to do so by treating them as ""sentinel"" values.\n",Represent stateless aggregates and processes as lightweight objects that do not require persistence.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\nlonger required to panic with an `UnrecognizedMessage` value when passed an\nunexpected message type.\nPrompted by this requirement, we relaxed the requirement for ALL handler\nmethods, which was likely too broad of a change.\nSpecifically, unlike when handling a message, the routing methods\n`AggregateMessageHandler.RouteCommandToInstance()` and\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of ""doing\nnothing"" when passed an unexpected message type.\n\n## Decision\n",Reinstate the hard requirement that the handlers MUST panic with\n`UnexpectedMessage` when asked to route a message type that was not configured\nas being consumed by that handler.\n,The `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` methods should continue to panic when passed an unexpected message type.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide on a mechanism for engine implementations to determine\nsuitable timeout durations to apply when handling a message.\nFor aggregate message handlers, which are not permitted to access external\nresources, a fairly constant timeout duration should be discernable by the\nengine developers.\nFor all other handler types, which may make network requests or perform CPU\nintensive work, there is no one timeout duration that makes sense in all\ncircumstances.\n\n## Decision\n","We have decided to allow process, integration and projection message handlers\nto provide a timeout ""hint"" on a per-message basis by way of a\n`TimeoutHint(dogma.Message) time.Duration` method.\nBy returning a zero-value duration, the handler indicates that it can provide no\nuseful ""hint"" and that the engine should choose a timeout by other means.\n","Engine implementations are free to choose any timeout mechanism they see fit, provided that it meets the following requirements:\n1. Aggregate message handlers must be given a constant timeout duration.\n2. All other message handlers must be given the opportunity to choose a suitable timeout duration based on the context in which the message is handled."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a documented process for proposing, discussing and ultimate accepting or\nrejecting ADRs via pull requests.\n\n## Decision\n","We have decided to adopt a process that favours rapid changes, at least while\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\ncapture discussion about the ADR. Such issues are given the `adr` label.\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\nthe ADR, or by rejecting it and reverting any associated code changes.\n","**Decision:**\n\nAll architectural decisions will be documented and tracked using the ADR (Architectural Decision Record) format. ADRs will be proposed via pull requests and must follow the following process:\n\n**Process:**\n\n1. **Proposal:** An ADR is proposed by creating a pull request against the ""architecture"" branch of the repository. The ADR should follow the ADR template and provide a clear and concise description of the architectural decision, its context, and the rationale behind it.\n2. **Review:** The proposed ADR is reviewed by the Architectural Review Board (ARB). The ARB provides feedback and may request revisions to the ADR.\n3. **Discussion:** The ARB and other stakeholders engage in discussion and debate on the proposed ADR. This discussion can take place in the pull request comments or in a dedicated forum or meeting.\n4. **Acceptance:** Once the ADR has been thoroughly reviewed and discussed, the ARB votes on its acceptance. If the ADR receives a majority vote, it is considered accepted.\n5. **Rejection:** If the ADR does not receive a majority vote, it is rejected. The ARB provides feedback on the reasons for rejection and may suggest alternative solutions.\n6. **Merging:** Accepted ADRs are merged into the ""architecture"" branch of the repository and added to the ADR documentation. Rejected ADRs are closed and archived.\n7. **Communication:** Accepted ADRs are communicated to stakeholders through documentation, presentations, or meetings."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether command messages can be routed directly to processes.\n\n## Decision\n,"We have decided to disallow this behavior - processes may only handle events\nand timeout messages.\nIf we were to allow processes to accept commands directly it may be tempting to\nimplement domain idempotency in the process instead of in an aggregate where\nsuch logic belongs.\nFurthermore, it's easier to allow commands to be routed to processes in a future\nversion than it is to remove it once it's in use.\n","Yes, command messages can be routed directly to processes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to advertise a meaningful history of changes to the Dogma API\nspecification for both application developers and engine developers.\nThe types of changes that have been made should be clearly identified, with\nspecial attention drawn to changes that are not backwards compatible.\n### Proposals\n- Maintain a `CHANGELOG.md` as per the recommendations of [Keep a Changelog]\n- Additionally, begin changelog entries that describe a BC break with `**[BC]**`\n- Periodically tag releases, using [semantic versioning]\n\n## Decision\n","A changelog will be maintained as per [Keep a Changelog]. Unreleased changes\nshould be added to the changelog as they are made.\nGit tags will be named according to the rules of [semantic versioning].\nAdditionally, tag names are to be prefixed with a `v` as required by [Go modules].\n","Maintain a `CHANGELOG.md` as per the recommendations of [Keep a Changelog]. Additionally, begin changelog entries that describe a BC break with `**[BC]**`. Periodically tag releases, using [semantic versioning]."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter reviewing and reworking the aggregate API in\n[ADR-16](0016-automatic-aggregate-creation.md) and\n[ADR-17](0017-recreate-aggregate-after-destruction.md) we have conducted a\nsimilar review of the process API in an effort to both simplify the API and\nimprove consistency between the aggregate and process APIs.\n\n## Decision\n,"- Remove `ProcessEventScope.Begin()`.\n- Remove `Process[Event|Timeout]Scope.HasBegun()`.\n- Remove `Process[Event|Timeout]Scope.Root()`.\n- Pass the process root directly to `ProcessMessageHandler.Handle[Event|Timeout]()`.\n- Allow `Process[EventTimeout]Scope.End()` to be called at any time.\n- Allow `Process[EventTimeout]Scope.ExecuteCommand()` and `ScheduleTimeout()` to\nbe called at any time. Doing so should ""negate"" any prior call to `End()` as\nthough it were never called.\n- Routing a command message to a process instance causes that instance to begin.\n","The process API will be reworked to:\n\n- **Remove unnecessary types**. The `StartProcess` and `ResumeProcess` messages will be merged into a single `Start/ResumeProcess` message. The `ProcessTerminated` message will be merged into the `AggregateTerminated` message.\n- **Make the syntax more consistent with the aggregate API**. The `Start/ResumeProcess` message will take a `ProcessId` as its first parameter, just like the `StartAggregate` and `ResumeAggregate` messages. The `AggregateTerminated` message will be renamed to `ProcessTerminated`.\n- **Improve the error handling**. The `Start/ResumeProcess` message will return a `Result` type, just like the `StartAggregate` and `ResumeAggregate` messages. This will allow us to provide more detailed error messages to the client.\n- **Add support for process snapshots**. We will add a new `CreateProcessSnapshot` message that will allow the client to create a snapshot of a running process. This will be useful for debugging and testing purposes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIdentifiers (the names and keys used to identify applications and handlers) must\nbe compared by engines to determine if two such entities are to be considered\nequivalent.\nThe documentation specifies that such keys must be non-empty UTF-8 strings\nconsisting of printable characters without whitespace, but it did not previously\nspecify how such strings would be compared.\nThese identifiers are either mostly or entirely immutable and generated as part\nof the source code. They do not need to be parsed and validated from user input.\n\n## Decision\n","In keeping with current behavior, we've decided to specify byte-wise comparison\nsemantics for identifiers.\n","All identifiers should be compared in a case-sensitive manner, using the UTF-8 encoding for all comparisons."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEvent sourcing engines need to call `AggregateRoot.ApplyEvent()` with\n""historical"" event types. That is, event types that have already been recorded\nagainst the instance but are no longer configured for production by that\naggregate.\nThe current specification language prohibits this, as per the `ApplyEvent()`\ndocumentation:\n> It MUST NOT be called with a message of any type that has not been\n> configured for production by a prior call to Configure().\nAdditionally, without adding some new features to `AggregateConfigurer` it is\nimpossible to declare an event as historical, meaning that there is no way to\ndiscover historical event types from the configuration.\n\n## Decision\n",We have chosen to relax the language in the specification to allow calling\n`ApplyEvent()` with any historical event types in addition to those configured\nfor production.\n,"Allow event sourcing engines to call `AggregateRoot.ApplyEvent()` with any event type, regardless of whether it has been configured for production."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to control the lifetime of an aggregate from the domain layer.\nIf you imagine an aggregate running atop a ""CRUD-based"" Dogma engine, it's easy\nto see that creation and destruction of aggregate data are necessary operations.\nThis is less obvious in an event-sourced scenario where the notion of ""deletion""\nis not present.\nWe need to define these operations in such as a way that the domain implementor\ncan use the operations in a meaningful way within their domain, but engine\nimplementors are free to determine their own persistence semantics.\n\n## Decision\n","We've opted to name the methods used to create and destroy aggregate instances\n`AggregateScope.Create()` and `Destroy()`, respectively. Note that\n`AggregateScope` has since been renamed to `AggregateCommandScope`.\n`Create()` is a fairly self explanatory name. This is an idempotent operation.\nThe method returns `true` if the call actually resulted in the creation of the\ninstance; or `false` if the instance already exists.\n`Destroy()` was chosen in preference to words such as ""delete"", as depending on\nthe engine implementation, no deletion necessarily occurs. It was chosen in\npreference to ""done"", as it more clearly indicates that the aggregate instance\nstate will be ""reset"". This could be implemented in an ES-based engine by\nrecording internal events to represent the create/destroy operations, and only\nloading those events that occurred since the most recent creation.\n### Mandatory Events\nIt was also decided to require an event message be recorded in the same scope\nas any successful call to `Create()` or `Destroy()`. This ensures that creation\nand deletion is always represented by a domain event.\nWe decided against having `Create()` or `Destroy()` take an event as a\nparameter for two reasons:\n1. This would necessitate a further decision about `Create()` as to whether the\nevent it is passed should be recorded in all cases or only if the instance\ndoes not already exist. Neither of which is appropriate in all cases.\n2. If we decide to relax this requirement in the future, those methods would\nhave to lose those event arguments, breaking backwards compatibilty.\n","**Decision:**\n\nTo introduce two new interfaces, `Aggregate` and `AggregateFactory`, to the domain layer.\n\n**Justification:**\n\nThe `Aggregate` interface will define the methods that allow clients to create and destroy aggregate instances. The `AggregateFactory` interface will be responsible for creating new aggregate instances.\n\nThis approach has the following advantages:\n\n* It allows domain implementors to use the operations in a meaningful way within their domain.\n* It allows engine implementors to determine their own persistence semantics.\n* It provides a clean separation between the domain layer and the persistence layer.\n\n**Consequences:**\n\n* The domain layer will become more complex.\n* The persistence layer will become more flexible.\n* The overall system will be more testable."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Use an Architectural Decision Record (ADR) template to document all architectural decisions made on this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether Dogma's examples should reside in the `dogma`\nrepository itself, or a separate `examples` repository.\n\n## Decision\n","We've decided to move the examples to a separate repository, so that we can\nprovide fully-functional examples that depend on modules/packages that we would\nnot want to have as dependants of `dogma` itself, such as `mysql`, etc.\n","The examples should reside in a separate `examples` repository.\n\n## Reason\nThere are several reasons for this:\n\n* **Separation of concerns.** The `dogma` repository should be focused on the core Dogma library, while the `examples` repository can contain examples of how to use Dogma in different applications.\n* **Easier maintenance.** It will be easier to maintain the examples in a separate repository, as they will not be affected by changes to the core Dogma library.\n* **Better organization.** A separate `examples` repository will help to keep the Dogma repository organized and easy to navigate.\n\n## Impact\nThis decision will have the following impact:\n\n* The `dogma` repository will be smaller and easier to maintain.\n* The `examples` repository will provide a centralized location for all Dogma examples.\n* Developers will be able to find and use Dogma examples more easily."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to provide metrics about the status of a monitored service.\n\n## Decision\n,We will use _Dropwizard_ for creating the metrics.\n,"We will create a custom dashboard that includes a variety of metrics about the status of a monitored service. The dashboard will be designed to be easy to read and understand, and it will be available to all users of the service."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to be able to monitor network services separately.\n\n## Decision\n,We will use _akka_ for managing the task to monitor a particular network service.\n,**Implement a separate monitoring system for network services.**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to provide metrics about the status of a monitored service.\n\n## Decision\n,We will use _Micrometer_ for creating the metrics and make them available through a _Prometheus_ endpoint.\n,Create a dashboard with the metrics we want to expose.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have to decide on a programming language to write our API code.\nCurrently the team members are mostly familiar with _Java_ as a programming language. Therefor we might want to stick to _Java_ because then we do not have to learn a new language and we can focus on the new tools and the problem domain that we address.\nHowever the support for _akka_ is much better in _Scala_. And we might also be eager to learn a new language.\n\n## Decision\n,We will use _Scala_ to program the monitoring part of our application.\n,"After careful consideration, we have decided to use **Scala** as the programming language for our API code.\n\n**Key Factors:**\n\n* **Improved support for Akka:** Scala provides excellent support for the Akka concurrency framework, which is essential for developing highly scalable and responsive APIs. Akka streamlines asynchronous programming and simplifies the handling of concurrent operations, making it an ideal choice for our project.\n* **Language Learning:** While the team is familiar with Java, we believe that embracing a new language like Scala will foster growth and innovation. Scala's functional programming features and powerful type system can enhance code maintainability, reduce errors, and improve performance.\n* **Modern Language:** Scala is a modern language that combines the best of object-oriented and functional programming. Its concise syntax, expressiveness, and scalability make it a strong choice for a wide range of applications, including API development."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** To use an Architectural Decision Record (ADR) template to document and record all architectural decisions made on this project.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture architectural decisions.\n* They ensure that decisions are well-documented, transparent, and accessible to all stakeholders.\n* ADRs facilitate knowledge sharing, collaboration, and reuse of decisions across the team and future projects.\n* They help prevent ad-hoc or uncoordinated decision-making that could lead to architectural drift or poor design.\n\n**Consequences:**\n\n* The use of an ADR template will require some upfront effort to establish and maintain.\n* All decision-makers and stakeholders must commit to using the template consistently.\n* ADRs will become part of the project documentation and will need to be reviewed and updated as the project evolves."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\n\n## Decision\n",This module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\n,Continue to use Terraform 0.11 for existing infrastructure and only use Terraform 0.12 for new infrastructure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are many IAM resources of which most, if not all, are relatively small in size & complexity. In order to minimise repo churn, the module will contain a collection of modules which can be invoked as required.\n\n## Decision\n",The module will be a mono repo of specific IAM resources.\n,"The decision is to create a collection of small, individual IAM resource modules that can be invoked as needed to minimize repo churn."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","We will use the ADR template to record architectural decisions. This template provides a consistent structure for documenting decisions, including the decision statement, context, consequences, and related documents."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\n\n## Decision\n",We'll package components with their own inline styles. No external dependencies needed.\n,"Exported components should be packaged assuming no dependencies.\n\n## Rationale\n- **Reduces dependency bloat in consuming applications.** Consuming applications may already have their own versions of the required dependencies, or may not need all of the dependencies required by this repository.\n- **Simplifies deployment and maintenance.** Consuming applications can simply drop in the exported components without having to worry about satisfying additional dependencies.\n- **Promotes modularity and reusability.** Components can be more easily shared and reused across different applications without the need for additional dependency management."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed a straight-forward, customizable UI style framework that was somewhat opinionated (we don't have a design team), customizable, intuitive in naming convention, and one which would not interfere with our JavaScript React component implementations.\nWe reviewed a few UI frameworks including Ant Design, Bootstrap, Bulma, and TailwindCSS.\n\n## Decision\n","We've chosen [Bulma](https://bulma.io/) for it's blend of simplicity, ease of use, and CSS-only approach, and current market share. It has a modern look & feel, seems to be gaining in popularity, and doesn't look like a lot of Bootstrap applications.\n","**Decision:** Use TailwindCSS\n\n**Rationale:**\n\n* **Straightforward and intuitive:** TailwindCSS uses a utility-first approach, making it easy to understand and use.\n* **Customizable:** TailwindCSS provides a wide range of utility classes, allowing for extensive customization.\n* **Opinionated but not prescriptive:** While TailwindCSS provides a set of design principles, it does not enforce specific design decisions, allowing us to tailor the UI to our specific needs.\n* **Minimal interference with JavaScript components:** TailwindCSS focuses on styling, without introducing additional JavaScript dependencies or interfering with React component implementation.\n* **Suited to our team's experience:** TailwindCSS's utility-based approach aligns well with our team's background, making it easy for us to learn and adopt."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",We should use an Architectural Decision Record (ADR) template.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a parser to be able to validate syntactically and semantically an Open QASM file. And a previous one with its grammar is already available from QX.\n\n## Decision\n,We will use Jison and the original Open QASM grammar from the QX project.\n,"**Decision:**\n\nUse the existing QX Open QASM parser to validate syntactically and semantically Open QASM files.\n\n**Rationale:**\n\n* **Avoid reinventing the wheel:** The QX Open QASM parser is a well-tested and reliable tool that already meets the requirements of our use case.\n* **Consistency:** Using the same parser as QX ensures consistency with their validation process.\n* **Cost-effectiveness:** Reusing an existing parser saves time and resources compared to developing a new one.\n* **Integration with existing tools:** The QX parser can be easily integrated into our existing development environment and workflow.\n* **Community support:** The QX parser is an open-source project with a large community of contributors and users, providing access to support and expertise.\n\n**Consequences:**\n\n* **Dependency on QX:** Our system will depend on the QX parser, introducing a potential vendor lock-in.\n* **Versioning considerations:** We need to ensure that the version of the QX parser we use is compatible with our version of Open QASM.\n* **Potential feature limitations:** The QX parser may not support all the features we require for our specific use case."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe visualization features of the project started as a spike, and the code is not very clear. We decided that a templates engine will solve some of the code problems and will make easier to create new visualizations in the future.\n\n## Decision\n","The recommended options from the team were, React, Lit-HTML and Nunjucks. React looks like an excellent option, but had some problems with the current templates, so Nunjucks replaced it as the best option at this moment.\n",We will use Jinja2 as the templating engine.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome parts of the code were quite different and some ""bad practices"" started to appear. Because no person in the team is an expert in TS, we believe a stricter static analysis process could help.\n\n## Decision\n","Because the community mainly accepts Google's TSLint rules as a good practice, we will use them.\n",Enforce ESLint rules and configure TypeScript compiler for stricter type checking.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe already have a parser for Open QASM files, but we are having problems to create the suggestions library because we haven't a valid AST available.\n\n## Decision\n",We will use ANTLR4 and ANTLR4-C3 library to generate a parser which can return an AST. With the AST the ANTLR4-C3 library can return the valid tokens at the caret position.\n,Create a new AST representation for Open QASM files.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLinShare App not only upload to `MySpace`, but also to `WorkSpace`.\nCurrently, `UploadWorker` is tightly with the logic of Uploading to MySpace, so we can not reuse it with other new uploading logic\n\n## Decision\n","#### Current Implementation\n```\nUploadWorker -> UploadInteractor\n```\n#### New Implementation\n```\n|-------> UploadToMySpaceCmd\n|\nUploadWorker -> UploadController -> UploadCommand\n|\n|-------> ***Cmd\n```\nWe will add `UploadController` to `UploadWorker`, it will execute the `Command` as we want to execute upload logic.\n","**Refactor the `UploadWorker` to be a generic uploading worker.**\n\nThis will allow it to be reused with other new uploading logic, such as uploading to WorkSpace. The refactored `UploadWorker` should be responsible for the following tasks:\n\n- Accepting a file to be uploaded\n- Determining the destination of the file (MySpace or WorkSpace)\n- Uploading the file to the destination\n- Notifying the user when the upload is complete\n\nBy making the `UploadWorker` more generic, we can avoid duplicating code and make it easier to maintain."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen to design an application for cloud file management, uploading or downloading always doing in the background, handle the network startup or any interruption is the requisite for mobile application.\nOver the experiences and supported nattily by androidx we have a lib to help us do all those things.\nWorkManager is a library used to en-queue deferrable work that is guaranteed to execute sometime after its Constraints are met. WorkManager allows observation of work status and the ability to create complex chains of work.\nTo summarize, WorkManager offers the following benefits:\n- Handles compatibility with different OS versions\n- Follows system health best practices\n- Supports asynchronous one-off and periodic tasks\n- Supports chained tasks with input/output\n- Lets you set constraints on when the task runs\n- Guarantees task execution, even if the app or device restarts\n\n## Decision\n",We designed to chose the lib to handle uploads and downloads with LinShare in the background\n,"Use WorkManager to manage background file uploads and downloads, ensuring reliable execution even in the face of network interruptions or device restarts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n#### Request permission\nApplication need `READ_EXTERNAL_STORAGE` to be allowed to perform file upload.\nEvery time upload is performed, application must check that permission\n```kotlin\nwhen (ContextCompat.checkSelfPermission(this, Manifest.permission.READ_EXTERNAL_STORAGE)) {\nPackageManager.PERMISSION_GRANTED -> { performUpload() }\nelse -> { requestReadStoragePermission() }\n}\n```\nIf the app has the permission, the method will returns `PERMISSION_GRANTED`.\nOtherwise, the app has to explicit ask the user for permission.\n#### Explain why the app needs permission\n> One approach you might use is to provide an explanation only if the user\nhas already denied that permission request. Android provides a utility method,\n`shouldShowRequestPermissionRationale()`, that returns `true` if the user has previously denied the request,\nand returns `false` if a user has denied a permission and selected the Don't ask again option\nin the permission request dialog, or if a device policy prohibits the permission.\nFollowing code check if the app has permission to read storage:\n```kotlin\n// Here, thisActivity is the current activity\nif (ContextCompat.checkSelfPermission(thisActivity,\nManifest.permission.READ_EXTERNAL_STORAGE)\n!= PackageManager.PERMISSION_GRANTED) {\n// Permission is not granted\n// Should we show an explanation?\nif (ActivityCompat.shouldShowRequestPermissionRationale(thisActivity,\nManifest.permission.READ_EXTERNAL_STORAGE)) {\n// Show an explanation to the user *asynchronously* -- don't block\n// this thread waiting for the user's response! After the user\n// sees the explanation, try again to request the permission.\n} else {\n// No explanation needed, we can request the permission.\nActivityCompat.requestPermissions(thisActivity,\narrayOf(Manifest.permission.READ_EXTERNAL_STORAGE),\nMY_PERMISSIONS_REQUEST_READ_EXTERNAL_STORAGE)\n// MY_PERMISSIONS_REQUEST_READ_EXTERNAL_STORAGE is an\n// app-defined int constant. The callback method gets the\n// result of the request.\n}\n} else {\n// Permission has already been granted\n}\n```\n#### Problem\n>`shouldShowRequestPermissionRationale()`, that returns `true` if the user has previously denied the request\nThis method return `false` in the first time app call it.\nSo that, System Permission request can not show and Explanation dialog always show.\n\n## Decision\n",Store a flags to indicate User has denied permission or not and change the code above\n,"**Decision:** Use the `Dexter` library to handle runtime permissions.\n\n**Rationale:**\n\nDexter is an Android library that simplifies the process of requesting and handling runtime permissions. It provides a concise and easy-to-use API that takes care of the following:\n\n* Automatically checks if the permission is already granted.\n* Shows a system permission request dialog if the permission is not granted.\n* Provides callbacks to handle the user's response to the permission request.\n\nBy using Dexter, the application can simplify the code for requesting and handling runtime permissions, making it more concise and easier to maintain.\n\n**Code example:**\n\n```kotlin\nDexter.withActivity(this)\n    .withPermission(Manifest.permission.READ_EXTERNAL_STORAGE)\n    .withListener(object : PermissionListener {\n        override fun onPermissionGranted(response: PermissionGrantedResponse?) {\n            // Permission granted\n            performUpload()\n        }\n\n        override fun onPermissionDenied(response: PermissionDeniedResponse?) {\n            // Permission denied or user checked ""Never ask again""\n        }\n\n        override fun onPermissionRationaleShouldBeShown(\n            response: PermissionRequest,\n            rationale: PermissionRequestRationale?\n        ) {\n            // Show a rationale to the user before asking again\n        }\n    })\n    .check()\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n- With mobile application, interaction with server base using REST API, in order for REST request to be accepted by LinShare server, they need to be authenticated.\n- One of the most common questions we get here - when talking about token authentication for mobile devices, is about token expiration. But how long should I allow my access tokens to exist before expiring them? I don’t want to force my users to re-authenticate every hour.\n- In order to supply a convenient user experience, we need to store a way to authenticate requests to the LinShare server.\n- Using directly authentication (and thus storing directly credentials) is discouraged, because of possible disclosure, and vulnerability to isolate a given device.\n- LinShare allow the use of `Long lived Token`. They can be revoked without changing credentials. They allows finger per device permission setting, and per-device activity review. Credential don't need to be stored (only the token does) which means user credentials cannot be leaked.\n\n## Decision\n","- For all that reason we using approach to keep the token and renew when need, that is the way to use long lived token authentication\n",Use `Long-lived Token` to authenticate requests to the LinShare server.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the android linshare application, we implement the list file from the space of user. Proposed user interface design includes the thumbnail of file preview when user wants to show\nIt is necessary to have an library to process first part is image thumbnail and preview.\nOver the best practice of image processing there are 2 libraries is very commons by android developer community: Glide and Picasso\nTo compare between them, the commonly usage is the same, but Glide have much more strengthen rather than Picasso.\nIt process the image source and have method to generate the thumbnail natively, it consume less the memory than Picasso and the library is have smaller packer with much more APIs to help process image, witch could be useful later when we implement more functionality in the application\n\n## Decision\n",We decided to use Glide instead of Picasso.\n,"Use Glide for image processing, thumbnail generation, and preview."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\nBut, an `Uri` has `UriPermission` protect the data which is represented.\nThis permission will be revoked by shared application when the received Activity no-longer run.\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\n\n## Decision\n","- We extract all requirement information of the Document at the time we receive the Intent.\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\n- We deliver temporary file path to Worker\n","**Decision:** Convert URIs to `FileDescriptors` before passing them to the Worker thread.\n\n**Rationale:**\n\nFileDescriptors provide a more reliable way to access data from URIs, as they are not subject to revocation by other applications. By converting URIs to FileDescriptors, we can ensure that the Worker thread will always have access to the necessary data, regardless of whether the corresponding Activity is still running.\n\n**Consequences:**\n\n* **Increased complexity:** Converting URIs to FileDescriptors requires additional code and may introduce potential performance overhead.\n* **Potential loss of data:** If the URI is revoked before it can be converted to a FileDescriptor, the data will be lost.\n* **Compatibility:** This approach may not be compatible with older versions of Android that do not support FileDescriptors."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n- In the concept of start to implement the new application for mobile we chose new and modern technology\n- For the long-term development we will have application with various functionality and cool feature\n- Besides we plan to develop many other mobile application base on the first success application\n### There are advantages\n- Compatibility: It’s compatible with JDK 6, so older devices aren’t left behind.\n- Performance: It’s on par with Java.\n- Inter-operability: It’s 100% inoperable with Java including annotations.\n- Footprint: The run-time library for Kotlin is tiny.\n- Compilation Time: There’s a little overhead on clean builds but it’s way faster with incremental builds.\n- Learning Curve: It’s easy to learn, especially for people used to modern languages. The Java to Kotlin converter in IntelliJ and Android Studio makes it even easier. You can also use a mix of Kotlin and Java in a project, so take your time learning Kotlin and add it in when you feel comfortable.\n- Kotlin is supported and advertised by google, android editor\n\n## Decision\n","We chose the Kotlin to develop the first android application, this is the way we keep the tech up to date for the longer target of development\n",Adopt Kotlin for the new mobile application development.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nApplication delegate download task to `DownloadManager` system service and get an `unique id` for this task.\nApplication need to store this Id to do further stuff with this task:\n- Query status\n- Get the completed state\n- Get error details\n- Cancel a download task\n\n## Decision\n,Creating a `DownloadingRepository` to manage downloading tasks\n,Store the id of download task in `NSUserDefaults` with a key that contains some identifier of current task.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nApplication delegate download task to `DownloadManager`. All tasks are enqueued and will be execute by system in background.\nThen, application created a `DownloadingRepository` to manage delegated downloading tasks. This repository need to persist data.\nAndroid provide Room which is an abstraction layer over SQLite to allow fluent database access while harnessing the full power of SQLite:\n- Room is an Object Relational Mapping library\n- SQL validation at compile time\n- Room maps database object to Kotlin object without boilerplate code\n- Room support coroutines\n\n## Decision\n",We will implement `DownloadRepository` by Room\n,"Use Room as the underlying database for the `DownloadingRepository`.\n\n**Benefits:**\n\n- Room provides an abstraction layer over SQLite, simplifying database access and reducing boilerplate code.\n- Room offers SQL validation at compile time, ensuring database integrity and reducing potential errors.\n- Room seamlessly maps database objects to Kotlin objects, eliminating the need for manual mapping.\n- Room supports coroutines, enabling asynchronous database operations and improving performance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\n\n## Decision\n","Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\n",Use the `DownloadManager` service from the system to perform stable background downloads.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUse-case:\n```\nAlice want to share a file to a workgroup\nAlice select file and select destination to upload to\n```\nWe have many UI flows of destination picker in Uploading case.\n\n## Decision\n,The accepted flows are implemented:\n![destination_picker](./images/destination_picker_in_uploading.png)\n,"Use a file picker for the user to select the file to upload, and a destination picker to select the destination to upload to."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** To use an Architectural Decision Record (ADR) to document architectural decisions.\n\n**Benefits:**\n\n* **Consistency:** Provides a standardized format for recording decisions.\n* **Transparency:** Makes decisions easily accessible to all stakeholders.\n* **Traceability:** Allows for tracing decisions back to their rationale and context.\n* **Knowledge sharing:** Facilitates knowledge transfer and alignment within the team.\n\n**Implementation:**\n\n* Create a dedicated repository for ADRs.\n* Establish a process for submitting and reviewing ADRs.\n* Use a consistent template for capturing the decision elements (e.g., title, context, decision, benefits, risks, alternatives considered).\n* Encourage team members to actively participate in the decision-making process and document their input.\n\n**Follow-up:**\n\n* Regularly review and update ADRs as the project evolves.\n* Share ADRs with stakeholders to ensure alignment and understanding.\n* Use ADRs as a basis for future architectural discussions and decisions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we start to develop the android application, we have to chose the right architecture\nIf you don’t choose the right architecture for your Android project, you will have a hard time maintaining it as your codebase grows and your team expands.\nHere it comes with mostly behavioral design patents, MVC, MVP and MVVM, for linshare application and target for code quality, where MVVM is suitable for our code base could be testable, reduce some middle code to biding between layers\n### Advantages of MVVM Architecture\n- Code is even more easily testable than with plain MVVM.\n- Code is further decoupled (the biggest advantage.)\n- The package structure is even easier to navigate.\n- The project is even easier to maintain.\n- Team can add new features even more quickly.\n### The Layers of MVVM with Clean Architecture\nThe code is divided into three separate layers:\n- Presentation Layer\n- Domain Layer\n- Data Layer\n\n## Decision\n",We agreed to implement the MVVM application design on android application.\n,"**Decision:** Utilize the Model-View-ViewModel (MVVM) architectural pattern with Clean Architecture principles for the Android application.\n\n**Rationale:**\n\nMVVM, combined with Clean Architecture, offers numerous advantages for the project:\n\n* Enhanced testability, simplifying unit and integration testing.\n* Decoupled codebase, ensuring loose coupling between layers and facilitating maintenance.\n* Improved code organization and navigability, making the project easier to understand and maintain.\n* Reduced middle code for binding between layers, streamlining the codebase.\n* Increased team productivity due to faster feature implementation and smoother collaboration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe recently discovered an issue with one of the classes in\n`service-api/module/Application/src/Model/DataAccess/Postgres/`, where\na search parameter was not correctly escaped. On investigation,\nwe found that test coverage of the files in that directory was\npractically zero.\nWe also found that writing unit tests for these classes was tricky, as\nthey were acting as factory classes for real database connections. This\nmade it difficult to mock out result sets etc. to test different\ncode paths.\nIn pseudo-code, the old code followed this pattern:\n```\nclass AbstractBase\n{\n// $adapter is a Laminas\Db\Adapter\Adapter;\n// this constructs AbstractBase with a reference\n// to a real database adapter\npublic __construct($adapter)\n{\n$this->adapter = $adapter;\n}\npublic getAdapter()\n{\nreturn $this->adapter;\n}\n}\nclass UserData\n{\npublic getData()\n{\n$query = new Sql($this->getAdapter());\n}\n}\n```\nNote that the `getData()` method is constructing the SQL directly\ninside the `UserData` class. This means we can't mock it out\nin a test context as we can't control how it's constructed.\n\n## Decision\n","Refactor the `*Data` database access classes so that database-specific\noperations and SQL construction can be mocked in a test context.\nWe decided to pull the adapter out of the data access\nclasses, instead wrapping it in a `DbWrapper` class. We then put some utility\nmethods on this class, such as a method for creating a `Sql` instance.\nThe `DbWrapper` then holds the reference to the adapter.\nIt looks like this:\n```\nclass DbWrapper\n{\npublic __construct($adapter)\n{\n$this->adapter = $adapter;\n}\npublic createSql()\n{\nreturn new Sql($this->adapter);\n}\n}\nclass AbstractBase\n{\npublic __construct($dbWrapper)\n{\n$this->dbWrapper = $dbWrapper;\n}\n}\nclass UserData\n{\npublic getData()\n{\n$query = $this->dbWrapper->createSql();\n}\n}\n```\nWe are now able to pass a mock `DbWrapper` instance to each data access\nclass; in turn, we can then return mock `Sql` instances from its\n`createSql()` method. This will make it possible to unit test classes\nwhich were previously very difficult to test.\n",We should create a new class - `DatabaseAdapter\PDO` - which will implement the `DatabaseAdapter` interface. This class will wrap around a PDO connection and provide a simplified API for interacting with the database. This will allow us to write unit tests for our data access classes without having to worry about mocking out the PDO connection.\n```\n// DatabaseAdapter/PDO.php\n<?php\nnamespace DatabaseAdapter;\n\nuse PDO;\nuse PDOException;\n\nclass PDO implements DatabaseAdapter\Adapter\n{\n// $connection must be a PDO instance\npublic function __construct(PDO $connection)\n{\n$this->connection = $connection;\n}\n\npublic function query(string $sql): ?Statement\n{\ntry {\nreturn new Statement($this->connection->query($sql));\n} catch (PDOException $e) {\nreturn null;\n}\n}\n}\n```
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\nespecially in contrast with Python. It's likely it will become increasingly difficult\nto find good PHP developers in future.\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\nlanguages of similar pedigree, such as Python.\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\nA mix of contractors and developers working on the code base over several years has\nresulted in a mix of styles and approaches. While we have already cleared out a lot\nof unused and/or broken code, there is likely to be more we haven't found yet.\nWe are also lagging behind the latest Design System guidelines, as our application was one\nof the first to go live, before the current iteration of the Design System existed.\nThis means that any changes to design have to be done piecemeal and manually: we can't\nsimply import the newest version of the design system and have everything magically update.\nThis combination of factors means that the code base can be difficult to work with:\nresistant to change and easy to break.\n\n## Decision\n","We have decided to modernise the code base to make it easier to work with and better\naligned with modern web architecture and standards. This is not a small job, but\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\n(""Modernising the code base"" is not to be confused with ""modernising LPAs"". Here\nwe're just talking about modernising the code base for the Make an LPA tool.)\n* **Don't rewrite everything at once**\nWhere possible, migrate part of an application to a new\ncomponent and split traffic coming into the domain so that some paths are diverted to that\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\nappropriate (e.g in a load balancer or application gateway).\nThis is challenging, but means that we don't have to do a ""big bang"" release of the new\nversion of the tool. Our aim is to gradually replace existing components with new\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\n* **Use Python for new work**\nWe considered golang, but don't have the experience in the team to build applications with it.\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\nminimal benefits: our application is not under heavy load and responds in an\nacceptable amount of time, so golang's super efficiency isn't essential.\nWe feel that we could scale horizontally if necessary and have not had any major issues\nwith capacity in the past.\n* **Choose containers or lambdas as appropriate**\nUse a container for components which stay up most of the time, and lambdas for\n""bursty"" applications (e.g. background processes like PDF generation, daily statistics aggregation).\n* **Choose the right lambda for the job**\nUse ""pure"" lambdas where possible. This is only the case where an application has simple dependencies\nwhich don't require unusual native libraries outside the\n[stock AWS Docker images for lambdas](https://gallery.ecr.aws/lambda/python)).\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\non one of the stock AWS Docker images for lambdas.\n* **Choose the right Docker image**\nWhen using Docker images, prefer the following:\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\n* Images based on Alpine (for other cases).\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\n* **Use Flask and gunicorn**\nUse [Flask](https://flask.palletsprojects.com/) for new Python web apps, fronted by\n[gunicorn](https://gunicorn.org/) for the WSGI implementation.\n* **Use the latest Design System**\nUse the [Government Design System](https://design-system.service.gov.uk/) guidelines for new UI. In\nparticular, use the\n[Land Registry's Python implementation of the design system](https://github.com/LandRegistry/govuk-frontend-jinja),\nwritten as [Jinja2 templates](https://jinja.palletsprojects.com/).\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\nif it is changed by developers at the Land Registry.\n* **Migrate legacy code to PHP 8**\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https://getlaminas.org/).\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\nas large parts of our stack are implemented on top of Laminas.\n* **Specify new APIs with OpenAPI**\nSpecify new APIs using [OpenAPI](https://swagger.io/specification/). Ideally, use tooling\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\ncode only when necessary, to avoid repetitive boilerplate.\n* **Controlled, incremental releases**\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\nwork on new components, moving them into the live environment as they are ready, but hidden\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\nof infrastructure live.\n* **Follow good practices for web security**\nBe aware of the [OWASP Top Ten](https://owasp.org/www-project-top-ten/) and code to avoid those\nissues. Use tools like [Talisman](https://github.com/GoogleCloudPlatform/flask-talisman) to\nimprove security.\n* **Be mindful of accessibility**\nConsider accessibility requirements at every step of the design and coding phases. Aim to\ncomply with [WCAG 2.1 Level AA](https://www.w3.org/WAI/WCAG22/quickref/) as a minimum. While the\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\nand custom components it doesn't cover.\n* **Be properly open source**\nMake the code base properly open source. While our code is open, there are still barriers to entry\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\nwill work towards removing these barriers so that onboarding of new developers (internally and\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\n* **Improve test coverage everywhere**\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\nwork on an issue (unless it's going to take longer than working on the issue!).\nWhere a whole category of testing is missing, add it (for example, we\nhave recently implemented the foundations for load testing; see\n[0004-implement-load-testing](./0004-implement-load-testing.md)).\n* **Automate code quality metrics**\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\n[0003-linting-and-scanning](./0003-linting-and-scanning.md)) for a starting point.\n* **Peer review everything**\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\n* **Be pragmatic**\nSee the [pragmatic quick reference](https://www.ccs.neu.edu/home/lieber/courses/csg110/sp08/Pragmatic%20Quick%20Reference.htm)\nfor a summary. These are generally good principles for software engineering.\n",We should migrate the code base to Python.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a requirement to publish data about the performance of the\nMake an LPA service. The key pieces of data we must publish are:\n* **User satisfaction:** Ratings of the service provided by users on a 5 point scale,\nfrom ""very dissatisfied"" to ""very satisfied"". This is currently collected through\na [centralised government feedback page](https://www.gov.uk/done/lasting-power-of-attorney).\n* **Completion rate:** Percentage of transactions started on the service by users\nwhich are subsequently completed. We derive this from Google Analytics and\nthe database.\n* **Digital take-up:** Number of users using the online service as a percentage of\nusers across all channels (online + paper + phone etc.). There is no API for this\npresently.\nCurrently, this data is being manually collated. We need to find a more efficient\nand accurate way of publishing it.\nAs a side objective, this is a greenfield project which gives us an opportunity\nto test the waters of our proposed new technology stack\n(see [0006-modernise-the-code-base](./0006-modernise-the-code-base.md)).\n\n## Decision\n","### Data sources\nWe agreed the following sources for the data:\n1. **User satisfaction:** To be collected by us through a new feedback form, based on\nthe existing central form.\n2. **Completion rate:** We propose to calculate this by counting the number of\nLPA applications which reach a ""created"" state. We propose not to count\napplications completed within a session (i.e. user logs in, starts an application,\nand completes it). Rather, we would count the applications completed by day,\nregardless of when they were started (e.g. user logs in on Monday, starts an\napplication, and completes it on Thursday: it's added to the ""completed""\npile for Thursday).\n3. **Digital take-up:** We will request a new endpoint on the Sirius data API to\nprovide this data. Investigation showed that this data is currently manually\ncollected from Sirius (OPG's case management system) through the client\napplication, so we believe that Sirius has the data we need.\n### High-level architecture\nWe agreed that an **embedded** design for the solution's high-level architecture was preferable:\n![Data platform - system landscape view](../images/structurizr-SystemLandscapeEmbedded.png)\n![Data platform - container view](../images/structurizr-ContainerEmbedded.png)\n(The above diagrams use the [C4 model](https://c4model.com/).)\nThis re-uses parts of the existing stack to implement ingress and storage for the data platform,\nin particular:\n* The data platform API (which accepts requests to generate the performance data) is\npositioned behind the api-web proxy currently used by api-app.\n* The data platform worker (which aggregates the performance data) and API write to the existing\npostgres database, albeit into new table(s).\nThe contrasting approach we discussed was a **standalone** one. Using this approach, we would\nimplement the entire data platform in isolation from the existing application: we would have\na separate database to store its data, and traffic in to the platform would be separate from\ntraffic intended for other components, for example:\n* The data platform has its own proxy which forwards requests to the API.\n* The API and the worker process write to a separate database.\nWe decided the latter would add unnecessary complexity, as we would need more\ningress rules for the API, extra Terraform/scripting to provision the additional database,\nand a new load balancer/proxy. This would be a lot of work and expense for a relatively simple service.\n(In some respects, the proposed architecture is a little over the top; but part of the reason for that\nis to provide a gateway into a more modern architecture over the long term.)\n### Implementation approach\nAfter agreeing on an embedded archtitecture, we also agreed the following approach to implementation:\n* We will implement the data platform in Python, using AWS lambdas to run the worker and API.\n* We will use a queue-based workflow, where we add a job to generate the perf data and notify\nclients when it is ready. We agreed that SQS was a good choice of queuing technology.\n* We will not merge the user satisfaction data into the existing feedback table,\nbut will store it in a separate table in the PostgreSQL database.\n* We’ll manage the new table, likely using SQLAlchemy, possibly with Alembic for migrations.\n* We'll provide an interface in the admin UI which enables admin users to view “end of journey”\nuser satisfaction data. This could either be in parallel with the existing feedback view,\nor integrated with it (by merging data from the two tables into one view). This is\nto be decided by the product team.\n* We'll gather user satisfaction data inside front-app initially, using a PHP form\nbased on the existing centralised feedback form. This will post to the new API (in Python).\nLonger term, this would be a good test case for migrating our PHP app to microfrontend\nslices: as the user satisfaction form needs no authentication, it would be a good place to\ninsert a first Python UI application into our infrastructure.\n* We generally agreed to the system landscape and container structure shown in the diagrams above.\nNote that a “container” in these diagrams doesn’t necessarily mean a docker container:\nit’s just a logical unit in the C4 level 2 (container) model diagram. These would be\nlambdas running under Docker or localstack in dev, but native lambdas (either pure code\nor lambda-ised containers) in live.\n",We will develop a data pipeline to automate the collection and publication of the required data. The pipeline will use [Apache Airflow](https://airflow.apache.org/) to orchestrate the data collection and processing tasks. The data will be stored in a [BigQuery](https://cloud.google.com/bigquery) dataset and made available through a [Looker](https://looker.com/) dashboard.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe service-front component, written in PHP, uses the default Redis save\nhandler for persisting session data. In certain situations, the\napplication may request a resource *A* which takes significant time to deliver,\nsuch as LPA statuses via the Sirius data API. If resource *A*\nis requested via an Ajax request, it's possible that the client\nwill request a new resource *B* before *A* is fully processed. If processing for\n*B* then completes before processing for *A*, the process for *A* can erroneously\noverwrite session data added by *B*, resulting in loss of session data required\nby *A*.\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\non service-front:\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\nAPI to get LPA statuses\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\n4.  replacement-attorney controller reads session data **S**\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\nsession by replacement-attorney...\n6.  replacement-attorney adds CSRF data to session, creating **S'**\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\n8.  replacement-attorney writes **S'** to session, including CSRF data\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\nthe correct data **S** and writes it to the session, losing the delta between\n**S** and **S'** (including the CSRF token!)\n10. user submits form to replacement-attorney controller with CSRF token in the form\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\n\n## Decision\n","Use a custom save handler to prevent certain Ajax requests from writing data to the session.\nThis will still use Redis as the storage back-end.\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\nimplying that the controller they invoke should only read from the session and never write to it.\nThe save handler inspects the header on the incoming request and ignores any requests to write\nthe session if accompanied by this header.\nPHP 7+ provides a mechanism to only read from the session, via:\n```\nsession_start(array('read_and_close' => true))\n```\nHowever, the complexity of the processing in the Laminas stack, which does its own session\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\nbehaviour there for requests we know to be problematic.\n","The service-front component should utilise a Redis session handler which supports multi-versioning, such as the Symfony\Lock\Store\RedisStore class. This will allow multiple simultaneous session writes without overwriting data, resolving the issue described in the context."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur code base is relatively large and complex, and was written\nover several years by multiple contributors. This has resulted in\nseveral issues:\n* The style and approach varies considerably across it, sometimes\nmaking it difficult to find the appropriate code, and often making\nit tricky to format code so that it is easy to read and follow.\n* Different developer preferences can result in\ninconsistencies in style. For example, different editor setups could\nintroduce or remove trailing spaces as part of a commit. These\nincidental edits increase the amount of churn in code files which\na developer has to review (e.g. if one developer's editor removes\ntrailing spaces, the diff for a commit may accidentally contains\ndozens of irrelevant lines which just remove space).\n* There is always the potential for a developer to accidentally commit\nsecret information to the code base, such as usernames and passwords.\nWhile good practice has prevented this so far, we have no automated\ngates to stop this happening.\n* There are redundant and incorrect import statements, blocks of\ncode which are unreachable, ambiguous or redundant variable definitions\nand the like, which potentially introduce bugs and vulnerabilities, and\ngenerally increase maintenance overhead.\n\n## Decision\n","We will gradually increase the quality of the code by introducing code scanning\nand linting to address the above issues.\nIn the first instance, we'll implement the following:\n* pre-commit hooks to check code before it is committed to the git repo\n* github actions to regularly scan our code as part of builds\nThe tools we'll use follow the ad hoc standards used by other teams, as follows:\n**pre-commit hooks**\n* General file linting: pre-commit-hooks (trailing space removal; line end fixing)\n* Secrets commit prevention: gitleaks; git-secrets\n* PHP linting/automated fixes: pre-commit-php (php-cbf PHP Code\nSniffer / Beautifier and Fixer with PSR-12 standard)\n* Terraform linting: pre-commit-terraform (formatting and validation)\n**github actions**\n* codeql (code security scanning across all languages)\n* gitleaks (check for secrets being committed)\n* psalm (PHP code security scanning)\n* trivy (container image scanning)\n* trufflehog (detect credential leaks)\n* tfsec (terraform security scanning)\n* dependabot (to detect updates to composer and npm packages)\nNote that github actions for code are currently focused on security scans *only*.\nThis is because the quality of our PHP code is especially suspect and would fail\nmore general lint scans. We will deal with this over time by manually\napplying PHPStan and Psalm (to PSR-12 standard) to individual components to\nbring them up to scratch.\nAs we add additional tools to these lists, we'll record the decisions for why we\nincluded them.\nWhen a linter fixes a file, we will isolate those changes (as far as possible)\nfrom changes which relate to the issue being addressed, putting them in\na commit with a title something like ""Fixes to satisfy PHP linter"".\n",We will use a combination of automated tools and code review to enforce code style and quality standards.\n\n**Automated Tools**\n\n* We will use a linter to enforce code style and formatting rules.\n* We will use a static analyzer to identify potential bugs and vulnerabilities.\n\n**Code Review**\n\n* We will require all code changes to be reviewed by at least one other developer before they are merged into the main branch.\n* Code reviewers will be responsible for ensuring that the code meets the following standards:\n    * Code style and formatting is consistent with the team's conventions.\n    * The code is free of potential bugs and vulnerabilities.\n    * The code is well-documented and easy to understand.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to test parallel login of users to ensure that\nsession data isn't accidentally shared between users\n(see [decision 0002](./0002-custom-save-handler-in-service-front.md)).\nAlso, Make an LPA currently has no load tests. These are useful\nfor capacity planning and finding bottlenecks which may be\ncausing errors for users on the site. While we are auto-scaling,\nwe don't have a way to verify that it is sufficient to manage\nexpected load, which load testing can provide.\nLoad testing can also provide a benchmark we can use to ensure\nthat any new work we do does not degrade performance of the stack\nas a whole.\n\n## Decision\n","Use [locust.io](https://locust.io/) to test parallel logins.\nWhile there are alternatives to locust, it is implemented in Python\n(our chosen language going forward), has an intuitive API, and some\nmembers of the team have experience with it already (and can reuse\npreviously-written code).\nLoad tests will be added to the tests/load directory in the project,\nas they are not component-specific and apply to the whole stack.\nDoing the above has the happy side effect of opening up the possibility of\nload testing the whole application stack in future.\nInitially, this will only run locally and is not integrated into\nthe CI pipeline. We will consider extending this testing into CI\nin future.\n",Conduct parallel login testing and implement load testing for Make an LPA.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we go live with the Flask part of service-front, we need to\nensure that this is secure enough, immediately for static pages\nand ultimately for forms as and when the first form is rolled out\n\n## Decision\n","We will follow OWASP guidelines to secure the flask site.\nWe will immediately do any obvious security protection.\nWe will scan the python code\n### OWASP 2021 top 10 on flask site\nA01:2021 Broken Access Control :\nThe initial release consists of static pages that are public. The first form will also be public. Ultimately, we aim to share session information with the php site. At this point tests will be required to ensure relevant content cannot be accessed without logging in.\nExcept for public resources, deny by default.  This does happen to some extent in that the flask site needs nginx to actively be configured to proxy_pass pages to it. However we need to be careful with any wildcard instructions to nginx.\nA02:2021 Cryptographic failure, (formerly known as Sensitive Data Exposure).\nWe do not keep any secrets in the repository and any in use will be protected using secrets management services, and accessed by code from there.\nWe use security scans and precommit hooks to check for secrets, in order to reduce the chance of these being committed\nsee : [OPG security policy](https://docs.opg.service.justice.gov.uk/documentation/guides/security_process.html#security-in-our-process)\nA03:2021 Injection (ths category now includes Cross Site Scripting)\nThis will be worth addressing when we have forms to submit. We already have some headers in place against this (see ""Other considerations"" section)\nA04:2021 Insecure Design  ( New category for 2021  ):\nThis includes error messages that reveal too much info e:g a server's IP\nOr code that has a variable saying whether a user is authenticated or not, that other code could fail to check\nThis will become more of an issue to check, as we develop the flask site further\nA05:2021 Security Misconfiguration\nIncludes ports being open when they shouldn't, default config which is too open,\nDefault account and password, not an issue yet as we do not have authentication\nError handling reveals stack traces -  This is avoided by the fact that flask has development mode switched off by default. Stack traces will appear in the logs only, the user would be shown a 500 error\nShould not install unnecessary features or frameworks. We base the flask container on a very basic docker image, not including irrelevant components\nA06:2021 Vulnerable and outdated components\nWe should ensure versions of components are kept up to date.  On initial release, we have an older version of flask pinned, but we have a story to get all\nthe necessary components working with later flask, and ultimately stop pinning the version and use dependabot to keep components updated.\nA07:2021  Identification and Authentication failures (formerly Broken Authentication) :\nWe aim to share session info with PHP , this will happen only when we get to forms that require authenticating to view.\nA08:2021 Software and Data Integrity Errors (new for 2021)\nWe address this by ensuring the security of the Circle CI/CD pipeline\nA09:2021 Security Logging and Monitoring failures (formerly insufficient logginng & monitoring)\nWhile page access is automatically logged in nginx and by flask, when we do get to the point of having authenticated pages, we will need to log all login attempts\nA10:2021  Server-side request forgery\nThis is addressed by restricting assets to our own servers, not pulling in anything external. In future we will allow a select few such as Google Analytics but it will still be tightly restricted.\n### Other Considerations\nCSRF - This is provided in the form framework Flask-WTForms and we make use of this\nThe question was asked, whether flask can uniwttingly expose any environment variables, but no discussion of this was to be found online.\nWe use Flask-Talisman to provide protection , including in the form of headers\nIt can be verified using Chrome inspector, that the following recommended headers are automatically set:  (2 more than is done for PHP site)\nContent-Security-Policy and X-Content-Security-Policy is set to the default very strict. This means that only assets from the same domain can be served\nWe will later relax this to allow Google Analytics, there is a story on the backlog for this. These aren't currently set on the PHP site,  we also have a story\non the backlog for that.\nX-Content-Type-Options to nosniff.  This forces browser to honour response content type instead of trying to guess it, which could leads to a cross-site-scripting attack  (done in PHP site too)\nX-Frame-Options to SAMEORIGIN  - prevents external sites embedding the site in an invisible iframe - ""clickjacking""  (done in PHP site too)\nX-XSS-Protection set to 1; mode=block .  Tries to prevent XSS attacks by preventing page from loading if request contains something that looks like js, and the response contains the same data  (done in PHP site too)\nstrict-transport-security (use https) is set by nginx if not already there (for php), and set on flask\nWe don't ask users to upload files, therefore shouldn't be vulnerable to security issues around that yet, however this could become an issue in future for example Modernise requiring uploads of documents\nWe should generally use jinja2 rather than ""hand-crank"" html, as the jinja2 templates generate safety tested html\nWe should always quote attributes with quotes when using Jinja expressions in them, to prevent an attacker inserting custom javascript handlers\n",The decision is not provided in the context.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\nWe will not burden the project with additional dependencies for creating\nADRs: the format is simple enough to copy an existing ADR and\nchange appropriate fields manually.\nThe name of the file will contain the number of the ADR, which\nwill be exactly copied into the title inside the ADR itself. For\nexample, this file is 0001-record-architecture-decisions.md,\nso the number ""0001"" (rather than ""1"") is in the title. (In other\nteams, the number ""1"" is used, but we will use the full number.)\nThe Date given in the decision is the date it was created.\nEach status will be recorded on the decision, along with the date it came into force.\nThis is to enable tracking of when decisions were proposed/accepted/superseded etc.,\nwhich should help identify stale decisions (e.g. proposed a long time ago\nbut never accepted). The latest status in the list is assumed current.\n","**Decision:** To record architectural decisions in a centralized and structured format using an Architectural Decision Record (ADR).\n\n**Rationale:**\n\n* ADRs provide a standardized template for capturing and documenting architectural decisions.\n* They facilitate collaboration and understanding among stakeholders by clearly defining the problem, solution, trade-offs, and implications of each decision.\n* ADRs create a historical record of the decision-making process, allowing future teams to understand why certain design choices were made.\n* By centralizing ADRs, they become easily accessible and searchable, ensuring that the project's architectural context remains clear and accessible.\n\n**Consequences:**\n\n* Requires the adoption of a consistent ADR format and process.\n* May require additional effort to document decisions in a structured manner.\n* Provides a clear and well-documented record of architectural decisions, benefiting both current and future teams."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n#### Context and Problem Statement\nA front end, client facing technology stack / language is needed.\n#### Considered Options\n- ASP.NET MVC (C#)\n- ASP.NET Web Forms (C#)\n- Angular (with TypeScript)\n#### Decision Outcome\nChosen option: ""Angular"", because\n- This is in-line with Trade Me's technical stack trajectory.\n- In line with what is considered industry standard for green field projects.\n\n## Decision\n","Chosen option: ""Angular"", because\n- This is in-line with Trade Me's technical stack trajectory.\n- In line with what is considered industry standard for green field projects.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n#### Context and Problem Statement\nA technology stack is required for the projects data access application programming interface (API).\n#### Considered Options\n- ASP.NET Web API (C#)\n#### Decision Outcome\nChosen option: ""ASP.NET Web API (C#)"", because\n- This can target .Net Core 2\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\n- A repository pattern can be applied facilitating Unit Testing.\n([back](README.md))\n\n## Decision\n","Chosen option: ""ASP.NET Web API (C#)"", because\n- This can target .Net Core 2\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\n- A repository pattern can be applied facilitating Unit Testing.\n([back](README.md))\n","The chosen option is ASP.NET Web API (C#). The main reasons for this decision are:\n\n- This can target .Net Core 2\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\n- A repository pattern can be applied facilitating Unit Testing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n#### Context and Problem Statement\nA relational database is required to persist the applications data.\n#### Considered Options\n- Microsoft SQL\n- MySQL\n- PostgreSQL\n- SQLite\n#### Decision Outcome\nChosen option: ""Microsoft SQL"", because\n- This is in-line with Trade Me's technical stack.\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\n\n## Decision\n","Chosen option: ""Microsoft SQL"", because\n- This is in-line with Trade Me's technical stack.\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\n",**Decision:** Use Microsoft SQL as the relational database.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n#### Context and Problem Statement\nAll game records need to be persisted to the relational database with reporting in mind.\n#### Considered Options\n- dbo.game for each game record, dbo.game_line for each shot attempt.\n#### Decision Outcome\nChosen option: dbo.game with dbo.game_line\n- Each shot attempt is recorded with a score of 0 for miss and 1 for a dunk\n- Verbose logging like this will make reporting easier and leave nothing to interpretation\n\n## Decision\n",Chosen option: dbo.game with dbo.game_line\n- Each shot attempt is recorded with a score of 0 for miss and 1 for a dunk\n- Verbose logging like this will make reporting easier and leave nothing to interpretation\n,"**Decision:** Choose the option of creating two tables, `dbo.game` and `dbo.game_line`, to store game records for efficient reporting.\n\n**Reasons:**\n\n* The verbose logging included in the `dbo.game_line` table will facilitate easier reporting by providing detailed information about each shot attempt.\n* Storing each shot attempt with a score of 0 for a miss and 1 for a dunk ensures clarity and eliminates the need for interpretation.\n* This approach aligns with the specified goal of persisting all game records to the relational database with reporting in mind."
